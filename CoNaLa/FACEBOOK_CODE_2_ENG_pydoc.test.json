[" \tcli.register('building-command-table.main', add_s3) \n\tcli.register('building-command-table.sync', register_sync_strategies)\n", 
" \tif (Info.version() is None): \n\t \treturn True \n\treturn False\n", 
" \tif (get_configured_provider() is False): \n\t \treturn False \n\tif (get_dependencies() is False): \n\t \treturn False \n\treturn __virtualname__\n", 
" \tbits = token.contents.split() \n\tif (len(bits) != 6): \n\t \traise template.TemplateSyntaxError((\"'%s' \ttag \ttakes \texactly \tfive \targuments\" % bits[0])) \n\tif (bits[2] != 'from'): \n\t \traise template.TemplateSyntaxError((\"second \targument \tto \t'%s' \ttag \tmust \tbe \t'from'\" % bits[0])) \n\tif (bits[4] != 'as'): \n\t \traise template.TemplateSyntaxError((\"fourth \targument \tto \t'%s' \ttag \tmust \tbe \t'as'\" % bits[0])) \n\treturn DictEntryForItemNode(bits[1], bits[3], bits[5])\n", 
" \tif isinstance(thing, _ThreadLocalProxy): \n\t \tthing = getattr(serving, thing.__attrname__) \n\treturn _pydoc._builtin_resolve(thing, forceload)\n", 
" \tif isinstance(s, bytearray): \n\t \ts = bytes(s) \n\tif isinstance(s, six.text_type): \n\t \treturn (u'utf-8', s) \n\telif isinstance(s, six.string_types): \n\t \ttry: \n\t \t \tenc = u'utf-8' \n\t \t \treturn (enc, six.text_type(s, enc)) \n\t \texcept UnicodeError: \n\t \t \tfor e in encoding_list: \n\t \t \t \ttry: \n\t \t \t \t \treturn (e, six.text_type(s, e)) \n\t \t \t \texcept (UnicodeError, LookupError): \n\t \t \t \t \tpass \n\t \t \ttry: \n\t \t \t \tenc = u'utf-8' \n\t \t \t \treturn (enc, six.text_type(s, enc, errors=u'replace')) \n\t \t \texcept UnicodeError: \n\t \t \t \traise Exception((_(u\"Diff \tcontent \tcouldn't \tbe \tconverted \tto \tunicode \tusing \tthe \tfollowing \tencodings: \t%s\") % ([u'utf-8'] + encoding_list))) \n\telse: \n\t \traise TypeError(u'Value \tto \tconvert \tis \tunexpected \ttype \t%s', type(s))\n", 
" \tsnap_name = _sdecode(snap_name) \n\tif re.match(GUID_REGEX, snap_name): \n\t \treturn snap_name.strip('{}') \n\telse: \n\t \treturn snapshot_name_to_id(name, snap_name, strict=strict, runas=runas)\n", 
" \tif isinstance(value, unicode): \n\t \treturn value.encode('utf-8') \n\treturn value\n", 
" \tssh_cmd = ['ssh', '-o', 'BatchMode=yes'] \n\tssh_cmd.append(dest) \n\tssh_cmd.extend(cmd) \n\treturn execute(*ssh_cmd, **kwargs)\n", 
" \twith open(fname, 'rb') as f: \n\t \tmd5sum = md5() \n\t \tfor block in iter((lambda : f.read(MD5_BLOCK_READ_BYTES)), ''): \n\t \t \tmd5sum.update(block) \n\treturn md5sum.hexdigest()\n", 
" \tif (seed is not None): \n\t \t_ = getCurrentThreadData().random \n\t \t_.seed(seed) \n\t \tchoice = _.choice \n\telse: \n\t \tchoice = random.choice \n\tif alphabet: \n\t \tretVal = ''.join((choice(alphabet) for _ in xrange(0, length))) \n\telif lowercase: \n\t \tretVal = ''.join((choice(string.ascii_lowercase) for _ in xrange(0, length))) \n\telse: \n\t \tretVal = ''.join((choice(string.ascii_letters) for _ in xrange(0, length))) \n\treturn retVal\n", 
" \tfn = [] \n\tfor i in range(0, 30): \n\t \tfn.append(choice(ALPHABET)) \n\treturn (''.join(fn) + file_ext)\n", 
" \tfn = [] \n\tfor i in range(0, 30): \n\t \tfn.append(choice(ALPHABET)) \n\treturn (''.join(fn) + file_ext)\n", 
" \tif ((',' in string) or ('|' in string)): \n\t \treturn string.replace('|', ',').split(',') \n\treturn string\n", 
" \ttotal_size = 0 \n\tfor (dirpath, dirnames, filenames) in os.walk(start_path): \n\t \tfor f in filenames: \n\t \t \tfp = os.path.join(dirpath, f) \n\t \t \tif (not os.path.islink(fp)): \n\t \t \t \ttotal_size += os.path.getsize(fp) \n\treturn total_size\n", 
" \treturn isinstance(object, types.MethodType)\n", 
" \treturn {'name': name, 'result': False, 'changes': {}, 'comment': ''}\n", 
" \tif skip: \n\t \tcontent = None \n\t \tcontent_append = (lambda x: None) \n\telse: \n\t \tcontent = [] \n\t \tcontent_append = content.append \n\tending_keyword = None \n\tif isinstance(keywords, (bytes, str_type)): \n\t \tkeywords = (keywords,) \n\tif ignore_first: \n\t \tfirst_line = descriptor_file.readline() \n\t \tif first_line: \n\t \t \tcontent_append(first_line) \n\tkeyword_match = re.compile((SPECIFIC_KEYWORD_LINE % '|'.join(keywords))) \n\twhile True: \n\t \tlast_position = descriptor_file.tell() \n\t \tif (end_position and (last_position >= end_position)): \n\t \t \tbreak \n\t \tline = descriptor_file.readline() \n\t \tif (not line): \n\t \t \tbreak \n\t \tline_match = keyword_match.match(stem.util.str_tools._to_unicode(line)) \n\t \tif line_match: \n\t \t \tending_keyword = line_match.groups()[0] \n\t \t \tif (not inclusive): \n\t \t \t \tdescriptor_file.seek(last_position) \n\t \t \telse: \n\t \t \t \tcontent_append(line) \n\t \t \tbreak \n\t \telse: \n\t \t \tcontent_append(line) \n\tif include_ending_keyword: \n\t \treturn (content, ending_keyword) \n\telse: \n\t \treturn content\n", 
" \tname = getattr(func, 'func_name', getattr(func, '__name__', '<unknown>')) \n\tfor (inp, expected) in pairs: \n\t \tout = func(inp) \n\t \tassert (out == expected), pair_fail_msg.format(name, inp, expected, out)\n", 
" \tmatch = date_re.match(value) \n\tif match: \n\t \tkw = dict(((k, int(v)) for (k, v) in six.iteritems(match.groupdict()))) \n\t \treturn datetime.date(**kw)\n", 
" \tmatch = date_re.match(value) \n\tif match: \n\t \tkw = dict(((k, int(v)) for (k, v) in six.iteritems(match.groupdict()))) \n\t \treturn datetime.date(**kw)\n", 
" \tmatch = date_re.match(value) \n\tif match: \n\t \tkw = dict(((k, int(v)) for (k, v) in six.iteritems(match.groupdict()))) \n\t \treturn datetime.date(**kw)\n", 
" \tmatch = date_re.match(value) \n\tif match: \n\t \tkw = dict(((k, int(v)) for (k, v) in six.iteritems(match.groupdict()))) \n\t \treturn datetime.date(**kw)\n", 
" \ttry: \n\t \tattrgetter(*attributes)(item) \n\t \treturn True \n\texcept AttributeError: \n\t \treturn False\n", 
" \ttry: \n\t \tattrgetter(*attributes)(item) \n\t \treturn True \n\texcept AttributeError: \n\t \treturn False\n", 
" \treturn {k: v for (k, v) in dict_.items() if ((k in names) and (v is not None))}\n", 
" \tlogging.info(('Configuration \tfor \tapp:' + str(config))) \n\ttry: \n\t \tconfig = json.loads(config) \n\texcept ValueError as error: \n\t \tlogging.error(('%s \tException--Unable \tto \tparse \tconfiguration: \t%s' % (error.__class__, str(error)))) \n\t \treturn None \n\texcept TypeError as error: \n\t \tlogging.error(('%s \tException--Unable \tto \tparse \tconfiguration: \t%s' % (error.__class__, str(error)))) \n\t \treturn None \n\tif is_config_valid(config): \n\t \treturn config \n\telse: \n\t \treturn None\n", 
" \tresult = [] \n\tfor name in names: \n\t \tmatch = pattern.match(name) \n\t \tif match: \n\t \t \tresult.append((name, match.group('name'))) \n\treturn result\n", 
" \texc = exc_info[1] \n\tif (exc is None): \n\t \tresult = exc_info[0] \n\telse: \n\t \ttry: \n\t \t \tresult = str(exc) \n\t \texcept UnicodeEncodeError: \n\t \t \ttry: \n\t \t \t \tresult = unicode(exc) \n\t \t \texcept UnicodeError: \n\t \t \t \tresult = exc.args[0] \n\tresult = force_unicode(result, 'UTF-8') \n\treturn xml_safe(result)\n", 
" \tfor key in dict2: \n\t \tif isinstance(dict2[key], dict): \n\t \t \tif ((key in dict1) and (key in dict2)): \n\t \t \t \tmerge_dicts(dict1[key], dict2[key]) \n\t \t \telse: \n\t \t \t \tdict1[key] = dict2[key] \n\t \telif (isinstance(dict2[key], list) and append_lists): \n\t \t \tif ((key in dict1) and isinstance(dict1[key], list)): \n\t \t \t \tdict1[key].extend(dict2[key]) \n\t \t \telse: \n\t \t \t \tdict1[key] = dict2[key] \n\t \telse: \n\t \t \tdict1[key] = dict2[key]\n", 
" \tfor arg in args: \n\t \tvalue = os.environ.get(arg) \n\t \tif value: \n\t \t \treturn value \n\treturn kwargs.get('default', '')\n", 
" \tfunc.unauthenticated = True \n\treturn func\n", 
" \treturn getattr(func, 'unauthenticated', False)\n", 
" \ttry: \n\t \tif issubclass(*args): \n\t \t \treturn True \n\texcept TypeError: \n\t \tpass \n\treturn False\n", 
" \ttry: \n\t \t(module, attr) = name.rsplit('.', 1) \n\texcept ValueError as error: \n\t \traise ImproperlyConfigured(('Error \timporting \tclass \t%s \tin \t%s: \t\"%s\"' % (name, setting, error))) \n\ttry: \n\t \tmod = import_module(module) \n\texcept ImportError as error: \n\t \traise ImproperlyConfigured(('Error \timporting \tmodule \t%s \tin \t%s: \t\"%s\"' % (module, setting, error))) \n\ttry: \n\t \tcls = getattr(mod, attr) \n\texcept AttributeError: \n\t \traise ImproperlyConfigured(('Module \t\"%s\" \tdoes \tnot \tdefine \ta \t\"%s\" \tclass \tin \t%s' % (module, attr, setting))) \n\treturn cls\n", 
" \tsuccessful_encoding = None \n\tencodings = ['utf-8'] \n\ttry: \n\t \tencodings.append(locale.nl_langinfo(locale.CODESET)) \n\texcept AttributeError: \n\t \tpass \n\ttry: \n\t \tencodings.append(locale.getlocale()[1]) \n\texcept (AttributeError, IndexError): \n\t \tpass \n\ttry: \n\t \tencodings.append(locale.getdefaultlocale()[1]) \n\texcept (AttributeError, IndexError): \n\t \tpass \n\tencodings.append('latin-1') \n\tfor enc in encodings: \n\t \tif (not enc): \n\t \t \tcontinue \n\t \ttry: \n\t \t \tdecoded = text_type(data, enc) \n\t \t \tsuccessful_encoding = enc \n\t \texcept (UnicodeError, LookupError): \n\t \t \tpass \n\t \telse: \n\t \t \tbreak \n\tif (not successful_encoding): \n\t \traise UnicodeError(('Unable \tto \tdecode \tinput \tdata. \t \tTried \tthe \tfollowing \tencodings: \t%s.' % ', \t'.join([repr(enc) for enc in encodings if enc]))) \n\telse: \n\t \treturn (decoded, successful_encoding)\n", 
" \tif shape: \n\t \treturn tf.random_normal(shape, mean=mean, stddev=stddev, seed=seed, dtype=dtype) \n\telse: \n\t \treturn tf.random_normal_initializer(mean=mean, stddev=stddev, seed=seed, dtype=dtype)\n", 
" \tstats = most_common_types(limit, objects, shortnames=shortnames) \n\twidth = max((len(name) for (name, count) in stats)) \n\tfor (name, count) in stats: \n\t \tprint ('%-*s \t%i' % (width, name, count))\n", 
" \tret = {} \n\tfor saltenv in __opts__['file_roots']: \n\t \tret[saltenv] = [] \n\t \tret[saltenv].append(list_env(saltenv)) \n\treturn ret\n", 
" \tif (value is None): \n\t \tvalue = '' \n\tlst = __salt__['cron.list_tab'](user) \n\tfor env in lst['env']: \n\t \tif (name == env['name']): \n\t \t \tif (value != env['value']): \n\t \t \t \treturn 'update' \n\t \t \treturn 'present' \n\treturn 'absent'\n", 
" \tfileserver = salt.fileserver.Fileserver(__opts__) \n\tload = {'saltenv': saltenv, 'fsbackend': backend} \n\treturn fileserver.dir_list(load=load)\n", 
" \treturn _file_lists(load, 'files')\n", 
" \treturn c[key]\n", 
" \tif (minute is not None): \n\t \tminute = str(minute).lower() \n\tif (hour is not None): \n\t \thour = str(hour).lower() \n\tif (daymonth is not None): \n\t \tdaymonth = str(daymonth).lower() \n\tif (month is not None): \n\t \tmonth = str(month).lower() \n\tif (dayweek is not None): \n\t \tdayweek = str(dayweek).lower() \n\tif (identifier is not None): \n\t \tidentifier = str(identifier) \n\tif (commented is not None): \n\t \tcommented = (commented is True) \n\tif (cmd is not None): \n\t \tcmd = str(cmd) \n\tlst = __salt__['cron.list_tab'](user) \n\tfor cron in lst['crons']: \n\t \tif _cron_matched(cron, cmd, identifier): \n\t \t \tif any([_needs_change(x, y) for (x, y) in ((cron['minute'], minute), (cron['hour'], hour), (cron['daymonth'], daymonth), (cron['month'], month), (cron['dayweek'], dayweek), (cron['identifier'], identifier), (cron['cmd'], cmd), (cron['comment'], comment), (cron['commented'], commented))]): \n\t \t \t \treturn 'update' \n\t \t \treturn 'present' \n\treturn 'absent'\n", 
" \tobj = None \n\tcontainer = content.viewManager.CreateContainerView(content.rootFolder, vimtype, True) \n\tfor c in container.view: \n\t \tif name: \n\t \t \tif (c.name == name): \n\t \t \t \tobj = c \n\t \t \t \tbreak \n\t \telse: \n\t \t \tobj = c \n\t \t \tbreak \n\tcontainer.Destroy() \n\treturn obj\n", 
" \tmetadata = {} \n\tif (('metadata' in inst.extra) and ('items' in inst.extra['metadata'])): \n\t \tfor md in inst.extra['metadata']['items']: \n\t \t \tmetadata[md['key']] = md['value'] \n\ttry: \n\t \tnetname = inst.extra['networkInterfaces'][0]['network'].split('/')[(-1)] \n\texcept: \n\t \tnetname = None \n\ttry: \n\t \tsubnetname = inst.extra['networkInterfaces'][0]['subnetwork'].split('/')[(-1)] \n\texcept: \n\t \tsubnetname = None \n\tif ('disks' in inst.extra): \n\t \tdisk_names = [disk_info['source'].split('/')[(-1)] for disk_info in sorted(inst.extra['disks'], key=(lambda disk_info: disk_info['index']))] \n\telse: \n\t \tdisk_names = [] \n\tif (len(inst.public_ips) == 0): \n\t \tpublic_ip = None \n\telse: \n\t \tpublic_ip = inst.public_ips[0] \n\treturn {'image': (((inst.image is not None) and inst.image.split('/')[(-1)]) or None), 'disks': disk_names, 'machine_type': inst.size, 'metadata': metadata, 'name': inst.name, 'network': netname, 'subnetwork': subnetname, 'private_ip': inst.private_ips[0], 'public_ip': public_ip, 'status': ((('status' in inst.extra) and inst.extra['status']) or None), 'tags': ((('tags' in inst.extra) and inst.extra['tags']) or []), 'zone': ((('zone' in inst.extra) and inst.extra['zone'].name) or None)}\n", 
" \tglobal FORMAT_READERS \n\tif (FORMAT_READERS is None): \n\t \t_import_readers() \n\treturn FORMAT_READERS.get(identity, None)\n", 
" \treturn True\n", 
" \treturn Reporter(sys.stdout, sys.stderr)\n", 
" \ttry: \n\t \tparsed_token = _verify_signed_jwt_with_certs(token, time_now, cache) \n\texcept _AppIdentityError as e: \n\t \tlogging.warning('id_token \tverification \tfailed: \t%s', e) \n\t \treturn None \n\texcept: \n\t \tlogging.warning('id_token \tverification \tfailed.') \n\t \treturn None \n\tif _verify_parsed_token(parsed_token, audiences, allowed_client_ids): \n\t \temail = parsed_token['email'] \n\t \treturn users.User(email)\n", 
" \tif (kb.passwordMgr and all(((_ is not None) for _ in (conf.scheme, conf.hostname, conf.port, conf.authUsername, conf.authPassword)))): \n\t \tkb.passwordMgr.add_password(None, ('%s://%s:%d' % (conf.scheme, conf.hostname, conf.port)), conf.authUsername, conf.authPassword)\n", 
" \tif _is_url(obj): \n\t \twith urlopen(obj) as url: \n\t \t \ttext = url.read() \n\telif hasattr(obj, 'read'): \n\t \ttext = obj.read() \n\telif isinstance(obj, char_types): \n\t \ttext = obj \n\t \ttry: \n\t \t \tif os.path.isfile(text): \n\t \t \t \twith open(text, 'rb') as f: \n\t \t \t \t \treturn f.read() \n\t \texcept (TypeError, ValueError): \n\t \t \tpass \n\telse: \n\t \traise TypeError(('Cannot \tread \tobject \tof \ttype \t%r' % type(obj).__name__)) \n\treturn text\n", 
" \tservice = __opts__.get('auth.pam.service', 'login') \n\t@CONV_FUNC \n\tdef my_conv(n_messages, messages, p_response, app_data): \n\t \t'\\n \t \t \t \t \t \t \t \tSimple \tconversation \tfunction \tthat \tresponds \tto \tany\\n \t \t \t \t \t \t \t \tprompt \twhere \tthe \techo \tis \toff \twith \tthe \tsupplied \tpassword\\n \t \t \t \t \t \t \t \t' \n\t \taddr = CALLOC(n_messages, sizeof(PamResponse)) \n\t \tp_response[0] = cast(addr, POINTER(PamResponse)) \n\t \tfor i in range(n_messages): \n\t \t \tif (messages[i].contents.msg_style == PAM_PROMPT_ECHO_OFF): \n\t \t \t \tpw_copy = STRDUP(str(password)) \n\t \t \t \tp_response.contents[i].resp = cast(pw_copy, c_char_p) \n\t \t \t \tp_response.contents[i].resp_retcode = 0 \n\t \treturn 0 \n\thandle = PamHandle() \n\tconv = PamConv(my_conv, 0) \n\tretval = PAM_START(service, username, pointer(conv), pointer(handle)) \n\tif (retval != 0): \n\t \tPAM_END(handle, retval) \n\t \treturn False \n\tretval = PAM_AUTHENTICATE(handle, 0) \n\tif (retval == 0): \n\t \tPAM_ACCT_MGMT(handle, 0) \n\tPAM_END(handle, 0) \n\treturn (retval == 0)\n", 
" \tif (name is None): \n\t \ts = inspect.stack()[(1 + moreFrames)] \n\t \tname = s[1] \n\t \tif name.endswith('.py'): \n\t \t \tname = name[0:(-3)] \n\t \telif name.endswith('.pyc'): \n\t \t \tname = name[0:(-4)] \n\t \tif name.startswith(_path): \n\t \t \tname = name[len(_path):] \n\t \telif name.startswith(_ext_path): \n\t \t \tname = name[len(_ext_path):] \n\t \tname = name.replace('/', '.').replace('\\\\', '.') \n\t \tif (name.find('.') != (-1)): \n\t \t \tn = name.split('.') \n\t \t \tif (len(n) >= 2): \n\t \t \t \tif (n[(-1)] == n[(-2)]): \n\t \t \t \t \tdel n[(-1)] \n\t \t \t \t \tname = '.'.join(n) \n\t \tif name.startswith('ext.'): \n\t \t \tname = name.split('ext.', 1)[1] \n\t \tif name.endswith('.__init__'): \n\t \t \tname = name.rsplit('.__init__', 1)[0] \n\tl = logging.getLogger(name) \n\tg = globals() \n\tif (not hasattr(l, 'print')): \n\t \tdef printmsg(*args, **kw): \n\t \t \tmsg = ' \t'.join((str(s) for s in args)) \n\t \t \ts = inspect.stack()[1] \n\t \t \to = '[' \n\t \t \tif ('self' in s[0].f_locals): \n\t \t \t \to += (s[0].f_locals['self'].__class__.__name__ + '.') \n\t \t \to += (((s[3] + ':') + str(s[2])) + '] \t') \n\t \t \to += msg \n\t \t \tif (o == _squelch): \n\t \t \t \tif (time.time() >= _squelchTime): \n\t \t \t \t \tl.debug(('[Previous \tmessage \trepeated \t%i \tmore \ttimes]' % ((g['_squelchCount'] + 1),))) \n\t \t \t \t \tg['_squelchCount'] = 0 \n\t \t \t \t \tg['_squelchTime'] = (time.time() + SQUELCH_TIME) \n\t \t \t \telse: \n\t \t \t \t \tg['_squelchCount'] += 1 \n\t \t \telse: \n\t \t \t \tg['_squelch'] = o \n\t \t \t \tif (g['_squelchCount'] > 0): \n\t \t \t \t \tl.debug(('[Previous \tmessage \trepeated \t%i \tmore \ttimes]' % (g['_squelchCount'],))) \n\t \t \t \tg['_squelchCount'] = 0 \n\t \t \t \tg['_squelchTime'] = (time.time() + SQUELCH_TIME) \n\t \t \t \tl.debug(o) \n\t \tsetattr(l, 'print', printmsg) \n\t \tsetattr(l, 'msg', printmsg) \n\treturn l\n", 
" \trequest.session.flush() \n\tif hasattr(request, 'user'): \n\t \tfrom django.contrib.auth.models import AnonymousUser \n\t \trequest.user = AnonymousUser()\n", 
" \tif (get_configured_provider() is False): \n\t \treturn False \n\tif (get_dependencies() is False): \n\t \treturn False \n\treturn __virtualname__\n", 
" \tconnection = boto3.session.Session(aws_access_key_id=access_key_id, aws_secret_access_key=secret_access_key, aws_session_token=session_token) \n\tconnection._session.set_config_variable('metadata_service_num_attempts', BOTO_NUM_RETRIES) \n\tec2_resource = connection.resource('ec2', region_name=region) \n\tif validate_region: \n\t \ttry: \n\t \t \tzones = ec2_resource.meta.client.describe_availability_zones() \n\t \texcept EndpointConnectionError: \n\t \t \traise InvalidRegionError(region) \n\t \tavailable_zones = [available_zone['ZoneName'] for available_zone in zones['AvailabilityZones']] \n\t \tif (zone not in available_zones): \n\t \t \traise InvalidZoneError(zone, available_zones) \n\treturn _EC2(zone=zone, connection=ec2_resource)\n", 
" \tservice_instance = salt.utils.vmware.get_service_instance(host=host, username=username, password=password, protocol=protocol, port=port) \n\tvalid_services = ['DCUI', 'TSM', 'SSH', 'ssh', 'lbtd', 'lsassd', 'lwiod', 'netlogond', 'ntpd', 'sfcbd-watchdog', 'snmpd', 'vprobed', 'vpxa', 'xorg'] \n\thost_names = _check_hosts(service_instance, host, host_names) \n\tret = {} \n\tfor host_name in host_names: \n\t \tif (service_name not in valid_services): \n\t \t \tret.update({host_name: {'Error': '{0} \tis \tnot \ta \tvalid \tservice \tname.'.format(service_name)}}) \n\t \t \treturn ret \n\t \thost_ref = _get_host_ref(service_instance, host, host_name=host_name) \n\t \tservices = host_ref.configManager.serviceSystem.serviceInfo.service \n\t \tif ((service_name == 'SSH') or (service_name == 'ssh')): \n\t \t \ttemp_service_name = 'TSM-SSH' \n\t \telse: \n\t \t \ttemp_service_name = service_name \n\t \tfor service in services: \n\t \t \tif (service.key == temp_service_name): \n\t \t \t \tret.update({host_name: {service_name: service.policy}}) \n\t \t \t \tbreak \n\t \t \telse: \n\t \t \t \tmsg = \"Could \tnot \tfind \tservice \t'{0}' \tfor \thost \t'{1}'.\".format(service_name, host_name) \n\t \t \t \tret.update({host_name: {'Error': msg}}) \n\t \tif (ret.get(host_name) is None): \n\t \t \tmsg = \"'vsphere.get_service_policy' \tfailed \tfor \thost \t{0}.\".format(host_name) \n\t \t \tlog.debug(msg) \n\t \t \tret.update({host_name: {'Error': msg}}) \n\treturn ret\n", 
" \tclient = salt.cloud.CloudClient(os.path.join(os.path.dirname(__opts__['conf_file']), 'cloud'), pillars=copy.deepcopy(__pillar__.get('cloud', {}))) \n\treturn client\n", 
" \tif (name is None): \n\t \tname = random_name(case=case) \n\texpected_container = Container(node_uuid=uuid4(), name=name, image=DockerImage.from_string(u'nginx')) \n\td = client.create_container(node_uuid=expected_container.node_uuid, name=expected_container.name, image=expected_container.image) \n\treturn (expected_container, d)\n", 
" \tclient = salt.cloud.CloudClient(os.path.join(os.path.dirname(__opts__['conf_file']), 'cloud'), pillars=copy.deepcopy(__pillar__.get('cloud', {}))) \n\treturn client\n", 
" \tclient = salt.cloud.CloudClient(os.path.join(os.path.dirname(__opts__['conf_file']), 'cloud'), pillars=copy.deepcopy(__pillar__.get('cloud', {}))) \n\treturn client\n", 
" \tclient = salt.cloud.CloudClient(os.path.join(os.path.dirname(__opts__['conf_file']), 'cloud'), pillars=copy.deepcopy(__pillar__.get('cloud', {}))) \n\treturn client\n", 
" \tclient = salt.cloud.CloudClient(os.path.join(os.path.dirname(__opts__['conf_file']), 'cloud'), pillars=copy.deepcopy(__pillar__.get('cloud', {}))) \n\treturn client\n", 
" \tclient = salt.cloud.CloudClient(os.path.join(os.path.dirname(__opts__['conf_file']), 'cloud'), pillars=copy.deepcopy(__pillar__.get('cloud', {}))) \n\treturn client\n", 
" \tclient = salt.cloud.CloudClient(os.path.join(os.path.dirname(__opts__['conf_file']), 'cloud'), pillars=copy.deepcopy(__pillar__.get('cloud', {}))) \n\treturn client\n", 
" \tclient = salt.cloud.CloudClient(os.path.join(os.path.dirname(__opts__['conf_file']), 'cloud'), pillars=copy.deepcopy(__pillar__.get('cloud', {}))) \n\treturn client\n", 
" \tconn = _get_conn(region=region, key=key, keyid=keyid, profile=profile) \n\tif isinstance(availability_zones, six.string_types): \n\t \tavailability_zones = json.loads(availability_zones) \n\tif isinstance(load_balancers, six.string_types): \n\t \tload_balancers = json.loads(load_balancers) \n\tif isinstance(vpc_zone_identifier, six.string_types): \n\t \tvpc_zone_identifier = json.loads(vpc_zone_identifier) \n\tif isinstance(tags, six.string_types): \n\t \ttags = json.loads(tags) \n\t_tags = [] \n\tif tags: \n\t \tfor tag in tags: \n\t \t \ttry: \n\t \t \t \tkey = tag.get('key') \n\t \t \texcept KeyError: \n\t \t \t \tlog.error('Tag \tmissing \tkey.') \n\t \t \t \treturn False \n\t \t \ttry: \n\t \t \t \tvalue = tag.get('value') \n\t \t \texcept KeyError: \n\t \t \t \tlog.error('Tag \tmissing \tvalue.') \n\t \t \t \treturn False \n\t \t \tpropagate_at_launch = tag.get('propagate_at_launch', False) \n\t \t \t_tag = autoscale.Tag(key=key, value=value, resource_id=name, propagate_at_launch=propagate_at_launch) \n\t \t \t_tags.append(_tag) \n\tif isinstance(termination_policies, six.string_types): \n\t \ttermination_policies = json.loads(termination_policies) \n\tif isinstance(suspended_processes, six.string_types): \n\t \tsuspended_processes = json.loads(suspended_processes) \n\tif isinstance(scheduled_actions, six.string_types): \n\t \tscheduled_actions = json.loads(scheduled_actions) \n\ttry: \n\t \t_asg = autoscale.AutoScalingGroup(name=name, launch_config=launch_config_name, availability_zones=availability_zones, min_size=min_size, max_size=max_size, desired_capacity=desired_capacity, load_balancers=load_balancers, default_cooldown=default_cooldown, health_check_type=health_check_type, health_check_period=health_check_period, placement_group=placement_group, tags=_tags, vpc_zone_identifier=vpc_zone_identifier, termination_policies=termination_policies, suspended_processes=suspended_processes) \n\t \tconn.create_auto_scaling_group(_asg) \n\t \t_create_scaling_policies(conn, name, scaling_policies) \n\t \t_create_scheduled_actions(conn, name, scheduled_actions) \n\t \tif (notification_arn and notification_types): \n\t \t \tconn.put_notification_configuration(_asg, notification_arn, notification_types) \n\t \tlog.info('Created \tASG \t{0}'.format(name)) \n\t \treturn True \n\texcept boto.exception.BotoServerError as e: \n\t \tlog.debug(e) \n\t \tmsg = 'Failed \tto \tcreate \tASG \t{0}'.format(name) \n\t \tlog.error(msg) \n\t \treturn False\n", 
" \tif ('cp.fileclient' not in __context__): \n\t \t__context__['cp.fileclient'] = salt.fileclient.get_file_client(__opts__)\n", 
" \tif ('cp.fileclient' not in __context__): \n\t \t__context__['cp.fileclient'] = salt.fileclient.get_file_client(__opts__)\n", 
" \turl = api_url(host, port, '/Users/Public') \n\tr = requests.get(url) \n\tuser = [i for i in r.json() if (i['Name'] == username)] \n\treturn user\n", 
" \tif (type(string) == unicode): \n\t \treturn string.encode('utf8', 'ignore') \n\treturn str(string)\n", 
" \tbits = token.contents.split() \n\tif (len(bits) != 6): \n\t \traise template.TemplateSyntaxError((\"'%s' \ttag \ttakes \texactly \tfive \targuments\" % bits[0])) \n\tif (bits[2] != 'from'): \n\t \traise template.TemplateSyntaxError((\"second \targument \tto \t'%s' \ttag \tmust \tbe \t'from'\" % bits[0])) \n\tif (bits[4] != 'as'): \n\t \traise template.TemplateSyntaxError((\"fourth \targument \tto \t'%s' \ttag \tmust \tbe \t'as'\" % bits[0])) \n\treturn DictEntryForItemNode(bits[1], bits[3], bits[5])\n", 
" \tif isinstance(s, bytearray): \n\t \ts = bytes(s) \n\tif isinstance(s, six.text_type): \n\t \treturn (u'utf-8', s) \n\telif isinstance(s, six.string_types): \n\t \ttry: \n\t \t \tenc = u'utf-8' \n\t \t \treturn (enc, six.text_type(s, enc)) \n\t \texcept UnicodeError: \n\t \t \tfor e in encoding_list: \n\t \t \t \ttry: \n\t \t \t \t \treturn (e, six.text_type(s, e)) \n\t \t \t \texcept (UnicodeError, LookupError): \n\t \t \t \t \tpass \n\t \t \ttry: \n\t \t \t \tenc = u'utf-8' \n\t \t \t \treturn (enc, six.text_type(s, enc, errors=u'replace')) \n\t \t \texcept UnicodeError: \n\t \t \t \traise Exception((_(u\"Diff \tcontent \tcouldn't \tbe \tconverted \tto \tunicode \tusing \tthe \tfollowing \tencodings: \t%s\") % ([u'utf-8'] + encoding_list))) \n\telse: \n\t \traise TypeError(u'Value \tto \tconvert \tis \tunexpected \ttype \t%s', type(s))\n", 
" \tif isinstance(s, bytearray): \n\t \ts = bytes(s) \n\tif isinstance(s, six.text_type): \n\t \treturn (u'utf-8', s) \n\telif isinstance(s, six.string_types): \n\t \ttry: \n\t \t \tenc = u'utf-8' \n\t \t \treturn (enc, six.text_type(s, enc)) \n\t \texcept UnicodeError: \n\t \t \tfor e in encoding_list: \n\t \t \t \ttry: \n\t \t \t \t \treturn (e, six.text_type(s, e)) \n\t \t \t \texcept (UnicodeError, LookupError): \n\t \t \t \t \tpass \n\t \t \ttry: \n\t \t \t \tenc = u'utf-8' \n\t \t \t \treturn (enc, six.text_type(s, enc, errors=u'replace')) \n\t \t \texcept UnicodeError: \n\t \t \t \traise Exception((_(u\"Diff \tcontent \tcouldn't \tbe \tconverted \tto \tunicode \tusing \tthe \tfollowing \tencodings: \t%s\") % ([u'utf-8'] + encoding_list))) \n\telse: \n\t \traise TypeError(u'Value \tto \tconvert \tis \tunexpected \ttype \t%s', type(s))\n", 
" \t_exists = (name in ls_(path=path)) \n\tif (not _exists): \n\t \t_exists = (name in ls_(cache=False, path=path)) \n\treturn _exists\n", 
" \tresult = {} \n\tfor (key, value) in six.iteritems(dictionary): \n\t \tresult[('%s%s' % (prefix, key))] = value \n\treturn result\n", 
" \treturn round((float(os.path.getsize(APP_PATH)) / (1024 * 1024)), 2)\n", 
" \tcls = _code_map.get(response.status_code, ClientException) \n\tkwargs = {'code': response.status_code, 'method': method, 'url': url, 'request_id': None} \n\tif response.headers: \n\t \tkwargs['request_id'] = response.headers.get('x-compute-request-id') \n\t \tif (issubclass(cls, RetryAfterException) and ('retry-after' in response.headers)): \n\t \t \tkwargs['retry_after'] = response.headers.get('retry-after') \n\tif body: \n\t \tmessage = 'n/a' \n\t \tdetails = 'n/a' \n\t \tif hasattr(body, 'keys'): \n\t \t \tif ('message' in body): \n\t \t \t \tmessage = body.get('message') \n\t \t \t \tdetails = body.get('details') \n\t \t \telse: \n\t \t \t \terror = body[list(body)[0]] \n\t \t \t \tmessage = error.get('message') \n\t \t \t \tdetails = error.get('details') \n\t \tkwargs['message'] = message \n\t \tkwargs['details'] = details \n\treturn cls(**kwargs)\n", 
" \tif (defaults is None): \n\t \tdefaults = {} \n\tif isinstance(settings, dict): \n\t \tdef getopt(key, default=None): \n\t \t \treturn settings.get(('SENTRY_%s' % key.upper()), defaults.get(key, default)) \n\t \toptions = copy.copy((settings.get('SENTRY_CONFIG') or settings.get('RAVEN_CONFIG') or {})) \n\telse: \n\t \tdef getopt(key, default=None): \n\t \t \treturn getattr(settings, ('SENTRY_%s' % key.upper()), defaults.get(key, default)) \n\t \toptions = copy.copy((getattr(settings, 'SENTRY_CONFIG', None) or getattr(settings, 'RAVEN_CONFIG', None) or {})) \n\toptions.setdefault('include_paths', getopt('include_paths', [])) \n\toptions.setdefault('exclude_paths', getopt('exclude_paths', [])) \n\toptions.setdefault('timeout', getopt('timeout')) \n\toptions.setdefault('name', getopt('name')) \n\toptions.setdefault('auto_log_stacks', getopt('auto_log_stacks')) \n\toptions.setdefault('string_max_length', getopt('string_max_length')) \n\toptions.setdefault('list_max_length', getopt('list_max_length')) \n\toptions.setdefault('site', getopt('site')) \n\toptions.setdefault('processors', getopt('processors')) \n\toptions.setdefault('dsn', getopt('dsn', os.environ.get('SENTRY_DSN'))) \n\toptions.setdefault('context', getopt('context')) \n\toptions.setdefault('tags', getopt('tags')) \n\toptions.setdefault('release', getopt('release')) \n\toptions.setdefault('environment', getopt('environment')) \n\toptions.setdefault('ignore_exceptions', getopt('ignore_exceptions')) \n\ttransport = (getopt('transport') or options.get('transport')) \n\tif isinstance(transport, string_types): \n\t \ttransport = import_string(transport) \n\toptions['transport'] = transport \n\treturn options\n", 
" \tif (not (values and all(((isinstance(value, string_types) and value) for value in values)))): \n\t \traise ValueError(('expected \ta \tnon-empty \tsequence \tof \tstrings, \tgot \t%s' % values)) \n\tif (len(values) != len(set(values))): \n\t \traise ValueError(('enumeration \titems \tmust \tbe \tunique, \tgot \t%s' % values)) \n\tattrs = dict([(value, value) for value in values]) \n\tattrs.update({'_values': list(values), '_default': values[0], '_case_sensitive': kwargs.get('case_sensitive', True)}) \n\treturn type('Enumeration', (Enumeration,), attrs)()\n", 
" \ttry: \n\t \treturn float(s) \n\texcept ValueError: \n\t \treturn s\n", 
" \ttry: \n\t \treturn float(s) \n\texcept ValueError: \n\t \treturn s\n", 
" \ttry: \n\t \treturn float(s) \n\texcept ValueError: \n\t \treturn s\n", 
" \ttry: \n\t \treturn float(s) \n\texcept ValueError: \n\t \treturn s\n", 
" \ttry: \n\t \treturn float(s) \n\texcept ValueError: \n\t \treturn s\n", 
" \treturn re.sub('<!--.*-->', '', re.sub('<!\\\\[CDATA\\\\[.*\\\\]\\\\]>', '', re.sub('(\\\\s|&nbsp;|//)+', ' \t', html_to_text(content))))\n", 
" \tif ('\\n\\n' in post): \n\t \t(headers, body) = post.split('\\n\\n', 1) \n\t \treturn body.lower() \n\telse: \n\t \treturn post.lower()\n", 
" \tcode_cell = {'cell_type': 'code', 'execution_count': None, 'metadata': {'collapsed': False}, 'outputs': [], 'source': [code.strip()]} \n\twork_notebook['cells'].append(code_cell)\n", 
" \td = _QueryString(title, prompt, **kw) \n\treturn d.result\n", 
" \tpatch_stdout = kwargs.pop(u'patch_stdout', False) \n\treturn_asyncio_coroutine = kwargs.pop(u'return_asyncio_coroutine', False) \n\ttrue_color = kwargs.pop(u'true_color', False) \n\trefresh_interval = kwargs.pop(u'refresh_interval', 0) \n\teventloop = kwargs.pop(u'eventloop', None) \n\tapplication = create_prompt_application(message, **kwargs) \n\treturn run_application(application, patch_stdout=patch_stdout, return_asyncio_coroutine=return_asyncio_coroutine, true_color=true_color, refresh_interval=refresh_interval, eventloop=eventloop)\n", 
" \tpatch_stdout = kwargs.pop(u'patch_stdout', False) \n\treturn_asyncio_coroutine = kwargs.pop(u'return_asyncio_coroutine', False) \n\ttrue_color = kwargs.pop(u'true_color', False) \n\trefresh_interval = kwargs.pop(u'refresh_interval', 0) \n\teventloop = kwargs.pop(u'eventloop', None) \n\tapplication = create_prompt_application(message, **kwargs) \n\treturn run_application(application, patch_stdout=patch_stdout, return_asyncio_coroutine=return_asyncio_coroutine, true_color=true_color, refresh_interval=refresh_interval, eventloop=eventloop)\n", 
" \tpatch_stdout = kwargs.pop(u'patch_stdout', False) \n\treturn_asyncio_coroutine = kwargs.pop(u'return_asyncio_coroutine', False) \n\ttrue_color = kwargs.pop(u'true_color', False) \n\trefresh_interval = kwargs.pop(u'refresh_interval', 0) \n\teventloop = kwargs.pop(u'eventloop', None) \n\tapplication = create_prompt_application(message, **kwargs) \n\treturn run_application(application, patch_stdout=patch_stdout, return_asyncio_coroutine=return_asyncio_coroutine, true_color=true_color, refresh_interval=refresh_interval, eventloop=eventloop)\n", 
" \tpatch_stdout = kwargs.pop(u'patch_stdout', False) \n\treturn_asyncio_coroutine = kwargs.pop(u'return_asyncio_coroutine', False) \n\ttrue_color = kwargs.pop(u'true_color', False) \n\trefresh_interval = kwargs.pop(u'refresh_interval', 0) \n\teventloop = kwargs.pop(u'eventloop', None) \n\tapplication = create_prompt_application(message, **kwargs) \n\treturn run_application(application, patch_stdout=patch_stdout, return_asyncio_coroutine=return_asyncio_coroutine, true_color=true_color, refresh_interval=refresh_interval, eventloop=eventloop)\n", 
" \tpatch_stdout = kwargs.pop(u'patch_stdout', False) \n\treturn_asyncio_coroutine = kwargs.pop(u'return_asyncio_coroutine', False) \n\ttrue_color = kwargs.pop(u'true_color', False) \n\trefresh_interval = kwargs.pop(u'refresh_interval', 0) \n\teventloop = kwargs.pop(u'eventloop', None) \n\tapplication = create_prompt_application(message, **kwargs) \n\treturn run_application(application, patch_stdout=patch_stdout, return_asyncio_coroutine=return_asyncio_coroutine, true_color=true_color, refresh_interval=refresh_interval, eventloop=eventloop)\n", 
" \tpatch_stdout = kwargs.pop(u'patch_stdout', False) \n\treturn_asyncio_coroutine = kwargs.pop(u'return_asyncio_coroutine', False) \n\ttrue_color = kwargs.pop(u'true_color', False) \n\trefresh_interval = kwargs.pop(u'refresh_interval', 0) \n\teventloop = kwargs.pop(u'eventloop', None) \n\tapplication = create_prompt_application(message, **kwargs) \n\treturn run_application(application, patch_stdout=patch_stdout, return_asyncio_coroutine=return_asyncio_coroutine, true_color=true_color, refresh_interval=refresh_interval, eventloop=eventloop)\n", 
" \tpatch_stdout = kwargs.pop(u'patch_stdout', False) \n\treturn_asyncio_coroutine = kwargs.pop(u'return_asyncio_coroutine', False) \n\ttrue_color = kwargs.pop(u'true_color', False) \n\trefresh_interval = kwargs.pop(u'refresh_interval', 0) \n\teventloop = kwargs.pop(u'eventloop', None) \n\tapplication = create_prompt_application(message, **kwargs) \n\treturn run_application(application, patch_stdout=patch_stdout, return_asyncio_coroutine=return_asyncio_coroutine, true_color=true_color, refresh_interval=refresh_interval, eventloop=eventloop)\n", 
" \td = _QueryString(title, prompt, **kw) \n\treturn d.result\n", 
" \tfor name in names: \n\t \ttry: \n\t \t \tgetattr(conf, name) \n\t \texcept (cfg.NoSuchOptError, cfg.ConfigFileValueError): \n\t \t \twith excutils.save_and_reraise_exception(): \n\t \t \t \toutput_log(MSG.INVALID_PARAMETER, param=name)\n", 
" \tname = name.strip() \n\tif ((not name) or (name == '0.0.0.0')): \n\t \tname = gethostname() \n\ttry: \n\t \t(hostname, aliases, ipaddrs) = gethostbyaddr(name) \n\texcept error: \n\t \tpass \n\telse: \n\t \taliases.insert(0, hostname) \n\t \tfor name in aliases: \n\t \t \tif ('.' in name): \n\t \t \t \tbreak \n\t \telse: \n\t \t \tname = hostname \n\treturn name\n", 
" \t(host, port) = address \n\terr = None \n\tfor res in socket.getaddrinfo(host, port, 0, socket.SOCK_STREAM): \n\t \t(af, socktype, proto, canonname, sa) = res \n\t \tsock = None \n\t \ttry: \n\t \t \tsock = socket.socket(af, socktype, proto) \n\t \t \t_set_socket_options(sock, socket_options) \n\t \t \tif (timeout is not socket._GLOBAL_DEFAULT_TIMEOUT): \n\t \t \t \tsock.settimeout(timeout) \n\t \t \tif source_address: \n\t \t \t \tsock.bind(source_address) \n\t \t \tsock.connect(sa) \n\t \t \treturn sock \n\t \texcept socket.error as _: \n\t \t \terr = _ \n\t \t \tif (sock is not None): \n\t \t \t \tsock.close() \n\t \t \t \tsock = None \n\tif (err is not None): \n\t \traise err \n\telse: \n\t \traise socket.error('getaddrinfo \treturns \tan \tempty \tlist')\n", 
" \treturn (IPV6_PATTERN.match(address) is not None)\n", 
" \tif check_pickle: \n\t \tpickle.dumps(function) \n\tdef delayed_function(*args, **kwargs): \n\t \treturn (function, args, kwargs) \n\ttry: \n\t \tdelayed_function = functools.wraps(function)(delayed_function) \n\texcept AttributeError: \n\t \t' \tfunctools.wraps \tfails \ton \tsome \tcallable \tobjects \t' \n\treturn delayed_function\n", 
" \tmod = sys.modules.get(import_name) \n\tif ((mod is not None) and hasattr(mod, '__file__')): \n\t \treturn os.path.dirname(os.path.abspath(mod.__file__)) \n\tloader = pkgutil.get_loader(import_name) \n\tif ((loader is None) or (import_name == '__main__')): \n\t \treturn os.getcwd() \n\tif hasattr(loader, 'get_filename'): \n\t \tfilepath = loader.get_filename(import_name) \n\telse: \n\t \t__import__(import_name) \n\t \tmod = sys.modules[import_name] \n\t \tfilepath = getattr(mod, '__file__', None) \n\t \tif (filepath is None): \n\t \t \traise RuntimeError(('No \troot \tpath \tcan \tbe \tfound \tfor \tthe \tprovided \tmodule \t\"%s\". \t \tThis \tcan \thappen \tbecause \tthe \tmodule \tcame \tfrom \tan \timport \thook \tthat \tdoes \tnot \tprovide \tfile \tname \tinformation \tor \tbecause \tit\\'s \ta \tnamespace \tpackage. \t \tIn \tthis \tcase \tthe \troot \tpath \tneeds \tto \tbe \texplicitly \tprovided.' % import_name)) \n\treturn os.path.dirname(os.path.abspath(filepath))\n", 
" \tcatch_api_errors = kwargs.pop('catch_api_errors', True) \n\tif kwargs.pop('client_auth', False): \n\t \tregistry_auth_config = {} \n\t \ttry: \n\t \t \thome = os.path.expanduser('~') \n\t \t \tdocker_auth_file = os.path.join(home, '.docker', 'config.json') \n\t \t \twith salt.utils.fopen(docker_auth_file) as fp: \n\t \t \t \ttry: \n\t \t \t \t \tdocker_auth = json.load(fp) \n\t \t \t \t \tfp.close() \n\t \t \t \texcept (OSError, IOError) as exc: \n\t \t \t \t \tif (exc.errno != errno.ENOENT): \n\t \t \t \t \t \tlog.error('Failed \tto \tread \tdocker \tauth \tfile \t%s: \t%s', docker_auth_file, exc) \n\t \t \t \t \t \tdocker_auth = {} \n\t \t \t \tif isinstance(docker_auth, dict): \n\t \t \t \t \tif (('auths' in docker_auth) and isinstance(docker_auth['auths'], dict)): \n\t \t \t \t \t \tfor (key, data) in six.iteritems(docker_auth['auths']): \n\t \t \t \t \t \t \tif isinstance(data, dict): \n\t \t \t \t \t \t \t \temail = str(data.get('email', '')) \n\t \t \t \t \t \t \t \tb64_auth = base64.b64decode(data.get('auth', '')) \n\t \t \t \t \t \t \t \t(username, password) = b64_auth.split(':') \n\t \t \t \t \t \t \t \tregistry = 'https://{registry}'.format(registry=key) \n\t \t \t \t \t \t \t \tregistry_auth_config.update({registry: {'username': username, 'password': password, 'email': email}}) \n\t \texcept Exception as e: \n\t \t \tlog.debug('dockerng \twas \tunable \tto \tload \tcredential \tfrom \t~/.docker/config.json \ttrying \twith \tpillar \tnow \t({0})'.format(e)) \n\t \tregistry_auth_config.update(__pillar__.get('docker-registries', {})) \n\t \tfor (key, data) in six.iteritems(__pillar__): \n\t \t \tif key.endswith('-docker-registries'): \n\t \t \t \tregistry_auth_config.update(data) \n\t \terr = '{0} \tDocker \tcredentials{1}. \tPlease \tsee \tthe \tdockerng \tremote \texecution \tmodule \tdocumentation \tfor \tinformation \ton \thow \tto \tconfigure \tauthentication.' \n\t \ttry: \n\t \t \tfor (registry, creds) in six.iteritems(registry_auth_config): \n\t \t \t \t__context__['docker.client'].login(creds['username'], password=creds['password'], email=creds.get('email'), registry=registry, reauth=creds.get('reauth', False)) \n\t \texcept KeyError: \n\t \t \traise SaltInvocationError(err.format('Incomplete', ' \tfor \tregistry \t{0}'.format(registry))) \n\t \tclient_timeout = kwargs.pop('client_timeout', None) \n\t \tif (client_timeout is not None): \n\t \t \t__context__['docker.client'].timeout = client_timeout \n\tfunc = getattr(__context__['docker.client'], attr) \n\tif (func is None): \n\t \traise SaltInvocationError(\"Invalid \tclient \taction \t'{0}'\".format(attr)) \n\tret = [] \n\ttry: \n\t \toutput = func(*args, **kwargs) \n\t \tif (not kwargs.get('stream', False)): \n\t \t \toutput = output.splitlines() \n\t \tfor line in output: \n\t \t \tret.append(json.loads(line)) \n\texcept docker.errors.APIError as exc: \n\t \tif catch_api_errors: \n\t \t \traise CommandExecutionError('Error \t{0}: \t{1}'.format(exc.response.status_code, exc.explanation)) \n\t \telse: \n\t \t \traise \n\texcept Exception as exc: \n\t \traise CommandExecutionError('Error \toccurred \tperforming \tdocker \t{0}: \t{1}'.format(attr, exc)) \n\treturn ret\n", 
" \tdef is_abstract(c): \n\t \tif (not hasattr(c, '__abstractmethods__')): \n\t \t \treturn False \n\t \tif (not len(c.__abstractmethods__)): \n\t \t \treturn False \n\t \treturn True \n\tall_classes = [] \n\tpath = sklearn.__path__ \n\tfor (importer, modname, ispkg) in pkgutil.walk_packages(path=path, prefix='sklearn.', onerror=(lambda x: None)): \n\t \tif ('.tests.' in modname): \n\t \t \tcontinue \n\t \tmodule = __import__(modname, fromlist='dummy') \n\t \tclasses = inspect.getmembers(module, inspect.isclass) \n\t \tall_classes.extend(classes) \n\tall_classes = set(all_classes) \n\testimators = [c for c in all_classes if (issubclass(c[1], BaseEstimator) and (c[0] != 'BaseEstimator'))] \n\testimators = [c for c in estimators if (not is_abstract(c[1]))] \n\tif (not include_dont_test): \n\t \testimators = [c for c in estimators if (not (c[0] in DONT_TEST))] \n\tif (not include_other): \n\t \testimators = [c for c in estimators if (not (c[0] in OTHER))] \n\tif (not include_meta_estimators): \n\t \testimators = [c for c in estimators if (not (c[0] in META_ESTIMATORS))] \n\tif (type_filter is not None): \n\t \tif (not isinstance(type_filter, list)): \n\t \t \ttype_filter = [type_filter] \n\t \telse: \n\t \t \ttype_filter = list(type_filter) \n\t \tfiltered_estimators = [] \n\t \tfilters = {'classifier': ClassifierMixin, 'regressor': RegressorMixin, 'transformer': TransformerMixin, 'cluster': ClusterMixin} \n\t \tfor (name, mixin) in filters.items(): \n\t \t \tif (name in type_filter): \n\t \t \t \ttype_filter.remove(name) \n\t \t \t \tfiltered_estimators.extend([est for est in estimators if issubclass(est[1], mixin)]) \n\t \testimators = filtered_estimators \n\t \tif type_filter: \n\t \t \traise ValueError((\"Parameter \ttype_filter \tmust \tbe \t'classifier', \t'regressor', \t'transformer', \t'cluster' \tor \tNone, \tgot \t%s.\" % repr(type_filter))) \n\treturn sorted(set(estimators), key=itemgetter(0))\n", 
" \tif ((value is None) and empty_ok): \n\t \treturn \n\tif (not isinstance(value, (int, long))): \n\t \traise exception(('%s \tshould \tbe \tan \tinteger; \treceived \t%s \t(a \t%s).' % (name, value, typename(value)))) \n\tif ((not value) and (not zero_ok)): \n\t \traise exception(('%s \tmust \tnot \tbe \t0 \t(zero)' % name)) \n\tif ((value < 0) and (not negative_ok)): \n\t \traise exception(('%s \tmust \tnot \tbe \tnegative.' % name))\n", 
" \treturn ChecklistModule(mpstate)\n", 
" \tif (not isinstance(html, _strings)): \n\t \traise TypeError('string \trequired') \n\taccept_leading_text = bool(create_parent) \n\telements = fragments_fromstring(html, guess_charset=guess_charset, parser=parser, no_leading_text=(not accept_leading_text)) \n\tif create_parent: \n\t \tif (not isinstance(create_parent, _strings)): \n\t \t \tcreate_parent = 'div' \n\t \tnew_root = Element(create_parent) \n\t \tif elements: \n\t \t \tif isinstance(elements[0], _strings): \n\t \t \t \tnew_root.text = elements[0] \n\t \t \t \tdel elements[0] \n\t \t \tnew_root.extend(elements) \n\t \treturn new_root \n\tif (not elements): \n\t \traise etree.ParserError('No \telements \tfound') \n\tif (len(elements) > 1): \n\t \traise etree.ParserError('Multiple \telements \tfound') \n\tresult = elements[0] \n\tif (result.tail and result.tail.strip()): \n\t \traise etree.ParserError(('Element \tfollowed \tby \ttext: \t%r' % result.tail)) \n\tresult.tail = None \n\treturn result\n", 
" \traise ValueError\n", 
" \trng = np.random.RandomState(42) \n\tt = rng.randn(100) \n\tedges = bayesian_blocks(t, fitness=u'events') \n\tassert_allclose(edges, [(-2.6197451), (-0.71094865), 0.36866702, 1.85227818]) \n\tt[80:] = t[:20] \n\tedges = bayesian_blocks(t, fitness=u'events', p0=0.01) \n\tassert_allclose(edges, [(-2.6197451), (-0.47432431), (-0.46202823), 1.85227818]) \n\tdt = 0.01 \n\tt = (dt * np.arange(1000)) \n\tx = np.zeros(len(t)) \n\tN = (len(t) // 10) \n\tx[rng.randint(0, len(t), N)] = 1 \n\tx[rng.randint(0, (len(t) // 2), N)] = 1 \n\tedges = bayesian_blocks(t, x, fitness=u'regular_events', dt=dt) \n\tassert_allclose(edges, [0, 5.105, 9.99]) \n\tt = (100 * rng.rand(20)) \n\tx = np.exp(((-0.5) * ((t - 50) ** 2))) \n\tsigma = 0.1 \n\tx_obs = (x + (sigma * rng.randn(len(x)))) \n\tedges = bayesian_blocks(t, x_obs, sigma, fitness=u'measures') \n\tassert_allclose(edges, [4.360377, 48.456895, 52.597917, 99.455051])\n", 
" \traise ValueError\n", 
" \tprobs0 = np.asarray(probs0, float) \n\tprobs1 = np.asarray(probs1, float) \n\tprobs0 = (probs0 / probs0.sum(axis)) \n\tprobs1 = (probs1 / probs1.sum(axis)) \n\td2 = (((probs1 - probs0) ** 2) / probs0).sum(axis) \n\tif (correction is not None): \n\t \t(nobs, df) = correction \n\t \tdiff = ((probs1 - probs0) / probs0).sum(axis) \n\t \td2 = np.maximum(((((d2 * nobs) - diff) - df) / (nobs - 1.0)), 0) \n\tif cohen: \n\t \treturn np.sqrt(d2) \n\telse: \n\t \treturn d2\n", 
" \tx = np.asarray(x) \n\tmask = isnull(x) \n\tx = x[(~ mask)] \n\tvalues = np.sort(x) \n\tdef _get_score(at): \n\t \tif (len(values) == 0): \n\t \t \treturn np.nan \n\t \tidx = (at * (len(values) - 1)) \n\t \tif ((idx % 1) == 0): \n\t \t \tscore = values[int(idx)] \n\t \telif (interpolation_method == 'fraction'): \n\t \t \tscore = _interpolate(values[int(idx)], values[(int(idx) + 1)], (idx % 1)) \n\t \telif (interpolation_method == 'lower'): \n\t \t \tscore = values[np.floor(idx)] \n\t \telif (interpolation_method == 'higher'): \n\t \t \tscore = values[np.ceil(idx)] \n\t \telse: \n\t \t \traise ValueError(\"interpolation_method \tcan \tonly \tbe \t'fraction' \t, \t'lower' \tor \t'higher'\") \n\t \treturn score \n\tif is_scalar(q): \n\t \treturn _get_score(q) \n\telse: \n\t \tq = np.asarray(q, np.float64) \n\t \treturn algos.arrmap_float64(q, _get_score)\n", 
" \tbest_model = None \n\tbest_inlier_num = 0 \n\tbest_inlier_residuals_sum = np.inf \n\tbest_inliers = None \n\trandom_state = check_random_state(random_state) \n\tif (min_samples < 0): \n\t \traise ValueError('`min_samples` \tmust \tbe \tgreater \tthan \tzero') \n\tif (max_trials < 0): \n\t \traise ValueError('`max_trials` \tmust \tbe \tgreater \tthan \tzero') \n\tif ((stop_probability < 0) or (stop_probability > 1)): \n\t \traise ValueError('`stop_probability` \tmust \tbe \tin \trange \t[0, \t1]') \n\tif ((not isinstance(data, list)) and (not isinstance(data, tuple))): \n\t \tdata = [data] \n\tdata = list(data) \n\tnum_samples = data[0].shape[0] \n\tfor num_trials in range(max_trials): \n\t \tsamples = [] \n\t \trandom_idxs = random_state.randint(0, num_samples, min_samples) \n\t \tfor d in data: \n\t \t \tsamples.append(d[random_idxs]) \n\t \tif ((is_data_valid is not None) and (not is_data_valid(*samples))): \n\t \t \tcontinue \n\t \tsample_model = model_class() \n\t \tsuccess = sample_model.estimate(*samples) \n\t \tif (success is not None): \n\t \t \tif (not success): \n\t \t \t \tcontinue \n\t \tif ((is_model_valid is not None) and (not is_model_valid(sample_model, *samples))): \n\t \t \tcontinue \n\t \tsample_model_residuals = np.abs(sample_model.residuals(*data)) \n\t \tsample_model_inliers = (sample_model_residuals < residual_threshold) \n\t \tsample_model_residuals_sum = np.sum((sample_model_residuals ** 2)) \n\t \tsample_inlier_num = np.sum(sample_model_inliers) \n\t \tif ((sample_inlier_num > best_inlier_num) or ((sample_inlier_num == best_inlier_num) and (sample_model_residuals_sum < best_inlier_residuals_sum))): \n\t \t \tbest_model = sample_model \n\t \t \tbest_inlier_num = sample_inlier_num \n\t \t \tbest_inlier_residuals_sum = sample_model_residuals_sum \n\t \t \tbest_inliers = sample_model_inliers \n\t \t \tif ((best_inlier_num >= stop_sample_num) or (best_inlier_residuals_sum <= stop_residuals_sum) or (num_trials >= _dynamic_max_trials(best_inlier_num, num_samples, min_samples, stop_probability))): \n\t \t \t \tbreak \n\tif (best_inliers is not None): \n\t \tfor i in range(len(data)): \n\t \t \tdata[i] = data[i][best_inliers] \n\t \tbest_model.estimate(*data) \n\treturn (best_model, best_inliers)\n", 
" \tn = len(rvs) \n\tnsupp = 20 \n\twsupp = (1.0 / nsupp) \n\tdistsupport = lrange(max(distfn.a, (-1000)), (min(distfn.b, 1000) + 1)) \n\tlast = 0 \n\tdistsupp = [max(distfn.a, (-1000))] \n\tdistmass = [] \n\tfor ii in distsupport: \n\t \tcurrent = distfn.cdf(ii, *arg) \n\t \tif ((current - last) >= (wsupp - 1e-14)): \n\t \t \tdistsupp.append(ii) \n\t \t \tdistmass.append((current - last)) \n\t \t \tlast = current \n\t \t \tif (current > (1 - wsupp)): \n\t \t \t \tbreak \n\tif (distsupp[(-1)] < distfn.b): \n\t \tdistsupp.append(distfn.b) \n\t \tdistmass.append((1 - last)) \n\tdistsupp = np.array(distsupp) \n\tdistmass = np.array(distmass) \n\thistsupp = (distsupp + 1e-08) \n\thistsupp[0] = distfn.a \n\t(freq, hsupp) = np.histogram(rvs, histsupp) \n\tcdfs = distfn.cdf(distsupp, *arg) \n\t(chis, pval) = stats.chisquare(np.array(freq), (n * distmass)) \n\treturn (chis, pval, (pval > alpha), ('chisquare \t- \ttest \tfor \t%sat \targ \t= \t%s \twith \tpval \t= \t%s' % (msg, str(arg), str(pval))))\n", 
" \tan = asarray(an) \n\tN = (len(an) - 1) \n\tn = (N - m) \n\tif (n < 0): \n\t \traise ValueError('Order \tof \tq \t<m> \tmust \tbe \tsmaller \tthan \tlen(an)-1.') \n\tAkj = eye((N + 1), (n + 1)) \n\tBkj = zeros(((N + 1), m), 'd') \n\tfor row in range(1, (m + 1)): \n\t \tBkj[row, :row] = (- an[:row][::(-1)]) \n\tfor row in range((m + 1), (N + 1)): \n\t \tBkj[row, :] = (- an[(row - m):row][::(-1)]) \n\tC = hstack((Akj, Bkj)) \n\tpq = linalg.solve(C, an) \n\tp = pq[:(n + 1)] \n\tq = r_[(1.0, pq[(n + 1):])] \n\treturn (poly1d(p[::(-1)]), poly1d(q[::(-1)]))\n", 
" \tpos2 = [5, 42, 80] \n\tcolor2 = np.array([[0.0, 1.0, 0.0, 0.5], [1.0, 0.0, 0.0, 0.75], [0.0, 0.0, 1.0, 1.0]], dtype=np.float32) \n\tcolor2_expected = np.array([color2[0], color2[0], color2[1], color2[1], color2[2], color2[2]], dtype=np.float32) \n\twith TestingCanvas() as c: \n\t \tregion = visuals.LinearRegion(pos=pos2, color=color2, parent=c.scene) \n\t \tassert np.all((region._color == color2_expected)) \n\t \tassert np.all((region.color == color2)) \n\t \tassert_image_approved(c.render(), 'visuals/linear_region2.png')\n", 
" \txs = np.asarray(xs) \n\tys = np.asarray(ys) \n\tres = (ys - (inter + (slope * xs))) \n\treturn res\n", 
" \traise _DecodeError('Tag \thad \tinvalid \twire \ttype.')\n", 
" \tbest_model = None \n\tbest_inlier_num = 0 \n\tbest_inlier_residuals_sum = np.inf \n\tbest_inliers = None \n\trandom_state = check_random_state(random_state) \n\tif (min_samples < 0): \n\t \traise ValueError('`min_samples` \tmust \tbe \tgreater \tthan \tzero') \n\tif (max_trials < 0): \n\t \traise ValueError('`max_trials` \tmust \tbe \tgreater \tthan \tzero') \n\tif ((stop_probability < 0) or (stop_probability > 1)): \n\t \traise ValueError('`stop_probability` \tmust \tbe \tin \trange \t[0, \t1]') \n\tif ((not isinstance(data, list)) and (not isinstance(data, tuple))): \n\t \tdata = [data] \n\tdata = list(data) \n\tnum_samples = data[0].shape[0] \n\tfor num_trials in range(max_trials): \n\t \tsamples = [] \n\t \trandom_idxs = random_state.randint(0, num_samples, min_samples) \n\t \tfor d in data: \n\t \t \tsamples.append(d[random_idxs]) \n\t \tif ((is_data_valid is not None) and (not is_data_valid(*samples))): \n\t \t \tcontinue \n\t \tsample_model = model_class() \n\t \tsuccess = sample_model.estimate(*samples) \n\t \tif (success is not None): \n\t \t \tif (not success): \n\t \t \t \tcontinue \n\t \tif ((is_model_valid is not None) and (not is_model_valid(sample_model, *samples))): \n\t \t \tcontinue \n\t \tsample_model_residuals = np.abs(sample_model.residuals(*data)) \n\t \tsample_model_inliers = (sample_model_residuals < residual_threshold) \n\t \tsample_model_residuals_sum = np.sum((sample_model_residuals ** 2)) \n\t \tsample_inlier_num = np.sum(sample_model_inliers) \n\t \tif ((sample_inlier_num > best_inlier_num) or ((sample_inlier_num == best_inlier_num) and (sample_model_residuals_sum < best_inlier_residuals_sum))): \n\t \t \tbest_model = sample_model \n\t \t \tbest_inlier_num = sample_inlier_num \n\t \t \tbest_inlier_residuals_sum = sample_model_residuals_sum \n\t \t \tbest_inliers = sample_model_inliers \n\t \t \tif ((best_inlier_num >= stop_sample_num) or (best_inlier_residuals_sum <= stop_residuals_sum) or (num_trials >= _dynamic_max_trials(best_inlier_num, num_samples, min_samples, stop_probability))): \n\t \t \t \tbreak \n\tif (best_inliers is not None): \n\t \tfor i in range(len(data)): \n\t \t \tdata[i] = data[i][best_inliers] \n\t \tbest_model.estimate(*data) \n\treturn (best_model, best_inliers)\n", 
" \t(n, dim) = (300, 2) \n\tnp.random.seed(0) \n\tC = np.array([[0.0, (-0.23)], [0.83, 0.23]]) \n\tX = np.r_[(np.dot(np.random.randn(n, dim), C), (np.dot(np.random.randn(n, dim), C) + np.array([1, 1])))] \n\ty = np.hstack((np.zeros(n), np.ones(n))) \n\treturn (X, y)\n", 
" \tclf = GaussianNB(priors=np.array([0.25, 0.25, 0.25, 0.25])) \n\tassert_raises(ValueError, clf.fit, X, y)\n", 
" \tdef g(target, *args, **kwargs): \n\t \tval = func(target, *args, **kwargs) \n\t \tif val: \n\t \t \treturn val \n\t \telse: \n\t \t \traise exception_class((value_error_message_template % target)) \n\tg.__name__ = func.__name__ \n\tg.__doc__ = func.__doc__ \n\treturn g\n", 
" \tclassifier = Classifier(training_filename) \n\tf = open(test_filename) \n\tlines = f.readlines() \n\tf.close() \n\tnumCorrect = 0.0 \n\tfor line in lines: \n\t \tdata = line.strip().split(' DCTB ') \n\t \tvector = [] \n\t \tclassInColumn = (-1) \n\t \tfor i in range(len(classifier.format)): \n\t \t \tif (classifier.format[i] == 'num'): \n\t \t \t \tvector.append(float(data[i])) \n\t \t \telif (classifier.format[i] == 'class'): \n\t \t \t \tclassInColumn = i \n\t \ttheClass = classifier.classify(vector) \n\t \tprefix = '-' \n\t \tif (theClass == data[classInColumn]): \n\t \t \tnumCorrect += 1 \n\t \t \tprefix = '+' \n\t \tprint ('%s \t \t%12s \t \t%s' % (prefix, theClass, line)) \n\tprint ('%4.2f%% \tcorrect' % ((numCorrect * 100) / len(lines)))\n", 
" \traise ValueError\n", 
" \tn = len(rvs) \n\tnsupp = 20 \n\twsupp = (1.0 / nsupp) \n\tdistsupport = lrange(max(distfn.a, (-1000)), (min(distfn.b, 1000) + 1)) \n\tlast = 0 \n\tdistsupp = [max(distfn.a, (-1000))] \n\tdistmass = [] \n\tfor ii in distsupport: \n\t \tcurrent = distfn.cdf(ii, *arg) \n\t \tif ((current - last) >= (wsupp - 1e-14)): \n\t \t \tdistsupp.append(ii) \n\t \t \tdistmass.append((current - last)) \n\t \t \tlast = current \n\t \t \tif (current > (1 - wsupp)): \n\t \t \t \tbreak \n\tif (distsupp[(-1)] < distfn.b): \n\t \tdistsupp.append(distfn.b) \n\t \tdistmass.append((1 - last)) \n\tdistsupp = np.array(distsupp) \n\tdistmass = np.array(distmass) \n\thistsupp = (distsupp + 1e-08) \n\thistsupp[0] = distfn.a \n\t(freq, hsupp) = np.histogram(rvs, histsupp) \n\tcdfs = distfn.cdf(distsupp, *arg) \n\t(chis, pval) = stats.chisquare(np.array(freq), (n * distmass)) \n\treturn (chis, pval, (pval > alpha), ('chisquare \t- \ttest \tfor \t%sat \targ \t= \t%s \twith \tpval \t= \t%s' % (msg, str(arg), str(pval))))\n", 
" \tdef is_abstract(c): \n\t \tif (not hasattr(c, '__abstractmethods__')): \n\t \t \treturn False \n\t \tif (not len(c.__abstractmethods__)): \n\t \t \treturn False \n\t \treturn True \n\tall_classes = [] \n\tpath = sklearn.__path__ \n\tfor (importer, modname, ispkg) in pkgutil.walk_packages(path=path, prefix='sklearn.', onerror=(lambda x: None)): \n\t \tif ('.tests.' in modname): \n\t \t \tcontinue \n\t \tmodule = __import__(modname, fromlist='dummy') \n\t \tclasses = inspect.getmembers(module, inspect.isclass) \n\t \tall_classes.extend(classes) \n\tall_classes = set(all_classes) \n\testimators = [c for c in all_classes if (issubclass(c[1], BaseEstimator) and (c[0] != 'BaseEstimator'))] \n\testimators = [c for c in estimators if (not is_abstract(c[1]))] \n\tif (not include_dont_test): \n\t \testimators = [c for c in estimators if (not (c[0] in DONT_TEST))] \n\tif (not include_other): \n\t \testimators = [c for c in estimators if (not (c[0] in OTHER))] \n\tif (not include_meta_estimators): \n\t \testimators = [c for c in estimators if (not (c[0] in META_ESTIMATORS))] \n\tif (type_filter is not None): \n\t \tif (not isinstance(type_filter, list)): \n\t \t \ttype_filter = [type_filter] \n\t \telse: \n\t \t \ttype_filter = list(type_filter) \n\t \tfiltered_estimators = [] \n\t \tfilters = {'classifier': ClassifierMixin, 'regressor': RegressorMixin, 'transformer': TransformerMixin, 'cluster': ClusterMixin} \n\t \tfor (name, mixin) in filters.items(): \n\t \t \tif (name in type_filter): \n\t \t \t \ttype_filter.remove(name) \n\t \t \t \tfiltered_estimators.extend([est for est in estimators if issubclass(est[1], mixin)]) \n\t \testimators = filtered_estimators \n\t \tif type_filter: \n\t \t \traise ValueError((\"Parameter \ttype_filter \tmust \tbe \t'classifier', \t'regressor', \t'transformer', \t'cluster' \tor \tNone, \tgot \t%s.\" % repr(type_filter))) \n\treturn sorted(set(estimators), key=itemgetter(0))\n", 
" \tif ((value is None) and empty_ok): \n\t \treturn \n\tif (not isinstance(value, (int, long))): \n\t \traise exception(('%s \tshould \tbe \tan \tinteger; \treceived \t%s \t(a \t%s).' % (name, value, typename(value)))) \n\tif ((not value) and (not zero_ok)): \n\t \traise exception(('%s \tmust \tnot \tbe \t0 \t(zero)' % name)) \n\tif ((value < 0) and (not negative_ok)): \n\t \traise exception(('%s \tmust \tnot \tbe \tnegative.' % name))\n", 
" \treturn ChecklistModule(mpstate)\n", 
" \tif (not isinstance(html, _strings)): \n\t \traise TypeError('string \trequired') \n\taccept_leading_text = bool(create_parent) \n\telements = fragments_fromstring(html, guess_charset=guess_charset, parser=parser, no_leading_text=(not accept_leading_text)) \n\tif create_parent: \n\t \tif (not isinstance(create_parent, _strings)): \n\t \t \tcreate_parent = 'div' \n\t \tnew_root = Element(create_parent) \n\t \tif elements: \n\t \t \tif isinstance(elements[0], _strings): \n\t \t \t \tnew_root.text = elements[0] \n\t \t \t \tdel elements[0] \n\t \t \tnew_root.extend(elements) \n\t \treturn new_root \n\tif (not elements): \n\t \traise etree.ParserError('No \telements \tfound') \n\tif (len(elements) > 1): \n\t \traise etree.ParserError('Multiple \telements \tfound') \n\tresult = elements[0] \n\tif (result.tail and result.tail.strip()): \n\t \traise etree.ParserError(('Element \tfollowed \tby \ttext: \t%r' % result.tail)) \n\tresult.tail = None \n\treturn result\n", 
" \traise ValueError\n", 
" \trng = np.random.RandomState(42) \n\tt = rng.randn(100) \n\tedges = bayesian_blocks(t, fitness=u'events') \n\tassert_allclose(edges, [(-2.6197451), (-0.71094865), 0.36866702, 1.85227818]) \n\tt[80:] = t[:20] \n\tedges = bayesian_blocks(t, fitness=u'events', p0=0.01) \n\tassert_allclose(edges, [(-2.6197451), (-0.47432431), (-0.46202823), 1.85227818]) \n\tdt = 0.01 \n\tt = (dt * np.arange(1000)) \n\tx = np.zeros(len(t)) \n\tN = (len(t) // 10) \n\tx[rng.randint(0, len(t), N)] = 1 \n\tx[rng.randint(0, (len(t) // 2), N)] = 1 \n\tedges = bayesian_blocks(t, x, fitness=u'regular_events', dt=dt) \n\tassert_allclose(edges, [0, 5.105, 9.99]) \n\tt = (100 * rng.rand(20)) \n\tx = np.exp(((-0.5) * ((t - 50) ** 2))) \n\tsigma = 0.1 \n\tx_obs = (x + (sigma * rng.randn(len(x)))) \n\tedges = bayesian_blocks(t, x_obs, sigma, fitness=u'measures') \n\tassert_allclose(edges, [4.360377, 48.456895, 52.597917, 99.455051])\n", 
" \traise ValueError\n", 
" \tprobs0 = np.asarray(probs0, float) \n\tprobs1 = np.asarray(probs1, float) \n\tprobs0 = (probs0 / probs0.sum(axis)) \n\tprobs1 = (probs1 / probs1.sum(axis)) \n\td2 = (((probs1 - probs0) ** 2) / probs0).sum(axis) \n\tif (correction is not None): \n\t \t(nobs, df) = correction \n\t \tdiff = ((probs1 - probs0) / probs0).sum(axis) \n\t \td2 = np.maximum(((((d2 * nobs) - diff) - df) / (nobs - 1.0)), 0) \n\tif cohen: \n\t \treturn np.sqrt(d2) \n\telse: \n\t \treturn d2\n", 
" \tx = np.asarray(x) \n\tmask = isnull(x) \n\tx = x[(~ mask)] \n\tvalues = np.sort(x) \n\tdef _get_score(at): \n\t \tif (len(values) == 0): \n\t \t \treturn np.nan \n\t \tidx = (at * (len(values) - 1)) \n\t \tif ((idx % 1) == 0): \n\t \t \tscore = values[int(idx)] \n\t \telif (interpolation_method == 'fraction'): \n\t \t \tscore = _interpolate(values[int(idx)], values[(int(idx) + 1)], (idx % 1)) \n\t \telif (interpolation_method == 'lower'): \n\t \t \tscore = values[np.floor(idx)] \n\t \telif (interpolation_method == 'higher'): \n\t \t \tscore = values[np.ceil(idx)] \n\t \telse: \n\t \t \traise ValueError(\"interpolation_method \tcan \tonly \tbe \t'fraction' \t, \t'lower' \tor \t'higher'\") \n\t \treturn score \n\tif is_scalar(q): \n\t \treturn _get_score(q) \n\telse: \n\t \tq = np.asarray(q, np.float64) \n\t \treturn algos.arrmap_float64(q, _get_score)\n", 
" \t_msg = 'is \tNone' \n\tif (obj is None): \n\t \tif (msg is None): \n\t \t \tmsg = _msg \n\t \telif (values is True): \n\t \t \tmsg = ('%s: \t%s' % (msg, _msg)) \n\t \t_report_failure(msg)\n", 
" \tclf = GaussianNB(priors=np.array([0.25, 0.25, 0.25, 0.25])) \n\tassert_raises(ValueError, clf.fit, X, y)\n", 
" \traise ValueError\n", 
" \tdef is_abstract(c): \n\t \tif (not hasattr(c, '__abstractmethods__')): \n\t \t \treturn False \n\t \tif (not len(c.__abstractmethods__)): \n\t \t \treturn False \n\t \treturn True \n\tall_classes = [] \n\tpath = sklearn.__path__ \n\tfor (importer, modname, ispkg) in pkgutil.walk_packages(path=path, prefix='sklearn.', onerror=(lambda x: None)): \n\t \tif ('.tests.' in modname): \n\t \t \tcontinue \n\t \tmodule = __import__(modname, fromlist='dummy') \n\t \tclasses = inspect.getmembers(module, inspect.isclass) \n\t \tall_classes.extend(classes) \n\tall_classes = set(all_classes) \n\testimators = [c for c in all_classes if (issubclass(c[1], BaseEstimator) and (c[0] != 'BaseEstimator'))] \n\testimators = [c for c in estimators if (not is_abstract(c[1]))] \n\tif (not include_dont_test): \n\t \testimators = [c for c in estimators if (not (c[0] in DONT_TEST))] \n\tif (not include_other): \n\t \testimators = [c for c in estimators if (not (c[0] in OTHER))] \n\tif (not include_meta_estimators): \n\t \testimators = [c for c in estimators if (not (c[0] in META_ESTIMATORS))] \n\tif (type_filter is not None): \n\t \tif (not isinstance(type_filter, list)): \n\t \t \ttype_filter = [type_filter] \n\t \telse: \n\t \t \ttype_filter = list(type_filter) \n\t \tfiltered_estimators = [] \n\t \tfilters = {'classifier': ClassifierMixin, 'regressor': RegressorMixin, 'transformer': TransformerMixin, 'cluster': ClusterMixin} \n\t \tfor (name, mixin) in filters.items(): \n\t \t \tif (name in type_filter): \n\t \t \t \ttype_filter.remove(name) \n\t \t \t \tfiltered_estimators.extend([est for est in estimators if issubclass(est[1], mixin)]) \n\t \testimators = filtered_estimators \n\t \tif type_filter: \n\t \t \traise ValueError((\"Parameter \ttype_filter \tmust \tbe \t'classifier', \t'regressor', \t'transformer', \t'cluster' \tor \tNone, \tgot \t%s.\" % repr(type_filter))) \n\treturn sorted(set(estimators), key=itemgetter(0))\n", 
" \treturn ChecklistModule(mpstate)\n", 
" \tif (not isinstance(html, _strings)): \n\t \traise TypeError('string \trequired') \n\taccept_leading_text = bool(create_parent) \n\telements = fragments_fromstring(html, guess_charset=guess_charset, parser=parser, no_leading_text=(not accept_leading_text)) \n\tif create_parent: \n\t \tif (not isinstance(create_parent, _strings)): \n\t \t \tcreate_parent = 'div' \n\t \tnew_root = Element(create_parent) \n\t \tif elements: \n\t \t \tif isinstance(elements[0], _strings): \n\t \t \t \tnew_root.text = elements[0] \n\t \t \t \tdel elements[0] \n\t \t \tnew_root.extend(elements) \n\t \treturn new_root \n\tif (not elements): \n\t \traise etree.ParserError('No \telements \tfound') \n\tif (len(elements) > 1): \n\t \traise etree.ParserError('Multiple \telements \tfound') \n\tresult = elements[0] \n\tif (result.tail and result.tail.strip()): \n\t \traise etree.ParserError(('Element \tfollowed \tby \ttext: \t%r' % result.tail)) \n\tresult.tail = None \n\treturn result\n", 
" \trng = np.random.RandomState(42) \n\tt = rng.randn(100) \n\tedges = bayesian_blocks(t, fitness=u'events') \n\tassert_allclose(edges, [(-2.6197451), (-0.71094865), 0.36866702, 1.85227818]) \n\tt[80:] = t[:20] \n\tedges = bayesian_blocks(t, fitness=u'events', p0=0.01) \n\tassert_allclose(edges, [(-2.6197451), (-0.47432431), (-0.46202823), 1.85227818]) \n\tdt = 0.01 \n\tt = (dt * np.arange(1000)) \n\tx = np.zeros(len(t)) \n\tN = (len(t) // 10) \n\tx[rng.randint(0, len(t), N)] = 1 \n\tx[rng.randint(0, (len(t) // 2), N)] = 1 \n\tedges = bayesian_blocks(t, x, fitness=u'regular_events', dt=dt) \n\tassert_allclose(edges, [0, 5.105, 9.99]) \n\tt = (100 * rng.rand(20)) \n\tx = np.exp(((-0.5) * ((t - 50) ** 2))) \n\tsigma = 0.1 \n\tx_obs = (x + (sigma * rng.randn(len(x)))) \n\tedges = bayesian_blocks(t, x_obs, sigma, fitness=u'measures') \n\tassert_allclose(edges, [4.360377, 48.456895, 52.597917, 99.455051])\n", 
" \traise ValueError\n", 
" \tamp = 1e-08 \n\ttempdir = _TempDir() \n\trng = np.random.RandomState(0) \n\tfname_dtemp = op.join(tempdir, 'test.dip') \n\tfname_sim = op.join(tempdir, 'test-ave.fif') \n\tfwd = convert_forward_solution(read_forward_solution(fname_fwd), surf_ori=False, force_fixed=True) \n\tevoked = read_evokeds(fname_evo)[0] \n\tcov = read_cov(fname_cov) \n\tn_per_hemi = 5 \n\tvertices = [np.sort(rng.permutation(s['vertno'])[:n_per_hemi]) for s in fwd['src']] \n\tnv = sum((len(v) for v in vertices)) \n\tstc = SourceEstimate((amp * np.eye(nv)), vertices, 0, 0.001) \n\tevoked = simulate_evoked(fwd, stc, evoked.info, cov, snr=20, random_state=rng) \n\tpicks = np.sort(np.concatenate([pick_types(evoked.info, meg=True, eeg=False)[::2], pick_types(evoked.info, meg=False, eeg=True)[::2]])) \n\tevoked.pick_channels([evoked.ch_names[p] for p in picks]) \n\tevoked.add_proj(make_eeg_average_ref_proj(evoked.info)) \n\twrite_evokeds(fname_sim, evoked) \n\trun_subprocess(['mne_dipole_fit', '--meas', fname_sim, '--meg', '--eeg', '--noise', fname_cov, '--dip', fname_dtemp, '--mri', fname_fwd, '--reg', '0', '--tmin', '0']) \n\tdip_c = read_dipole(fname_dtemp) \n\tsphere = make_sphere_model(head_radius=0.1) \n\t(dip, residuals) = fit_dipole(evoked, fname_cov, sphere, fname_fwd) \n\tdata_rms = np.sqrt(np.sum((evoked.data ** 2), axis=0)) \n\tresi_rms = np.sqrt(np.sum((residuals ** 2), axis=0)) \n\tfactor = 1.0 \n\tif ((os.getenv('TRAVIS', 'false') == 'true') and (sys.version[:3] in ('3.5', '2.7'))): \n\t \tfactor = 0.8 \n\tassert_true((data_rms > (factor * resi_rms)).all(), msg=('%s \t(factor: \t%s)' % ((data_rms / resi_rms).min(), factor))) \n\ttransform_surface_to(fwd['src'][0], 'head', fwd['mri_head_t']) \n\ttransform_surface_to(fwd['src'][1], 'head', fwd['mri_head_t']) \n\tsrc_rr = np.concatenate([s['rr'][v] for (s, v) in zip(fwd['src'], vertices)], axis=0) \n\tsrc_nn = np.concatenate([s['nn'][v] for (s, v) in zip(fwd['src'], vertices)], axis=0) \n\tdip.crop(dip_c.times[0], dip_c.times[(-1)]) \n\t(src_rr, src_nn) = (src_rr[:(-1)], src_nn[:(-1)]) \n\t(corrs, dists, gc_dists, amp_errs, gofs) = ([], [], [], [], []) \n\tfor d in (dip_c, dip): \n\t \tnew = d.pos \n\t \tdiffs = (new - src_rr) \n\t \tcorrs += [np.corrcoef(src_rr.ravel(), new.ravel())[(0, 1)]] \n\t \tdists += [np.sqrt(np.mean(np.sum((diffs * diffs), axis=1)))] \n\t \tgc_dists += [((180 / np.pi) * np.mean(np.arccos(np.sum((src_nn * d.ori), axis=1))))] \n\t \tamp_errs += [np.sqrt(np.mean(((amp - d.amplitude) ** 2)))] \n\t \tgofs += [np.mean(d.gof)] \n\tassert_true((dists[0] >= (dists[1] * factor)), ('dists: \t%s' % dists)) \n\tassert_true((corrs[0] <= (corrs[1] / factor)), ('corrs: \t%s' % corrs)) \n\tassert_true((gc_dists[0] >= (gc_dists[1] * factor)), ('gc-dists \t(ori): \t%s' % gc_dists)) \n\tassert_true((amp_errs[0] >= (amp_errs[1] * factor)), ('amplitude \terrors: \t%s' % amp_errs)) \n\tassert_true((gofs[0] <= (gofs[1] / factor)), ('gof: \t%s' % gofs))\n", 
" \tif (scope is None): \n\t \tscope = ('https://www.googleapis.com/auth/' + name) \n\tparent_parsers = [tools.argparser] \n\tparent_parsers.extend(parents) \n\tparser = argparse.ArgumentParser(description=doc, formatter_class=argparse.RawDescriptionHelpFormatter, parents=parent_parsers) \n\tflags = parser.parse_args(argv[1:]) \n\tclient_secrets = os.path.join(os.path.dirname(filename), 'client_secrets.json') \n\tflow = client.flow_from_clientsecrets(client_secrets, scope=scope, message=tools.message_if_missing(client_secrets)) \n\tstorage = file.Storage((name + '.dat')) \n\tcredentials = storage.get() \n\tif ((credentials is None) or credentials.invalid): \n\t \tcredentials = tools.run_flow(flow, storage, flags) \n\thttp = credentials.authorize(http=build_http()) \n\tif (discovery_filename is None): \n\t \tservice = discovery.build(name, version, http=http) \n\telse: \n\t \twith open(discovery_filename) as discovery_file: \n\t \t \tservice = discovery.build_from_document(discovery_file.read(), base='https://www.googleapis.com/', http=http) \n\treturn (service, flags)\n", 
" \tclf = GaussianNB(priors=np.array([0.25, 0.25, 0.25, 0.25])) \n\tassert_raises(ValueError, clf.fit, X, y)\n", 
" \traise ValueError\n", 
" \t[Xt, Yt] = listOfFeatures2Matrix(features) \n\tknn = kNN(Xt, Yt, K) \n\treturn knn\n", 
" \traise ValueError\n", 
" \tdef is_abstract(c): \n\t \tif (not hasattr(c, '__abstractmethods__')): \n\t \t \treturn False \n\t \tif (not len(c.__abstractmethods__)): \n\t \t \treturn False \n\t \treturn True \n\tall_classes = [] \n\tpath = sklearn.__path__ \n\tfor (importer, modname, ispkg) in pkgutil.walk_packages(path=path, prefix='sklearn.', onerror=(lambda x: None)): \n\t \tif ('.tests.' in modname): \n\t \t \tcontinue \n\t \tmodule = __import__(modname, fromlist='dummy') \n\t \tclasses = inspect.getmembers(module, inspect.isclass) \n\t \tall_classes.extend(classes) \n\tall_classes = set(all_classes) \n\testimators = [c for c in all_classes if (issubclass(c[1], BaseEstimator) and (c[0] != 'BaseEstimator'))] \n\testimators = [c for c in estimators if (not is_abstract(c[1]))] \n\tif (not include_dont_test): \n\t \testimators = [c for c in estimators if (not (c[0] in DONT_TEST))] \n\tif (not include_other): \n\t \testimators = [c for c in estimators if (not (c[0] in OTHER))] \n\tif (not include_meta_estimators): \n\t \testimators = [c for c in estimators if (not (c[0] in META_ESTIMATORS))] \n\tif (type_filter is not None): \n\t \tif (not isinstance(type_filter, list)): \n\t \t \ttype_filter = [type_filter] \n\t \telse: \n\t \t \ttype_filter = list(type_filter) \n\t \tfiltered_estimators = [] \n\t \tfilters = {'classifier': ClassifierMixin, 'regressor': RegressorMixin, 'transformer': TransformerMixin, 'cluster': ClusterMixin} \n\t \tfor (name, mixin) in filters.items(): \n\t \t \tif (name in type_filter): \n\t \t \t \ttype_filter.remove(name) \n\t \t \t \tfiltered_estimators.extend([est for est in estimators if issubclass(est[1], mixin)]) \n\t \testimators = filtered_estimators \n\t \tif type_filter: \n\t \t \traise ValueError((\"Parameter \ttype_filter \tmust \tbe \t'classifier', \t'regressor', \t'transformer', \t'cluster' \tor \tNone, \tgot \t%s.\" % repr(type_filter))) \n\treturn sorted(set(estimators), key=itemgetter(0))\n", 
" \tif ((value is None) and empty_ok): \n\t \treturn \n\tif (not isinstance(value, (int, long))): \n\t \traise exception(('%s \tshould \tbe \tan \tinteger; \treceived \t%s \t(a \t%s).' % (name, value, typename(value)))) \n\tif ((not value) and (not zero_ok)): \n\t \traise exception(('%s \tmust \tnot \tbe \t0 \t(zero)' % name)) \n\tif ((value < 0) and (not negative_ok)): \n\t \traise exception(('%s \tmust \tnot \tbe \tnegative.' % name))\n", 
" \tsupported = ('\\x00', '\\x00', '\\x00', '\\x01') \n\ti = 0 \n\tfor item in version: \n\t \tif (version[i] != supported[i]): \n\t \t \traise RuntimeError('SFF \tversion \tnot \tsupported. \tPlease \tcontact \tthe \tauthor \tof \tthe \tsoftware.') \n\t \ti += 1\n", 
" \treturn ChecklistModule(mpstate)\n", 
" \tif (not isinstance(html, _strings)): \n\t \traise TypeError('string \trequired') \n\taccept_leading_text = bool(create_parent) \n\telements = fragments_fromstring(html, guess_charset=guess_charset, parser=parser, no_leading_text=(not accept_leading_text)) \n\tif create_parent: \n\t \tif (not isinstance(create_parent, _strings)): \n\t \t \tcreate_parent = 'div' \n\t \tnew_root = Element(create_parent) \n\t \tif elements: \n\t \t \tif isinstance(elements[0], _strings): \n\t \t \t \tnew_root.text = elements[0] \n\t \t \t \tdel elements[0] \n\t \t \tnew_root.extend(elements) \n\t \treturn new_root \n\tif (not elements): \n\t \traise etree.ParserError('No \telements \tfound') \n\tif (len(elements) > 1): \n\t \traise etree.ParserError('Multiple \telements \tfound') \n\tresult = elements[0] \n\tif (result.tail and result.tail.strip()): \n\t \traise etree.ParserError(('Element \tfollowed \tby \ttext: \t%r' % result.tail)) \n\tresult.tail = None \n\treturn result\n", 
" \traise ValueError\n", 
" \trng = np.random.RandomState(42) \n\tt = rng.randn(100) \n\tedges = bayesian_blocks(t, fitness=u'events') \n\tassert_allclose(edges, [(-2.6197451), (-0.71094865), 0.36866702, 1.85227818]) \n\tt[80:] = t[:20] \n\tedges = bayesian_blocks(t, fitness=u'events', p0=0.01) \n\tassert_allclose(edges, [(-2.6197451), (-0.47432431), (-0.46202823), 1.85227818]) \n\tdt = 0.01 \n\tt = (dt * np.arange(1000)) \n\tx = np.zeros(len(t)) \n\tN = (len(t) // 10) \n\tx[rng.randint(0, len(t), N)] = 1 \n\tx[rng.randint(0, (len(t) // 2), N)] = 1 \n\tedges = bayesian_blocks(t, x, fitness=u'regular_events', dt=dt) \n\tassert_allclose(edges, [0, 5.105, 9.99]) \n\tt = (100 * rng.rand(20)) \n\tx = np.exp(((-0.5) * ((t - 50) ** 2))) \n\tsigma = 0.1 \n\tx_obs = (x + (sigma * rng.randn(len(x)))) \n\tedges = bayesian_blocks(t, x_obs, sigma, fitness=u'measures') \n\tassert_allclose(edges, [4.360377, 48.456895, 52.597917, 99.455051])\n", 
" \traise ValueError\n", 
" \timport vispy.color.colormap as c \n\tassert_raises(AssertionError, c._glsl_step, [0.0, 1.0]) \n\tc._glsl_mix(controls=[0.0, 1.0]) \n\tc._glsl_mix(controls=[0.0, 0.25, 1.0]) \n\tfor fun in (c._glsl_step, c._glsl_mix): \n\t \tassert_raises(AssertionError, fun, controls=[0.1, 1.0]) \n\t \tassert_raises(AssertionError, fun, controls=[0.0, 0.9]) \n\t \tassert_raises(AssertionError, fun, controls=[0.1, 0.9]) \n\tcolor_0 = np.array([1.0, 0.0, 0.0]) \n\tcolor_1 = np.array([0.0, 1.0, 0.0]) \n\tcolor_2 = np.array([0.0, 0.0, 1.0]) \n\tcolors_00 = np.vstack((color_0, color_0)) \n\tcolors_01 = np.vstack((color_0, color_1)) \n\tcolors_11 = np.vstack((color_1, color_1)) \n\tcolors_021 = np.vstack((color_0, color_2, color_1)) \n\tcontrols_2 = np.array([0.0, 1.0]) \n\tcontrols_3 = np.array([0.0, 0.25, 1.0]) \n\tx = np.array([(-1.0), 0.0, 0.1, 0.4, 0.5, 0.6, 1.0, 2.0])[:, None] \n\tmixed_2 = c.mix(colors_01, x, controls_2) \n\tmixed_3 = c.mix(colors_021, x, controls_3) \n\tfor y in (mixed_2, mixed_3): \n\t \tassert_allclose(y[:2, :], colors_00) \n\t \tassert_allclose(y[(-2):, :], colors_11) \n\tassert_allclose(mixed_2[:, (-1)], np.zeros(len(y)))\n", 
" \tprobs0 = np.asarray(probs0, float) \n\tprobs1 = np.asarray(probs1, float) \n\tprobs0 = (probs0 / probs0.sum(axis)) \n\tprobs1 = (probs1 / probs1.sum(axis)) \n\td2 = (((probs1 - probs0) ** 2) / probs0).sum(axis) \n\tif (correction is not None): \n\t \t(nobs, df) = correction \n\t \tdiff = ((probs1 - probs0) / probs0).sum(axis) \n\t \td2 = np.maximum(((((d2 * nobs) - diff) - df) / (nobs - 1.0)), 0) \n\tif cohen: \n\t \treturn np.sqrt(d2) \n\telse: \n\t \treturn d2\n", 
" \treturn Gaussian()(mean, ln_var)\n", 
" \tclf = GaussianNB(priors=np.array([0.25, 0.25, 0.25, 0.25])) \n\tassert_raises(ValueError, clf.fit, X, y)\n", 
" \traise ValueError\n", 
" \treturn (k, features.copy(), labels.copy())\n", 
" \traise ValueError\n", 
" \tdef is_abstract(c): \n\t \tif (not hasattr(c, '__abstractmethods__')): \n\t \t \treturn False \n\t \tif (not len(c.__abstractmethods__)): \n\t \t \treturn False \n\t \treturn True \n\tall_classes = [] \n\tpath = sklearn.__path__ \n\tfor (importer, modname, ispkg) in pkgutil.walk_packages(path=path, prefix='sklearn.', onerror=(lambda x: None)): \n\t \tif ('.tests.' in modname): \n\t \t \tcontinue \n\t \tmodule = __import__(modname, fromlist='dummy') \n\t \tclasses = inspect.getmembers(module, inspect.isclass) \n\t \tall_classes.extend(classes) \n\tall_classes = set(all_classes) \n\testimators = [c for c in all_classes if (issubclass(c[1], BaseEstimator) and (c[0] != 'BaseEstimator'))] \n\testimators = [c for c in estimators if (not is_abstract(c[1]))] \n\tif (not include_dont_test): \n\t \testimators = [c for c in estimators if (not (c[0] in DONT_TEST))] \n\tif (not include_other): \n\t \testimators = [c for c in estimators if (not (c[0] in OTHER))] \n\tif (not include_meta_estimators): \n\t \testimators = [c for c in estimators if (not (c[0] in META_ESTIMATORS))] \n\tif (type_filter is not None): \n\t \tif (not isinstance(type_filter, list)): \n\t \t \ttype_filter = [type_filter] \n\t \telse: \n\t \t \ttype_filter = list(type_filter) \n\t \tfiltered_estimators = [] \n\t \tfilters = {'classifier': ClassifierMixin, 'regressor': RegressorMixin, 'transformer': TransformerMixin, 'cluster': ClusterMixin} \n\t \tfor (name, mixin) in filters.items(): \n\t \t \tif (name in type_filter): \n\t \t \t \ttype_filter.remove(name) \n\t \t \t \tfiltered_estimators.extend([est for est in estimators if issubclass(est[1], mixin)]) \n\t \testimators = filtered_estimators \n\t \tif type_filter: \n\t \t \traise ValueError((\"Parameter \ttype_filter \tmust \tbe \t'classifier', \t'regressor', \t'transformer', \t'cluster' \tor \tNone, \tgot \t%s.\" % repr(type_filter))) \n\treturn sorted(set(estimators), key=itemgetter(0))\n", 
" \tif ((value is None) and empty_ok): \n\t \treturn \n\tif (not isinstance(value, (int, long))): \n\t \traise exception(('%s \tshould \tbe \tan \tinteger; \treceived \t%s \t(a \t%s).' % (name, value, typename(value)))) \n\tif ((not value) and (not zero_ok)): \n\t \traise exception(('%s \tmust \tnot \tbe \t0 \t(zero)' % name)) \n\tif ((value < 0) and (not negative_ok)): \n\t \traise exception(('%s \tmust \tnot \tbe \tnegative.' % name))\n", 
" \treturn ChecklistModule(mpstate)\n", 
" \tif (not isinstance(html, _strings)): \n\t \traise TypeError('string \trequired') \n\taccept_leading_text = bool(create_parent) \n\telements = fragments_fromstring(html, guess_charset=guess_charset, parser=parser, no_leading_text=(not accept_leading_text)) \n\tif create_parent: \n\t \tif (not isinstance(create_parent, _strings)): \n\t \t \tcreate_parent = 'div' \n\t \tnew_root = Element(create_parent) \n\t \tif elements: \n\t \t \tif isinstance(elements[0], _strings): \n\t \t \t \tnew_root.text = elements[0] \n\t \t \t \tdel elements[0] \n\t \t \tnew_root.extend(elements) \n\t \treturn new_root \n\tif (not elements): \n\t \traise etree.ParserError('No \telements \tfound') \n\tif (len(elements) > 1): \n\t \traise etree.ParserError('Multiple \telements \tfound') \n\tresult = elements[0] \n\tif (result.tail and result.tail.strip()): \n\t \traise etree.ParserError(('Element \tfollowed \tby \ttext: \t%r' % result.tail)) \n\tresult.tail = None \n\treturn result\n", 
" \traise ValueError\n", 
" \trng = np.random.RandomState(42) \n\tt = rng.randn(100) \n\tedges = bayesian_blocks(t, fitness=u'events') \n\tassert_allclose(edges, [(-2.6197451), (-0.71094865), 0.36866702, 1.85227818]) \n\tt[80:] = t[:20] \n\tedges = bayesian_blocks(t, fitness=u'events', p0=0.01) \n\tassert_allclose(edges, [(-2.6197451), (-0.47432431), (-0.46202823), 1.85227818]) \n\tdt = 0.01 \n\tt = (dt * np.arange(1000)) \n\tx = np.zeros(len(t)) \n\tN = (len(t) // 10) \n\tx[rng.randint(0, len(t), N)] = 1 \n\tx[rng.randint(0, (len(t) // 2), N)] = 1 \n\tedges = bayesian_blocks(t, x, fitness=u'regular_events', dt=dt) \n\tassert_allclose(edges, [0, 5.105, 9.99]) \n\tt = (100 * rng.rand(20)) \n\tx = np.exp(((-0.5) * ((t - 50) ** 2))) \n\tsigma = 0.1 \n\tx_obs = (x + (sigma * rng.randn(len(x)))) \n\tedges = bayesian_blocks(t, x_obs, sigma, fitness=u'measures') \n\tassert_allclose(edges, [4.360377, 48.456895, 52.597917, 99.455051])\n", 
" \tclf = GaussianNB(priors=np.array([0.25, 0.25, 0.25, 0.25])) \n\tassert_raises(ValueError, clf.fit, X, y)\n", 
" \traise ValueError\n", 
" \timport vispy.color.colormap as c \n\tassert_raises(AssertionError, c._glsl_step, [0.0, 1.0]) \n\tc._glsl_mix(controls=[0.0, 1.0]) \n\tc._glsl_mix(controls=[0.0, 0.25, 1.0]) \n\tfor fun in (c._glsl_step, c._glsl_mix): \n\t \tassert_raises(AssertionError, fun, controls=[0.1, 1.0]) \n\t \tassert_raises(AssertionError, fun, controls=[0.0, 0.9]) \n\t \tassert_raises(AssertionError, fun, controls=[0.1, 0.9]) \n\tcolor_0 = np.array([1.0, 0.0, 0.0]) \n\tcolor_1 = np.array([0.0, 1.0, 0.0]) \n\tcolor_2 = np.array([0.0, 0.0, 1.0]) \n\tcolors_00 = np.vstack((color_0, color_0)) \n\tcolors_01 = np.vstack((color_0, color_1)) \n\tcolors_11 = np.vstack((color_1, color_1)) \n\tcolors_021 = np.vstack((color_0, color_2, color_1)) \n\tcontrols_2 = np.array([0.0, 1.0]) \n\tcontrols_3 = np.array([0.0, 0.25, 1.0]) \n\tx = np.array([(-1.0), 0.0, 0.1, 0.4, 0.5, 0.6, 1.0, 2.0])[:, None] \n\tmixed_2 = c.mix(colors_01, x, controls_2) \n\tmixed_3 = c.mix(colors_021, x, controls_3) \n\tfor y in (mixed_2, mixed_3): \n\t \tassert_allclose(y[:2, :], colors_00) \n\t \tassert_allclose(y[(-2):, :], colors_11) \n\tassert_allclose(mixed_2[:, (-1)], np.zeros(len(y)))\n", 
" \tif (x < 0.27): \n\t \treturn 1.0 \n\tif (x > 3.2): \n\t \treturn 0.0 \n\tx = (((-2.0) * x) * x) \n\tk = 0 \n\tfor i in reversed(xrange(1, (27 + 1), 2)): \n\t \tk = ((1 - k) * exp((x * i))) \n\treturn (2.0 * k)\n", 
" \tclf = GaussianNB(priors=np.array([0.25, 0.25, 0.25, 0.25])) \n\tassert_raises(ValueError, clf.fit, X, y)\n", 
" \traise ValueError\n", 
" \tclasses = [] \n\tn_classes = [] \n\tclass_prior = [] \n\t(n_samples, n_outputs) = y.shape \n\tif issparse(y): \n\t \ty = y.tocsc() \n\t \ty_nnz = np.diff(y.indptr) \n\t \tfor k in range(n_outputs): \n\t \t \tcol_nonzero = y.indices[y.indptr[k]:y.indptr[(k + 1)]] \n\t \t \tif (sample_weight is not None): \n\t \t \t \tnz_samp_weight = np.asarray(sample_weight)[col_nonzero] \n\t \t \t \tzeros_samp_weight_sum = (np.sum(sample_weight) - np.sum(nz_samp_weight)) \n\t \t \telse: \n\t \t \t \tnz_samp_weight = None \n\t \t \t \tzeros_samp_weight_sum = (y.shape[0] - y_nnz[k]) \n\t \t \t(classes_k, y_k) = np.unique(y.data[y.indptr[k]:y.indptr[(k + 1)]], return_inverse=True) \n\t \t \tclass_prior_k = bincount(y_k, weights=nz_samp_weight) \n\t \t \tif (0 in classes_k): \n\t \t \t \tclass_prior_k[(classes_k == 0)] += zeros_samp_weight_sum \n\t \t \tif ((0 not in classes_k) and (y_nnz[k] < y.shape[0])): \n\t \t \t \tclasses_k = np.insert(classes_k, 0, 0) \n\t \t \t \tclass_prior_k = np.insert(class_prior_k, 0, zeros_samp_weight_sum) \n\t \t \tclasses.append(classes_k) \n\t \t \tn_classes.append(classes_k.shape[0]) \n\t \t \tclass_prior.append((class_prior_k / class_prior_k.sum())) \n\telse: \n\t \tfor k in range(n_outputs): \n\t \t \t(classes_k, y_k) = np.unique(y[:, k], return_inverse=True) \n\t \t \tclasses.append(classes_k) \n\t \t \tn_classes.append(classes_k.shape[0]) \n\t \t \tclass_prior_k = bincount(y_k, weights=sample_weight) \n\t \t \tclass_prior.append((class_prior_k / class_prior_k.sum())) \n\treturn (classes, n_classes, class_prior)\n", 
" \tX = np.random.random(size=(100, 10)) \n\tY = np.random.randint(5, size=(100, 1)) \n\tdataset = DenseDesignMatrix(X, y=Y) \n\tmodel = KMeans(k=5, nvis=10) \n\ttrain = Train(model=model, dataset=dataset) \n\ttrain.main_loop()\n", 
" \tX = np.random.random(size=(100, 10)) \n\tY = np.random.randint(5, size=(100, 1)) \n\tdataset = DenseDesignMatrix(X, y=Y) \n\tmodel = KMeans(k=5, nvis=10) \n\ttrain = Train(model=model, dataset=dataset) \n\ttrain.main_loop()\n", 
" \tdef is_abstract(c): \n\t \tif (not hasattr(c, '__abstractmethods__')): \n\t \t \treturn False \n\t \tif (not len(c.__abstractmethods__)): \n\t \t \treturn False \n\t \treturn True \n\tall_classes = [] \n\tpath = sklearn.__path__ \n\tfor (importer, modname, ispkg) in pkgutil.walk_packages(path=path, prefix='sklearn.', onerror=(lambda x: None)): \n\t \tif ('.tests.' in modname): \n\t \t \tcontinue \n\t \tmodule = __import__(modname, fromlist='dummy') \n\t \tclasses = inspect.getmembers(module, inspect.isclass) \n\t \tall_classes.extend(classes) \n\tall_classes = set(all_classes) \n\testimators = [c for c in all_classes if (issubclass(c[1], BaseEstimator) and (c[0] != 'BaseEstimator'))] \n\testimators = [c for c in estimators if (not is_abstract(c[1]))] \n\tif (not include_dont_test): \n\t \testimators = [c for c in estimators if (not (c[0] in DONT_TEST))] \n\tif (not include_other): \n\t \testimators = [c for c in estimators if (not (c[0] in OTHER))] \n\tif (not include_meta_estimators): \n\t \testimators = [c for c in estimators if (not (c[0] in META_ESTIMATORS))] \n\tif (type_filter is not None): \n\t \tif (not isinstance(type_filter, list)): \n\t \t \ttype_filter = [type_filter] \n\t \telse: \n\t \t \ttype_filter = list(type_filter) \n\t \tfiltered_estimators = [] \n\t \tfilters = {'classifier': ClassifierMixin, 'regressor': RegressorMixin, 'transformer': TransformerMixin, 'cluster': ClusterMixin} \n\t \tfor (name, mixin) in filters.items(): \n\t \t \tif (name in type_filter): \n\t \t \t \ttype_filter.remove(name) \n\t \t \t \tfiltered_estimators.extend([est for est in estimators if issubclass(est[1], mixin)]) \n\t \testimators = filtered_estimators \n\t \tif type_filter: \n\t \t \traise ValueError((\"Parameter \ttype_filter \tmust \tbe \t'classifier', \t'regressor', \t'transformer', \t'cluster' \tor \tNone, \tgot \t%s.\" % repr(type_filter))) \n\treturn sorted(set(estimators), key=itemgetter(0))\n", 
" \treturn ChecklistModule(mpstate)\n", 
" \tdef g(target, *args, **kwargs): \n\t \tval = func(target, *args, **kwargs) \n\t \tif val: \n\t \t \treturn val \n\t \telse: \n\t \t \traise exception_class((value_error_message_template % target)) \n\tg.__name__ = func.__name__ \n\tg.__doc__ = func.__doc__ \n\treturn g\n", 
" \tif (not isinstance(html, _strings)): \n\t \traise TypeError('string \trequired') \n\taccept_leading_text = bool(create_parent) \n\telements = fragments_fromstring(html, guess_charset=guess_charset, parser=parser, no_leading_text=(not accept_leading_text)) \n\tif create_parent: \n\t \tif (not isinstance(create_parent, _strings)): \n\t \t \tcreate_parent = 'div' \n\t \tnew_root = Element(create_parent) \n\t \tif elements: \n\t \t \tif isinstance(elements[0], _strings): \n\t \t \t \tnew_root.text = elements[0] \n\t \t \t \tdel elements[0] \n\t \t \tnew_root.extend(elements) \n\t \treturn new_root \n\tif (not elements): \n\t \traise etree.ParserError('No \telements \tfound') \n\tif (len(elements) > 1): \n\t \traise etree.ParserError('Multiple \telements \tfound') \n\tresult = elements[0] \n\tif (result.tail and result.tail.strip()): \n\t \traise etree.ParserError(('Element \tfollowed \tby \ttext: \t%r' % result.tail)) \n\tresult.tail = None \n\treturn result\n", 
" \trng = np.random.RandomState(42) \n\tt = rng.randn(100) \n\tedges = bayesian_blocks(t, fitness=u'events') \n\tassert_allclose(edges, [(-2.6197451), (-0.71094865), 0.36866702, 1.85227818]) \n\tt[80:] = t[:20] \n\tedges = bayesian_blocks(t, fitness=u'events', p0=0.01) \n\tassert_allclose(edges, [(-2.6197451), (-0.47432431), (-0.46202823), 1.85227818]) \n\tdt = 0.01 \n\tt = (dt * np.arange(1000)) \n\tx = np.zeros(len(t)) \n\tN = (len(t) // 10) \n\tx[rng.randint(0, len(t), N)] = 1 \n\tx[rng.randint(0, (len(t) // 2), N)] = 1 \n\tedges = bayesian_blocks(t, x, fitness=u'regular_events', dt=dt) \n\tassert_allclose(edges, [0, 5.105, 9.99]) \n\tt = (100 * rng.rand(20)) \n\tx = np.exp(((-0.5) * ((t - 50) ** 2))) \n\tsigma = 0.1 \n\tx_obs = (x + (sigma * rng.randn(len(x)))) \n\tedges = bayesian_blocks(t, x_obs, sigma, fitness=u'measures') \n\tassert_allclose(edges, [4.360377, 48.456895, 52.597917, 99.455051])\n", 
" \traise ValueError\n", 
" \tamp = 1e-08 \n\ttempdir = _TempDir() \n\trng = np.random.RandomState(0) \n\tfname_dtemp = op.join(tempdir, 'test.dip') \n\tfname_sim = op.join(tempdir, 'test-ave.fif') \n\tfwd = convert_forward_solution(read_forward_solution(fname_fwd), surf_ori=False, force_fixed=True) \n\tevoked = read_evokeds(fname_evo)[0] \n\tcov = read_cov(fname_cov) \n\tn_per_hemi = 5 \n\tvertices = [np.sort(rng.permutation(s['vertno'])[:n_per_hemi]) for s in fwd['src']] \n\tnv = sum((len(v) for v in vertices)) \n\tstc = SourceEstimate((amp * np.eye(nv)), vertices, 0, 0.001) \n\tevoked = simulate_evoked(fwd, stc, evoked.info, cov, snr=20, random_state=rng) \n\tpicks = np.sort(np.concatenate([pick_types(evoked.info, meg=True, eeg=False)[::2], pick_types(evoked.info, meg=False, eeg=True)[::2]])) \n\tevoked.pick_channels([evoked.ch_names[p] for p in picks]) \n\tevoked.add_proj(make_eeg_average_ref_proj(evoked.info)) \n\twrite_evokeds(fname_sim, evoked) \n\trun_subprocess(['mne_dipole_fit', '--meas', fname_sim, '--meg', '--eeg', '--noise', fname_cov, '--dip', fname_dtemp, '--mri', fname_fwd, '--reg', '0', '--tmin', '0']) \n\tdip_c = read_dipole(fname_dtemp) \n\tsphere = make_sphere_model(head_radius=0.1) \n\t(dip, residuals) = fit_dipole(evoked, fname_cov, sphere, fname_fwd) \n\tdata_rms = np.sqrt(np.sum((evoked.data ** 2), axis=0)) \n\tresi_rms = np.sqrt(np.sum((residuals ** 2), axis=0)) \n\tfactor = 1.0 \n\tif ((os.getenv('TRAVIS', 'false') == 'true') and (sys.version[:3] in ('3.5', '2.7'))): \n\t \tfactor = 0.8 \n\tassert_true((data_rms > (factor * resi_rms)).all(), msg=('%s \t(factor: \t%s)' % ((data_rms / resi_rms).min(), factor))) \n\ttransform_surface_to(fwd['src'][0], 'head', fwd['mri_head_t']) \n\ttransform_surface_to(fwd['src'][1], 'head', fwd['mri_head_t']) \n\tsrc_rr = np.concatenate([s['rr'][v] for (s, v) in zip(fwd['src'], vertices)], axis=0) \n\tsrc_nn = np.concatenate([s['nn'][v] for (s, v) in zip(fwd['src'], vertices)], axis=0) \n\tdip.crop(dip_c.times[0], dip_c.times[(-1)]) \n\t(src_rr, src_nn) = (src_rr[:(-1)], src_nn[:(-1)]) \n\t(corrs, dists, gc_dists, amp_errs, gofs) = ([], [], [], [], []) \n\tfor d in (dip_c, dip): \n\t \tnew = d.pos \n\t \tdiffs = (new - src_rr) \n\t \tcorrs += [np.corrcoef(src_rr.ravel(), new.ravel())[(0, 1)]] \n\t \tdists += [np.sqrt(np.mean(np.sum((diffs * diffs), axis=1)))] \n\t \tgc_dists += [((180 / np.pi) * np.mean(np.arccos(np.sum((src_nn * d.ori), axis=1))))] \n\t \tamp_errs += [np.sqrt(np.mean(((amp - d.amplitude) ** 2)))] \n\t \tgofs += [np.mean(d.gof)] \n\tassert_true((dists[0] >= (dists[1] * factor)), ('dists: \t%s' % dists)) \n\tassert_true((corrs[0] <= (corrs[1] / factor)), ('corrs: \t%s' % corrs)) \n\tassert_true((gc_dists[0] >= (gc_dists[1] * factor)), ('gc-dists \t(ori): \t%s' % gc_dists)) \n\tassert_true((amp_errs[0] >= (amp_errs[1] * factor)), ('amplitude \terrors: \t%s' % amp_errs)) \n\tassert_true((gofs[0] <= (gofs[1] / factor)), ('gof: \t%s' % gofs))\n", 
" \tif (scope is None): \n\t \tscope = ('https://www.googleapis.com/auth/' + name) \n\tparent_parsers = [tools.argparser] \n\tparent_parsers.extend(parents) \n\tparser = argparse.ArgumentParser(description=doc, formatter_class=argparse.RawDescriptionHelpFormatter, parents=parent_parsers) \n\tflags = parser.parse_args(argv[1:]) \n\tclient_secrets = os.path.join(os.path.dirname(filename), 'client_secrets.json') \n\tflow = client.flow_from_clientsecrets(client_secrets, scope=scope, message=tools.message_if_missing(client_secrets)) \n\tstorage = file.Storage((name + '.dat')) \n\tcredentials = storage.get() \n\tif ((credentials is None) or credentials.invalid): \n\t \tcredentials = tools.run_flow(flow, storage, flags) \n\thttp = credentials.authorize(http=build_http()) \n\tif (discovery_filename is None): \n\t \tservice = discovery.build(name, version, http=http) \n\telse: \n\t \twith open(discovery_filename) as discovery_file: \n\t \t \tservice = discovery.build_from_document(discovery_file.read(), base='https://www.googleapis.com/', http=http) \n\treturn (service, flags)\n", 
" \tfor table in ('t/html.html', 'http://blah.com/table.html', 'https://blah.com/table.html', 'file://blah/table.htm', 'ftp://blah.com/table.html', 'file://blah.com/table.htm', ' \t<! \tdoctype \thtml \t> \thello \tworld', 'junk \t< \ttable \tbaz> \t<tr \tfoo \t> \t<td \tbar> \t</td> \t</tr> \t</table> \tjunk', ['junk \t< \ttable \tbaz>', ' \t<tr \tfoo \t>', ' \t<td \tbar> \t', '</td> \t</tr>', '</table> \tjunk'], (' \t<! \tdoctype \thtml \t> \t', ' \thello \tworld')): \n\t \tassert (_probably_html(table) is True) \n\tfor table in ('t/html.htms', 'Xhttp://blah.com/table.html', ' \thttps://blah.com/table.htm', 'fole://blah/table.htm', ' \t< \tdoctype \thtml \t> \thello \tworld', 'junk \t< \ttble \tbaz> \t<tr \tfoo \t> \t<td \tbar> \t</td> \t</tr> \t</table> \tjunk', ['junk \t< \ttable \tbaz>', ' \t<t \tfoo \t>', ' \t<td \tbar> \t', '</td> \t</tr>', '</table> \tjunk'], (' \t<! \tdoctype \thtm \t> \t', ' \thello \tworld'), [[1, 2, 3]]): \n\t \tassert (_probably_html(table) is False)\n", 
" \tclf = GaussianNB(priors=np.array([0.25, 0.25, 0.25, 0.25])) \n\tassert_raises(ValueError, clf.fit, X, y)\n", 
" \traise ValueError\n", 
" \tch_names = ['MEG \t2211', 'MEG \t0223', 'MEG \t1312', 'MEG \t0412', 'MEG \t1043', 'MEG \t2042', 'MEG \t2032', 'MEG \t0522', 'MEG \t1031'] \n\tsel_names = ['Vertex', 'Left-temporal', 'Right-temporal', 'Left-parietal', 'Right-parietal', 'Left-occipital', 'Right-occipital', 'Left-frontal', 'Right-frontal'] \n\traw = read_raw_fif(raw_fname) \n\tfor (i, name) in enumerate(sel_names): \n\t \tsel = read_selection(name) \n\t \tassert_true((ch_names[i] in sel)) \n\t \tsel_info = read_selection(name, info=raw.info) \n\t \tassert_equal(sel, sel_info) \n\tall_ch = read_selection(['L', 'R']) \n\tleft = read_selection('L') \n\tright = read_selection('R') \n\tassert_true((len(all_ch) == (len(left) + len(right)))) \n\tassert_true((len(set(left).intersection(set(right))) == 0)) \n\tfrontal = read_selection('frontal') \n\toccipital = read_selection('Right-occipital') \n\tassert_true((len(set(frontal).intersection(set(occipital))) == 0)) \n\tch_names_new = [ch.replace(' \t', '') for ch in ch_names] \n\traw_new = read_raw_fif(raw_new_fname) \n\tfor (i, name) in enumerate(sel_names): \n\t \tsel = read_selection(name, info=raw_new.info) \n\t \tassert_true((ch_names_new[i] in sel)) \n\tassert_raises(TypeError, read_selection, name, info='foo')\n", 
" \traise ValueError\n", 
" \tdef is_abstract(c): \n\t \tif (not hasattr(c, '__abstractmethods__')): \n\t \t \treturn False \n\t \tif (not len(c.__abstractmethods__)): \n\t \t \treturn False \n\t \treturn True \n\tall_classes = [] \n\tpath = sklearn.__path__ \n\tfor (importer, modname, ispkg) in pkgutil.walk_packages(path=path, prefix='sklearn.', onerror=(lambda x: None)): \n\t \tif ('.tests.' in modname): \n\t \t \tcontinue \n\t \tmodule = __import__(modname, fromlist='dummy') \n\t \tclasses = inspect.getmembers(module, inspect.isclass) \n\t \tall_classes.extend(classes) \n\tall_classes = set(all_classes) \n\testimators = [c for c in all_classes if (issubclass(c[1], BaseEstimator) and (c[0] != 'BaseEstimator'))] \n\testimators = [c for c in estimators if (not is_abstract(c[1]))] \n\tif (not include_dont_test): \n\t \testimators = [c for c in estimators if (not (c[0] in DONT_TEST))] \n\tif (not include_other): \n\t \testimators = [c for c in estimators if (not (c[0] in OTHER))] \n\tif (not include_meta_estimators): \n\t \testimators = [c for c in estimators if (not (c[0] in META_ESTIMATORS))] \n\tif (type_filter is not None): \n\t \tif (not isinstance(type_filter, list)): \n\t \t \ttype_filter = [type_filter] \n\t \telse: \n\t \t \ttype_filter = list(type_filter) \n\t \tfiltered_estimators = [] \n\t \tfilters = {'classifier': ClassifierMixin, 'regressor': RegressorMixin, 'transformer': TransformerMixin, 'cluster': ClusterMixin} \n\t \tfor (name, mixin) in filters.items(): \n\t \t \tif (name in type_filter): \n\t \t \t \ttype_filter.remove(name) \n\t \t \t \tfiltered_estimators.extend([est for est in estimators if issubclass(est[1], mixin)]) \n\t \testimators = filtered_estimators \n\t \tif type_filter: \n\t \t \traise ValueError((\"Parameter \ttype_filter \tmust \tbe \t'classifier', \t'regressor', \t'transformer', \t'cluster' \tor \tNone, \tgot \t%s.\" % repr(type_filter))) \n\treturn sorted(set(estimators), key=itemgetter(0))\n", 
" \tif ((value is None) and empty_ok): \n\t \treturn \n\tif (not isinstance(value, (int, long))): \n\t \traise exception(('%s \tshould \tbe \tan \tinteger; \treceived \t%s \t(a \t%s).' % (name, value, typename(value)))) \n\tif ((not value) and (not zero_ok)): \n\t \traise exception(('%s \tmust \tnot \tbe \t0 \t(zero)' % name)) \n\tif ((value < 0) and (not negative_ok)): \n\t \traise exception(('%s \tmust \tnot \tbe \tnegative.' % name))\n", 
" \tsupported = ('\\x00', '\\x00', '\\x00', '\\x01') \n\ti = 0 \n\tfor item in version: \n\t \tif (version[i] != supported[i]): \n\t \t \traise RuntimeError('SFF \tversion \tnot \tsupported. \tPlease \tcontact \tthe \tauthor \tof \tthe \tsoftware.') \n\t \ti += 1\n", 
" \treturn ChecklistModule(mpstate)\n", 
" \tif (not isinstance(html, _strings)): \n\t \traise TypeError('string \trequired') \n\taccept_leading_text = bool(create_parent) \n\telements = fragments_fromstring(html, guess_charset=guess_charset, parser=parser, no_leading_text=(not accept_leading_text)) \n\tif create_parent: \n\t \tif (not isinstance(create_parent, _strings)): \n\t \t \tcreate_parent = 'div' \n\t \tnew_root = Element(create_parent) \n\t \tif elements: \n\t \t \tif isinstance(elements[0], _strings): \n\t \t \t \tnew_root.text = elements[0] \n\t \t \t \tdel elements[0] \n\t \t \tnew_root.extend(elements) \n\t \treturn new_root \n\tif (not elements): \n\t \traise etree.ParserError('No \telements \tfound') \n\tif (len(elements) > 1): \n\t \traise etree.ParserError('Multiple \telements \tfound') \n\tresult = elements[0] \n\tif (result.tail and result.tail.strip()): \n\t \traise etree.ParserError(('Element \tfollowed \tby \ttext: \t%r' % result.tail)) \n\tresult.tail = None \n\treturn result\n", 
" \traise ValueError\n", 
" \trng = np.random.RandomState(42) \n\tt = rng.randn(100) \n\tedges = bayesian_blocks(t, fitness=u'events') \n\tassert_allclose(edges, [(-2.6197451), (-0.71094865), 0.36866702, 1.85227818]) \n\tt[80:] = t[:20] \n\tedges = bayesian_blocks(t, fitness=u'events', p0=0.01) \n\tassert_allclose(edges, [(-2.6197451), (-0.47432431), (-0.46202823), 1.85227818]) \n\tdt = 0.01 \n\tt = (dt * np.arange(1000)) \n\tx = np.zeros(len(t)) \n\tN = (len(t) // 10) \n\tx[rng.randint(0, len(t), N)] = 1 \n\tx[rng.randint(0, (len(t) // 2), N)] = 1 \n\tedges = bayesian_blocks(t, x, fitness=u'regular_events', dt=dt) \n\tassert_allclose(edges, [0, 5.105, 9.99]) \n\tt = (100 * rng.rand(20)) \n\tx = np.exp(((-0.5) * ((t - 50) ** 2))) \n\tsigma = 0.1 \n\tx_obs = (x + (sigma * rng.randn(len(x)))) \n\tedges = bayesian_blocks(t, x_obs, sigma, fitness=u'measures') \n\tassert_allclose(edges, [4.360377, 48.456895, 52.597917, 99.455051])\n", 
" \traise ValueError\n", 
" \timport vispy.color.colormap as c \n\tassert_raises(AssertionError, c._glsl_step, [0.0, 1.0]) \n\tc._glsl_mix(controls=[0.0, 1.0]) \n\tc._glsl_mix(controls=[0.0, 0.25, 1.0]) \n\tfor fun in (c._glsl_step, c._glsl_mix): \n\t \tassert_raises(AssertionError, fun, controls=[0.1, 1.0]) \n\t \tassert_raises(AssertionError, fun, controls=[0.0, 0.9]) \n\t \tassert_raises(AssertionError, fun, controls=[0.1, 0.9]) \n\tcolor_0 = np.array([1.0, 0.0, 0.0]) \n\tcolor_1 = np.array([0.0, 1.0, 0.0]) \n\tcolor_2 = np.array([0.0, 0.0, 1.0]) \n\tcolors_00 = np.vstack((color_0, color_0)) \n\tcolors_01 = np.vstack((color_0, color_1)) \n\tcolors_11 = np.vstack((color_1, color_1)) \n\tcolors_021 = np.vstack((color_0, color_2, color_1)) \n\tcontrols_2 = np.array([0.0, 1.0]) \n\tcontrols_3 = np.array([0.0, 0.25, 1.0]) \n\tx = np.array([(-1.0), 0.0, 0.1, 0.4, 0.5, 0.6, 1.0, 2.0])[:, None] \n\tmixed_2 = c.mix(colors_01, x, controls_2) \n\tmixed_3 = c.mix(colors_021, x, controls_3) \n\tfor y in (mixed_2, mixed_3): \n\t \tassert_allclose(y[:2, :], colors_00) \n\t \tassert_allclose(y[(-2):, :], colors_11) \n\tassert_allclose(mixed_2[:, (-1)], np.zeros(len(y)))\n", 
" \tprobs0 = np.asarray(probs0, float) \n\tprobs1 = np.asarray(probs1, float) \n\tprobs0 = (probs0 / probs0.sum(axis)) \n\tprobs1 = (probs1 / probs1.sum(axis)) \n\td2 = (((probs1 - probs0) ** 2) / probs0).sum(axis) \n\tif (correction is not None): \n\t \t(nobs, df) = correction \n\t \tdiff = ((probs1 - probs0) / probs0).sum(axis) \n\t \td2 = np.maximum(((((d2 * nobs) - diff) - df) / (nobs - 1.0)), 0) \n\tif cohen: \n\t \treturn np.sqrt(d2) \n\telse: \n\t \treturn d2\n", 
" \treturn Gaussian()(mean, ln_var)\n", 
" \tclf = GaussianNB(priors=np.array([0.25, 0.25, 0.25, 0.25])) \n\tassert_raises(ValueError, clf.fit, X, y)\n", 
" \traise ValueError\n", 
" \treturn (k, features.copy(), labels.copy())\n", 
" \traise ValueError\n", 
" \tdef is_abstract(c): \n\t \tif (not hasattr(c, '__abstractmethods__')): \n\t \t \treturn False \n\t \tif (not len(c.__abstractmethods__)): \n\t \t \treturn False \n\t \treturn True \n\tall_classes = [] \n\tpath = sklearn.__path__ \n\tfor (importer, modname, ispkg) in pkgutil.walk_packages(path=path, prefix='sklearn.', onerror=(lambda x: None)): \n\t \tif ('.tests.' in modname): \n\t \t \tcontinue \n\t \tmodule = __import__(modname, fromlist='dummy') \n\t \tclasses = inspect.getmembers(module, inspect.isclass) \n\t \tall_classes.extend(classes) \n\tall_classes = set(all_classes) \n\testimators = [c for c in all_classes if (issubclass(c[1], BaseEstimator) and (c[0] != 'BaseEstimator'))] \n\testimators = [c for c in estimators if (not is_abstract(c[1]))] \n\tif (not include_dont_test): \n\t \testimators = [c for c in estimators if (not (c[0] in DONT_TEST))] \n\tif (not include_other): \n\t \testimators = [c for c in estimators if (not (c[0] in OTHER))] \n\tif (not include_meta_estimators): \n\t \testimators = [c for c in estimators if (not (c[0] in META_ESTIMATORS))] \n\tif (type_filter is not None): \n\t \tif (not isinstance(type_filter, list)): \n\t \t \ttype_filter = [type_filter] \n\t \telse: \n\t \t \ttype_filter = list(type_filter) \n\t \tfiltered_estimators = [] \n\t \tfilters = {'classifier': ClassifierMixin, 'regressor': RegressorMixin, 'transformer': TransformerMixin, 'cluster': ClusterMixin} \n\t \tfor (name, mixin) in filters.items(): \n\t \t \tif (name in type_filter): \n\t \t \t \ttype_filter.remove(name) \n\t \t \t \tfiltered_estimators.extend([est for est in estimators if issubclass(est[1], mixin)]) \n\t \testimators = filtered_estimators \n\t \tif type_filter: \n\t \t \traise ValueError((\"Parameter \ttype_filter \tmust \tbe \t'classifier', \t'regressor', \t'transformer', \t'cluster' \tor \tNone, \tgot \t%s.\" % repr(type_filter))) \n\treturn sorted(set(estimators), key=itemgetter(0))\n", 
" \tif ((value is None) and empty_ok): \n\t \treturn \n\tif (not isinstance(value, (int, long))): \n\t \traise exception(('%s \tshould \tbe \tan \tinteger; \treceived \t%s \t(a \t%s).' % (name, value, typename(value)))) \n\tif ((not value) and (not zero_ok)): \n\t \traise exception(('%s \tmust \tnot \tbe \t0 \t(zero)' % name)) \n\tif ((value < 0) and (not negative_ok)): \n\t \traise exception(('%s \tmust \tnot \tbe \tnegative.' % name))\n", 
" \traise ValueError\n", 
" \treturn ChecklistModule(mpstate)\n", 
" \tif (not isinstance(html, _strings)): \n\t \traise TypeError('string \trequired') \n\taccept_leading_text = bool(create_parent) \n\telements = fragments_fromstring(html, guess_charset=guess_charset, parser=parser, no_leading_text=(not accept_leading_text)) \n\tif create_parent: \n\t \tif (not isinstance(create_parent, _strings)): \n\t \t \tcreate_parent = 'div' \n\t \tnew_root = Element(create_parent) \n\t \tif elements: \n\t \t \tif isinstance(elements[0], _strings): \n\t \t \t \tnew_root.text = elements[0] \n\t \t \t \tdel elements[0] \n\t \t \tnew_root.extend(elements) \n\t \treturn new_root \n\tif (not elements): \n\t \traise etree.ParserError('No \telements \tfound') \n\tif (len(elements) > 1): \n\t \traise etree.ParserError('Multiple \telements \tfound') \n\tresult = elements[0] \n\tif (result.tail and result.tail.strip()): \n\t \traise etree.ParserError(('Element \tfollowed \tby \ttext: \t%r' % result.tail)) \n\tresult.tail = None \n\treturn result\n", 
" \traise ValueError\n", 
" \trng = np.random.RandomState(42) \n\tt = rng.randn(100) \n\tedges = bayesian_blocks(t, fitness=u'events') \n\tassert_allclose(edges, [(-2.6197451), (-0.71094865), 0.36866702, 1.85227818]) \n\tt[80:] = t[:20] \n\tedges = bayesian_blocks(t, fitness=u'events', p0=0.01) \n\tassert_allclose(edges, [(-2.6197451), (-0.47432431), (-0.46202823), 1.85227818]) \n\tdt = 0.01 \n\tt = (dt * np.arange(1000)) \n\tx = np.zeros(len(t)) \n\tN = (len(t) // 10) \n\tx[rng.randint(0, len(t), N)] = 1 \n\tx[rng.randint(0, (len(t) // 2), N)] = 1 \n\tedges = bayesian_blocks(t, x, fitness=u'regular_events', dt=dt) \n\tassert_allclose(edges, [0, 5.105, 9.99]) \n\tt = (100 * rng.rand(20)) \n\tx = np.exp(((-0.5) * ((t - 50) ** 2))) \n\tsigma = 0.1 \n\tx_obs = (x + (sigma * rng.randn(len(x)))) \n\tedges = bayesian_blocks(t, x_obs, sigma, fitness=u'measures') \n\tassert_allclose(edges, [4.360377, 48.456895, 52.597917, 99.455051])\n", 
" \traise ValueError\n", 
" \tamp = 1e-08 \n\ttempdir = _TempDir() \n\trng = np.random.RandomState(0) \n\tfname_dtemp = op.join(tempdir, 'test.dip') \n\tfname_sim = op.join(tempdir, 'test-ave.fif') \n\tfwd = convert_forward_solution(read_forward_solution(fname_fwd), surf_ori=False, force_fixed=True) \n\tevoked = read_evokeds(fname_evo)[0] \n\tcov = read_cov(fname_cov) \n\tn_per_hemi = 5 \n\tvertices = [np.sort(rng.permutation(s['vertno'])[:n_per_hemi]) for s in fwd['src']] \n\tnv = sum((len(v) for v in vertices)) \n\tstc = SourceEstimate((amp * np.eye(nv)), vertices, 0, 0.001) \n\tevoked = simulate_evoked(fwd, stc, evoked.info, cov, snr=20, random_state=rng) \n\tpicks = np.sort(np.concatenate([pick_types(evoked.info, meg=True, eeg=False)[::2], pick_types(evoked.info, meg=False, eeg=True)[::2]])) \n\tevoked.pick_channels([evoked.ch_names[p] for p in picks]) \n\tevoked.add_proj(make_eeg_average_ref_proj(evoked.info)) \n\twrite_evokeds(fname_sim, evoked) \n\trun_subprocess(['mne_dipole_fit', '--meas', fname_sim, '--meg', '--eeg', '--noise', fname_cov, '--dip', fname_dtemp, '--mri', fname_fwd, '--reg', '0', '--tmin', '0']) \n\tdip_c = read_dipole(fname_dtemp) \n\tsphere = make_sphere_model(head_radius=0.1) \n\t(dip, residuals) = fit_dipole(evoked, fname_cov, sphere, fname_fwd) \n\tdata_rms = np.sqrt(np.sum((evoked.data ** 2), axis=0)) \n\tresi_rms = np.sqrt(np.sum((residuals ** 2), axis=0)) \n\tfactor = 1.0 \n\tif ((os.getenv('TRAVIS', 'false') == 'true') and (sys.version[:3] in ('3.5', '2.7'))): \n\t \tfactor = 0.8 \n\tassert_true((data_rms > (factor * resi_rms)).all(), msg=('%s \t(factor: \t%s)' % ((data_rms / resi_rms).min(), factor))) \n\ttransform_surface_to(fwd['src'][0], 'head', fwd['mri_head_t']) \n\ttransform_surface_to(fwd['src'][1], 'head', fwd['mri_head_t']) \n\tsrc_rr = np.concatenate([s['rr'][v] for (s, v) in zip(fwd['src'], vertices)], axis=0) \n\tsrc_nn = np.concatenate([s['nn'][v] for (s, v) in zip(fwd['src'], vertices)], axis=0) \n\tdip.crop(dip_c.times[0], dip_c.times[(-1)]) \n\t(src_rr, src_nn) = (src_rr[:(-1)], src_nn[:(-1)]) \n\t(corrs, dists, gc_dists, amp_errs, gofs) = ([], [], [], [], []) \n\tfor d in (dip_c, dip): \n\t \tnew = d.pos \n\t \tdiffs = (new - src_rr) \n\t \tcorrs += [np.corrcoef(src_rr.ravel(), new.ravel())[(0, 1)]] \n\t \tdists += [np.sqrt(np.mean(np.sum((diffs * diffs), axis=1)))] \n\t \tgc_dists += [((180 / np.pi) * np.mean(np.arccos(np.sum((src_nn * d.ori), axis=1))))] \n\t \tamp_errs += [np.sqrt(np.mean(((amp - d.amplitude) ** 2)))] \n\t \tgofs += [np.mean(d.gof)] \n\tassert_true((dists[0] >= (dists[1] * factor)), ('dists: \t%s' % dists)) \n\tassert_true((corrs[0] <= (corrs[1] / factor)), ('corrs: \t%s' % corrs)) \n\tassert_true((gc_dists[0] >= (gc_dists[1] * factor)), ('gc-dists \t(ori): \t%s' % gc_dists)) \n\tassert_true((amp_errs[0] >= (amp_errs[1] * factor)), ('amplitude \terrors: \t%s' % amp_errs)) \n\tassert_true((gofs[0] <= (gofs[1] / factor)), ('gof: \t%s' % gofs))\n", 
" \tif (scope is None): \n\t \tscope = ('https://www.googleapis.com/auth/' + name) \n\tparent_parsers = [tools.argparser] \n\tparent_parsers.extend(parents) \n\tparser = argparse.ArgumentParser(description=doc, formatter_class=argparse.RawDescriptionHelpFormatter, parents=parent_parsers) \n\tflags = parser.parse_args(argv[1:]) \n\tclient_secrets = os.path.join(os.path.dirname(filename), 'client_secrets.json') \n\tflow = client.flow_from_clientsecrets(client_secrets, scope=scope, message=tools.message_if_missing(client_secrets)) \n\tstorage = file.Storage((name + '.dat')) \n\tcredentials = storage.get() \n\tif ((credentials is None) or credentials.invalid): \n\t \tcredentials = tools.run_flow(flow, storage, flags) \n\thttp = credentials.authorize(http=build_http()) \n\tif (discovery_filename is None): \n\t \tservice = discovery.build(name, version, http=http) \n\telse: \n\t \twith open(discovery_filename) as discovery_file: \n\t \t \tservice = discovery.build_from_document(discovery_file.read(), base='https://www.googleapis.com/', http=http) \n\treturn (service, flags)\n", 
" \tx = np.asarray(x) \n\tmask = isnull(x) \n\tx = x[(~ mask)] \n\tvalues = np.sort(x) \n\tdef _get_score(at): \n\t \tif (len(values) == 0): \n\t \t \treturn np.nan \n\t \tidx = (at * (len(values) - 1)) \n\t \tif ((idx % 1) == 0): \n\t \t \tscore = values[int(idx)] \n\t \telif (interpolation_method == 'fraction'): \n\t \t \tscore = _interpolate(values[int(idx)], values[(int(idx) + 1)], (idx % 1)) \n\t \telif (interpolation_method == 'lower'): \n\t \t \tscore = values[np.floor(idx)] \n\t \telif (interpolation_method == 'higher'): \n\t \t \tscore = values[np.ceil(idx)] \n\t \telse: \n\t \t \traise ValueError(\"interpolation_method \tcan \tonly \tbe \t'fraction' \t, \t'lower' \tor \t'higher'\") \n\t \treturn score \n\tif is_scalar(q): \n\t \treturn _get_score(q) \n\telse: \n\t \tq = np.asarray(q, np.float64) \n\t \treturn algos.arrmap_float64(q, _get_score)\n", 
" \t[Xt, Yt] = listOfFeatures2Matrix(features) \n\tknn = kNN(Xt, Yt, K) \n\treturn knn\n", 
" \tif (tree in [True, False]): \n\t \treturn tree \n\t(attribute, subtree_dict) = tree \n\tsubtree_key = input.get(attribute) \n\tif (subtree_key not in subtree_dict): \n\t \tsubtree_key = None \n\tsubtree = subtree_dict[subtree_key] \n\treturn classify(subtree, input)\n", 
" \tbest_model = None \n\tbest_inlier_num = 0 \n\tbest_inlier_residuals_sum = np.inf \n\tbest_inliers = None \n\trandom_state = check_random_state(random_state) \n\tif (min_samples < 0): \n\t \traise ValueError('`min_samples` \tmust \tbe \tgreater \tthan \tzero') \n\tif (max_trials < 0): \n\t \traise ValueError('`max_trials` \tmust \tbe \tgreater \tthan \tzero') \n\tif ((stop_probability < 0) or (stop_probability > 1)): \n\t \traise ValueError('`stop_probability` \tmust \tbe \tin \trange \t[0, \t1]') \n\tif ((not isinstance(data, list)) and (not isinstance(data, tuple))): \n\t \tdata = [data] \n\tdata = list(data) \n\tnum_samples = data[0].shape[0] \n\tfor num_trials in range(max_trials): \n\t \tsamples = [] \n\t \trandom_idxs = random_state.randint(0, num_samples, min_samples) \n\t \tfor d in data: \n\t \t \tsamples.append(d[random_idxs]) \n\t \tif ((is_data_valid is not None) and (not is_data_valid(*samples))): \n\t \t \tcontinue \n\t \tsample_model = model_class() \n\t \tsuccess = sample_model.estimate(*samples) \n\t \tif (success is not None): \n\t \t \tif (not success): \n\t \t \t \tcontinue \n\t \tif ((is_model_valid is not None) and (not is_model_valid(sample_model, *samples))): \n\t \t \tcontinue \n\t \tsample_model_residuals = np.abs(sample_model.residuals(*data)) \n\t \tsample_model_inliers = (sample_model_residuals < residual_threshold) \n\t \tsample_model_residuals_sum = np.sum((sample_model_residuals ** 2)) \n\t \tsample_inlier_num = np.sum(sample_model_inliers) \n\t \tif ((sample_inlier_num > best_inlier_num) or ((sample_inlier_num == best_inlier_num) and (sample_model_residuals_sum < best_inlier_residuals_sum))): \n\t \t \tbest_model = sample_model \n\t \t \tbest_inlier_num = sample_inlier_num \n\t \t \tbest_inlier_residuals_sum = sample_model_residuals_sum \n\t \t \tbest_inliers = sample_model_inliers \n\t \t \tif ((best_inlier_num >= stop_sample_num) or (best_inlier_residuals_sum <= stop_residuals_sum) or (num_trials >= _dynamic_max_trials(best_inlier_num, num_samples, min_samples, stop_probability))): \n\t \t \t \tbreak \n\tif (best_inliers is not None): \n\t \tfor i in range(len(data)): \n\t \t \tdata[i] = data[i][best_inliers] \n\t \tbest_model.estimate(*data) \n\treturn (best_model, best_inliers)\n", 
" \tif (scope is None): \n\t \tscope = ('https://www.googleapis.com/auth/' + name) \n\tparent_parsers = [tools.argparser] \n\tparent_parsers.extend(parents) \n\tparser = argparse.ArgumentParser(description=doc, formatter_class=argparse.RawDescriptionHelpFormatter, parents=parent_parsers) \n\tflags = parser.parse_args(argv[1:]) \n\tclient_secrets = os.path.join(os.path.dirname(filename), 'client_secrets.json') \n\tflow = client.flow_from_clientsecrets(client_secrets, scope=scope, message=tools.message_if_missing(client_secrets)) \n\tstorage = file.Storage((name + '.dat')) \n\tcredentials = storage.get() \n\tif ((credentials is None) or credentials.invalid): \n\t \tcredentials = tools.run_flow(flow, storage, flags) \n\thttp = credentials.authorize(http=build_http()) \n\tif (discovery_filename is None): \n\t \tservice = discovery.build(name, version, http=http) \n\telse: \n\t \twith open(discovery_filename) as discovery_file: \n\t \t \tservice = discovery.build_from_document(discovery_file.read(), base='https://www.googleapis.com/', http=http) \n\treturn (service, flags)\n", 
" \t[X, Y] = listOfFeatures2Matrix(features) \n\tet = sklearn.ensemble.ExtraTreesClassifier(n_estimators=n_estimators) \n\tet.fit(X, Y) \n\treturn et\n", 
" \tfor k in range((u['nterms'] - 1)): \n\t \tk1 = (k + 1) \n\t \tmu1n = np.power(mu[0], k1) \n\t \tu['y'][k] = (u['w'][k] * (u['fn'][k1] - (mu1n * u['fn'][0]))) \n\t \tfor p in range((u['nfit'] - 1)): \n\t \t \tu['M'][k][p] = (u['w'][k] * (np.power(mu[(p + 1)], k1) - mu1n))\n", 
" \tclf = GaussianNB(priors=np.array([0.25, 0.25, 0.25, 0.25])) \n\tassert_raises(ValueError, clf.fit, X, y)\n", 
" \tdef g(target, *args, **kwargs): \n\t \tval = func(target, *args, **kwargs) \n\t \tif val: \n\t \t \treturn val \n\t \telse: \n\t \t \traise exception_class((value_error_message_template % target)) \n\tg.__name__ = func.__name__ \n\tg.__doc__ = func.__doc__ \n\treturn g\n", 
" \tif (sample_weight is not None): \n\t \testimator.fit(X, y, sample_weight) \n\telse: \n\t \testimator.fit(X, y) \n\treturn estimator\n", 
" \traise ValueError\n", 
" \tdef is_abstract(c): \n\t \tif (not hasattr(c, '__abstractmethods__')): \n\t \t \treturn False \n\t \tif (not len(c.__abstractmethods__)): \n\t \t \treturn False \n\t \treturn True \n\tall_classes = [] \n\tpath = sklearn.__path__ \n\tfor (importer, modname, ispkg) in pkgutil.walk_packages(path=path, prefix='sklearn.', onerror=(lambda x: None)): \n\t \tif ('.tests.' in modname): \n\t \t \tcontinue \n\t \tmodule = __import__(modname, fromlist='dummy') \n\t \tclasses = inspect.getmembers(module, inspect.isclass) \n\t \tall_classes.extend(classes) \n\tall_classes = set(all_classes) \n\testimators = [c for c in all_classes if (issubclass(c[1], BaseEstimator) and (c[0] != 'BaseEstimator'))] \n\testimators = [c for c in estimators if (not is_abstract(c[1]))] \n\tif (not include_dont_test): \n\t \testimators = [c for c in estimators if (not (c[0] in DONT_TEST))] \n\tif (not include_other): \n\t \testimators = [c for c in estimators if (not (c[0] in OTHER))] \n\tif (not include_meta_estimators): \n\t \testimators = [c for c in estimators if (not (c[0] in META_ESTIMATORS))] \n\tif (type_filter is not None): \n\t \tif (not isinstance(type_filter, list)): \n\t \t \ttype_filter = [type_filter] \n\t \telse: \n\t \t \ttype_filter = list(type_filter) \n\t \tfiltered_estimators = [] \n\t \tfilters = {'classifier': ClassifierMixin, 'regressor': RegressorMixin, 'transformer': TransformerMixin, 'cluster': ClusterMixin} \n\t \tfor (name, mixin) in filters.items(): \n\t \t \tif (name in type_filter): \n\t \t \t \ttype_filter.remove(name) \n\t \t \t \tfiltered_estimators.extend([est for est in estimators if issubclass(est[1], mixin)]) \n\t \testimators = filtered_estimators \n\t \tif type_filter: \n\t \t \traise ValueError((\"Parameter \ttype_filter \tmust \tbe \t'classifier', \t'regressor', \t'transformer', \t'cluster' \tor \tNone, \tgot \t%s.\" % repr(type_filter))) \n\treturn sorted(set(estimators), key=itemgetter(0))\n", 
" \treturn ChecklistModule(mpstate)\n", 
" \tif (not isinstance(html, _strings)): \n\t \traise TypeError('string \trequired') \n\taccept_leading_text = bool(create_parent) \n\telements = fragments_fromstring(html, guess_charset=guess_charset, parser=parser, no_leading_text=(not accept_leading_text)) \n\tif create_parent: \n\t \tif (not isinstance(create_parent, _strings)): \n\t \t \tcreate_parent = 'div' \n\t \tnew_root = Element(create_parent) \n\t \tif elements: \n\t \t \tif isinstance(elements[0], _strings): \n\t \t \t \tnew_root.text = elements[0] \n\t \t \t \tdel elements[0] \n\t \t \tnew_root.extend(elements) \n\t \treturn new_root \n\tif (not elements): \n\t \traise etree.ParserError('No \telements \tfound') \n\tif (len(elements) > 1): \n\t \traise etree.ParserError('Multiple \telements \tfound') \n\tresult = elements[0] \n\tif (result.tail and result.tail.strip()): \n\t \traise etree.ParserError(('Element \tfollowed \tby \ttext: \t%r' % result.tail)) \n\tresult.tail = None \n\treturn result\n", 
" \trng = np.random.RandomState(42) \n\tt = rng.randn(100) \n\tedges = bayesian_blocks(t, fitness=u'events') \n\tassert_allclose(edges, [(-2.6197451), (-0.71094865), 0.36866702, 1.85227818]) \n\tt[80:] = t[:20] \n\tedges = bayesian_blocks(t, fitness=u'events', p0=0.01) \n\tassert_allclose(edges, [(-2.6197451), (-0.47432431), (-0.46202823), 1.85227818]) \n\tdt = 0.01 \n\tt = (dt * np.arange(1000)) \n\tx = np.zeros(len(t)) \n\tN = (len(t) // 10) \n\tx[rng.randint(0, len(t), N)] = 1 \n\tx[rng.randint(0, (len(t) // 2), N)] = 1 \n\tedges = bayesian_blocks(t, x, fitness=u'regular_events', dt=dt) \n\tassert_allclose(edges, [0, 5.105, 9.99]) \n\tt = (100 * rng.rand(20)) \n\tx = np.exp(((-0.5) * ((t - 50) ** 2))) \n\tsigma = 0.1 \n\tx_obs = (x + (sigma * rng.randn(len(x)))) \n\tedges = bayesian_blocks(t, x_obs, sigma, fitness=u'measures') \n\tassert_allclose(edges, [4.360377, 48.456895, 52.597917, 99.455051])\n", 
" \traise ValueError\n", 
" \tamp = 1e-08 \n\ttempdir = _TempDir() \n\trng = np.random.RandomState(0) \n\tfname_dtemp = op.join(tempdir, 'test.dip') \n\tfname_sim = op.join(tempdir, 'test-ave.fif') \n\tfwd = convert_forward_solution(read_forward_solution(fname_fwd), surf_ori=False, force_fixed=True) \n\tevoked = read_evokeds(fname_evo)[0] \n\tcov = read_cov(fname_cov) \n\tn_per_hemi = 5 \n\tvertices = [np.sort(rng.permutation(s['vertno'])[:n_per_hemi]) for s in fwd['src']] \n\tnv = sum((len(v) for v in vertices)) \n\tstc = SourceEstimate((amp * np.eye(nv)), vertices, 0, 0.001) \n\tevoked = simulate_evoked(fwd, stc, evoked.info, cov, snr=20, random_state=rng) \n\tpicks = np.sort(np.concatenate([pick_types(evoked.info, meg=True, eeg=False)[::2], pick_types(evoked.info, meg=False, eeg=True)[::2]])) \n\tevoked.pick_channels([evoked.ch_names[p] for p in picks]) \n\tevoked.add_proj(make_eeg_average_ref_proj(evoked.info)) \n\twrite_evokeds(fname_sim, evoked) \n\trun_subprocess(['mne_dipole_fit', '--meas', fname_sim, '--meg', '--eeg', '--noise', fname_cov, '--dip', fname_dtemp, '--mri', fname_fwd, '--reg', '0', '--tmin', '0']) \n\tdip_c = read_dipole(fname_dtemp) \n\tsphere = make_sphere_model(head_radius=0.1) \n\t(dip, residuals) = fit_dipole(evoked, fname_cov, sphere, fname_fwd) \n\tdata_rms = np.sqrt(np.sum((evoked.data ** 2), axis=0)) \n\tresi_rms = np.sqrt(np.sum((residuals ** 2), axis=0)) \n\tfactor = 1.0 \n\tif ((os.getenv('TRAVIS', 'false') == 'true') and (sys.version[:3] in ('3.5', '2.7'))): \n\t \tfactor = 0.8 \n\tassert_true((data_rms > (factor * resi_rms)).all(), msg=('%s \t(factor: \t%s)' % ((data_rms / resi_rms).min(), factor))) \n\ttransform_surface_to(fwd['src'][0], 'head', fwd['mri_head_t']) \n\ttransform_surface_to(fwd['src'][1], 'head', fwd['mri_head_t']) \n\tsrc_rr = np.concatenate([s['rr'][v] for (s, v) in zip(fwd['src'], vertices)], axis=0) \n\tsrc_nn = np.concatenate([s['nn'][v] for (s, v) in zip(fwd['src'], vertices)], axis=0) \n\tdip.crop(dip_c.times[0], dip_c.times[(-1)]) \n\t(src_rr, src_nn) = (src_rr[:(-1)], src_nn[:(-1)]) \n\t(corrs, dists, gc_dists, amp_errs, gofs) = ([], [], [], [], []) \n\tfor d in (dip_c, dip): \n\t \tnew = d.pos \n\t \tdiffs = (new - src_rr) \n\t \tcorrs += [np.corrcoef(src_rr.ravel(), new.ravel())[(0, 1)]] \n\t \tdists += [np.sqrt(np.mean(np.sum((diffs * diffs), axis=1)))] \n\t \tgc_dists += [((180 / np.pi) * np.mean(np.arccos(np.sum((src_nn * d.ori), axis=1))))] \n\t \tamp_errs += [np.sqrt(np.mean(((amp - d.amplitude) ** 2)))] \n\t \tgofs += [np.mean(d.gof)] \n\tassert_true((dists[0] >= (dists[1] * factor)), ('dists: \t%s' % dists)) \n\tassert_true((corrs[0] <= (corrs[1] / factor)), ('corrs: \t%s' % corrs)) \n\tassert_true((gc_dists[0] >= (gc_dists[1] * factor)), ('gc-dists \t(ori): \t%s' % gc_dists)) \n\tassert_true((amp_errs[0] >= (amp_errs[1] * factor)), ('amplitude \terrors: \t%s' % amp_errs)) \n\tassert_true((gofs[0] <= (gofs[1] / factor)), ('gof: \t%s' % gofs))\n", 
" \tif (scope is None): \n\t \tscope = ('https://www.googleapis.com/auth/' + name) \n\tparent_parsers = [tools.argparser] \n\tparent_parsers.extend(parents) \n\tparser = argparse.ArgumentParser(description=doc, formatter_class=argparse.RawDescriptionHelpFormatter, parents=parent_parsers) \n\tflags = parser.parse_args(argv[1:]) \n\tclient_secrets = os.path.join(os.path.dirname(filename), 'client_secrets.json') \n\tflow = client.flow_from_clientsecrets(client_secrets, scope=scope, message=tools.message_if_missing(client_secrets)) \n\tstorage = file.Storage((name + '.dat')) \n\tcredentials = storage.get() \n\tif ((credentials is None) or credentials.invalid): \n\t \tcredentials = tools.run_flow(flow, storage, flags) \n\thttp = credentials.authorize(http=build_http()) \n\tif (discovery_filename is None): \n\t \tservice = discovery.build(name, version, http=http) \n\telse: \n\t \twith open(discovery_filename) as discovery_file: \n\t \t \tservice = discovery.build_from_document(discovery_file.read(), base='https://www.googleapis.com/', http=http) \n\treturn (service, flags)\n", 
" \tclf = GaussianNB(priors=np.array([0.25, 0.25, 0.25, 0.25])) \n\tassert_raises(ValueError, clf.fit, X, y)\n", 
" \traise ValueError\n", 
" \treturn (k, features.copy(), labels.copy())\n", 
" \traise ValueError\n", 
" \tdef is_abstract(c): \n\t \tif (not hasattr(c, '__abstractmethods__')): \n\t \t \treturn False \n\t \tif (not len(c.__abstractmethods__)): \n\t \t \treturn False \n\t \treturn True \n\tall_classes = [] \n\tpath = sklearn.__path__ \n\tfor (importer, modname, ispkg) in pkgutil.walk_packages(path=path, prefix='sklearn.', onerror=(lambda x: None)): \n\t \tif ('.tests.' in modname): \n\t \t \tcontinue \n\t \tmodule = __import__(modname, fromlist='dummy') \n\t \tclasses = inspect.getmembers(module, inspect.isclass) \n\t \tall_classes.extend(classes) \n\tall_classes = set(all_classes) \n\testimators = [c for c in all_classes if (issubclass(c[1], BaseEstimator) and (c[0] != 'BaseEstimator'))] \n\testimators = [c for c in estimators if (not is_abstract(c[1]))] \n\tif (not include_dont_test): \n\t \testimators = [c for c in estimators if (not (c[0] in DONT_TEST))] \n\tif (not include_other): \n\t \testimators = [c for c in estimators if (not (c[0] in OTHER))] \n\tif (not include_meta_estimators): \n\t \testimators = [c for c in estimators if (not (c[0] in META_ESTIMATORS))] \n\tif (type_filter is not None): \n\t \tif (not isinstance(type_filter, list)): \n\t \t \ttype_filter = [type_filter] \n\t \telse: \n\t \t \ttype_filter = list(type_filter) \n\t \tfiltered_estimators = [] \n\t \tfilters = {'classifier': ClassifierMixin, 'regressor': RegressorMixin, 'transformer': TransformerMixin, 'cluster': ClusterMixin} \n\t \tfor (name, mixin) in filters.items(): \n\t \t \tif (name in type_filter): \n\t \t \t \ttype_filter.remove(name) \n\t \t \t \tfiltered_estimators.extend([est for est in estimators if issubclass(est[1], mixin)]) \n\t \testimators = filtered_estimators \n\t \tif type_filter: \n\t \t \traise ValueError((\"Parameter \ttype_filter \tmust \tbe \t'classifier', \t'regressor', \t'transformer', \t'cluster' \tor \tNone, \tgot \t%s.\" % repr(type_filter))) \n\treturn sorted(set(estimators), key=itemgetter(0))\n", 
" \tif ((value is None) and empty_ok): \n\t \treturn \n\tif (not isinstance(value, (int, long))): \n\t \traise exception(('%s \tshould \tbe \tan \tinteger; \treceived \t%s \t(a \t%s).' % (name, value, typename(value)))) \n\tif ((not value) and (not zero_ok)): \n\t \traise exception(('%s \tmust \tnot \tbe \t0 \t(zero)' % name)) \n\tif ((value < 0) and (not negative_ok)): \n\t \traise exception(('%s \tmust \tnot \tbe \tnegative.' % name))\n", 
" \treturn ChecklistModule(mpstate)\n", 
" \tif (not isinstance(html, _strings)): \n\t \traise TypeError('string \trequired') \n\taccept_leading_text = bool(create_parent) \n\telements = fragments_fromstring(html, guess_charset=guess_charset, parser=parser, no_leading_text=(not accept_leading_text)) \n\tif create_parent: \n\t \tif (not isinstance(create_parent, _strings)): \n\t \t \tcreate_parent = 'div' \n\t \tnew_root = Element(create_parent) \n\t \tif elements: \n\t \t \tif isinstance(elements[0], _strings): \n\t \t \t \tnew_root.text = elements[0] \n\t \t \t \tdel elements[0] \n\t \t \tnew_root.extend(elements) \n\t \treturn new_root \n\tif (not elements): \n\t \traise etree.ParserError('No \telements \tfound') \n\tif (len(elements) > 1): \n\t \traise etree.ParserError('Multiple \telements \tfound') \n\tresult = elements[0] \n\tif (result.tail and result.tail.strip()): \n\t \traise etree.ParserError(('Element \tfollowed \tby \ttext: \t%r' % result.tail)) \n\tresult.tail = None \n\treturn result\n", 
" \traise ValueError\n", 
" \trng = np.random.RandomState(42) \n\tt = rng.randn(100) \n\tedges = bayesian_blocks(t, fitness=u'events') \n\tassert_allclose(edges, [(-2.6197451), (-0.71094865), 0.36866702, 1.85227818]) \n\tt[80:] = t[:20] \n\tedges = bayesian_blocks(t, fitness=u'events', p0=0.01) \n\tassert_allclose(edges, [(-2.6197451), (-0.47432431), (-0.46202823), 1.85227818]) \n\tdt = 0.01 \n\tt = (dt * np.arange(1000)) \n\tx = np.zeros(len(t)) \n\tN = (len(t) // 10) \n\tx[rng.randint(0, len(t), N)] = 1 \n\tx[rng.randint(0, (len(t) // 2), N)] = 1 \n\tedges = bayesian_blocks(t, x, fitness=u'regular_events', dt=dt) \n\tassert_allclose(edges, [0, 5.105, 9.99]) \n\tt = (100 * rng.rand(20)) \n\tx = np.exp(((-0.5) * ((t - 50) ** 2))) \n\tsigma = 0.1 \n\tx_obs = (x + (sigma * rng.randn(len(x)))) \n\tedges = bayesian_blocks(t, x_obs, sigma, fitness=u'measures') \n\tassert_allclose(edges, [4.360377, 48.456895, 52.597917, 99.455051])\n", 
" \traise ValueError\n", 
" \tamp = 1e-08 \n\ttempdir = _TempDir() \n\trng = np.random.RandomState(0) \n\tfname_dtemp = op.join(tempdir, 'test.dip') \n\tfname_sim = op.join(tempdir, 'test-ave.fif') \n\tfwd = convert_forward_solution(read_forward_solution(fname_fwd), surf_ori=False, force_fixed=True) \n\tevoked = read_evokeds(fname_evo)[0] \n\tcov = read_cov(fname_cov) \n\tn_per_hemi = 5 \n\tvertices = [np.sort(rng.permutation(s['vertno'])[:n_per_hemi]) for s in fwd['src']] \n\tnv = sum((len(v) for v in vertices)) \n\tstc = SourceEstimate((amp * np.eye(nv)), vertices, 0, 0.001) \n\tevoked = simulate_evoked(fwd, stc, evoked.info, cov, snr=20, random_state=rng) \n\tpicks = np.sort(np.concatenate([pick_types(evoked.info, meg=True, eeg=False)[::2], pick_types(evoked.info, meg=False, eeg=True)[::2]])) \n\tevoked.pick_channels([evoked.ch_names[p] for p in picks]) \n\tevoked.add_proj(make_eeg_average_ref_proj(evoked.info)) \n\twrite_evokeds(fname_sim, evoked) \n\trun_subprocess(['mne_dipole_fit', '--meas', fname_sim, '--meg', '--eeg', '--noise', fname_cov, '--dip', fname_dtemp, '--mri', fname_fwd, '--reg', '0', '--tmin', '0']) \n\tdip_c = read_dipole(fname_dtemp) \n\tsphere = make_sphere_model(head_radius=0.1) \n\t(dip, residuals) = fit_dipole(evoked, fname_cov, sphere, fname_fwd) \n\tdata_rms = np.sqrt(np.sum((evoked.data ** 2), axis=0)) \n\tresi_rms = np.sqrt(np.sum((residuals ** 2), axis=0)) \n\tfactor = 1.0 \n\tif ((os.getenv('TRAVIS', 'false') == 'true') and (sys.version[:3] in ('3.5', '2.7'))): \n\t \tfactor = 0.8 \n\tassert_true((data_rms > (factor * resi_rms)).all(), msg=('%s \t(factor: \t%s)' % ((data_rms / resi_rms).min(), factor))) \n\ttransform_surface_to(fwd['src'][0], 'head', fwd['mri_head_t']) \n\ttransform_surface_to(fwd['src'][1], 'head', fwd['mri_head_t']) \n\tsrc_rr = np.concatenate([s['rr'][v] for (s, v) in zip(fwd['src'], vertices)], axis=0) \n\tsrc_nn = np.concatenate([s['nn'][v] for (s, v) in zip(fwd['src'], vertices)], axis=0) \n\tdip.crop(dip_c.times[0], dip_c.times[(-1)]) \n\t(src_rr, src_nn) = (src_rr[:(-1)], src_nn[:(-1)]) \n\t(corrs, dists, gc_dists, amp_errs, gofs) = ([], [], [], [], []) \n\tfor d in (dip_c, dip): \n\t \tnew = d.pos \n\t \tdiffs = (new - src_rr) \n\t \tcorrs += [np.corrcoef(src_rr.ravel(), new.ravel())[(0, 1)]] \n\t \tdists += [np.sqrt(np.mean(np.sum((diffs * diffs), axis=1)))] \n\t \tgc_dists += [((180 / np.pi) * np.mean(np.arccos(np.sum((src_nn * d.ori), axis=1))))] \n\t \tamp_errs += [np.sqrt(np.mean(((amp - d.amplitude) ** 2)))] \n\t \tgofs += [np.mean(d.gof)] \n\tassert_true((dists[0] >= (dists[1] * factor)), ('dists: \t%s' % dists)) \n\tassert_true((corrs[0] <= (corrs[1] / factor)), ('corrs: \t%s' % corrs)) \n\tassert_true((gc_dists[0] >= (gc_dists[1] * factor)), ('gc-dists \t(ori): \t%s' % gc_dists)) \n\tassert_true((amp_errs[0] >= (amp_errs[1] * factor)), ('amplitude \terrors: \t%s' % amp_errs)) \n\tassert_true((gofs[0] <= (gofs[1] / factor)), ('gof: \t%s' % gofs))\n", 
" \tif (scope is None): \n\t \tscope = ('https://www.googleapis.com/auth/' + name) \n\tparent_parsers = [tools.argparser] \n\tparent_parsers.extend(parents) \n\tparser = argparse.ArgumentParser(description=doc, formatter_class=argparse.RawDescriptionHelpFormatter, parents=parent_parsers) \n\tflags = parser.parse_args(argv[1:]) \n\tclient_secrets = os.path.join(os.path.dirname(filename), 'client_secrets.json') \n\tflow = client.flow_from_clientsecrets(client_secrets, scope=scope, message=tools.message_if_missing(client_secrets)) \n\tstorage = file.Storage((name + '.dat')) \n\tcredentials = storage.get() \n\tif ((credentials is None) or credentials.invalid): \n\t \tcredentials = tools.run_flow(flow, storage, flags) \n\thttp = credentials.authorize(http=build_http()) \n\tif (discovery_filename is None): \n\t \tservice = discovery.build(name, version, http=http) \n\telse: \n\t \twith open(discovery_filename) as discovery_file: \n\t \t \tservice = discovery.build_from_document(discovery_file.read(), base='https://www.googleapis.com/', http=http) \n\treturn (service, flags)\n", 
" \tx = np.asarray(x) \n\tmask = isnull(x) \n\tx = x[(~ mask)] \n\tvalues = np.sort(x) \n\tdef _get_score(at): \n\t \tif (len(values) == 0): \n\t \t \treturn np.nan \n\t \tidx = (at * (len(values) - 1)) \n\t \tif ((idx % 1) == 0): \n\t \t \tscore = values[int(idx)] \n\t \telif (interpolation_method == 'fraction'): \n\t \t \tscore = _interpolate(values[int(idx)], values[(int(idx) + 1)], (idx % 1)) \n\t \telif (interpolation_method == 'lower'): \n\t \t \tscore = values[np.floor(idx)] \n\t \telif (interpolation_method == 'higher'): \n\t \t \tscore = values[np.ceil(idx)] \n\t \telse: \n\t \t \traise ValueError(\"interpolation_method \tcan \tonly \tbe \t'fraction' \t, \t'lower' \tor \t'higher'\") \n\t \treturn score \n\tif is_scalar(q): \n\t \treturn _get_score(q) \n\telse: \n\t \tq = np.asarray(q, np.float64) \n\t \treturn algos.arrmap_float64(q, _get_score)\n", 
" \tclf = GaussianNB(priors=np.array([0.25, 0.25, 0.25, 0.25])) \n\tassert_raises(ValueError, clf.fit, X, y)\n", 
" \traise ValueError\n", 
" \tclasses = [] \n\tn_classes = [] \n\tclass_prior = [] \n\t(n_samples, n_outputs) = y.shape \n\tif issparse(y): \n\t \ty = y.tocsc() \n\t \ty_nnz = np.diff(y.indptr) \n\t \tfor k in range(n_outputs): \n\t \t \tcol_nonzero = y.indices[y.indptr[k]:y.indptr[(k + 1)]] \n\t \t \tif (sample_weight is not None): \n\t \t \t \tnz_samp_weight = np.asarray(sample_weight)[col_nonzero] \n\t \t \t \tzeros_samp_weight_sum = (np.sum(sample_weight) - np.sum(nz_samp_weight)) \n\t \t \telse: \n\t \t \t \tnz_samp_weight = None \n\t \t \t \tzeros_samp_weight_sum = (y.shape[0] - y_nnz[k]) \n\t \t \t(classes_k, y_k) = np.unique(y.data[y.indptr[k]:y.indptr[(k + 1)]], return_inverse=True) \n\t \t \tclass_prior_k = bincount(y_k, weights=nz_samp_weight) \n\t \t \tif (0 in classes_k): \n\t \t \t \tclass_prior_k[(classes_k == 0)] += zeros_samp_weight_sum \n\t \t \tif ((0 not in classes_k) and (y_nnz[k] < y.shape[0])): \n\t \t \t \tclasses_k = np.insert(classes_k, 0, 0) \n\t \t \t \tclass_prior_k = np.insert(class_prior_k, 0, zeros_samp_weight_sum) \n\t \t \tclasses.append(classes_k) \n\t \t \tn_classes.append(classes_k.shape[0]) \n\t \t \tclass_prior.append((class_prior_k / class_prior_k.sum())) \n\telse: \n\t \tfor k in range(n_outputs): \n\t \t \t(classes_k, y_k) = np.unique(y[:, k], return_inverse=True) \n\t \t \tclasses.append(classes_k) \n\t \t \tn_classes.append(classes_k.shape[0]) \n\t \t \tclass_prior_k = bincount(y_k, weights=sample_weight) \n\t \t \tclass_prior.append((class_prior_k / class_prior_k.sum())) \n\treturn (classes, n_classes, class_prior)\n", 
" \tdef is_abstract(c): \n\t \tif (not hasattr(c, '__abstractmethods__')): \n\t \t \treturn False \n\t \tif (not len(c.__abstractmethods__)): \n\t \t \treturn False \n\t \treturn True \n\tall_classes = [] \n\tpath = sklearn.__path__ \n\tfor (importer, modname, ispkg) in pkgutil.walk_packages(path=path, prefix='sklearn.', onerror=(lambda x: None)): \n\t \tif ('.tests.' in modname): \n\t \t \tcontinue \n\t \tmodule = __import__(modname, fromlist='dummy') \n\t \tclasses = inspect.getmembers(module, inspect.isclass) \n\t \tall_classes.extend(classes) \n\tall_classes = set(all_classes) \n\testimators = [c for c in all_classes if (issubclass(c[1], BaseEstimator) and (c[0] != 'BaseEstimator'))] \n\testimators = [c for c in estimators if (not is_abstract(c[1]))] \n\tif (not include_dont_test): \n\t \testimators = [c for c in estimators if (not (c[0] in DONT_TEST))] \n\tif (not include_other): \n\t \testimators = [c for c in estimators if (not (c[0] in OTHER))] \n\tif (not include_meta_estimators): \n\t \testimators = [c for c in estimators if (not (c[0] in META_ESTIMATORS))] \n\tif (type_filter is not None): \n\t \tif (not isinstance(type_filter, list)): \n\t \t \ttype_filter = [type_filter] \n\t \telse: \n\t \t \ttype_filter = list(type_filter) \n\t \tfiltered_estimators = [] \n\t \tfilters = {'classifier': ClassifierMixin, 'regressor': RegressorMixin, 'transformer': TransformerMixin, 'cluster': ClusterMixin} \n\t \tfor (name, mixin) in filters.items(): \n\t \t \tif (name in type_filter): \n\t \t \t \ttype_filter.remove(name) \n\t \t \t \tfiltered_estimators.extend([est for est in estimators if issubclass(est[1], mixin)]) \n\t \testimators = filtered_estimators \n\t \tif type_filter: \n\t \t \traise ValueError((\"Parameter \ttype_filter \tmust \tbe \t'classifier', \t'regressor', \t'transformer', \t'cluster' \tor \tNone, \tgot \t%s.\" % repr(type_filter))) \n\treturn sorted(set(estimators), key=itemgetter(0))\n", 
" \treturn ChecklistModule(mpstate)\n", 
" \tif (not isinstance(html, _strings)): \n\t \traise TypeError('string \trequired') \n\taccept_leading_text = bool(create_parent) \n\telements = fragments_fromstring(html, guess_charset=guess_charset, parser=parser, no_leading_text=(not accept_leading_text)) \n\tif create_parent: \n\t \tif (not isinstance(create_parent, _strings)): \n\t \t \tcreate_parent = 'div' \n\t \tnew_root = Element(create_parent) \n\t \tif elements: \n\t \t \tif isinstance(elements[0], _strings): \n\t \t \t \tnew_root.text = elements[0] \n\t \t \t \tdel elements[0] \n\t \t \tnew_root.extend(elements) \n\t \treturn new_root \n\tif (not elements): \n\t \traise etree.ParserError('No \telements \tfound') \n\tif (len(elements) > 1): \n\t \traise etree.ParserError('Multiple \telements \tfound') \n\tresult = elements[0] \n\tif (result.tail and result.tail.strip()): \n\t \traise etree.ParserError(('Element \tfollowed \tby \ttext: \t%r' % result.tail)) \n\tresult.tail = None \n\treturn result\n", 
" \trng = np.random.RandomState(42) \n\tt = rng.randn(100) \n\tedges = bayesian_blocks(t, fitness=u'events') \n\tassert_allclose(edges, [(-2.6197451), (-0.71094865), 0.36866702, 1.85227818]) \n\tt[80:] = t[:20] \n\tedges = bayesian_blocks(t, fitness=u'events', p0=0.01) \n\tassert_allclose(edges, [(-2.6197451), (-0.47432431), (-0.46202823), 1.85227818]) \n\tdt = 0.01 \n\tt = (dt * np.arange(1000)) \n\tx = np.zeros(len(t)) \n\tN = (len(t) // 10) \n\tx[rng.randint(0, len(t), N)] = 1 \n\tx[rng.randint(0, (len(t) // 2), N)] = 1 \n\tedges = bayesian_blocks(t, x, fitness=u'regular_events', dt=dt) \n\tassert_allclose(edges, [0, 5.105, 9.99]) \n\tt = (100 * rng.rand(20)) \n\tx = np.exp(((-0.5) * ((t - 50) ** 2))) \n\tsigma = 0.1 \n\tx_obs = (x + (sigma * rng.randn(len(x)))) \n\tedges = bayesian_blocks(t, x_obs, sigma, fitness=u'measures') \n\tassert_allclose(edges, [4.360377, 48.456895, 52.597917, 99.455051])\n", 
" \traise ValueError\n", 
" \tamp = 1e-08 \n\ttempdir = _TempDir() \n\trng = np.random.RandomState(0) \n\tfname_dtemp = op.join(tempdir, 'test.dip') \n\tfname_sim = op.join(tempdir, 'test-ave.fif') \n\tfwd = convert_forward_solution(read_forward_solution(fname_fwd), surf_ori=False, force_fixed=True) \n\tevoked = read_evokeds(fname_evo)[0] \n\tcov = read_cov(fname_cov) \n\tn_per_hemi = 5 \n\tvertices = [np.sort(rng.permutation(s['vertno'])[:n_per_hemi]) for s in fwd['src']] \n\tnv = sum((len(v) for v in vertices)) \n\tstc = SourceEstimate((amp * np.eye(nv)), vertices, 0, 0.001) \n\tevoked = simulate_evoked(fwd, stc, evoked.info, cov, snr=20, random_state=rng) \n\tpicks = np.sort(np.concatenate([pick_types(evoked.info, meg=True, eeg=False)[::2], pick_types(evoked.info, meg=False, eeg=True)[::2]])) \n\tevoked.pick_channels([evoked.ch_names[p] for p in picks]) \n\tevoked.add_proj(make_eeg_average_ref_proj(evoked.info)) \n\twrite_evokeds(fname_sim, evoked) \n\trun_subprocess(['mne_dipole_fit', '--meas', fname_sim, '--meg', '--eeg', '--noise', fname_cov, '--dip', fname_dtemp, '--mri', fname_fwd, '--reg', '0', '--tmin', '0']) \n\tdip_c = read_dipole(fname_dtemp) \n\tsphere = make_sphere_model(head_radius=0.1) \n\t(dip, residuals) = fit_dipole(evoked, fname_cov, sphere, fname_fwd) \n\tdata_rms = np.sqrt(np.sum((evoked.data ** 2), axis=0)) \n\tresi_rms = np.sqrt(np.sum((residuals ** 2), axis=0)) \n\tfactor = 1.0 \n\tif ((os.getenv('TRAVIS', 'false') == 'true') and (sys.version[:3] in ('3.5', '2.7'))): \n\t \tfactor = 0.8 \n\tassert_true((data_rms > (factor * resi_rms)).all(), msg=('%s \t(factor: \t%s)' % ((data_rms / resi_rms).min(), factor))) \n\ttransform_surface_to(fwd['src'][0], 'head', fwd['mri_head_t']) \n\ttransform_surface_to(fwd['src'][1], 'head', fwd['mri_head_t']) \n\tsrc_rr = np.concatenate([s['rr'][v] for (s, v) in zip(fwd['src'], vertices)], axis=0) \n\tsrc_nn = np.concatenate([s['nn'][v] for (s, v) in zip(fwd['src'], vertices)], axis=0) \n\tdip.crop(dip_c.times[0], dip_c.times[(-1)]) \n\t(src_rr, src_nn) = (src_rr[:(-1)], src_nn[:(-1)]) \n\t(corrs, dists, gc_dists, amp_errs, gofs) = ([], [], [], [], []) \n\tfor d in (dip_c, dip): \n\t \tnew = d.pos \n\t \tdiffs = (new - src_rr) \n\t \tcorrs += [np.corrcoef(src_rr.ravel(), new.ravel())[(0, 1)]] \n\t \tdists += [np.sqrt(np.mean(np.sum((diffs * diffs), axis=1)))] \n\t \tgc_dists += [((180 / np.pi) * np.mean(np.arccos(np.sum((src_nn * d.ori), axis=1))))] \n\t \tamp_errs += [np.sqrt(np.mean(((amp - d.amplitude) ** 2)))] \n\t \tgofs += [np.mean(d.gof)] \n\tassert_true((dists[0] >= (dists[1] * factor)), ('dists: \t%s' % dists)) \n\tassert_true((corrs[0] <= (corrs[1] / factor)), ('corrs: \t%s' % corrs)) \n\tassert_true((gc_dists[0] >= (gc_dists[1] * factor)), ('gc-dists \t(ori): \t%s' % gc_dists)) \n\tassert_true((amp_errs[0] >= (amp_errs[1] * factor)), ('amplitude \terrors: \t%s' % amp_errs)) \n\tassert_true((gofs[0] <= (gofs[1] / factor)), ('gof: \t%s' % gofs))\n", 
" \tif (scope is None): \n\t \tscope = ('https://www.googleapis.com/auth/' + name) \n\tparent_parsers = [tools.argparser] \n\tparent_parsers.extend(parents) \n\tparser = argparse.ArgumentParser(description=doc, formatter_class=argparse.RawDescriptionHelpFormatter, parents=parent_parsers) \n\tflags = parser.parse_args(argv[1:]) \n\tclient_secrets = os.path.join(os.path.dirname(filename), 'client_secrets.json') \n\tflow = client.flow_from_clientsecrets(client_secrets, scope=scope, message=tools.message_if_missing(client_secrets)) \n\tstorage = file.Storage((name + '.dat')) \n\tcredentials = storage.get() \n\tif ((credentials is None) or credentials.invalid): \n\t \tcredentials = tools.run_flow(flow, storage, flags) \n\thttp = credentials.authorize(http=build_http()) \n\tif (discovery_filename is None): \n\t \tservice = discovery.build(name, version, http=http) \n\telse: \n\t \twith open(discovery_filename) as discovery_file: \n\t \t \tservice = discovery.build_from_document(discovery_file.read(), base='https://www.googleapis.com/', http=http) \n\treturn (service, flags)\n", 
" \tch_names = ['MEG \t2211', 'MEG \t0223', 'MEG \t1312', 'MEG \t0412', 'MEG \t1043', 'MEG \t2042', 'MEG \t2032', 'MEG \t0522', 'MEG \t1031'] \n\tsel_names = ['Vertex', 'Left-temporal', 'Right-temporal', 'Left-parietal', 'Right-parietal', 'Left-occipital', 'Right-occipital', 'Left-frontal', 'Right-frontal'] \n\traw = read_raw_fif(raw_fname) \n\tfor (i, name) in enumerate(sel_names): \n\t \tsel = read_selection(name) \n\t \tassert_true((ch_names[i] in sel)) \n\t \tsel_info = read_selection(name, info=raw.info) \n\t \tassert_equal(sel, sel_info) \n\tall_ch = read_selection(['L', 'R']) \n\tleft = read_selection('L') \n\tright = read_selection('R') \n\tassert_true((len(all_ch) == (len(left) + len(right)))) \n\tassert_true((len(set(left).intersection(set(right))) == 0)) \n\tfrontal = read_selection('frontal') \n\toccipital = read_selection('Right-occipital') \n\tassert_true((len(set(frontal).intersection(set(occipital))) == 0)) \n\tch_names_new = [ch.replace(' \t', '') for ch in ch_names] \n\traw_new = read_raw_fif(raw_new_fname) \n\tfor (i, name) in enumerate(sel_names): \n\t \tsel = read_selection(name, info=raw_new.info) \n\t \tassert_true((ch_names_new[i] in sel)) \n\tassert_raises(TypeError, read_selection, name, info='foo')\n", 
" \tclf = GaussianNB(priors=np.array([0.25, 0.25, 0.25, 0.25])) \n\tassert_raises(ValueError, clf.fit, X, y)\n", 
" \traise ValueError\n", 
" \treturn (k, features.copy(), labels.copy())\n", 
" \traise ValueError\n", 
" \tdef is_abstract(c): \n\t \tif (not hasattr(c, '__abstractmethods__')): \n\t \t \treturn False \n\t \tif (not len(c.__abstractmethods__)): \n\t \t \treturn False \n\t \treturn True \n\tall_classes = [] \n\tpath = sklearn.__path__ \n\tfor (importer, modname, ispkg) in pkgutil.walk_packages(path=path, prefix='sklearn.', onerror=(lambda x: None)): \n\t \tif ('.tests.' in modname): \n\t \t \tcontinue \n\t \tmodule = __import__(modname, fromlist='dummy') \n\t \tclasses = inspect.getmembers(module, inspect.isclass) \n\t \tall_classes.extend(classes) \n\tall_classes = set(all_classes) \n\testimators = [c for c in all_classes if (issubclass(c[1], BaseEstimator) and (c[0] != 'BaseEstimator'))] \n\testimators = [c for c in estimators if (not is_abstract(c[1]))] \n\tif (not include_dont_test): \n\t \testimators = [c for c in estimators if (not (c[0] in DONT_TEST))] \n\tif (not include_other): \n\t \testimators = [c for c in estimators if (not (c[0] in OTHER))] \n\tif (not include_meta_estimators): \n\t \testimators = [c for c in estimators if (not (c[0] in META_ESTIMATORS))] \n\tif (type_filter is not None): \n\t \tif (not isinstance(type_filter, list)): \n\t \t \ttype_filter = [type_filter] \n\t \telse: \n\t \t \ttype_filter = list(type_filter) \n\t \tfiltered_estimators = [] \n\t \tfilters = {'classifier': ClassifierMixin, 'regressor': RegressorMixin, 'transformer': TransformerMixin, 'cluster': ClusterMixin} \n\t \tfor (name, mixin) in filters.items(): \n\t \t \tif (name in type_filter): \n\t \t \t \ttype_filter.remove(name) \n\t \t \t \tfiltered_estimators.extend([est for est in estimators if issubclass(est[1], mixin)]) \n\t \testimators = filtered_estimators \n\t \tif type_filter: \n\t \t \traise ValueError((\"Parameter \ttype_filter \tmust \tbe \t'classifier', \t'regressor', \t'transformer', \t'cluster' \tor \tNone, \tgot \t%s.\" % repr(type_filter))) \n\treturn sorted(set(estimators), key=itemgetter(0))\n", 
" \treturn ChecklistModule(mpstate)\n", 
" \tif (not isinstance(html, _strings)): \n\t \traise TypeError('string \trequired') \n\taccept_leading_text = bool(create_parent) \n\telements = fragments_fromstring(html, guess_charset=guess_charset, parser=parser, no_leading_text=(not accept_leading_text)) \n\tif create_parent: \n\t \tif (not isinstance(create_parent, _strings)): \n\t \t \tcreate_parent = 'div' \n\t \tnew_root = Element(create_parent) \n\t \tif elements: \n\t \t \tif isinstance(elements[0], _strings): \n\t \t \t \tnew_root.text = elements[0] \n\t \t \t \tdel elements[0] \n\t \t \tnew_root.extend(elements) \n\t \treturn new_root \n\tif (not elements): \n\t \traise etree.ParserError('No \telements \tfound') \n\tif (len(elements) > 1): \n\t \traise etree.ParserError('Multiple \telements \tfound') \n\tresult = elements[0] \n\tif (result.tail and result.tail.strip()): \n\t \traise etree.ParserError(('Element \tfollowed \tby \ttext: \t%r' % result.tail)) \n\tresult.tail = None \n\treturn result\n", 
" \trng = np.random.RandomState(42) \n\tt = rng.randn(100) \n\tedges = bayesian_blocks(t, fitness=u'events') \n\tassert_allclose(edges, [(-2.6197451), (-0.71094865), 0.36866702, 1.85227818]) \n\tt[80:] = t[:20] \n\tedges = bayesian_blocks(t, fitness=u'events', p0=0.01) \n\tassert_allclose(edges, [(-2.6197451), (-0.47432431), (-0.46202823), 1.85227818]) \n\tdt = 0.01 \n\tt = (dt * np.arange(1000)) \n\tx = np.zeros(len(t)) \n\tN = (len(t) // 10) \n\tx[rng.randint(0, len(t), N)] = 1 \n\tx[rng.randint(0, (len(t) // 2), N)] = 1 \n\tedges = bayesian_blocks(t, x, fitness=u'regular_events', dt=dt) \n\tassert_allclose(edges, [0, 5.105, 9.99]) \n\tt = (100 * rng.rand(20)) \n\tx = np.exp(((-0.5) * ((t - 50) ** 2))) \n\tsigma = 0.1 \n\tx_obs = (x + (sigma * rng.randn(len(x)))) \n\tedges = bayesian_blocks(t, x_obs, sigma, fitness=u'measures') \n\tassert_allclose(edges, [4.360377, 48.456895, 52.597917, 99.455051])\n", 
" \traise ValueError\n", 
" \tamp = 1e-08 \n\ttempdir = _TempDir() \n\trng = np.random.RandomState(0) \n\tfname_dtemp = op.join(tempdir, 'test.dip') \n\tfname_sim = op.join(tempdir, 'test-ave.fif') \n\tfwd = convert_forward_solution(read_forward_solution(fname_fwd), surf_ori=False, force_fixed=True) \n\tevoked = read_evokeds(fname_evo)[0] \n\tcov = read_cov(fname_cov) \n\tn_per_hemi = 5 \n\tvertices = [np.sort(rng.permutation(s['vertno'])[:n_per_hemi]) for s in fwd['src']] \n\tnv = sum((len(v) for v in vertices)) \n\tstc = SourceEstimate((amp * np.eye(nv)), vertices, 0, 0.001) \n\tevoked = simulate_evoked(fwd, stc, evoked.info, cov, snr=20, random_state=rng) \n\tpicks = np.sort(np.concatenate([pick_types(evoked.info, meg=True, eeg=False)[::2], pick_types(evoked.info, meg=False, eeg=True)[::2]])) \n\tevoked.pick_channels([evoked.ch_names[p] for p in picks]) \n\tevoked.add_proj(make_eeg_average_ref_proj(evoked.info)) \n\twrite_evokeds(fname_sim, evoked) \n\trun_subprocess(['mne_dipole_fit', '--meas', fname_sim, '--meg', '--eeg', '--noise', fname_cov, '--dip', fname_dtemp, '--mri', fname_fwd, '--reg', '0', '--tmin', '0']) \n\tdip_c = read_dipole(fname_dtemp) \n\tsphere = make_sphere_model(head_radius=0.1) \n\t(dip, residuals) = fit_dipole(evoked, fname_cov, sphere, fname_fwd) \n\tdata_rms = np.sqrt(np.sum((evoked.data ** 2), axis=0)) \n\tresi_rms = np.sqrt(np.sum((residuals ** 2), axis=0)) \n\tfactor = 1.0 \n\tif ((os.getenv('TRAVIS', 'false') == 'true') and (sys.version[:3] in ('3.5', '2.7'))): \n\t \tfactor = 0.8 \n\tassert_true((data_rms > (factor * resi_rms)).all(), msg=('%s \t(factor: \t%s)' % ((data_rms / resi_rms).min(), factor))) \n\ttransform_surface_to(fwd['src'][0], 'head', fwd['mri_head_t']) \n\ttransform_surface_to(fwd['src'][1], 'head', fwd['mri_head_t']) \n\tsrc_rr = np.concatenate([s['rr'][v] for (s, v) in zip(fwd['src'], vertices)], axis=0) \n\tsrc_nn = np.concatenate([s['nn'][v] for (s, v) in zip(fwd['src'], vertices)], axis=0) \n\tdip.crop(dip_c.times[0], dip_c.times[(-1)]) \n\t(src_rr, src_nn) = (src_rr[:(-1)], src_nn[:(-1)]) \n\t(corrs, dists, gc_dists, amp_errs, gofs) = ([], [], [], [], []) \n\tfor d in (dip_c, dip): \n\t \tnew = d.pos \n\t \tdiffs = (new - src_rr) \n\t \tcorrs += [np.corrcoef(src_rr.ravel(), new.ravel())[(0, 1)]] \n\t \tdists += [np.sqrt(np.mean(np.sum((diffs * diffs), axis=1)))] \n\t \tgc_dists += [((180 / np.pi) * np.mean(np.arccos(np.sum((src_nn * d.ori), axis=1))))] \n\t \tamp_errs += [np.sqrt(np.mean(((amp - d.amplitude) ** 2)))] \n\t \tgofs += [np.mean(d.gof)] \n\tassert_true((dists[0] >= (dists[1] * factor)), ('dists: \t%s' % dists)) \n\tassert_true((corrs[0] <= (corrs[1] / factor)), ('corrs: \t%s' % corrs)) \n\tassert_true((gc_dists[0] >= (gc_dists[1] * factor)), ('gc-dists \t(ori): \t%s' % gc_dists)) \n\tassert_true((amp_errs[0] >= (amp_errs[1] * factor)), ('amplitude \terrors: \t%s' % amp_errs)) \n\tassert_true((gofs[0] <= (gofs[1] / factor)), ('gof: \t%s' % gofs))\n", 
" \tif (scope is None): \n\t \tscope = ('https://www.googleapis.com/auth/' + name) \n\tparent_parsers = [tools.argparser] \n\tparent_parsers.extend(parents) \n\tparser = argparse.ArgumentParser(description=doc, formatter_class=argparse.RawDescriptionHelpFormatter, parents=parent_parsers) \n\tflags = parser.parse_args(argv[1:]) \n\tclient_secrets = os.path.join(os.path.dirname(filename), 'client_secrets.json') \n\tflow = client.flow_from_clientsecrets(client_secrets, scope=scope, message=tools.message_if_missing(client_secrets)) \n\tstorage = file.Storage((name + '.dat')) \n\tcredentials = storage.get() \n\tif ((credentials is None) or credentials.invalid): \n\t \tcredentials = tools.run_flow(flow, storage, flags) \n\thttp = credentials.authorize(http=build_http()) \n\tif (discovery_filename is None): \n\t \tservice = discovery.build(name, version, http=http) \n\telse: \n\t \twith open(discovery_filename) as discovery_file: \n\t \t \tservice = discovery.build_from_document(discovery_file.read(), base='https://www.googleapis.com/', http=http) \n\treturn (service, flags)\n", 
" \tclf = GaussianNB(priors=np.array([0.25, 0.25, 0.25, 0.25])) \n\tassert_raises(ValueError, clf.fit, X, y)\n", 
" \tdef g(target, *args, **kwargs): \n\t \tval = func(target, *args, **kwargs) \n\t \tif val: \n\t \t \treturn val \n\t \telse: \n\t \t \traise exception_class((value_error_message_template % target)) \n\tg.__name__ = func.__name__ \n\tg.__doc__ = func.__doc__ \n\treturn g\n", 
" \tdef is_abstract(c): \n\t \tif (not hasattr(c, '__abstractmethods__')): \n\t \t \treturn False \n\t \tif (not len(c.__abstractmethods__)): \n\t \t \treturn False \n\t \treturn True \n\tall_classes = [] \n\tpath = sklearn.__path__ \n\tfor (importer, modname, ispkg) in pkgutil.walk_packages(path=path, prefix='sklearn.', onerror=(lambda x: None)): \n\t \tif ('.tests.' in modname): \n\t \t \tcontinue \n\t \tmodule = __import__(modname, fromlist='dummy') \n\t \tclasses = inspect.getmembers(module, inspect.isclass) \n\t \tall_classes.extend(classes) \n\tall_classes = set(all_classes) \n\testimators = [c for c in all_classes if (issubclass(c[1], BaseEstimator) and (c[0] != 'BaseEstimator'))] \n\testimators = [c for c in estimators if (not is_abstract(c[1]))] \n\tif (not include_dont_test): \n\t \testimators = [c for c in estimators if (not (c[0] in DONT_TEST))] \n\tif (not include_other): \n\t \testimators = [c for c in estimators if (not (c[0] in OTHER))] \n\tif (not include_meta_estimators): \n\t \testimators = [c for c in estimators if (not (c[0] in META_ESTIMATORS))] \n\tif (type_filter is not None): \n\t \tif (not isinstance(type_filter, list)): \n\t \t \ttype_filter = [type_filter] \n\t \telse: \n\t \t \ttype_filter = list(type_filter) \n\t \tfiltered_estimators = [] \n\t \tfilters = {'classifier': ClassifierMixin, 'regressor': RegressorMixin, 'transformer': TransformerMixin, 'cluster': ClusterMixin} \n\t \tfor (name, mixin) in filters.items(): \n\t \t \tif (name in type_filter): \n\t \t \t \ttype_filter.remove(name) \n\t \t \t \tfiltered_estimators.extend([est for est in estimators if issubclass(est[1], mixin)]) \n\t \testimators = filtered_estimators \n\t \tif type_filter: \n\t \t \traise ValueError((\"Parameter \ttype_filter \tmust \tbe \t'classifier', \t'regressor', \t'transformer', \t'cluster' \tor \tNone, \tgot \t%s.\" % repr(type_filter))) \n\treturn sorted(set(estimators), key=itemgetter(0))\n", 
" \treturn ChecklistModule(mpstate)\n", 
" \tif (not isinstance(html, _strings)): \n\t \traise TypeError('string \trequired') \n\taccept_leading_text = bool(create_parent) \n\telements = fragments_fromstring(html, guess_charset=guess_charset, parser=parser, no_leading_text=(not accept_leading_text)) \n\tif create_parent: \n\t \tif (not isinstance(create_parent, _strings)): \n\t \t \tcreate_parent = 'div' \n\t \tnew_root = Element(create_parent) \n\t \tif elements: \n\t \t \tif isinstance(elements[0], _strings): \n\t \t \t \tnew_root.text = elements[0] \n\t \t \t \tdel elements[0] \n\t \t \tnew_root.extend(elements) \n\t \treturn new_root \n\tif (not elements): \n\t \traise etree.ParserError('No \telements \tfound') \n\tif (len(elements) > 1): \n\t \traise etree.ParserError('Multiple \telements \tfound') \n\tresult = elements[0] \n\tif (result.tail and result.tail.strip()): \n\t \traise etree.ParserError(('Element \tfollowed \tby \ttext: \t%r' % result.tail)) \n\tresult.tail = None \n\treturn result\n", 
" \trng = np.random.RandomState(42) \n\tt = rng.randn(100) \n\tedges = bayesian_blocks(t, fitness=u'events') \n\tassert_allclose(edges, [(-2.6197451), (-0.71094865), 0.36866702, 1.85227818]) \n\tt[80:] = t[:20] \n\tedges = bayesian_blocks(t, fitness=u'events', p0=0.01) \n\tassert_allclose(edges, [(-2.6197451), (-0.47432431), (-0.46202823), 1.85227818]) \n\tdt = 0.01 \n\tt = (dt * np.arange(1000)) \n\tx = np.zeros(len(t)) \n\tN = (len(t) // 10) \n\tx[rng.randint(0, len(t), N)] = 1 \n\tx[rng.randint(0, (len(t) // 2), N)] = 1 \n\tedges = bayesian_blocks(t, x, fitness=u'regular_events', dt=dt) \n\tassert_allclose(edges, [0, 5.105, 9.99]) \n\tt = (100 * rng.rand(20)) \n\tx = np.exp(((-0.5) * ((t - 50) ** 2))) \n\tsigma = 0.1 \n\tx_obs = (x + (sigma * rng.randn(len(x)))) \n\tedges = bayesian_blocks(t, x_obs, sigma, fitness=u'measures') \n\tassert_allclose(edges, [4.360377, 48.456895, 52.597917, 99.455051])\n", 
" \traise ValueError\n", 
" \tamp = 1e-08 \n\ttempdir = _TempDir() \n\trng = np.random.RandomState(0) \n\tfname_dtemp = op.join(tempdir, 'test.dip') \n\tfname_sim = op.join(tempdir, 'test-ave.fif') \n\tfwd = convert_forward_solution(read_forward_solution(fname_fwd), surf_ori=False, force_fixed=True) \n\tevoked = read_evokeds(fname_evo)[0] \n\tcov = read_cov(fname_cov) \n\tn_per_hemi = 5 \n\tvertices = [np.sort(rng.permutation(s['vertno'])[:n_per_hemi]) for s in fwd['src']] \n\tnv = sum((len(v) for v in vertices)) \n\tstc = SourceEstimate((amp * np.eye(nv)), vertices, 0, 0.001) \n\tevoked = simulate_evoked(fwd, stc, evoked.info, cov, snr=20, random_state=rng) \n\tpicks = np.sort(np.concatenate([pick_types(evoked.info, meg=True, eeg=False)[::2], pick_types(evoked.info, meg=False, eeg=True)[::2]])) \n\tevoked.pick_channels([evoked.ch_names[p] for p in picks]) \n\tevoked.add_proj(make_eeg_average_ref_proj(evoked.info)) \n\twrite_evokeds(fname_sim, evoked) \n\trun_subprocess(['mne_dipole_fit', '--meas', fname_sim, '--meg', '--eeg', '--noise', fname_cov, '--dip', fname_dtemp, '--mri', fname_fwd, '--reg', '0', '--tmin', '0']) \n\tdip_c = read_dipole(fname_dtemp) \n\tsphere = make_sphere_model(head_radius=0.1) \n\t(dip, residuals) = fit_dipole(evoked, fname_cov, sphere, fname_fwd) \n\tdata_rms = np.sqrt(np.sum((evoked.data ** 2), axis=0)) \n\tresi_rms = np.sqrt(np.sum((residuals ** 2), axis=0)) \n\tfactor = 1.0 \n\tif ((os.getenv('TRAVIS', 'false') == 'true') and (sys.version[:3] in ('3.5', '2.7'))): \n\t \tfactor = 0.8 \n\tassert_true((data_rms > (factor * resi_rms)).all(), msg=('%s \t(factor: \t%s)' % ((data_rms / resi_rms).min(), factor))) \n\ttransform_surface_to(fwd['src'][0], 'head', fwd['mri_head_t']) \n\ttransform_surface_to(fwd['src'][1], 'head', fwd['mri_head_t']) \n\tsrc_rr = np.concatenate([s['rr'][v] for (s, v) in zip(fwd['src'], vertices)], axis=0) \n\tsrc_nn = np.concatenate([s['nn'][v] for (s, v) in zip(fwd['src'], vertices)], axis=0) \n\tdip.crop(dip_c.times[0], dip_c.times[(-1)]) \n\t(src_rr, src_nn) = (src_rr[:(-1)], src_nn[:(-1)]) \n\t(corrs, dists, gc_dists, amp_errs, gofs) = ([], [], [], [], []) \n\tfor d in (dip_c, dip): \n\t \tnew = d.pos \n\t \tdiffs = (new - src_rr) \n\t \tcorrs += [np.corrcoef(src_rr.ravel(), new.ravel())[(0, 1)]] \n\t \tdists += [np.sqrt(np.mean(np.sum((diffs * diffs), axis=1)))] \n\t \tgc_dists += [((180 / np.pi) * np.mean(np.arccos(np.sum((src_nn * d.ori), axis=1))))] \n\t \tamp_errs += [np.sqrt(np.mean(((amp - d.amplitude) ** 2)))] \n\t \tgofs += [np.mean(d.gof)] \n\tassert_true((dists[0] >= (dists[1] * factor)), ('dists: \t%s' % dists)) \n\tassert_true((corrs[0] <= (corrs[1] / factor)), ('corrs: \t%s' % corrs)) \n\tassert_true((gc_dists[0] >= (gc_dists[1] * factor)), ('gc-dists \t(ori): \t%s' % gc_dists)) \n\tassert_true((amp_errs[0] >= (amp_errs[1] * factor)), ('amplitude \terrors: \t%s' % amp_errs)) \n\tassert_true((gofs[0] <= (gofs[1] / factor)), ('gof: \t%s' % gofs))\n", 
" \tif (scope is None): \n\t \tscope = ('https://www.googleapis.com/auth/' + name) \n\tparent_parsers = [tools.argparser] \n\tparent_parsers.extend(parents) \n\tparser = argparse.ArgumentParser(description=doc, formatter_class=argparse.RawDescriptionHelpFormatter, parents=parent_parsers) \n\tflags = parser.parse_args(argv[1:]) \n\tclient_secrets = os.path.join(os.path.dirname(filename), 'client_secrets.json') \n\tflow = client.flow_from_clientsecrets(client_secrets, scope=scope, message=tools.message_if_missing(client_secrets)) \n\tstorage = file.Storage((name + '.dat')) \n\tcredentials = storage.get() \n\tif ((credentials is None) or credentials.invalid): \n\t \tcredentials = tools.run_flow(flow, storage, flags) \n\thttp = credentials.authorize(http=build_http()) \n\tif (discovery_filename is None): \n\t \tservice = discovery.build(name, version, http=http) \n\telse: \n\t \twith open(discovery_filename) as discovery_file: \n\t \t \tservice = discovery.build_from_document(discovery_file.read(), base='https://www.googleapis.com/', http=http) \n\treturn (service, flags)\n", 
" \tch_names = ['MEG \t2211', 'MEG \t0223', 'MEG \t1312', 'MEG \t0412', 'MEG \t1043', 'MEG \t2042', 'MEG \t2032', 'MEG \t0522', 'MEG \t1031'] \n\tsel_names = ['Vertex', 'Left-temporal', 'Right-temporal', 'Left-parietal', 'Right-parietal', 'Left-occipital', 'Right-occipital', 'Left-frontal', 'Right-frontal'] \n\traw = read_raw_fif(raw_fname) \n\tfor (i, name) in enumerate(sel_names): \n\t \tsel = read_selection(name) \n\t \tassert_true((ch_names[i] in sel)) \n\t \tsel_info = read_selection(name, info=raw.info) \n\t \tassert_equal(sel, sel_info) \n\tall_ch = read_selection(['L', 'R']) \n\tleft = read_selection('L') \n\tright = read_selection('R') \n\tassert_true((len(all_ch) == (len(left) + len(right)))) \n\tassert_true((len(set(left).intersection(set(right))) == 0)) \n\tfrontal = read_selection('frontal') \n\toccipital = read_selection('Right-occipital') \n\tassert_true((len(set(frontal).intersection(set(occipital))) == 0)) \n\tch_names_new = [ch.replace(' \t', '') for ch in ch_names] \n\traw_new = read_raw_fif(raw_new_fname) \n\tfor (i, name) in enumerate(sel_names): \n\t \tsel = read_selection(name, info=raw_new.info) \n\t \tassert_true((ch_names_new[i] in sel)) \n\tassert_raises(TypeError, read_selection, name, info='foo')\n", 
" \tclf = GaussianNB(priors=np.array([0.25, 0.25, 0.25, 0.25])) \n\tassert_raises(ValueError, clf.fit, X, y)\n", 
" \traise ValueError\n", 
" \treturn (k, features.copy(), labels.copy())\n", 
" \traise ValueError\n", 
" \tdef is_abstract(c): \n\t \tif (not hasattr(c, '__abstractmethods__')): \n\t \t \treturn False \n\t \tif (not len(c.__abstractmethods__)): \n\t \t \treturn False \n\t \treturn True \n\tall_classes = [] \n\tpath = sklearn.__path__ \n\tfor (importer, modname, ispkg) in pkgutil.walk_packages(path=path, prefix='sklearn.', onerror=(lambda x: None)): \n\t \tif ('.tests.' in modname): \n\t \t \tcontinue \n\t \tmodule = __import__(modname, fromlist='dummy') \n\t \tclasses = inspect.getmembers(module, inspect.isclass) \n\t \tall_classes.extend(classes) \n\tall_classes = set(all_classes) \n\testimators = [c for c in all_classes if (issubclass(c[1], BaseEstimator) and (c[0] != 'BaseEstimator'))] \n\testimators = [c for c in estimators if (not is_abstract(c[1]))] \n\tif (not include_dont_test): \n\t \testimators = [c for c in estimators if (not (c[0] in DONT_TEST))] \n\tif (not include_other): \n\t \testimators = [c for c in estimators if (not (c[0] in OTHER))] \n\tif (not include_meta_estimators): \n\t \testimators = [c for c in estimators if (not (c[0] in META_ESTIMATORS))] \n\tif (type_filter is not None): \n\t \tif (not isinstance(type_filter, list)): \n\t \t \ttype_filter = [type_filter] \n\t \telse: \n\t \t \ttype_filter = list(type_filter) \n\t \tfiltered_estimators = [] \n\t \tfilters = {'classifier': ClassifierMixin, 'regressor': RegressorMixin, 'transformer': TransformerMixin, 'cluster': ClusterMixin} \n\t \tfor (name, mixin) in filters.items(): \n\t \t \tif (name in type_filter): \n\t \t \t \ttype_filter.remove(name) \n\t \t \t \tfiltered_estimators.extend([est for est in estimators if issubclass(est[1], mixin)]) \n\t \testimators = filtered_estimators \n\t \tif type_filter: \n\t \t \traise ValueError((\"Parameter \ttype_filter \tmust \tbe \t'classifier', \t'regressor', \t'transformer', \t'cluster' \tor \tNone, \tgot \t%s.\" % repr(type_filter))) \n\treturn sorted(set(estimators), key=itemgetter(0))\n", 
" \tif ((value is None) and empty_ok): \n\t \treturn \n\tif (not isinstance(value, (int, long))): \n\t \traise exception(('%s \tshould \tbe \tan \tinteger; \treceived \t%s \t(a \t%s).' % (name, value, typename(value)))) \n\tif ((not value) and (not zero_ok)): \n\t \traise exception(('%s \tmust \tnot \tbe \t0 \t(zero)' % name)) \n\tif ((value < 0) and (not negative_ok)): \n\t \traise exception(('%s \tmust \tnot \tbe \tnegative.' % name))\n", 
" \tsupported = ('\\x00', '\\x00', '\\x00', '\\x01') \n\ti = 0 \n\tfor item in version: \n\t \tif (version[i] != supported[i]): \n\t \t \traise RuntimeError('SFF \tversion \tnot \tsupported. \tPlease \tcontact \tthe \tauthor \tof \tthe \tsoftware.') \n\t \ti += 1\n", 
" \treturn ChecklistModule(mpstate)\n", 
" \tif (not isinstance(html, _strings)): \n\t \traise TypeError('string \trequired') \n\taccept_leading_text = bool(create_parent) \n\telements = fragments_fromstring(html, guess_charset=guess_charset, parser=parser, no_leading_text=(not accept_leading_text)) \n\tif create_parent: \n\t \tif (not isinstance(create_parent, _strings)): \n\t \t \tcreate_parent = 'div' \n\t \tnew_root = Element(create_parent) \n\t \tif elements: \n\t \t \tif isinstance(elements[0], _strings): \n\t \t \t \tnew_root.text = elements[0] \n\t \t \t \tdel elements[0] \n\t \t \tnew_root.extend(elements) \n\t \treturn new_root \n\tif (not elements): \n\t \traise etree.ParserError('No \telements \tfound') \n\tif (len(elements) > 1): \n\t \traise etree.ParserError('Multiple \telements \tfound') \n\tresult = elements[0] \n\tif (result.tail and result.tail.strip()): \n\t \traise etree.ParserError(('Element \tfollowed \tby \ttext: \t%r' % result.tail)) \n\tresult.tail = None \n\treturn result\n", 
" \traise ValueError\n", 
" \trng = np.random.RandomState(42) \n\tt = rng.randn(100) \n\tedges = bayesian_blocks(t, fitness=u'events') \n\tassert_allclose(edges, [(-2.6197451), (-0.71094865), 0.36866702, 1.85227818]) \n\tt[80:] = t[:20] \n\tedges = bayesian_blocks(t, fitness=u'events', p0=0.01) \n\tassert_allclose(edges, [(-2.6197451), (-0.47432431), (-0.46202823), 1.85227818]) \n\tdt = 0.01 \n\tt = (dt * np.arange(1000)) \n\tx = np.zeros(len(t)) \n\tN = (len(t) // 10) \n\tx[rng.randint(0, len(t), N)] = 1 \n\tx[rng.randint(0, (len(t) // 2), N)] = 1 \n\tedges = bayesian_blocks(t, x, fitness=u'regular_events', dt=dt) \n\tassert_allclose(edges, [0, 5.105, 9.99]) \n\tt = (100 * rng.rand(20)) \n\tx = np.exp(((-0.5) * ((t - 50) ** 2))) \n\tsigma = 0.1 \n\tx_obs = (x + (sigma * rng.randn(len(x)))) \n\tedges = bayesian_blocks(t, x_obs, sigma, fitness=u'measures') \n\tassert_allclose(edges, [4.360377, 48.456895, 52.597917, 99.455051])\n", 
" \traise ValueError\n", 
" \timport vispy.color.colormap as c \n\tassert_raises(AssertionError, c._glsl_step, [0.0, 1.0]) \n\tc._glsl_mix(controls=[0.0, 1.0]) \n\tc._glsl_mix(controls=[0.0, 0.25, 1.0]) \n\tfor fun in (c._glsl_step, c._glsl_mix): \n\t \tassert_raises(AssertionError, fun, controls=[0.1, 1.0]) \n\t \tassert_raises(AssertionError, fun, controls=[0.0, 0.9]) \n\t \tassert_raises(AssertionError, fun, controls=[0.1, 0.9]) \n\tcolor_0 = np.array([1.0, 0.0, 0.0]) \n\tcolor_1 = np.array([0.0, 1.0, 0.0]) \n\tcolor_2 = np.array([0.0, 0.0, 1.0]) \n\tcolors_00 = np.vstack((color_0, color_0)) \n\tcolors_01 = np.vstack((color_0, color_1)) \n\tcolors_11 = np.vstack((color_1, color_1)) \n\tcolors_021 = np.vstack((color_0, color_2, color_1)) \n\tcontrols_2 = np.array([0.0, 1.0]) \n\tcontrols_3 = np.array([0.0, 0.25, 1.0]) \n\tx = np.array([(-1.0), 0.0, 0.1, 0.4, 0.5, 0.6, 1.0, 2.0])[:, None] \n\tmixed_2 = c.mix(colors_01, x, controls_2) \n\tmixed_3 = c.mix(colors_021, x, controls_3) \n\tfor y in (mixed_2, mixed_3): \n\t \tassert_allclose(y[:2, :], colors_00) \n\t \tassert_allclose(y[(-2):, :], colors_11) \n\tassert_allclose(mixed_2[:, (-1)], np.zeros(len(y)))\n", 
" \tprobs0 = np.asarray(probs0, float) \n\tprobs1 = np.asarray(probs1, float) \n\tprobs0 = (probs0 / probs0.sum(axis)) \n\tprobs1 = (probs1 / probs1.sum(axis)) \n\td2 = (((probs1 - probs0) ** 2) / probs0).sum(axis) \n\tif (correction is not None): \n\t \t(nobs, df) = correction \n\t \tdiff = ((probs1 - probs0) / probs0).sum(axis) \n\t \td2 = np.maximum(((((d2 * nobs) - diff) - df) / (nobs - 1.0)), 0) \n\tif cohen: \n\t \treturn np.sqrt(d2) \n\telse: \n\t \treturn d2\n", 
" \treturn Gaussian()(mean, ln_var)\n", 
" \tclf = GaussianNB(priors=np.array([0.25, 0.25, 0.25, 0.25])) \n\tassert_raises(ValueError, clf.fit, X, y)\n", 
" \traise ValueError\n", 
" \treturn (k, features.copy(), labels.copy())\n", 
" \traise ValueError\n", 
" \tdef is_abstract(c): \n\t \tif (not hasattr(c, '__abstractmethods__')): \n\t \t \treturn False \n\t \tif (not len(c.__abstractmethods__)): \n\t \t \treturn False \n\t \treturn True \n\tall_classes = [] \n\tpath = sklearn.__path__ \n\tfor (importer, modname, ispkg) in pkgutil.walk_packages(path=path, prefix='sklearn.', onerror=(lambda x: None)): \n\t \tif ('.tests.' in modname): \n\t \t \tcontinue \n\t \tmodule = __import__(modname, fromlist='dummy') \n\t \tclasses = inspect.getmembers(module, inspect.isclass) \n\t \tall_classes.extend(classes) \n\tall_classes = set(all_classes) \n\testimators = [c for c in all_classes if (issubclass(c[1], BaseEstimator) and (c[0] != 'BaseEstimator'))] \n\testimators = [c for c in estimators if (not is_abstract(c[1]))] \n\tif (not include_dont_test): \n\t \testimators = [c for c in estimators if (not (c[0] in DONT_TEST))] \n\tif (not include_other): \n\t \testimators = [c for c in estimators if (not (c[0] in OTHER))] \n\tif (not include_meta_estimators): \n\t \testimators = [c for c in estimators if (not (c[0] in META_ESTIMATORS))] \n\tif (type_filter is not None): \n\t \tif (not isinstance(type_filter, list)): \n\t \t \ttype_filter = [type_filter] \n\t \telse: \n\t \t \ttype_filter = list(type_filter) \n\t \tfiltered_estimators = [] \n\t \tfilters = {'classifier': ClassifierMixin, 'regressor': RegressorMixin, 'transformer': TransformerMixin, 'cluster': ClusterMixin} \n\t \tfor (name, mixin) in filters.items(): \n\t \t \tif (name in type_filter): \n\t \t \t \ttype_filter.remove(name) \n\t \t \t \tfiltered_estimators.extend([est for est in estimators if issubclass(est[1], mixin)]) \n\t \testimators = filtered_estimators \n\t \tif type_filter: \n\t \t \traise ValueError((\"Parameter \ttype_filter \tmust \tbe \t'classifier', \t'regressor', \t'transformer', \t'cluster' \tor \tNone, \tgot \t%s.\" % repr(type_filter))) \n\treturn sorted(set(estimators), key=itemgetter(0))\n", 
" \treturn ChecklistModule(mpstate)\n", 
" \tif (not isinstance(html, _strings)): \n\t \traise TypeError('string \trequired') \n\taccept_leading_text = bool(create_parent) \n\telements = fragments_fromstring(html, guess_charset=guess_charset, parser=parser, no_leading_text=(not accept_leading_text)) \n\tif create_parent: \n\t \tif (not isinstance(create_parent, _strings)): \n\t \t \tcreate_parent = 'div' \n\t \tnew_root = Element(create_parent) \n\t \tif elements: \n\t \t \tif isinstance(elements[0], _strings): \n\t \t \t \tnew_root.text = elements[0] \n\t \t \t \tdel elements[0] \n\t \t \tnew_root.extend(elements) \n\t \treturn new_root \n\tif (not elements): \n\t \traise etree.ParserError('No \telements \tfound') \n\tif (len(elements) > 1): \n\t \traise etree.ParserError('Multiple \telements \tfound') \n\tresult = elements[0] \n\tif (result.tail and result.tail.strip()): \n\t \traise etree.ParserError(('Element \tfollowed \tby \ttext: \t%r' % result.tail)) \n\tresult.tail = None \n\treturn result\n", 
" \trng = np.random.RandomState(42) \n\tt = rng.randn(100) \n\tedges = bayesian_blocks(t, fitness=u'events') \n\tassert_allclose(edges, [(-2.6197451), (-0.71094865), 0.36866702, 1.85227818]) \n\tt[80:] = t[:20] \n\tedges = bayesian_blocks(t, fitness=u'events', p0=0.01) \n\tassert_allclose(edges, [(-2.6197451), (-0.47432431), (-0.46202823), 1.85227818]) \n\tdt = 0.01 \n\tt = (dt * np.arange(1000)) \n\tx = np.zeros(len(t)) \n\tN = (len(t) // 10) \n\tx[rng.randint(0, len(t), N)] = 1 \n\tx[rng.randint(0, (len(t) // 2), N)] = 1 \n\tedges = bayesian_blocks(t, x, fitness=u'regular_events', dt=dt) \n\tassert_allclose(edges, [0, 5.105, 9.99]) \n\tt = (100 * rng.rand(20)) \n\tx = np.exp(((-0.5) * ((t - 50) ** 2))) \n\tsigma = 0.1 \n\tx_obs = (x + (sigma * rng.randn(len(x)))) \n\tedges = bayesian_blocks(t, x_obs, sigma, fitness=u'measures') \n\tassert_allclose(edges, [4.360377, 48.456895, 52.597917, 99.455051])\n", 
" \traise ValueError\n", 
" \tamp = 1e-08 \n\ttempdir = _TempDir() \n\trng = np.random.RandomState(0) \n\tfname_dtemp = op.join(tempdir, 'test.dip') \n\tfname_sim = op.join(tempdir, 'test-ave.fif') \n\tfwd = convert_forward_solution(read_forward_solution(fname_fwd), surf_ori=False, force_fixed=True) \n\tevoked = read_evokeds(fname_evo)[0] \n\tcov = read_cov(fname_cov) \n\tn_per_hemi = 5 \n\tvertices = [np.sort(rng.permutation(s['vertno'])[:n_per_hemi]) for s in fwd['src']] \n\tnv = sum((len(v) for v in vertices)) \n\tstc = SourceEstimate((amp * np.eye(nv)), vertices, 0, 0.001) \n\tevoked = simulate_evoked(fwd, stc, evoked.info, cov, snr=20, random_state=rng) \n\tpicks = np.sort(np.concatenate([pick_types(evoked.info, meg=True, eeg=False)[::2], pick_types(evoked.info, meg=False, eeg=True)[::2]])) \n\tevoked.pick_channels([evoked.ch_names[p] for p in picks]) \n\tevoked.add_proj(make_eeg_average_ref_proj(evoked.info)) \n\twrite_evokeds(fname_sim, evoked) \n\trun_subprocess(['mne_dipole_fit', '--meas', fname_sim, '--meg', '--eeg', '--noise', fname_cov, '--dip', fname_dtemp, '--mri', fname_fwd, '--reg', '0', '--tmin', '0']) \n\tdip_c = read_dipole(fname_dtemp) \n\tsphere = make_sphere_model(head_radius=0.1) \n\t(dip, residuals) = fit_dipole(evoked, fname_cov, sphere, fname_fwd) \n\tdata_rms = np.sqrt(np.sum((evoked.data ** 2), axis=0)) \n\tresi_rms = np.sqrt(np.sum((residuals ** 2), axis=0)) \n\tfactor = 1.0 \n\tif ((os.getenv('TRAVIS', 'false') == 'true') and (sys.version[:3] in ('3.5', '2.7'))): \n\t \tfactor = 0.8 \n\tassert_true((data_rms > (factor * resi_rms)).all(), msg=('%s \t(factor: \t%s)' % ((data_rms / resi_rms).min(), factor))) \n\ttransform_surface_to(fwd['src'][0], 'head', fwd['mri_head_t']) \n\ttransform_surface_to(fwd['src'][1], 'head', fwd['mri_head_t']) \n\tsrc_rr = np.concatenate([s['rr'][v] for (s, v) in zip(fwd['src'], vertices)], axis=0) \n\tsrc_nn = np.concatenate([s['nn'][v] for (s, v) in zip(fwd['src'], vertices)], axis=0) \n\tdip.crop(dip_c.times[0], dip_c.times[(-1)]) \n\t(src_rr, src_nn) = (src_rr[:(-1)], src_nn[:(-1)]) \n\t(corrs, dists, gc_dists, amp_errs, gofs) = ([], [], [], [], []) \n\tfor d in (dip_c, dip): \n\t \tnew = d.pos \n\t \tdiffs = (new - src_rr) \n\t \tcorrs += [np.corrcoef(src_rr.ravel(), new.ravel())[(0, 1)]] \n\t \tdists += [np.sqrt(np.mean(np.sum((diffs * diffs), axis=1)))] \n\t \tgc_dists += [((180 / np.pi) * np.mean(np.arccos(np.sum((src_nn * d.ori), axis=1))))] \n\t \tamp_errs += [np.sqrt(np.mean(((amp - d.amplitude) ** 2)))] \n\t \tgofs += [np.mean(d.gof)] \n\tassert_true((dists[0] >= (dists[1] * factor)), ('dists: \t%s' % dists)) \n\tassert_true((corrs[0] <= (corrs[1] / factor)), ('corrs: \t%s' % corrs)) \n\tassert_true((gc_dists[0] >= (gc_dists[1] * factor)), ('gc-dists \t(ori): \t%s' % gc_dists)) \n\tassert_true((amp_errs[0] >= (amp_errs[1] * factor)), ('amplitude \terrors: \t%s' % amp_errs)) \n\tassert_true((gofs[0] <= (gofs[1] / factor)), ('gof: \t%s' % gofs))\n", 
" \tif (scope is None): \n\t \tscope = ('https://www.googleapis.com/auth/' + name) \n\tparent_parsers = [tools.argparser] \n\tparent_parsers.extend(parents) \n\tparser = argparse.ArgumentParser(description=doc, formatter_class=argparse.RawDescriptionHelpFormatter, parents=parent_parsers) \n\tflags = parser.parse_args(argv[1:]) \n\tclient_secrets = os.path.join(os.path.dirname(filename), 'client_secrets.json') \n\tflow = client.flow_from_clientsecrets(client_secrets, scope=scope, message=tools.message_if_missing(client_secrets)) \n\tstorage = file.Storage((name + '.dat')) \n\tcredentials = storage.get() \n\tif ((credentials is None) or credentials.invalid): \n\t \tcredentials = tools.run_flow(flow, storage, flags) \n\thttp = credentials.authorize(http=build_http()) \n\tif (discovery_filename is None): \n\t \tservice = discovery.build(name, version, http=http) \n\telse: \n\t \twith open(discovery_filename) as discovery_file: \n\t \t \tservice = discovery.build_from_document(discovery_file.read(), base='https://www.googleapis.com/', http=http) \n\treturn (service, flags)\n", 
" \tclf = GaussianNB(priors=np.array([0.25, 0.25, 0.25, 0.25])) \n\tassert_raises(ValueError, clf.fit, X, y)\n", 
" \tdef g(target, *args, **kwargs): \n\t \tval = func(target, *args, **kwargs) \n\t \tif val: \n\t \t \treturn val \n\t \telse: \n\t \t \traise exception_class((value_error_message_template % target)) \n\tg.__name__ = func.__name__ \n\tg.__doc__ = func.__doc__ \n\treturn g\n", 
" \trng = np.random.RandomState(42) \n\tt = rng.randn(100) \n\tedges = bayesian_blocks(t, fitness=u'events') \n\tassert_allclose(edges, [(-2.6197451), (-0.71094865), 0.36866702, 1.85227818]) \n\tt[80:] = t[:20] \n\tedges = bayesian_blocks(t, fitness=u'events', p0=0.01) \n\tassert_allclose(edges, [(-2.6197451), (-0.47432431), (-0.46202823), 1.85227818]) \n\tdt = 0.01 \n\tt = (dt * np.arange(1000)) \n\tx = np.zeros(len(t)) \n\tN = (len(t) // 10) \n\tx[rng.randint(0, len(t), N)] = 1 \n\tx[rng.randint(0, (len(t) // 2), N)] = 1 \n\tedges = bayesian_blocks(t, x, fitness=u'regular_events', dt=dt) \n\tassert_allclose(edges, [0, 5.105, 9.99]) \n\tt = (100 * rng.rand(20)) \n\tx = np.exp(((-0.5) * ((t - 50) ** 2))) \n\tsigma = 0.1 \n\tx_obs = (x + (sigma * rng.randn(len(x)))) \n\tedges = bayesian_blocks(t, x_obs, sigma, fitness=u'measures') \n\tassert_allclose(edges, [4.360377, 48.456895, 52.597917, 99.455051])\n", 
" \traise ValueError\n", 
" \treturn Pipeline(_name_estimators(steps))\n", 
" \tdef is_abstract(c): \n\t \tif (not hasattr(c, '__abstractmethods__')): \n\t \t \treturn False \n\t \tif (not len(c.__abstractmethods__)): \n\t \t \treturn False \n\t \treturn True \n\tall_classes = [] \n\tpath = sklearn.__path__ \n\tfor (importer, modname, ispkg) in pkgutil.walk_packages(path=path, prefix='sklearn.', onerror=(lambda x: None)): \n\t \tif ('.tests.' in modname): \n\t \t \tcontinue \n\t \tmodule = __import__(modname, fromlist='dummy') \n\t \tclasses = inspect.getmembers(module, inspect.isclass) \n\t \tall_classes.extend(classes) \n\tall_classes = set(all_classes) \n\testimators = [c for c in all_classes if (issubclass(c[1], BaseEstimator) and (c[0] != 'BaseEstimator'))] \n\testimators = [c for c in estimators if (not is_abstract(c[1]))] \n\tif (not include_dont_test): \n\t \testimators = [c for c in estimators if (not (c[0] in DONT_TEST))] \n\tif (not include_other): \n\t \testimators = [c for c in estimators if (not (c[0] in OTHER))] \n\tif (not include_meta_estimators): \n\t \testimators = [c for c in estimators if (not (c[0] in META_ESTIMATORS))] \n\tif (type_filter is not None): \n\t \tif (not isinstance(type_filter, list)): \n\t \t \ttype_filter = [type_filter] \n\t \telse: \n\t \t \ttype_filter = list(type_filter) \n\t \tfiltered_estimators = [] \n\t \tfilters = {'classifier': ClassifierMixin, 'regressor': RegressorMixin, 'transformer': TransformerMixin, 'cluster': ClusterMixin} \n\t \tfor (name, mixin) in filters.items(): \n\t \t \tif (name in type_filter): \n\t \t \t \ttype_filter.remove(name) \n\t \t \t \tfiltered_estimators.extend([est for est in estimators if issubclass(est[1], mixin)]) \n\t \testimators = filtered_estimators \n\t \tif type_filter: \n\t \t \traise ValueError((\"Parameter \ttype_filter \tmust \tbe \t'classifier', \t'regressor', \t'transformer', \t'cluster' \tor \tNone, \tgot \t%s.\" % repr(type_filter))) \n\treturn sorted(set(estimators), key=itemgetter(0))\n", 
" \tif ((value is None) and empty_ok): \n\t \treturn \n\tif (not isinstance(value, (int, long))): \n\t \traise exception(('%s \tshould \tbe \tan \tinteger; \treceived \t%s \t(a \t%s).' % (name, value, typename(value)))) \n\tif ((not value) and (not zero_ok)): \n\t \traise exception(('%s \tmust \tnot \tbe \t0 \t(zero)' % name)) \n\tif ((value < 0) and (not negative_ok)): \n\t \traise exception(('%s \tmust \tnot \tbe \tnegative.' % name))\n", 
" \treturn ChecklistModule(mpstate)\n", 
" \tif (not isinstance(html, _strings)): \n\t \traise TypeError('string \trequired') \n\taccept_leading_text = bool(create_parent) \n\telements = fragments_fromstring(html, guess_charset=guess_charset, parser=parser, no_leading_text=(not accept_leading_text)) \n\tif create_parent: \n\t \tif (not isinstance(create_parent, _strings)): \n\t \t \tcreate_parent = 'div' \n\t \tnew_root = Element(create_parent) \n\t \tif elements: \n\t \t \tif isinstance(elements[0], _strings): \n\t \t \t \tnew_root.text = elements[0] \n\t \t \t \tdel elements[0] \n\t \t \tnew_root.extend(elements) \n\t \treturn new_root \n\tif (not elements): \n\t \traise etree.ParserError('No \telements \tfound') \n\tif (len(elements) > 1): \n\t \traise etree.ParserError('Multiple \telements \tfound') \n\tresult = elements[0] \n\tif (result.tail and result.tail.strip()): \n\t \traise etree.ParserError(('Element \tfollowed \tby \ttext: \t%r' % result.tail)) \n\tresult.tail = None \n\treturn result\n", 
" \traise ValueError\n", 
" \trng = np.random.RandomState(42) \n\tt = rng.randn(100) \n\tedges = bayesian_blocks(t, fitness=u'events') \n\tassert_allclose(edges, [(-2.6197451), (-0.71094865), 0.36866702, 1.85227818]) \n\tt[80:] = t[:20] \n\tedges = bayesian_blocks(t, fitness=u'events', p0=0.01) \n\tassert_allclose(edges, [(-2.6197451), (-0.47432431), (-0.46202823), 1.85227818]) \n\tdt = 0.01 \n\tt = (dt * np.arange(1000)) \n\tx = np.zeros(len(t)) \n\tN = (len(t) // 10) \n\tx[rng.randint(0, len(t), N)] = 1 \n\tx[rng.randint(0, (len(t) // 2), N)] = 1 \n\tedges = bayesian_blocks(t, x, fitness=u'regular_events', dt=dt) \n\tassert_allclose(edges, [0, 5.105, 9.99]) \n\tt = (100 * rng.rand(20)) \n\tx = np.exp(((-0.5) * ((t - 50) ** 2))) \n\tsigma = 0.1 \n\tx_obs = (x + (sigma * rng.randn(len(x)))) \n\tedges = bayesian_blocks(t, x_obs, sigma, fitness=u'measures') \n\tassert_allclose(edges, [4.360377, 48.456895, 52.597917, 99.455051])\n", 
" \traise ValueError\n", 
" \tamp = 1e-08 \n\ttempdir = _TempDir() \n\trng = np.random.RandomState(0) \n\tfname_dtemp = op.join(tempdir, 'test.dip') \n\tfname_sim = op.join(tempdir, 'test-ave.fif') \n\tfwd = convert_forward_solution(read_forward_solution(fname_fwd), surf_ori=False, force_fixed=True) \n\tevoked = read_evokeds(fname_evo)[0] \n\tcov = read_cov(fname_cov) \n\tn_per_hemi = 5 \n\tvertices = [np.sort(rng.permutation(s['vertno'])[:n_per_hemi]) for s in fwd['src']] \n\tnv = sum((len(v) for v in vertices)) \n\tstc = SourceEstimate((amp * np.eye(nv)), vertices, 0, 0.001) \n\tevoked = simulate_evoked(fwd, stc, evoked.info, cov, snr=20, random_state=rng) \n\tpicks = np.sort(np.concatenate([pick_types(evoked.info, meg=True, eeg=False)[::2], pick_types(evoked.info, meg=False, eeg=True)[::2]])) \n\tevoked.pick_channels([evoked.ch_names[p] for p in picks]) \n\tevoked.add_proj(make_eeg_average_ref_proj(evoked.info)) \n\twrite_evokeds(fname_sim, evoked) \n\trun_subprocess(['mne_dipole_fit', '--meas', fname_sim, '--meg', '--eeg', '--noise', fname_cov, '--dip', fname_dtemp, '--mri', fname_fwd, '--reg', '0', '--tmin', '0']) \n\tdip_c = read_dipole(fname_dtemp) \n\tsphere = make_sphere_model(head_radius=0.1) \n\t(dip, residuals) = fit_dipole(evoked, fname_cov, sphere, fname_fwd) \n\tdata_rms = np.sqrt(np.sum((evoked.data ** 2), axis=0)) \n\tresi_rms = np.sqrt(np.sum((residuals ** 2), axis=0)) \n\tfactor = 1.0 \n\tif ((os.getenv('TRAVIS', 'false') == 'true') and (sys.version[:3] in ('3.5', '2.7'))): \n\t \tfactor = 0.8 \n\tassert_true((data_rms > (factor * resi_rms)).all(), msg=('%s \t(factor: \t%s)' % ((data_rms / resi_rms).min(), factor))) \n\ttransform_surface_to(fwd['src'][0], 'head', fwd['mri_head_t']) \n\ttransform_surface_to(fwd['src'][1], 'head', fwd['mri_head_t']) \n\tsrc_rr = np.concatenate([s['rr'][v] for (s, v) in zip(fwd['src'], vertices)], axis=0) \n\tsrc_nn = np.concatenate([s['nn'][v] for (s, v) in zip(fwd['src'], vertices)], axis=0) \n\tdip.crop(dip_c.times[0], dip_c.times[(-1)]) \n\t(src_rr, src_nn) = (src_rr[:(-1)], src_nn[:(-1)]) \n\t(corrs, dists, gc_dists, amp_errs, gofs) = ([], [], [], [], []) \n\tfor d in (dip_c, dip): \n\t \tnew = d.pos \n\t \tdiffs = (new - src_rr) \n\t \tcorrs += [np.corrcoef(src_rr.ravel(), new.ravel())[(0, 1)]] \n\t \tdists += [np.sqrt(np.mean(np.sum((diffs * diffs), axis=1)))] \n\t \tgc_dists += [((180 / np.pi) * np.mean(np.arccos(np.sum((src_nn * d.ori), axis=1))))] \n\t \tamp_errs += [np.sqrt(np.mean(((amp - d.amplitude) ** 2)))] \n\t \tgofs += [np.mean(d.gof)] \n\tassert_true((dists[0] >= (dists[1] * factor)), ('dists: \t%s' % dists)) \n\tassert_true((corrs[0] <= (corrs[1] / factor)), ('corrs: \t%s' % corrs)) \n\tassert_true((gc_dists[0] >= (gc_dists[1] * factor)), ('gc-dists \t(ori): \t%s' % gc_dists)) \n\tassert_true((amp_errs[0] >= (amp_errs[1] * factor)), ('amplitude \terrors: \t%s' % amp_errs)) \n\tassert_true((gofs[0] <= (gofs[1] / factor)), ('gof: \t%s' % gofs))\n", 
" \tx = np.asarray(x) \n\tmask = isnull(x) \n\tx = x[(~ mask)] \n\tvalues = np.sort(x) \n\tdef _get_score(at): \n\t \tif (len(values) == 0): \n\t \t \treturn np.nan \n\t \tidx = (at * (len(values) - 1)) \n\t \tif ((idx % 1) == 0): \n\t \t \tscore = values[int(idx)] \n\t \telif (interpolation_method == 'fraction'): \n\t \t \tscore = _interpolate(values[int(idx)], values[(int(idx) + 1)], (idx % 1)) \n\t \telif (interpolation_method == 'lower'): \n\t \t \tscore = values[np.floor(idx)] \n\t \telif (interpolation_method == 'higher'): \n\t \t \tscore = values[np.ceil(idx)] \n\t \telse: \n\t \t \traise ValueError(\"interpolation_method \tcan \tonly \tbe \t'fraction' \t, \t'lower' \tor \t'higher'\") \n\t \treturn score \n\tif is_scalar(q): \n\t \treturn _get_score(q) \n\telse: \n\t \tq = np.asarray(q, np.float64) \n\t \treturn algos.arrmap_float64(q, _get_score)\n", 
" \tclf = GaussianNB(priors=np.array([0.25, 0.25, 0.25, 0.25])) \n\tassert_raises(ValueError, clf.fit, X, y)\n", 
" \traise ValueError\n", 
" \tclasses = [] \n\tn_classes = [] \n\tclass_prior = [] \n\t(n_samples, n_outputs) = y.shape \n\tif issparse(y): \n\t \ty = y.tocsc() \n\t \ty_nnz = np.diff(y.indptr) \n\t \tfor k in range(n_outputs): \n\t \t \tcol_nonzero = y.indices[y.indptr[k]:y.indptr[(k + 1)]] \n\t \t \tif (sample_weight is not None): \n\t \t \t \tnz_samp_weight = np.asarray(sample_weight)[col_nonzero] \n\t \t \t \tzeros_samp_weight_sum = (np.sum(sample_weight) - np.sum(nz_samp_weight)) \n\t \t \telse: \n\t \t \t \tnz_samp_weight = None \n\t \t \t \tzeros_samp_weight_sum = (y.shape[0] - y_nnz[k]) \n\t \t \t(classes_k, y_k) = np.unique(y.data[y.indptr[k]:y.indptr[(k + 1)]], return_inverse=True) \n\t \t \tclass_prior_k = bincount(y_k, weights=nz_samp_weight) \n\t \t \tif (0 in classes_k): \n\t \t \t \tclass_prior_k[(classes_k == 0)] += zeros_samp_weight_sum \n\t \t \tif ((0 not in classes_k) and (y_nnz[k] < y.shape[0])): \n\t \t \t \tclasses_k = np.insert(classes_k, 0, 0) \n\t \t \t \tclass_prior_k = np.insert(class_prior_k, 0, zeros_samp_weight_sum) \n\t \t \tclasses.append(classes_k) \n\t \t \tn_classes.append(classes_k.shape[0]) \n\t \t \tclass_prior.append((class_prior_k / class_prior_k.sum())) \n\telse: \n\t \tfor k in range(n_outputs): \n\t \t \t(classes_k, y_k) = np.unique(y[:, k], return_inverse=True) \n\t \t \tclasses.append(classes_k) \n\t \t \tn_classes.append(classes_k.shape[0]) \n\t \t \tclass_prior_k = bincount(y_k, weights=sample_weight) \n\t \t \tclass_prior.append((class_prior_k / class_prior_k.sum())) \n\treturn (classes, n_classes, class_prior)\n", 
" \tdef is_abstract(c): \n\t \tif (not hasattr(c, '__abstractmethods__')): \n\t \t \treturn False \n\t \tif (not len(c.__abstractmethods__)): \n\t \t \treturn False \n\t \treturn True \n\tall_classes = [] \n\tpath = sklearn.__path__ \n\tfor (importer, modname, ispkg) in pkgutil.walk_packages(path=path, prefix='sklearn.', onerror=(lambda x: None)): \n\t \tif ('.tests.' in modname): \n\t \t \tcontinue \n\t \tmodule = __import__(modname, fromlist='dummy') \n\t \tclasses = inspect.getmembers(module, inspect.isclass) \n\t \tall_classes.extend(classes) \n\tall_classes = set(all_classes) \n\testimators = [c for c in all_classes if (issubclass(c[1], BaseEstimator) and (c[0] != 'BaseEstimator'))] \n\testimators = [c for c in estimators if (not is_abstract(c[1]))] \n\tif (not include_dont_test): \n\t \testimators = [c for c in estimators if (not (c[0] in DONT_TEST))] \n\tif (not include_other): \n\t \testimators = [c for c in estimators if (not (c[0] in OTHER))] \n\tif (not include_meta_estimators): \n\t \testimators = [c for c in estimators if (not (c[0] in META_ESTIMATORS))] \n\tif (type_filter is not None): \n\t \tif (not isinstance(type_filter, list)): \n\t \t \ttype_filter = [type_filter] \n\t \telse: \n\t \t \ttype_filter = list(type_filter) \n\t \tfiltered_estimators = [] \n\t \tfilters = {'classifier': ClassifierMixin, 'regressor': RegressorMixin, 'transformer': TransformerMixin, 'cluster': ClusterMixin} \n\t \tfor (name, mixin) in filters.items(): \n\t \t \tif (name in type_filter): \n\t \t \t \ttype_filter.remove(name) \n\t \t \t \tfiltered_estimators.extend([est for est in estimators if issubclass(est[1], mixin)]) \n\t \testimators = filtered_estimators \n\t \tif type_filter: \n\t \t \traise ValueError((\"Parameter \ttype_filter \tmust \tbe \t'classifier', \t'regressor', \t'transformer', \t'cluster' \tor \tNone, \tgot \t%s.\" % repr(type_filter))) \n\treturn sorted(set(estimators), key=itemgetter(0))\n", 
" \tif ((value is None) and empty_ok): \n\t \treturn \n\tif (not isinstance(value, (int, long))): \n\t \traise exception(('%s \tshould \tbe \tan \tinteger; \treceived \t%s \t(a \t%s).' % (name, value, typename(value)))) \n\tif ((not value) and (not zero_ok)): \n\t \traise exception(('%s \tmust \tnot \tbe \t0 \t(zero)' % name)) \n\tif ((value < 0) and (not negative_ok)): \n\t \traise exception(('%s \tmust \tnot \tbe \tnegative.' % name))\n", 
" \treturn ChecklistModule(mpstate)\n", 
" \tif (not isinstance(html, _strings)): \n\t \traise TypeError('string \trequired') \n\taccept_leading_text = bool(create_parent) \n\telements = fragments_fromstring(html, guess_charset=guess_charset, parser=parser, no_leading_text=(not accept_leading_text)) \n\tif create_parent: \n\t \tif (not isinstance(create_parent, _strings)): \n\t \t \tcreate_parent = 'div' \n\t \tnew_root = Element(create_parent) \n\t \tif elements: \n\t \t \tif isinstance(elements[0], _strings): \n\t \t \t \tnew_root.text = elements[0] \n\t \t \t \tdel elements[0] \n\t \t \tnew_root.extend(elements) \n\t \treturn new_root \n\tif (not elements): \n\t \traise etree.ParserError('No \telements \tfound') \n\tif (len(elements) > 1): \n\t \traise etree.ParserError('Multiple \telements \tfound') \n\tresult = elements[0] \n\tif (result.tail and result.tail.strip()): \n\t \traise etree.ParserError(('Element \tfollowed \tby \ttext: \t%r' % result.tail)) \n\tresult.tail = None \n\treturn result\n", 
" \traise ValueError\n", 
" \trng = np.random.RandomState(42) \n\tt = rng.randn(100) \n\tedges = bayesian_blocks(t, fitness=u'events') \n\tassert_allclose(edges, [(-2.6197451), (-0.71094865), 0.36866702, 1.85227818]) \n\tt[80:] = t[:20] \n\tedges = bayesian_blocks(t, fitness=u'events', p0=0.01) \n\tassert_allclose(edges, [(-2.6197451), (-0.47432431), (-0.46202823), 1.85227818]) \n\tdt = 0.01 \n\tt = (dt * np.arange(1000)) \n\tx = np.zeros(len(t)) \n\tN = (len(t) // 10) \n\tx[rng.randint(0, len(t), N)] = 1 \n\tx[rng.randint(0, (len(t) // 2), N)] = 1 \n\tedges = bayesian_blocks(t, x, fitness=u'regular_events', dt=dt) \n\tassert_allclose(edges, [0, 5.105, 9.99]) \n\tt = (100 * rng.rand(20)) \n\tx = np.exp(((-0.5) * ((t - 50) ** 2))) \n\tsigma = 0.1 \n\tx_obs = (x + (sigma * rng.randn(len(x)))) \n\tedges = bayesian_blocks(t, x_obs, sigma, fitness=u'measures') \n\tassert_allclose(edges, [4.360377, 48.456895, 52.597917, 99.455051])\n", 
" \traise ValueError\n", 
" \tamp = 1e-08 \n\ttempdir = _TempDir() \n\trng = np.random.RandomState(0) \n\tfname_dtemp = op.join(tempdir, 'test.dip') \n\tfname_sim = op.join(tempdir, 'test-ave.fif') \n\tfwd = convert_forward_solution(read_forward_solution(fname_fwd), surf_ori=False, force_fixed=True) \n\tevoked = read_evokeds(fname_evo)[0] \n\tcov = read_cov(fname_cov) \n\tn_per_hemi = 5 \n\tvertices = [np.sort(rng.permutation(s['vertno'])[:n_per_hemi]) for s in fwd['src']] \n\tnv = sum((len(v) for v in vertices)) \n\tstc = SourceEstimate((amp * np.eye(nv)), vertices, 0, 0.001) \n\tevoked = simulate_evoked(fwd, stc, evoked.info, cov, snr=20, random_state=rng) \n\tpicks = np.sort(np.concatenate([pick_types(evoked.info, meg=True, eeg=False)[::2], pick_types(evoked.info, meg=False, eeg=True)[::2]])) \n\tevoked.pick_channels([evoked.ch_names[p] for p in picks]) \n\tevoked.add_proj(make_eeg_average_ref_proj(evoked.info)) \n\twrite_evokeds(fname_sim, evoked) \n\trun_subprocess(['mne_dipole_fit', '--meas', fname_sim, '--meg', '--eeg', '--noise', fname_cov, '--dip', fname_dtemp, '--mri', fname_fwd, '--reg', '0', '--tmin', '0']) \n\tdip_c = read_dipole(fname_dtemp) \n\tsphere = make_sphere_model(head_radius=0.1) \n\t(dip, residuals) = fit_dipole(evoked, fname_cov, sphere, fname_fwd) \n\tdata_rms = np.sqrt(np.sum((evoked.data ** 2), axis=0)) \n\tresi_rms = np.sqrt(np.sum((residuals ** 2), axis=0)) \n\tfactor = 1.0 \n\tif ((os.getenv('TRAVIS', 'false') == 'true') and (sys.version[:3] in ('3.5', '2.7'))): \n\t \tfactor = 0.8 \n\tassert_true((data_rms > (factor * resi_rms)).all(), msg=('%s \t(factor: \t%s)' % ((data_rms / resi_rms).min(), factor))) \n\ttransform_surface_to(fwd['src'][0], 'head', fwd['mri_head_t']) \n\ttransform_surface_to(fwd['src'][1], 'head', fwd['mri_head_t']) \n\tsrc_rr = np.concatenate([s['rr'][v] for (s, v) in zip(fwd['src'], vertices)], axis=0) \n\tsrc_nn = np.concatenate([s['nn'][v] for (s, v) in zip(fwd['src'], vertices)], axis=0) \n\tdip.crop(dip_c.times[0], dip_c.times[(-1)]) \n\t(src_rr, src_nn) = (src_rr[:(-1)], src_nn[:(-1)]) \n\t(corrs, dists, gc_dists, amp_errs, gofs) = ([], [], [], [], []) \n\tfor d in (dip_c, dip): \n\t \tnew = d.pos \n\t \tdiffs = (new - src_rr) \n\t \tcorrs += [np.corrcoef(src_rr.ravel(), new.ravel())[(0, 1)]] \n\t \tdists += [np.sqrt(np.mean(np.sum((diffs * diffs), axis=1)))] \n\t \tgc_dists += [((180 / np.pi) * np.mean(np.arccos(np.sum((src_nn * d.ori), axis=1))))] \n\t \tamp_errs += [np.sqrt(np.mean(((amp - d.amplitude) ** 2)))] \n\t \tgofs += [np.mean(d.gof)] \n\tassert_true((dists[0] >= (dists[1] * factor)), ('dists: \t%s' % dists)) \n\tassert_true((corrs[0] <= (corrs[1] / factor)), ('corrs: \t%s' % corrs)) \n\tassert_true((gc_dists[0] >= (gc_dists[1] * factor)), ('gc-dists \t(ori): \t%s' % gc_dists)) \n\tassert_true((amp_errs[0] >= (amp_errs[1] * factor)), ('amplitude \terrors: \t%s' % amp_errs)) \n\tassert_true((gofs[0] <= (gofs[1] / factor)), ('gof: \t%s' % gofs))\n", 
" \tx = np.asarray(x) \n\tmask = isnull(x) \n\tx = x[(~ mask)] \n\tvalues = np.sort(x) \n\tdef _get_score(at): \n\t \tif (len(values) == 0): \n\t \t \treturn np.nan \n\t \tidx = (at * (len(values) - 1)) \n\t \tif ((idx % 1) == 0): \n\t \t \tscore = values[int(idx)] \n\t \telif (interpolation_method == 'fraction'): \n\t \t \tscore = _interpolate(values[int(idx)], values[(int(idx) + 1)], (idx % 1)) \n\t \telif (interpolation_method == 'lower'): \n\t \t \tscore = values[np.floor(idx)] \n\t \telif (interpolation_method == 'higher'): \n\t \t \tscore = values[np.ceil(idx)] \n\t \telse: \n\t \t \traise ValueError(\"interpolation_method \tcan \tonly \tbe \t'fraction' \t, \t'lower' \tor \t'higher'\") \n\t \treturn score \n\tif is_scalar(q): \n\t \treturn _get_score(q) \n\telse: \n\t \tq = np.asarray(q, np.float64) \n\t \treturn algos.arrmap_float64(q, _get_score)\n", 
" \tclf = GaussianNB(priors=np.array([0.25, 0.25, 0.25, 0.25])) \n\tassert_raises(ValueError, clf.fit, X, y)\n", 
" \tdef g(target, *args, **kwargs): \n\t \tval = func(target, *args, **kwargs) \n\t \tif val: \n\t \t \treturn val \n\t \telse: \n\t \t \traise exception_class((value_error_message_template % target)) \n\tg.__name__ = func.__name__ \n\tg.__doc__ = func.__doc__ \n\treturn g\n", 
" \treturn (k, features.copy(), labels.copy())\n", 
" \traise ValueError\n", 
" \tdef is_abstract(c): \n\t \tif (not hasattr(c, '__abstractmethods__')): \n\t \t \treturn False \n\t \tif (not len(c.__abstractmethods__)): \n\t \t \treturn False \n\t \treturn True \n\tall_classes = [] \n\tpath = sklearn.__path__ \n\tfor (importer, modname, ispkg) in pkgutil.walk_packages(path=path, prefix='sklearn.', onerror=(lambda x: None)): \n\t \tif ('.tests.' in modname): \n\t \t \tcontinue \n\t \tmodule = __import__(modname, fromlist='dummy') \n\t \tclasses = inspect.getmembers(module, inspect.isclass) \n\t \tall_classes.extend(classes) \n\tall_classes = set(all_classes) \n\testimators = [c for c in all_classes if (issubclass(c[1], BaseEstimator) and (c[0] != 'BaseEstimator'))] \n\testimators = [c for c in estimators if (not is_abstract(c[1]))] \n\tif (not include_dont_test): \n\t \testimators = [c for c in estimators if (not (c[0] in DONT_TEST))] \n\tif (not include_other): \n\t \testimators = [c for c in estimators if (not (c[0] in OTHER))] \n\tif (not include_meta_estimators): \n\t \testimators = [c for c in estimators if (not (c[0] in META_ESTIMATORS))] \n\tif (type_filter is not None): \n\t \tif (not isinstance(type_filter, list)): \n\t \t \ttype_filter = [type_filter] \n\t \telse: \n\t \t \ttype_filter = list(type_filter) \n\t \tfiltered_estimators = [] \n\t \tfilters = {'classifier': ClassifierMixin, 'regressor': RegressorMixin, 'transformer': TransformerMixin, 'cluster': ClusterMixin} \n\t \tfor (name, mixin) in filters.items(): \n\t \t \tif (name in type_filter): \n\t \t \t \ttype_filter.remove(name) \n\t \t \t \tfiltered_estimators.extend([est for est in estimators if issubclass(est[1], mixin)]) \n\t \testimators = filtered_estimators \n\t \tif type_filter: \n\t \t \traise ValueError((\"Parameter \ttype_filter \tmust \tbe \t'classifier', \t'regressor', \t'transformer', \t'cluster' \tor \tNone, \tgot \t%s.\" % repr(type_filter))) \n\treturn sorted(set(estimators), key=itemgetter(0))\n", 
" \tif ((value is None) and empty_ok): \n\t \treturn \n\tif (not isinstance(value, (int, long))): \n\t \traise exception(('%s \tshould \tbe \tan \tinteger; \treceived \t%s \t(a \t%s).' % (name, value, typename(value)))) \n\tif ((not value) and (not zero_ok)): \n\t \traise exception(('%s \tmust \tnot \tbe \t0 \t(zero)' % name)) \n\tif ((value < 0) and (not negative_ok)): \n\t \traise exception(('%s \tmust \tnot \tbe \tnegative.' % name))\n", 
" \traise ValueError\n", 
" \tif (not isinstance(html, _strings)): \n\t \traise TypeError('string \trequired') \n\taccept_leading_text = bool(create_parent) \n\telements = fragments_fromstring(html, guess_charset=guess_charset, parser=parser, no_leading_text=(not accept_leading_text)) \n\tif create_parent: \n\t \tif (not isinstance(create_parent, _strings)): \n\t \t \tcreate_parent = 'div' \n\t \tnew_root = Element(create_parent) \n\t \tif elements: \n\t \t \tif isinstance(elements[0], _strings): \n\t \t \t \tnew_root.text = elements[0] \n\t \t \t \tdel elements[0] \n\t \t \tnew_root.extend(elements) \n\t \treturn new_root \n\tif (not elements): \n\t \traise etree.ParserError('No \telements \tfound') \n\tif (len(elements) > 1): \n\t \traise etree.ParserError('Multiple \telements \tfound') \n\tresult = elements[0] \n\tif (result.tail and result.tail.strip()): \n\t \traise etree.ParserError(('Element \tfollowed \tby \ttext: \t%r' % result.tail)) \n\tresult.tail = None \n\treturn result\n", 
" \trng = np.random.RandomState(42) \n\tt = rng.randn(100) \n\tedges = bayesian_blocks(t, fitness=u'events') \n\tassert_allclose(edges, [(-2.6197451), (-0.71094865), 0.36866702, 1.85227818]) \n\tt[80:] = t[:20] \n\tedges = bayesian_blocks(t, fitness=u'events', p0=0.01) \n\tassert_allclose(edges, [(-2.6197451), (-0.47432431), (-0.46202823), 1.85227818]) \n\tdt = 0.01 \n\tt = (dt * np.arange(1000)) \n\tx = np.zeros(len(t)) \n\tN = (len(t) // 10) \n\tx[rng.randint(0, len(t), N)] = 1 \n\tx[rng.randint(0, (len(t) // 2), N)] = 1 \n\tedges = bayesian_blocks(t, x, fitness=u'regular_events', dt=dt) \n\tassert_allclose(edges, [0, 5.105, 9.99]) \n\tt = (100 * rng.rand(20)) \n\tx = np.exp(((-0.5) * ((t - 50) ** 2))) \n\tsigma = 0.1 \n\tx_obs = (x + (sigma * rng.randn(len(x)))) \n\tedges = bayesian_blocks(t, x_obs, sigma, fitness=u'measures') \n\tassert_allclose(edges, [4.360377, 48.456895, 52.597917, 99.455051])\n", 
" \traise ValueError\n", 
" \tx_arr = np.asarray(x) \n\tparams = x_arr[:k_params].ravel() \n\tu = x_arr[k_params:] \n\tobjective_func_arr = (f(params, *args) + (alpha * u).sum()) \n\treturn matrix(objective_func_arr)\n", 
" \tx = np.asarray(x) \n\tmask = isnull(x) \n\tx = x[(~ mask)] \n\tvalues = np.sort(x) \n\tdef _get_score(at): \n\t \tif (len(values) == 0): \n\t \t \treturn np.nan \n\t \tidx = (at * (len(values) - 1)) \n\t \tif ((idx % 1) == 0): \n\t \t \tscore = values[int(idx)] \n\t \telif (interpolation_method == 'fraction'): \n\t \t \tscore = _interpolate(values[int(idx)], values[(int(idx) + 1)], (idx % 1)) \n\t \telif (interpolation_method == 'lower'): \n\t \t \tscore = values[np.floor(idx)] \n\t \telif (interpolation_method == 'higher'): \n\t \t \tscore = values[np.ceil(idx)] \n\t \telse: \n\t \t \traise ValueError(\"interpolation_method \tcan \tonly \tbe \t'fraction' \t, \t'lower' \tor \t'higher'\") \n\t \treturn score \n\tif is_scalar(q): \n\t \treturn _get_score(q) \n\telse: \n\t \tq = np.asarray(q, np.float64) \n\t \treturn algos.arrmap_float64(q, _get_score)\n", 
" \treturn [[((-1), 1), (2, 0), (0, 4)], [(0, 1), (None, 2), (3, 2)], [((-3), 3), (1, 3), (1, 1)], [(1, 1), (Decimal('1.'), 1), (1, 1)], [(3, 2), (2, 1), (1.0, 1)]][i]\n", 
" \tpass\n", 
" \ttry: \n\t \tfo = open((RFmodelName + 'MEANS'), 'rb') \n\texcept IOError: \n\t \tprint \"Load \tRandom \tForest \tModel: \tDidn't \tfind \tfile\" \n\t \treturn \n\ttry: \n\t \tMEAN = cPickle.load(fo) \n\t \tSTD = cPickle.load(fo) \n\t \tif (not isRegression): \n\t \t \tclassNames = cPickle.load(fo) \n\t \tmtWin = cPickle.load(fo) \n\t \tmtStep = cPickle.load(fo) \n\t \tstWin = cPickle.load(fo) \n\t \tstStep = cPickle.load(fo) \n\t \tcomputeBEAT = cPickle.load(fo) \n\texcept: \n\t \tfo.close() \n\tfo.close() \n\tMEAN = numpy.array(MEAN) \n\tSTD = numpy.array(STD) \n\tCOEFF = [] \n\twith open(RFmodelName, 'rb') as fid: \n\t \tRF = cPickle.load(fid) \n\tif isRegression: \n\t \treturn (RF, MEAN, STD, mtWin, mtStep, stWin, stStep, computeBEAT) \n\telse: \n\t \treturn (RF, MEAN, STD, classNames, mtWin, mtStep, stWin, stStep, computeBEAT)\n", 
" \tclf = GaussianNB(priors=np.array([0.25, 0.25, 0.25, 0.25])) \n\tassert_raises(ValueError, clf.fit, X, y)\n", 
" \tdef g(target, *args, **kwargs): \n\t \tval = func(target, *args, **kwargs) \n\t \tif val: \n\t \t \treturn val \n\t \telse: \n\t \t \traise exception_class((value_error_message_template % target)) \n\tg.__name__ = func.__name__ \n\tg.__doc__ = func.__doc__ \n\treturn g\n", 
" \tif (initializers is None): \n\t \treturn None \n\tif (isinstance(initializers, dict) and (layer_name in initializers)): \n\t \treturn _get_initializers(initializers[layer_name], fields) \n\treturn _get_initializers(initializers, fields)\n", 
" \tx_arr = np.asarray(x) \n\tparams = x_arr[:k_params].ravel() \n\tu = x_arr[k_params:] \n\tobjective_func_arr = (f(params, *args) + (alpha * u).sum()) \n\treturn matrix(objective_func_arr)\n", 
" \traise ValueError\n", 
" \tx_arr = np.asarray(x) \n\tparams = x_arr[:k_params].ravel() \n\tu = x_arr[k_params:] \n\tobjective_func_arr = (f(params, *args) + (alpha * u).sum()) \n\treturn matrix(objective_func_arr)\n", 
" \tx_arr = np.asarray(x) \n\tparams = x_arr[:k_params].ravel() \n\tu = x_arr[k_params:] \n\tobjective_func_arr = (f(params, *args) + (alpha * u).sum()) \n\treturn matrix(objective_func_arr)\n", 
" \ttry: \n\t \timport_module(module) \n\texcept ImportError: \n\t \treturn False \n\telse: \n\t \treturn True\n", 
" \tresult = run_and_monitor(command, echo=echo, input=input) \n\tif (result.returncode != 0): \n\t \terror = 'FAILED \t{command} \twith \texit \tcode \t{code}\\n{err}'.format(command=command, code=result.returncode, err=result.stderr.strip()) \n\t \tsys.stderr.write((error + '\\n')) \n\t \traise RuntimeError(error) \n\treturn result\n", 
" \tfwd = mne.read_forward_solution(fwd_fname, surf_ori=True) \n\twith warnings.catch_warnings(record=True) as w: \n\t \twarnings.simplefilter('always') \n\t \tprojs = read_proj(eog_fname) \n\t \tprojs.extend(read_proj(ecg_fname)) \n\tdecim = 6 \n\tfor ch_type in ['eeg', 'grad', 'mag']: \n\t \tw = read_source_estimate((sensmap_fname % (ch_type, 'lh'))).data \n\t \tstc = sensitivity_map(fwd, projs=None, ch_type=ch_type, mode='free', exclude='bads') \n\t \tassert_array_almost_equal(stc.data, w, decim) \n\t \tassert_true((stc.subject == 'sample')) \n\t \tif (ch_type == 'grad'): \n\t \t \tw = read_source_estimate((sensmap_fname % (ch_type, '2-lh'))).data \n\t \t \tstc = sensitivity_map(fwd, projs=None, mode='fixed', ch_type=ch_type, exclude='bads') \n\t \t \tassert_array_almost_equal(stc.data, w, decim) \n\t \tif (ch_type == 'mag'): \n\t \t \tw = read_source_estimate((sensmap_fname % (ch_type, '3-lh'))).data \n\t \t \tstc = sensitivity_map(fwd, projs=None, mode='ratio', ch_type=ch_type, exclude='bads') \n\t \t \tassert_array_almost_equal(stc.data, w, decim) \n\t \tif (ch_type == 'eeg'): \n\t \t \tmodes = ['radiality', 'angle', 'remaining', 'dampening'] \n\t \t \tends = ['4-lh', '5-lh', '6-lh', '7-lh'] \n\t \t \tfor (mode, end) in zip(modes, ends): \n\t \t \t \tw = read_source_estimate((sensmap_fname % (ch_type, end))).data \n\t \t \t \tstc = sensitivity_map(fwd, projs=projs, mode=mode, ch_type=ch_type, exclude='bads') \n\t \t \t \tassert_array_almost_equal(stc.data, w, decim) \n\tstc = sensitivity_map(fwd, projs=[make_eeg_average_ref_proj(fwd['info'])], ch_type='eeg', exclude='bads') \n\tassert_raises(ValueError, sensitivity_map, fwd, projs=None, mode='angle') \n\tassert_raises(RuntimeError, sensitivity_map, fwd, projs=[], mode='angle') \n\tfname = op.join(sample_path, 'sample_audvis_trunc-meg-vol-7-fwd.fif') \n\tfwd = mne.read_forward_solution(fname) \n\tsensitivity_map(fwd)\n", 
" \tif (ch_type not in ['eeg', 'grad', 'mag']): \n\t \traise ValueError((\"ch_type \tshould \tbe \t'eeg', \t'mag' \tor \t'grad \t(got \t%s)\" % ch_type)) \n\tif (mode not in ['free', 'fixed', 'ratio', 'radiality', 'angle', 'remaining', 'dampening']): \n\t \traise ValueError(('Unknown \tmode \ttype \t(got \t%s)' % mode)) \n\tif is_fixed_orient(fwd, orig=True): \n\t \traise ValueError('fwd \tshould \tmust \tbe \tcomputed \twith \tfree \torientation') \n\tif (ch_type == 'eeg'): \n\t \tfwd = pick_types_forward(fwd, meg=False, eeg=True, exclude=exclude) \n\telse: \n\t \tfwd = pick_types_forward(fwd, meg=ch_type, eeg=False, exclude=exclude) \n\tconvert_forward_solution(fwd, surf_ori=True, force_fixed=False, copy=False, verbose=False) \n\tif ((not fwd['surf_ori']) or is_fixed_orient(fwd)): \n\t \traise RuntimeError('Error \tconverting \tsolution, \tplease \tnotify \tmne-python \tdevelopers') \n\tgain = fwd['sol']['data'] \n\tif (ch_type == 'eeg'): \n\t \tif ((projs is None) or (not _has_eeg_average_ref_proj(projs))): \n\t \t \teeg_ave = [make_eeg_average_ref_proj(fwd['info'])] \n\t \telse: \n\t \t \teeg_ave = [] \n\t \tprojs = (eeg_ave if (projs is None) else (projs + eeg_ave)) \n\tresidual_types = ['angle', 'remaining', 'dampening'] \n\tif (projs is not None): \n\t \t(proj, ncomp, U) = make_projector(projs, fwd['sol']['row_names'], include_active=True) \n\t \tif (mode not in residual_types): \n\t \t \tgain = np.dot(proj, gain) \n\t \telif (ncomp == 0): \n\t \t \traise RuntimeError(('No \tvalid \tprojectors \tfound \tfor \tchannel \ttype \t%s, \tcannot \tcompute \t%s' % (ch_type, mode))) \n\telif (mode in residual_types): \n\t \traise ValueError(('No \tprojectors \tused, \tcannot \tcompute \t%s' % mode)) \n\t(n_sensors, n_dipoles) = gain.shape \n\tn_locations = (n_dipoles // 3) \n\tsensitivity_map = np.empty(n_locations) \n\tfor k in range(n_locations): \n\t \tgg = gain[:, (3 * k):(3 * (k + 1))] \n\t \tif (mode != 'fixed'): \n\t \t \ts = linalg.svd(gg, full_matrices=False, compute_uv=False) \n\t \tif (mode == 'free'): \n\t \t \tsensitivity_map[k] = s[0] \n\t \telse: \n\t \t \tgz = linalg.norm(gg[:, 2]) \n\t \t \tif (mode == 'fixed'): \n\t \t \t \tsensitivity_map[k] = gz \n\t \t \telif (mode == 'ratio'): \n\t \t \t \tsensitivity_map[k] = (gz / s[0]) \n\t \t \telif (mode == 'radiality'): \n\t \t \t \tsensitivity_map[k] = (1.0 - (gz / s[0])) \n\t \t \telif (mode == 'angle'): \n\t \t \t \tco = linalg.norm(np.dot(gg[:, 2], U)) \n\t \t \t \tsensitivity_map[k] = (co / gz) \n\t \t \telse: \n\t \t \t \tp = linalg.norm(np.dot(proj, gg[:, 2])) \n\t \t \t \tif (mode == 'remaining'): \n\t \t \t \t \tsensitivity_map[k] = (p / gz) \n\t \t \t \telif (mode == 'dampening'): \n\t \t \t \t \tsensitivity_map[k] = (1.0 - (p / gz)) \n\t \t \t \telse: \n\t \t \t \t \traise ValueError(('Unknown \tmode \ttype \t(got \t%s)' % mode)) \n\tif (mode in ['fixed', 'free']): \n\t \tsensitivity_map /= np.max(sensitivity_map) \n\tsubject = _subject_from_forward(fwd) \n\tif (fwd['src'][0]['type'] == 'vol'): \n\t \tvertices = fwd['src'][0]['vertno'] \n\t \tSEClass = VolSourceEstimate \n\telse: \n\t \tvertices = [fwd['src'][0]['vertno'], fwd['src'][1]['vertno']] \n\t \tSEClass = SourceEstimate \n\tstc = SEClass(sensitivity_map[:, np.newaxis], vertices=vertices, tmin=0, tstep=1, subject=subject) \n\treturn stc\n", 
" \tret = salt.utils.mac_utils.execute_return_result('systemsetup \t-getcomputername') \n\treturn salt.utils.mac_utils.parse_return(ret)\n", 
" \tif (not isinstance(a, np.ndarray)): \n\t \tlog_a = np.log(np.array(a, dtype=dtype)) \n\telif dtype: \n\t \tif isinstance(a, np.ma.MaskedArray): \n\t \t \tlog_a = np.log(np.ma.asarray(a, dtype=dtype)) \n\t \telse: \n\t \t \tlog_a = np.log(np.asarray(a, dtype=dtype)) \n\telse: \n\t \tlog_a = np.log(a) \n\treturn np.exp(log_a.mean(axis=axis))\n", 
" \tall_classes = list(set(np.append(y_true, y_pred))) \n\tall_class_accuracies = [] \n\tfor this_class in all_classes: \n\t \tthis_class_sensitivity = (float(sum(((y_pred == this_class) & (y_true == this_class)))) / float(sum((y_true == this_class)))) \n\t \tthis_class_specificity = (float(sum(((y_pred != this_class) & (y_true != this_class)))) / float(sum((y_true != this_class)))) \n\t \tthis_class_accuracy = ((this_class_sensitivity + this_class_specificity) / 2.0) \n\t \tall_class_accuracies.append(this_class_accuracy) \n\treturn np.mean(all_class_accuracies)\n", 
" \tif (labels is None): \n\t \tlabels = unique_labels(y_true, y_pred) \n\telse: \n\t \tlabels = np.asarray(labels) \n\tif ((target_names is not None) and (len(labels) != len(target_names))): \n\t \twarnings.warn('labels \tsize, \t{0}, \tdoes \tnot \tmatch \tsize \tof \ttarget_names, \t{1}'.format(len(labels), len(target_names))) \n\tlast_line_heading = 'avg \t/ \ttotal' \n\tif (target_names is None): \n\t \ttarget_names = [(u'%s' % l) for l in labels] \n\tname_width = max((len(cn) for cn in target_names)) \n\twidth = max(name_width, len(last_line_heading), digits) \n\theaders = ['precision', 'recall', 'f1-score', 'support'] \n\thead_fmt = (u'{:>{width}s} \t' + (u' \t{:>9}' * len(headers))) \n\treport = head_fmt.format(u'', width=width, *headers) \n\treport += u'\\n\\n' \n\t(p, r, f1, s) = precision_recall_fscore_support(y_true, y_pred, labels=labels, average=None, sample_weight=sample_weight) \n\trow_fmt = ((u'{:>{width}s} \t' + (u' \t{:>9.{digits}f}' * 3)) + u' \t{:>9}\\n') \n\trows = zip(target_names, p, r, f1, s) \n\tfor row in rows: \n\t \treport += row_fmt.format(width=width, digits=digits, *row) \n\treport += u'\\n' \n\treport += row_fmt.format(last_line_heading, np.average(p, weights=s), np.average(r, weights=s), np.average(f1, weights=s), np.sum(s), width=width, digits=digits) \n\treturn report\n", 
" \tif (dataset is None): \n\t \tdataset = datasets.load_iris() \n\tX = dataset.data \n\ty = dataset.target \n\tif binary: \n\t \t(X, y) = (X[(y < 2)], y[(y < 2)]) \n\t(n_samples, n_features) = X.shape \n\tp = np.arange(n_samples) \n\trng = check_random_state(37) \n\trng.shuffle(p) \n\t(X, y) = (X[p], y[p]) \n\thalf = int((n_samples / 2)) \n\trng = np.random.RandomState(0) \n\tX = np.c_[(X, rng.randn(n_samples, (200 * n_features)))] \n\tclf = svm.SVC(kernel='linear', probability=True, random_state=0) \n\tprobas_pred = clf.fit(X[:half], y[:half]).predict_proba(X[half:]) \n\tif binary: \n\t \tprobas_pred = probas_pred[:, 1] \n\ty_pred = clf.predict(X[half:]) \n\ty_true = y[half:] \n\treturn (y_true, y_pred, probas_pred)\n", 
" \tif (CM.shape[0] != len(ClassNames)): \n\t \tprint 'printConfusionMatrix: \tWrong \targument \tsizes\\n' \n\t \treturn \n\tfor c in ClassNames: \n\t \tif (len(c) > 4): \n\t \t \tc = c[0:3] \n\t \tprint ' DCTB {0:s}'.format(c), \n\tprint \n\tfor (i, c) in enumerate(ClassNames): \n\t \tif (len(c) > 4): \n\t \t \tc = c[0:3] \n\t \tprint '{0:s}'.format(c), \n\t \tfor j in range(len(ClassNames)): \n\t \t \tprint ' DCTB {0:.1f}'.format(((100.0 * CM[i][j]) / numpy.sum(CM))), \n\t \tprint\n", 
" \tdef final_decorator(f): \n\t \tfor d in decorators: \n\t \t \tif (not hasattr(d, '__code__')): \n\t \t \t \tsetattr(d, '__code__', Fakecode()) \n\t \t \tf = d(f) \n\t \treturn f \n\treturn final_decorator\n", 
" \tpos = np.random.RandomState(0).rand(10, 3) \n\tvalues = (np.arange(10.0) / 10) \n\tidx0 = list(range(7)) \n\tidx1 = list(range(7, 10)) \n\tidx2 = list(range(5, 10)) \n\tl0 = Label(idx0, pos[idx0], values[idx0], 'lh', color='red') \n\tl1 = Label(idx1, pos[idx1], values[idx1], 'lh') \n\tl2 = Label(idx2, pos[idx2], values[idx2], 'lh', color=(0, 1, 0, 0.5)) \n\tassert_equal(len(l0), len(idx0)) \n\tl_good = l0.copy() \n\tl_good.subject = 'sample' \n\tl_bad = l1.copy() \n\tl_bad.subject = 'foo' \n\tassert_raises(ValueError, l_good.__add__, l_bad) \n\tassert_raises(TypeError, l_good.__add__, 'foo') \n\tassert_raises(ValueError, l_good.__sub__, l_bad) \n\tassert_raises(TypeError, l_good.__sub__, 'foo') \n\tl01 = (l0 + l1) \n\tassert_equal(len(l01), (len(l0) + len(l1))) \n\tassert_array_equal(l01.values[:len(l0)], l0.values) \n\tassert_equal(l01.color, l0.color) \n\tassert_labels_equal((l01 - l0), l1, comment=False, color=False) \n\tassert_labels_equal((l01 - l1), l0, comment=False, color=False) \n\tl = (l0 + l2) \n\ti0 = np.where((l0.vertices == 6))[0][0] \n\ti2 = np.where((l2.vertices == 6))[0][0] \n\ti = np.where((l.vertices == 6))[0][0] \n\tassert_equal(l.values[i], (l0.values[i0] + l2.values[i2])) \n\tassert_equal(l.values[0], l0.values[0]) \n\tassert_array_equal(np.unique(l.vertices), np.unique((idx0 + idx2))) \n\tassert_equal(l.color, _blend_colors(l0.color, l2.color)) \n\tl2.hemi = 'rh' \n\tbhl = (l0 + l2) \n\tassert_equal(bhl.hemi, 'both') \n\tassert_equal(len(bhl), (len(l0) + len(l2))) \n\tassert_equal(bhl.color, l.color) \n\tassert_true(('BiHemiLabel' in repr(bhl))) \n\tassert_labels_equal((bhl - l0), l2) \n\tassert_labels_equal((bhl - l2), l0) \n\tbhl2 = (l1 + bhl) \n\tassert_labels_equal(bhl2.lh, l01) \n\tassert_equal(bhl2.color, _blend_colors(l1.color, bhl.color)) \n\tassert_array_equal((l2 + bhl).rh.vertices, bhl.rh.vertices) \n\tassert_array_equal((bhl + bhl).lh.vertices, bhl.lh.vertices) \n\tassert_raises(TypeError, bhl.__add__, 5) \n\tbhl_ = (bhl2 - l1) \n\tassert_labels_equal(bhl_.lh, bhl.lh, comment=False, color=False) \n\tassert_labels_equal(bhl_.rh, bhl.rh) \n\tassert_labels_equal((bhl2 - l2), (l0 + l1)) \n\tassert_labels_equal(((bhl2 - l1) - l0), l2) \n\tbhl_ = (bhl2 - bhl2) \n\tassert_array_equal(bhl_.vertices, [])\n", 
" \telts = time_string.split(':') \n\tif (len(elts) == 1): \n\t \treturn time_string \n\treturn str(((int(elts[0]) * 60) + float(elts[1])))\n", 
" \t(X, y) = make_multilabel_classification(n_classes=2, n_labels=1, allow_unlabeled=False, random_state=123) \n\tclf = OneVsRestClassifier(SVC(kernel='linear')) \n\teclf = VotingClassifier(estimators=[('ovr', clf)], voting='hard') \n\ttry: \n\t \teclf.fit(X, y) \n\texcept NotImplementedError: \n\t \treturn\n", 
" \treturn (isinstance(value, str) or isinstance(value, int) or isinstance(value, float) or isinstance(value, bool))\n", 
" \tif isinstance(value, basestring): \n\t \treturn (value in ('None', '')) \n\telif isListLike(value): \n\t \treturn all((isNoneValue(_) for _ in value)) \n\telif isinstance(value, dict): \n\t \treturn (not any(value)) \n\telse: \n\t \treturn (value is None)\n", 
" \ty_prob = np.clip(y_prob, 1e-10, (1 - 1e-10)) \n\treturn ((- np.sum(((y_true * np.log(y_prob)) + ((1 - y_true) * np.log((1 - y_prob)))))) / y_prob.shape[0])\n", 
" \treturn _apply_scalar_per_pixel(generic_cy._geometric_mean, image, selem, out=out, mask=mask, shift_x=shift_x, shift_y=shift_y)\n", 
" \tdesired_keys = ['id', 'name', 'state', 'public_ips', 'private_ips', 'size', 'image', 'location'] \n\titem['private_ips'] = [] \n\titem['public_ips'] = [] \n\tif ('ips' in item): \n\t \tfor ip in item['ips']: \n\t \t \tif is_public_ip(ip): \n\t \t \t \titem['public_ips'].append(ip) \n\t \t \telse: \n\t \t \t \titem['private_ips'].append(ip) \n\tfor key in desired_keys: \n\t \tif (key not in item): \n\t \t \titem[key] = None \n\tto_del = [] \n\tif (not full): \n\t \tfor key in six.iterkeys(item): \n\t \t \tif (key not in desired_keys): \n\t \t \t \tto_del.append(key) \n\tfor key in to_del: \n\t \tdel item[key] \n\tif ('state' in item): \n\t \titem['state'] = joyent_node_state(item['state']) \n\treturn item\n", 
" \tclf = LinearSVC() \n\t(X, y_idx) = make_blobs(n_samples=100, n_features=2, random_state=42, centers=3, cluster_std=3.0) \n\ttarget_names = np.array(['a', 'b', 'c']) \n\ty = target_names[y_idx] \n\t(X_train, y_train) = (X[::2], y[::2]) \n\t(X_test, y_test) = (X[1::2], y[1::2]) \n\tclf.fit(X_train, y_train) \n\tfor method in ['isotonic', 'sigmoid']: \n\t \tcal_clf = CalibratedClassifierCV(clf, method=method, cv=2) \n\t \tcal_clf.fit(X_train, y_train) \n\t \tprobas = cal_clf.predict_proba(X_test) \n\t \tassert_array_almost_equal(np.sum(probas, axis=1), np.ones(len(X_test))) \n\t \tdef softmax(y_pred): \n\t \t \te = np.exp((- y_pred)) \n\t \t \treturn (e / e.sum(axis=1).reshape((-1), 1)) \n\t \tuncalibrated_log_loss = log_loss(y_test, softmax(clf.decision_function(X_test))) \n\t \tcalibrated_log_loss = log_loss(y_test, probas) \n\t \tassert_greater_equal(uncalibrated_log_loss, calibrated_log_loss) \n\t(X, y) = make_blobs(n_samples=100, n_features=2, random_state=42, cluster_std=3.0) \n\t(X_train, y_train) = (X[::2], y[::2]) \n\t(X_test, y_test) = (X[1::2], y[1::2]) \n\tclf = RandomForestClassifier(n_estimators=10, random_state=42) \n\tclf.fit(X_train, y_train) \n\tclf_probs = clf.predict_proba(X_test) \n\tloss = log_loss(y_test, clf_probs) \n\tfor method in ['isotonic', 'sigmoid']: \n\t \tcal_clf = CalibratedClassifierCV(clf, method=method, cv=3) \n\t \tcal_clf.fit(X_train, y_train) \n\t \tcal_clf_probs = cal_clf.predict_proba(X_test) \n\t \tcal_loss = log_loss(y_test, cal_clf_probs) \n\t \tassert_greater(loss, cal_loss)\n", 
" \ttry: \n\t \treturn float(value) \n\texcept (TypeError, ValueError): \n\t \treturn default\n", 
" \t(p, options) = list(zip(*p_options)) \n\tn_options = len(options) \n\tch = scope.hyperopt_param(label, scope.categorical(p, upper=n_options)) \n\treturn scope.switch(ch, *options)\n", 
" \tchart = Chart() \n\tchart.add(u('S\\xc3\\xa9rie1'), [{'value': 1, 'xlink': 'http://1/', 'label': u('{\\\\}\\xc3\\x82\\xc2\\xb0\\xc4\\xb3\\xc3\\xa6\\xc3\\xb0\\xc2\\xa9&\\xc3\\x97&<\\xe2\\x80\\x94\\xc3\\x97\\xe2\\x82\\xac\\xc2\\xbf_\\xe2\\x80\\xa6\\\\{_\\xe2\\x80\\xa6')}, {'value': 2, 'xlink': {'href': 'http://6.example.com/'}, 'label': u('\\xc3\\xa6\\xc3\\x82\\xc2\\xb0\\xe2\\x82\\xac\\xe2\\x89\\xa0|\\xe2\\x82\\xac\\xc3\\xa6\\xc3\\x82\\xc2\\xb0\\xe2\\x82\\xac\\xc9\\x99\\xc3\\xa6')}, {'value': 3, 'label': 'unicode \t<3'}]) \n\tif (not chart._dual): \n\t \tchart.x_labels = [u('&\\xc5\\x93'), u('\\xc2\\xbf?'), u('\\xe2\\x80\\xa0\\xe2\\x80\\xa0\\xe2\\x80\\xa0\\xe2\\x80\\xa0\\xe2\\x80\\xa0\\xe2\\x80\\xa0\\xe2\\x80\\xa0\\xe2\\x80\\xa0'), 'unicode \t<3'] \n\tchart.render_pyquery()\n", 
" \tclf1 = LogisticRegression(random_state=123) \n\tclf2 = RandomForestClassifier(random_state=123) \n\tclf3 = GaussianNB() \n\teclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='hard') \n\tscores = cross_val_score(eclf, X, y, cv=5, scoring='accuracy') \n\tassert_almost_equal(scores.mean(), 0.95, decimal=2)\n", 
" \tdef is_abstract(c): \n\t \tif (not hasattr(c, '__abstractmethods__')): \n\t \t \treturn False \n\t \tif (not len(c.__abstractmethods__)): \n\t \t \treturn False \n\t \treturn True \n\tall_classes = [] \n\tpath = sklearn.__path__ \n\tfor (importer, modname, ispkg) in pkgutil.walk_packages(path=path, prefix='sklearn.', onerror=(lambda x: None)): \n\t \tif ('.tests.' in modname): \n\t \t \tcontinue \n\t \tmodule = __import__(modname, fromlist='dummy') \n\t \tclasses = inspect.getmembers(module, inspect.isclass) \n\t \tall_classes.extend(classes) \n\tall_classes = set(all_classes) \n\testimators = [c for c in all_classes if (issubclass(c[1], BaseEstimator) and (c[0] != 'BaseEstimator'))] \n\testimators = [c for c in estimators if (not is_abstract(c[1]))] \n\tif (not include_dont_test): \n\t \testimators = [c for c in estimators if (not (c[0] in DONT_TEST))] \n\tif (not include_other): \n\t \testimators = [c for c in estimators if (not (c[0] in OTHER))] \n\tif (not include_meta_estimators): \n\t \testimators = [c for c in estimators if (not (c[0] in META_ESTIMATORS))] \n\tif (type_filter is not None): \n\t \tif (not isinstance(type_filter, list)): \n\t \t \ttype_filter = [type_filter] \n\t \telse: \n\t \t \ttype_filter = list(type_filter) \n\t \tfiltered_estimators = [] \n\t \tfilters = {'classifier': ClassifierMixin, 'regressor': RegressorMixin, 'transformer': TransformerMixin, 'cluster': ClusterMixin} \n\t \tfor (name, mixin) in filters.items(): \n\t \t \tif (name in type_filter): \n\t \t \t \ttype_filter.remove(name) \n\t \t \t \tfiltered_estimators.extend([est for est in estimators if issubclass(est[1], mixin)]) \n\t \testimators = filtered_estimators \n\t \tif type_filter: \n\t \t \traise ValueError((\"Parameter \ttype_filter \tmust \tbe \t'classifier', \t'regressor', \t'transformer', \t'cluster' \tor \tNone, \tgot \t%s.\" % repr(type_filter))) \n\treturn sorted(set(estimators), key=itemgetter(0))\n", 
" \tclf = GaussianNB() \n\tclf.fit(X, y) \n\tassert_raises(ValueError, clf.partial_fit, np.hstack((X, X)), y)\n", 
" \tfeature = Fisherfaces() \n\tclassifier = NearestNeighbor(dist_metric=EuclideanDistance(), k=1) \n\treturn ExtendedPredictableModel(feature=feature, classifier=classifier, image_size=image_size, subject_names=subject_names)\n", 
" \treturn {dataset.dataset_id: dataset for dataset in iterable}\n", 
" \twith pytest.raises(ValueError): \n\t \tdat = ascii.read(['c \td', 'e \tf'], names=('a',), guess=False, fast_reader=fast_reader)\n", 
" \traise ValueError\n", 
" \tif (not isinstance(html, _strings)): \n\t \traise TypeError('string \trequired') \n\taccept_leading_text = bool(create_parent) \n\telements = fragments_fromstring(html, guess_charset=guess_charset, parser=parser, no_leading_text=(not accept_leading_text)) \n\tif create_parent: \n\t \tif (not isinstance(create_parent, _strings)): \n\t \t \tcreate_parent = 'div' \n\t \tnew_root = Element(create_parent) \n\t \tif elements: \n\t \t \tif isinstance(elements[0], _strings): \n\t \t \t \tnew_root.text = elements[0] \n\t \t \t \tdel elements[0] \n\t \t \tnew_root.extend(elements) \n\t \treturn new_root \n\tif (not elements): \n\t \traise etree.ParserError('No \telements \tfound') \n\tif (len(elements) > 1): \n\t \traise etree.ParserError('Multiple \telements \tfound') \n\tresult = elements[0] \n\tif (result.tail and result.tail.strip()): \n\t \traise etree.ParserError(('Element \tfollowed \tby \ttext: \t%r' % result.tail)) \n\tresult.tail = None \n\treturn result\n", 
" \ttable = EvTable('{yHeading1{n', '{gHeading2{n', table=[[1, 2, 3], [4, 5, 6], [7, 8, 9]], border='cells', align='l') \n\ttable.add_column('{rThis \tis \tlong \tdata{n', '{bThis \tis \teven \tlonger \tdata{n') \n\ttable.add_row('This \tis \ta \tsingle \trow') \n\tprint(unicode(table)) \n\ttable.reformat(width=50) \n\tprint(unicode(table)) \n\ttable.reformat_column(3, width=30, align='r') \n\tprint(unicode(table)) \n\treturn table\n", 
" \ttable = EvTable('{yHeading1{n', '{gHeading2{n', table=[[1, 2, 3], [4, 5, 6], [7, 8, 9]], border='cells', align='l') \n\ttable.add_column('{rThis \tis \tlong \tdata{n', '{bThis \tis \teven \tlonger \tdata{n') \n\ttable.add_row('This \tis \ta \tsingle \trow') \n\tprint(unicode(table)) \n\ttable.reformat(width=50) \n\tprint(unicode(table)) \n\ttable.reformat_column(3, width=30, align='r') \n\tprint(unicode(table)) \n\treturn table\n", 
" \ttable = EvTable('{yHeading1{n', '{gHeading2{n', table=[[1, 2, 3], [4, 5, 6], [7, 8, 9]], border='cells', align='l') \n\ttable.add_column('{rThis \tis \tlong \tdata{n', '{bThis \tis \teven \tlonger \tdata{n') \n\ttable.add_row('This \tis \ta \tsingle \trow') \n\tprint(unicode(table)) \n\ttable.reformat(width=50) \n\tprint(unicode(table)) \n\ttable.reformat_column(3, width=30, align='r') \n\tprint(unicode(table)) \n\treturn table\n", 
" \ttable = EvTable('{yHeading1{n', '{gHeading2{n', table=[[1, 2, 3], [4, 5, 6], [7, 8, 9]], border='cells', align='l') \n\ttable.add_column('{rThis \tis \tlong \tdata{n', '{bThis \tis \teven \tlonger \tdata{n') \n\ttable.add_row('This \tis \ta \tsingle \trow') \n\tprint(unicode(table)) \n\ttable.reformat(width=50) \n\tprint(unicode(table)) \n\ttable.reformat_column(3, width=30, align='r') \n\tprint(unicode(table)) \n\treturn table\n", 
" \ttable = EvTable('{yHeading1{n', '{gHeading2{n', table=[[1, 2, 3], [4, 5, 6], [7, 8, 9]], border='cells', align='l') \n\ttable.add_column('{rThis \tis \tlong \tdata{n', '{bThis \tis \teven \tlonger \tdata{n') \n\ttable.add_row('This \tis \ta \tsingle \trow') \n\tprint(unicode(table)) \n\ttable.reformat(width=50) \n\tprint(unicode(table)) \n\ttable.reformat_column(3, width=30, align='r') \n\tprint(unicode(table)) \n\treturn table\n", 
" \talpha = 0.1 \n\tn_samples = 20 \n\ttol = 1e-05 \n\tmax_iter = 50 \n\tclass_weight = {0: 0.45, 1: 0.55, 2: 0.75} \n\tfit_intercept = True \n\t(X, y) = make_blobs(n_samples=n_samples, centers=3, random_state=0, cluster_std=0.1) \n\tstep_size = get_step_size(X, alpha, fit_intercept, classification=True) \n\tclasses = np.unique(y) \n\tclf1 = LogisticRegression(solver='sag', C=((1.0 / alpha) / n_samples), max_iter=max_iter, tol=tol, random_state=77, fit_intercept=fit_intercept, class_weight=class_weight) \n\tclf2 = clone(clf1) \n\tclf1.fit(X, y) \n\tclf2.fit(sp.csr_matrix(X), y) \n\tle = LabelEncoder() \n\tclass_weight_ = compute_class_weight(class_weight, np.unique(y), y) \n\tsample_weight = class_weight_[le.fit_transform(y)] \n\tcoef1 = [] \n\tintercept1 = [] \n\tcoef2 = [] \n\tintercept2 = [] \n\tfor cl in classes: \n\t \ty_encoded = np.ones(n_samples) \n\t \ty_encoded[(y != cl)] = (-1) \n\t \t(spweights1, spintercept1) = sag_sparse(X, y_encoded, step_size, alpha, n_iter=max_iter, dloss=log_dloss, sample_weight=sample_weight) \n\t \t(spweights2, spintercept2) = sag_sparse(X, y_encoded, step_size, alpha, n_iter=max_iter, dloss=log_dloss, sample_weight=sample_weight, sparse=True) \n\t \tcoef1.append(spweights1) \n\t \tintercept1.append(spintercept1) \n\t \tcoef2.append(spweights2) \n\t \tintercept2.append(spintercept2) \n\tcoef1 = np.vstack(coef1) \n\tintercept1 = np.array(intercept1) \n\tcoef2 = np.vstack(coef2) \n\tintercept2 = np.array(intercept2) \n\tfor (i, cl) in enumerate(classes): \n\t \tassert_array_almost_equal(clf1.coef_[i].ravel(), coef1[i].ravel(), decimal=2) \n\t \tassert_almost_equal(clf1.intercept_[i], intercept1[i], decimal=1) \n\t \tassert_array_almost_equal(clf2.coef_[i].ravel(), coef2[i].ravel(), decimal=2) \n\t \tassert_almost_equal(clf2.intercept_[i], intercept2[i], decimal=1)\n", 
" \tdef is_abstract(c): \n\t \tif (not hasattr(c, '__abstractmethods__')): \n\t \t \treturn False \n\t \tif (not len(c.__abstractmethods__)): \n\t \t \treturn False \n\t \treturn True \n\tall_classes = [] \n\tpath = sklearn.__path__ \n\tfor (importer, modname, ispkg) in pkgutil.walk_packages(path=path, prefix='sklearn.', onerror=(lambda x: None)): \n\t \tif ('.tests.' in modname): \n\t \t \tcontinue \n\t \tmodule = __import__(modname, fromlist='dummy') \n\t \tclasses = inspect.getmembers(module, inspect.isclass) \n\t \tall_classes.extend(classes) \n\tall_classes = set(all_classes) \n\testimators = [c for c in all_classes if (issubclass(c[1], BaseEstimator) and (c[0] != 'BaseEstimator'))] \n\testimators = [c for c in estimators if (not is_abstract(c[1]))] \n\tif (not include_dont_test): \n\t \testimators = [c for c in estimators if (not (c[0] in DONT_TEST))] \n\tif (not include_other): \n\t \testimators = [c for c in estimators if (not (c[0] in OTHER))] \n\tif (not include_meta_estimators): \n\t \testimators = [c for c in estimators if (not (c[0] in META_ESTIMATORS))] \n\tif (type_filter is not None): \n\t \tif (not isinstance(type_filter, list)): \n\t \t \ttype_filter = [type_filter] \n\t \telse: \n\t \t \ttype_filter = list(type_filter) \n\t \tfiltered_estimators = [] \n\t \tfilters = {'classifier': ClassifierMixin, 'regressor': RegressorMixin, 'transformer': TransformerMixin, 'cluster': ClusterMixin} \n\t \tfor (name, mixin) in filters.items(): \n\t \t \tif (name in type_filter): \n\t \t \t \ttype_filter.remove(name) \n\t \t \t \tfiltered_estimators.extend([est for est in estimators if issubclass(est[1], mixin)]) \n\t \testimators = filtered_estimators \n\t \tif type_filter: \n\t \t \traise ValueError((\"Parameter \ttype_filter \tmust \tbe \t'classifier', \t'regressor', \t'transformer', \t'cluster' \tor \tNone, \tgot \t%s.\" % repr(type_filter))) \n\treturn sorted(set(estimators), key=itemgetter(0))\n", 
" \tif ((value is None) and empty_ok): \n\t \treturn \n\tif (not isinstance(value, (int, long))): \n\t \traise exception(('%s \tshould \tbe \tan \tinteger; \treceived \t%s \t(a \t%s).' % (name, value, typename(value)))) \n\tif ((not value) and (not zero_ok)): \n\t \traise exception(('%s \tmust \tnot \tbe \t0 \t(zero)' % name)) \n\tif ((value < 0) and (not negative_ok)): \n\t \traise exception(('%s \tmust \tnot \tbe \tnegative.' % name))\n", 
" \tif (not isinstance(html, _strings)): \n\t \traise TypeError('string \trequired') \n\taccept_leading_text = bool(create_parent) \n\telements = fragments_fromstring(html, guess_charset=guess_charset, parser=parser, no_leading_text=(not accept_leading_text)) \n\tif create_parent: \n\t \tif (not isinstance(create_parent, _strings)): \n\t \t \tcreate_parent = 'div' \n\t \tnew_root = Element(create_parent) \n\t \tif elements: \n\t \t \tif isinstance(elements[0], _strings): \n\t \t \t \tnew_root.text = elements[0] \n\t \t \t \tdel elements[0] \n\t \t \tnew_root.extend(elements) \n\t \treturn new_root \n\tif (not elements): \n\t \traise etree.ParserError('No \telements \tfound') \n\tif (len(elements) > 1): \n\t \traise etree.ParserError('Multiple \telements \tfound') \n\tresult = elements[0] \n\tif (result.tail and result.tail.strip()): \n\t \traise etree.ParserError(('Element \tfollowed \tby \ttext: \t%r' % result.tail)) \n\tresult.tail = None \n\treturn result\n", 
" \trng = np.random.RandomState(42) \n\tt = rng.randn(100) \n\tedges = bayesian_blocks(t, fitness=u'events') \n\tassert_allclose(edges, [(-2.6197451), (-0.71094865), 0.36866702, 1.85227818]) \n\tt[80:] = t[:20] \n\tedges = bayesian_blocks(t, fitness=u'events', p0=0.01) \n\tassert_allclose(edges, [(-2.6197451), (-0.47432431), (-0.46202823), 1.85227818]) \n\tdt = 0.01 \n\tt = (dt * np.arange(1000)) \n\tx = np.zeros(len(t)) \n\tN = (len(t) // 10) \n\tx[rng.randint(0, len(t), N)] = 1 \n\tx[rng.randint(0, (len(t) // 2), N)] = 1 \n\tedges = bayesian_blocks(t, x, fitness=u'regular_events', dt=dt) \n\tassert_allclose(edges, [0, 5.105, 9.99]) \n\tt = (100 * rng.rand(20)) \n\tx = np.exp(((-0.5) * ((t - 50) ** 2))) \n\tsigma = 0.1 \n\tx_obs = (x + (sigma * rng.randn(len(x)))) \n\tedges = bayesian_blocks(t, x_obs, sigma, fitness=u'measures') \n\tassert_allclose(edges, [4.360377, 48.456895, 52.597917, 99.455051])\n", 
" \traise ValueError\n", 
" \tx_arr = np.asarray(x) \n\tparams = x_arr[:k_params].ravel() \n\tu = x_arr[k_params:] \n\tobjective_func_arr = (f(params, *args) + (alpha * u).sum()) \n\treturn matrix(objective_func_arr)\n", 
" \tx = np.asarray(x) \n\tmask = isnull(x) \n\tx = x[(~ mask)] \n\tvalues = np.sort(x) \n\tdef _get_score(at): \n\t \tif (len(values) == 0): \n\t \t \treturn np.nan \n\t \tidx = (at * (len(values) - 1)) \n\t \tif ((idx % 1) == 0): \n\t \t \tscore = values[int(idx)] \n\t \telif (interpolation_method == 'fraction'): \n\t \t \tscore = _interpolate(values[int(idx)], values[(int(idx) + 1)], (idx % 1)) \n\t \telif (interpolation_method == 'lower'): \n\t \t \tscore = values[np.floor(idx)] \n\t \telif (interpolation_method == 'higher'): \n\t \t \tscore = values[np.ceil(idx)] \n\t \telse: \n\t \t \traise ValueError(\"interpolation_method \tcan \tonly \tbe \t'fraction' \t, \t'lower' \tor \t'higher'\") \n\t \treturn score \n\tif is_scalar(q): \n\t \treturn _get_score(q) \n\telse: \n\t \tq = np.asarray(q, np.float64) \n\t \treturn algos.arrmap_float64(q, _get_score)\n", 
" \tclf = GaussianNB(priors=np.array([0.25, 0.25, 0.25, 0.25])) \n\tassert_raises(ValueError, clf.fit, X, y)\n", 
" \tdef g(target, *args, **kwargs): \n\t \tval = func(target, *args, **kwargs) \n\t \tif val: \n\t \t \treturn val \n\t \telse: \n\t \t \traise exception_class((value_error_message_template % target)) \n\tg.__name__ = func.__name__ \n\tg.__doc__ = func.__doc__ \n\treturn g\n", 
" \tapp = app_or_default(app) \n\tapp.set_current() \n\tplatforms.signals.reset(*WORKER_SIGRESET) \n\tplatforms.signals.ignore(*WORKER_SIGIGNORE) \n\tplatforms.set_mp_process_title('celeryd', hostname=hostname) \n\tapp.loader.init_worker() \n\tapp.loader.init_worker_process() \n\tsignals.worker_process_init.send(sender=None)\n", 
" \tapp = app_or_default(app) \n\tapp.set_current() \n\tplatforms.signals.reset(*WORKER_SIGRESET) \n\tplatforms.signals.ignore(*WORKER_SIGIGNORE) \n\tplatforms.set_mp_process_title('celeryd', hostname=hostname) \n\tapp.loader.init_worker() \n\tapp.loader.init_worker_process() \n\tsignals.worker_process_init.send(sender=None)\n", 
" \tif (headers is None): \n\t \theaders = {} \n\tif (data and (not method)): \n\t \tmethod = 'POST' \n\telif (not method): \n\t \tmethod = 'GET' \n\tif ((method == 'GET') and data): \n\t \turi = add_params_to_uri(uri, data) \n\t \tdata = None \n\treturn (uri, headers, data, method)\n", 
" \traise ValueError\n", 
" \tdef is_abstract(c): \n\t \tif (not hasattr(c, '__abstractmethods__')): \n\t \t \treturn False \n\t \tif (not len(c.__abstractmethods__)): \n\t \t \treturn False \n\t \treturn True \n\tall_classes = [] \n\tpath = sklearn.__path__ \n\tfor (importer, modname, ispkg) in pkgutil.walk_packages(path=path, prefix='sklearn.', onerror=(lambda x: None)): \n\t \tif ('.tests.' in modname): \n\t \t \tcontinue \n\t \tmodule = __import__(modname, fromlist='dummy') \n\t \tclasses = inspect.getmembers(module, inspect.isclass) \n\t \tall_classes.extend(classes) \n\tall_classes = set(all_classes) \n\testimators = [c for c in all_classes if (issubclass(c[1], BaseEstimator) and (c[0] != 'BaseEstimator'))] \n\testimators = [c for c in estimators if (not is_abstract(c[1]))] \n\tif (not include_dont_test): \n\t \testimators = [c for c in estimators if (not (c[0] in DONT_TEST))] \n\tif (not include_other): \n\t \testimators = [c for c in estimators if (not (c[0] in OTHER))] \n\tif (not include_meta_estimators): \n\t \testimators = [c for c in estimators if (not (c[0] in META_ESTIMATORS))] \n\tif (type_filter is not None): \n\t \tif (not isinstance(type_filter, list)): \n\t \t \ttype_filter = [type_filter] \n\t \telse: \n\t \t \ttype_filter = list(type_filter) \n\t \tfiltered_estimators = [] \n\t \tfilters = {'classifier': ClassifierMixin, 'regressor': RegressorMixin, 'transformer': TransformerMixin, 'cluster': ClusterMixin} \n\t \tfor (name, mixin) in filters.items(): \n\t \t \tif (name in type_filter): \n\t \t \t \ttype_filter.remove(name) \n\t \t \t \tfiltered_estimators.extend([est for est in estimators if issubclass(est[1], mixin)]) \n\t \testimators = filtered_estimators \n\t \tif type_filter: \n\t \t \traise ValueError((\"Parameter \ttype_filter \tmust \tbe \t'classifier', \t'regressor', \t'transformer', \t'cluster' \tor \tNone, \tgot \t%s.\" % repr(type_filter))) \n\treturn sorted(set(estimators), key=itemgetter(0))\n", 
" \tif ((value is None) and empty_ok): \n\t \treturn \n\tif (not isinstance(value, (int, long))): \n\t \traise exception(('%s \tshould \tbe \tan \tinteger; \treceived \t%s \t(a \t%s).' % (name, value, typename(value)))) \n\tif ((not value) and (not zero_ok)): \n\t \traise exception(('%s \tmust \tnot \tbe \t0 \t(zero)' % name)) \n\tif ((value < 0) and (not negative_ok)): \n\t \traise exception(('%s \tmust \tnot \tbe \tnegative.' % name))\n", 
" \tif (not isinstance(html, _strings)): \n\t \traise TypeError('string \trequired') \n\taccept_leading_text = bool(create_parent) \n\telements = fragments_fromstring(html, guess_charset=guess_charset, parser=parser, no_leading_text=(not accept_leading_text)) \n\tif create_parent: \n\t \tif (not isinstance(create_parent, _strings)): \n\t \t \tcreate_parent = 'div' \n\t \tnew_root = Element(create_parent) \n\t \tif elements: \n\t \t \tif isinstance(elements[0], _strings): \n\t \t \t \tnew_root.text = elements[0] \n\t \t \t \tdel elements[0] \n\t \t \tnew_root.extend(elements) \n\t \treturn new_root \n\tif (not elements): \n\t \traise etree.ParserError('No \telements \tfound') \n\tif (len(elements) > 1): \n\t \traise etree.ParserError('Multiple \telements \tfound') \n\tresult = elements[0] \n\tif (result.tail and result.tail.strip()): \n\t \traise etree.ParserError(('Element \tfollowed \tby \ttext: \t%r' % result.tail)) \n\tresult.tail = None \n\treturn result\n", 
" \trng = np.random.RandomState(42) \n\tt = rng.randn(100) \n\tedges = bayesian_blocks(t, fitness=u'events') \n\tassert_allclose(edges, [(-2.6197451), (-0.71094865), 0.36866702, 1.85227818]) \n\tt[80:] = t[:20] \n\tedges = bayesian_blocks(t, fitness=u'events', p0=0.01) \n\tassert_allclose(edges, [(-2.6197451), (-0.47432431), (-0.46202823), 1.85227818]) \n\tdt = 0.01 \n\tt = (dt * np.arange(1000)) \n\tx = np.zeros(len(t)) \n\tN = (len(t) // 10) \n\tx[rng.randint(0, len(t), N)] = 1 \n\tx[rng.randint(0, (len(t) // 2), N)] = 1 \n\tedges = bayesian_blocks(t, x, fitness=u'regular_events', dt=dt) \n\tassert_allclose(edges, [0, 5.105, 9.99]) \n\tt = (100 * rng.rand(20)) \n\tx = np.exp(((-0.5) * ((t - 50) ** 2))) \n\tsigma = 0.1 \n\tx_obs = (x + (sigma * rng.randn(len(x)))) \n\tedges = bayesian_blocks(t, x_obs, sigma, fitness=u'measures') \n\tassert_allclose(edges, [4.360377, 48.456895, 52.597917, 99.455051])\n", 
" \traise ValueError\n", 
" \tx_arr = np.asarray(x) \n\tparams = x_arr[:k_params].ravel() \n\tu = x_arr[k_params:] \n\tobjective_func_arr = (f(params, *args) + (alpha * u).sum()) \n\treturn matrix(objective_func_arr)\n", 
" \tx = np.asarray(x) \n\tmask = isnull(x) \n\tx = x[(~ mask)] \n\tvalues = np.sort(x) \n\tdef _get_score(at): \n\t \tif (len(values) == 0): \n\t \t \treturn np.nan \n\t \tidx = (at * (len(values) - 1)) \n\t \tif ((idx % 1) == 0): \n\t \t \tscore = values[int(idx)] \n\t \telif (interpolation_method == 'fraction'): \n\t \t \tscore = _interpolate(values[int(idx)], values[(int(idx) + 1)], (idx % 1)) \n\t \telif (interpolation_method == 'lower'): \n\t \t \tscore = values[np.floor(idx)] \n\t \telif (interpolation_method == 'higher'): \n\t \t \tscore = values[np.ceil(idx)] \n\t \telse: \n\t \t \traise ValueError(\"interpolation_method \tcan \tonly \tbe \t'fraction' \t, \t'lower' \tor \t'higher'\") \n\t \treturn score \n\tif is_scalar(q): \n\t \treturn _get_score(q) \n\telse: \n\t \tq = np.asarray(q, np.float64) \n\t \treturn algos.arrmap_float64(q, _get_score)\n", 
" \tclf = GaussianNB(priors=np.array([0.25, 0.25, 0.25, 0.25])) \n\tassert_raises(ValueError, clf.fit, X, y)\n", 
" \tdef g(target, *args, **kwargs): \n\t \tval = func(target, *args, **kwargs) \n\t \tif val: \n\t \t \treturn val \n\t \telse: \n\t \t \traise exception_class((value_error_message_template % target)) \n\tg.__name__ = func.__name__ \n\tg.__doc__ = func.__doc__ \n\treturn g\n", 
" \tapp = app_or_default(app) \n\tapp.set_current() \n\tplatforms.signals.reset(*WORKER_SIGRESET) \n\tplatforms.signals.ignore(*WORKER_SIGIGNORE) \n\tplatforms.set_mp_process_title('celeryd', hostname=hostname) \n\tapp.loader.init_worker() \n\tapp.loader.init_worker_process() \n\tsignals.worker_process_init.send(sender=None)\n", 
" \tapp = app_or_default(app) \n\tapp.set_current() \n\tplatforms.signals.reset(*WORKER_SIGRESET) \n\tplatforms.signals.ignore(*WORKER_SIGIGNORE) \n\tplatforms.set_mp_process_title('celeryd', hostname=hostname) \n\tapp.loader.init_worker() \n\tapp.loader.init_worker_process() \n\tsignals.worker_process_init.send(sender=None)\n", 
" \tif (headers is None): \n\t \theaders = {} \n\tif (data and (not method)): \n\t \tmethod = 'POST' \n\telif (not method): \n\t \tmethod = 'GET' \n\tif ((method == 'GET') and data): \n\t \turi = add_params_to_uri(uri, data) \n\t \tdata = None \n\treturn (uri, headers, data, method)\n", 
" \traise ValueError\n", 
" \t(listing_dict['content_type'], swift_bytes) = extract_swift_bytes(listing_dict['content_type']) \n\tif (swift_bytes is not None): \n\t \ttry: \n\t \t \tlisting_dict['bytes'] = int(swift_bytes) \n\t \texcept ValueError: \n\t \t \tif logger: \n\t \t \t \tlogger.exception(_('Invalid \tswift_bytes'))\n", 
" \tdef getter(self): \n\t \tif ('content-type' in self.headers): \n\t \t \treturn self.headers.get('content-type').split(';')[0] \n\tdef setter(self, value): \n\t \tself.headers['content-type'] = value \n\treturn property(getter, setter, doc='Retrieve \tand \tset \tthe \tresponse \tContent-Type \theader')\n", 
" \timport string \n\tfor c in string.ascii_uppercase: \n\t \tname = name.replace(c, ('_%c' % c)) \n\treturn name.strip('_').lower()\n", 
" \timport string \n\tfor c in string.ascii_uppercase: \n\t \tname = name.replace(c, ('_%c' % c)) \n\treturn name.strip('_').lower()\n", 
" \tcls = None \n\tif (compat_version in DiffCompatVersion.MYERS_VERSIONS): \n\t \tfrom reviewboard.diffviewer.myersdiff import MyersDiffer \n\t \tcls = MyersDiffer \n\telif (compat_version == DiffCompatVersion.SMDIFFER): \n\t \tfrom reviewboard.diffviewer.smdiff import SMDiffer \n\t \tcls = SMDiffer \n\telse: \n\t \traise DiffCompatError((u'Invalid \tdiff \tcompatibility \tversion \t(%s) \tpassed \tto \tDiffer' % compat_version)) \n\treturn cls(a, b, ignore_space, compat_version=compat_version)\n", 
" \tparts = name.rsplit('#', 1) \n\tparts[0] = ('%s_%s' % (parts[0], suffix)) \n\treturn '#'.join(parts)\n", 
" \treturn filter((lambda f: f.lower().endswith(extension)), file_names)\n", 
" \tassert_nD(data, 2, 'data') \n\tif (predefined_filter is None): \n\t \tpredefined_filter = LPIFilter2D(impulse_response, **filter_params) \n\treturn predefined_filter(data)\n", 
" \ttry: \n\t \tfrom cStringIO import StringIO \n\texcept ImportError: \n\t \tfrom StringIO import StringIO \n\ts = StringIO() \n\tnum_errors = get_validation_errors(s, app) \n\tif num_errors: \n\t \tif app: \n\t \t \tsys.stderr.write(style.ERROR((\"Error: \t%s \tcouldn't \tbe \tinstalled, \tbecause \tthere \twere \terrors \tin \tyour \tmodel:\\n\" % app))) \n\t \telse: \n\t \t \tsys.stderr.write(style.ERROR(\"Error: \tCouldn't \tinstall \tapps, \tbecause \tthere \twere \terrors \tin \tone \tor \tmore \tmodels:\\n\")) \n\t \ts.seek(0) \n\t \tsys.stderr.write(s.read()) \n\t \tsys.exit(1)\n", 
" \ttry: \n\t \treturn bool(clean_path(opts['pki_dir'], id_)) \n\texcept (AttributeError, KeyError) as e: \n\t \treturn False\n", 
" \tret = {'name': name, 'changes': {}, 'comment': str(), 'result': None} \n\tif (not settings): \n\t \tret['comment'] = 'No \tsettings \tto \tchange \tprovided.' \n\t \tret['result'] = True \n\t \treturn ret \n\tret_settings = dict() \n\tret_settings['changes'] = {} \n\tret_settings['failures'] = {} \n\tcurrent_settings = __salt__['win_smtp_server.get_server_setting'](settings=settings.keys(), server=server) \n\tfor key in settings: \n\t \tsettings = _normalize_server_settings(**settings) \n\t \tif (str(settings[key]) != str(current_settings[key])): \n\t \t \tret_settings['changes'][key] = {'old': current_settings[key], 'new': settings[key]} \n\tif (not ret_settings['changes']): \n\t \tret['comment'] = 'Settings \talready \tcontain \tthe \tprovided \tvalues.' \n\t \tret['result'] = True \n\t \treturn ret \n\telif __opts__['test']: \n\t \tret['comment'] = 'Settings \twill \tbe \tchanged.' \n\t \tret['changes'] = ret_settings \n\t \treturn ret \n\t__salt__['win_smtp_server.set_server_setting'](settings=settings, server=server) \n\tnew_settings = __salt__['win_smtp_server.get_server_setting'](settings=settings.keys(), server=server) \n\tfor key in settings: \n\t \tif (str(new_settings[key]) != str(settings[key])): \n\t \t \tret_settings['failures'][key] = {'old': current_settings[key], 'new': new_settings[key]} \n\t \t \tret_settings['changes'].pop(key, None) \n\tif ret_settings['failures']: \n\t \tret['comment'] = 'Some \tsettings \tfailed \tto \tchange.' \n\t \tret['changes'] = ret_settings \n\t \tret['result'] = False \n\telse: \n\t \tret['comment'] = 'Set \tsettings \tto \tcontain \tthe \tprovided \tvalues.' \n\t \tret['changes'] = ret_settings['changes'] \n\t \tret['result'] = True \n\treturn ret\n", 
" \tparent = node.parent \n\twhile (parent is not None): \n\t \tif isinstance(parent, astroid.Decorators): \n\t \t \treturn True \n\t \tif (parent.is_statement or isinstance(parent, astroid.Lambda) or isinstance(parent, (scoped_nodes.ComprehensionScope, scoped_nodes.ListComp))): \n\t \t \tbreak \n\t \tparent = parent.parent \n\treturn False\n", 
" \treturn (getattr(func, '_is_coroutine', False) or (hasattr(inspect, 'iscoroutinefunction') and inspect.iscoroutinefunction(func)))\n", 
" \tif (names is None): \n\t \tnames = [] \n\tfor arg in args: \n\t \tif isinstance(arg, Tuple): \n\t \t \t_rec_get_names(arg.elts, names) \n\t \telse: \n\t \t \tnames.append(arg.name) \n\treturn names\n", 
" \tparser = _parse_signature(func) \n\t(args, kwargs, missing, extra, extra_positional) = parser(args, kwargs)[:5] \n\tif missing: \n\t \traise ArgumentValidationError(tuple(missing)) \n\telif ((extra or extra_positional) and (not drop_extra)): \n\t \traise ArgumentValidationError(None, extra, extra_positional) \n\treturn (tuple(args), kwargs)\n", 
" \tparser = _parse_signature(func) \n\t(args, kwargs, missing, extra, extra_positional) = parser(args, kwargs)[:5] \n\tif missing: \n\t \traise ArgumentValidationError(tuple(missing)) \n\telif ((extra or extra_positional) and (not drop_extra)): \n\t \traise ArgumentValidationError(None, extra, extra_positional) \n\treturn (tuple(args), kwargs)\n", 
" \tdef _matcher(seq, *args, **kwargs): \n\t \tfor cls in seq: \n\t \t \ttry: \n\t \t \t \tanswer = getattr(maybe_promise(cls), method)(*args, **kwargs) \n\t \t \t \tif (answer is not None): \n\t \t \t \t \treturn answer \n\t \t \texcept AttributeError: \n\t \t \t \tpass \n\treturn _matcher\n", 
" \tdata = get_instance(name, provider) \n\tif (data is None): \n\t \treturn False \n\treturn True\n", 
" \tassert isinstance(args, (list, tuple)) \n\tsignature = getattr(inspect, u'signature', None) \n\tif (signature is not None): \n\t \ttry: \n\t \t \tsig = _signatures_cache[func] \n\t \texcept KeyError: \n\t \t \tsig = signature(func) \n\t \t \t_signatures_cache[func] = sig \n\t \ttry: \n\t \t \tsig.bind(*args) \n\t \texcept TypeError: \n\t \t \treturn False \n\t \telse: \n\t \t \treturn True \n\telse: \n\t \tspec = inspect.getargspec(func) \n\t \tdef drop_self(spec): \n\t \t \t(args, varargs, varkw, defaults) = spec \n\t \t \tif (args[0:1] == [u'self']): \n\t \t \t \targs = args[1:] \n\t \t \treturn inspect.ArgSpec(args, varargs, varkw, defaults) \n\t \tspec = drop_self(spec) \n\t \tif (spec.varargs is not None): \n\t \t \treturn True \n\t \treturn ((len(spec.args) - len((spec.defaults or []))) <= len(args) <= len(spec.args))\n", 
" \tif isinstance(URL, unicode): \n\t \traise TypeError('Unicode \tobject \tnot \tallowed \tas \tURL') \n\trequest.setHeader('Content-Type', 'text/html; \tcharset=utf-8') \n\trequest.redirect(URL) \n\tcontent = ('\\n<html>\\n \t \t \t \t<head>\\n \t \t \t \t \t \t \t \t<meta \thttp-equiv=\"refresh\" \tcontent=\"0;URL=%(url)s\">\\n \t \t \t \t</head>\\n \t \t \t \t<body \tbgcolor=\"#FFFFFF\" \ttext=\"#000000\">\\n \t \t \t \t<a \thref=\"%(url)s\">click \there</a>\\n \t \t \t \t</body>\\n</html>\\n' % {'url': nativeString(URL)}) \n\tif _PY3: \n\t \tcontent = content.encode('utf8') \n\treturn content\n", 
" \tdef _redirect(environ, start_response): \n\t \t(args, kwargs) = environ['wsgiorg.routing_args'] \n\t \tstart_response('301 \tMOVED \tPERMANENTLY', [('Location', to.format(*args, **kwargs))]) \n\t \treturn [] \n\treturn _redirect\n", 
" \treturn (300 <= status <= 399)\n", 
" \treturn (300 <= status <= 399)\n", 
" \treturn (300 <= status <= 399)\n", 
" \treturn HttpResponseRedirect('/test_client/permanent_redirect_view/')\n", 
" \tmember = trans_member \n\twith pytest.raises(EmailAddress.DoesNotExist): \n\t \tEmailAddress.objects.get(user=member) \n\tmember.email = 'member@this.test' \n\taccounts.utils.verify_user(member) \n\tEmailAddress.objects.get(user=member, email='member@this.test', primary=True, verified=True) \n\tassert (get_user_model().objects.get(pk=member.pk).email == '')\n", 
" \tif (not isinstance(auth, text_type)): \n\t \tauth = auth.decode('iso-8859-1') \n\ttry: \n\t \t(method, data) = auth.split(None, 1) \n\t \tif (method.lower() == 'basic'): \n\t \t \t(username, code) = b64decode(data).decode('iso-8859-1').split(':', 1) \n\t \t \ttry: \n\t \t \t \tuser = User.objects.get(username=username, auth_token__key=code) \n\t \t \texcept User.DoesNotExist: \n\t \t \t \treturn False \n\t \t \tif (not user.is_active): \n\t \t \t \treturn False \n\t \t \trequest.user = user \n\t \t \treturn True \n\t \telse: \n\t \t \treturn False \n\texcept (ValueError, TypeError): \n\t \treturn False\n", 
" \tproxy_username = None \n\tproxy_password = None \n\tproxy_username = os.environ.get('proxy-username') \n\tif (not proxy_username): \n\t \tproxy_username = os.environ.get('proxy_username') \n\tproxy_password = os.environ.get('proxy-password') \n\tif (not proxy_password): \n\t \tproxy_password = os.environ.get('proxy_password') \n\tif (not proxy_username): \n\t \tif ('@' in proxy_settings): \n\t \t \tprotocol_and_proxy_auth = proxy_settings.split('@')[0].split(':') \n\t \t \tif (len(protocol_and_proxy_auth) == 3): \n\t \t \t \tproxy_username = protocol_and_proxy_auth[1].lstrip('/') \n\t \t \t \tproxy_password = protocol_and_proxy_auth[2] \n\t \t \telif (len(protocol_and_proxy_auth) == 2): \n\t \t \t \tproxy_username = protocol_and_proxy_auth[0] \n\t \t \t \tproxy_password = protocol_and_proxy_auth[1] \n\tif proxy_username: \n\t \tuser_auth = base64.encodestring(('%s:%s' % (proxy_username, proxy_password))) \n\t \treturn ('Basic \t%s\\r\\n' % user_auth.strip()) \n\telse: \n\t \treturn ''\n", 
" \tif (not strategy.request.session.session_key): \n\t \tstrategy.request.session.create() \n\ttemplate = u'activation' \n\tif strategy.request.session.pop(u'password_reset', False): \n\t \ttemplate = u'reset' \n\turl = u'{}?verification_code={}&id={}&type={}'.format(reverse(u'social:complete', args=(backend.name,)), code.code, strategy.request.session.session_key, template) \n\tsend_notification_email(None, code.email, template, info=code.code, context={u'url': url})\n", 
" \tkstone = auth(profile, **connection_args) \n\tif ('connection_endpoint' in connection_args): \n\t \tauth_url = connection_args.get('connection_endpoint') \n\telif (_OS_IDENTITY_API_VERSION > 2): \n\t \tauth_url = __salt__['config.option']('keystone.endpoint', 'http://127.0.0.1:35357/v3') \n\telse: \n\t \tauth_url = __salt__['config.option']('keystone.endpoint', 'http://127.0.0.1:35357/v2.0') \n\tif user_id: \n\t \tfor user in kstone.users.list(): \n\t \t \tif (user.id == user_id): \n\t \t \t \tname = user.name \n\t \t \t \tbreak \n\tif (not name): \n\t \treturn {'Error': 'Unable \tto \tresolve \tuser \tname'} \n\tkwargs = {'username': name, 'password': password, 'auth_url': auth_url} \n\ttry: \n\t \tif (_OS_IDENTITY_API_VERSION > 2): \n\t \t \tclient3.Client(**kwargs) \n\t \telse: \n\t \t \tclient.Client(**kwargs) \n\texcept (keystoneclient.exceptions.Unauthorized, keystoneclient.exceptions.AuthorizationFailure): \n\t \treturn False \n\treturn True\n", 
" \tif (schema_type == 'string'): \n\t \tif ((type(value) == type('')) or (type(value) == type(u''))): \n\t \t \treturn value \n\t \telse: \n\t \t \treturn str(value) \n\telif (schema_type == 'integer'): \n\t \treturn str(int(value)) \n\telif (schema_type == 'number'): \n\t \treturn str(float(value)) \n\telif (schema_type == 'boolean'): \n\t \treturn str(bool(value)).lower() \n\telif ((type(value) == type('')) or (type(value) == type(u''))): \n\t \treturn value \n\telse: \n\t \treturn str(value)\n", 
" \treturn _dumps(s, cls=(cls or _default_encoder), **dict(default_kwargs, **kwargs))\n", 
" \treturn isinstance(obj, CLASS_TYPES)\n", 
" \tif (text is None): \n\t \treturn '' \n\tif (sys.version_info < (3, 0)): \n\t \timport __builtin__ \n\t \treturn __builtin__.bytes(text) \n\telse: \n\t \timport builtins \n\t \tif isinstance(text, builtins.bytes): \n\t \t \treturn text \n\t \tif isinstance(text, list): \n\t \t \treturn builtins.bytes(text) \n\t \telse: \n\t \t \treturn builtins.bytes(text, encoding='utf-8')\n", 
" \treturn WikiParser().parse(wiki_markup, show_toc=False, locale=locale, nofollow=nofollow)\n", 
" \treturn _dumps(s, cls=(cls or _default_encoder), **dict(default_kwargs, **kwargs))\n", 
" \treturn json.dumps(d, indent=4, separators=(',', ': \t'), sort_keys=sort_keys)\n", 
" \tsource = module.params.get('source') \n\tzone = module.params.get('zone') \n\tdesc = module.params.get('description') \n\ttimeout = module.params.get('timeout') \n\tfamily = module.params.get('family') \n\tif (not source): \n\t \tmodule.fail_json(msg='Must \tsupply \ta \tsource', changed=False) \n\tif source.startswith(GCS_URI): \n\t \tvolume = source \n\telif source.startswith('gs://'): \n\t \tvolume = source.replace('gs://', GCS_URI) \n\telse: \n\t \ttry: \n\t \t \tvolume = gce.ex_get_volume(source, zone) \n\t \texcept ResourceNotFoundError: \n\t \t \tmodule.fail_json(msg=('Disk \t%s \tnot \tfound \tin \tzone \t%s' % (source, zone)), changed=False) \n\t \texcept GoogleBaseError as e: \n\t \t \tmodule.fail_json(msg=str(e), changed=False) \n\tgce_extra_args = {} \n\tif (family is not None): \n\t \tgce_extra_args['family'] = family \n\told_timeout = gce.connection.timeout \n\ttry: \n\t \tgce.connection.timeout = timeout \n\t \tgce.ex_create_image(name, volume, desc, use_existing=False, **gce_extra_args) \n\t \treturn True \n\texcept ResourceExistsError: \n\t \treturn False \n\texcept GoogleBaseError as e: \n\t \tmodule.fail_json(msg=str(e), changed=False) \n\tfinally: \n\t \tgce.connection.timeout = old_timeout\n", 
" \tkwargs_for_reverse = ({key_name: unicode(key_value)} if key_name else None) \n\tif kwargs: \n\t \tkwargs_for_reverse.update(kwargs) \n\treturn reverse(('contentstore.views.' + handler_name), kwargs=kwargs_for_reverse)\n", 
" \tpath = package.__path__ \n\tpkg_prefix = (package.__name__ + '.') \n\tfor (importer, module_name, is_package) in pkgutil.walk_packages(path, pkg_prefix): \n\t \tif (('.tests.' in module_name) or module_name.endswith('.setup')): \n\t \t \tcontinue \n\t \ttry: \n\t \t \tmodule = _import_module(importer, module_name, package) \n\t \texcept ImportError: \n\t \t \tLOG.error(_LE('Failed \tto \timport \tmodule \t%s'), module_name) \n\t \t \tif (not ignore_error): \n\t \t \t \traise \n\t \telse: \n\t \t \tif (module is not None): \n\t \t \t \t(yield module)\n", 
" \tformats = [] \n\tformats = action_alias_db.get_format_strings() \n\tif (format_str not in formats): \n\t \traise ValueError(('Format \tstring \t\"%s\" \tis \tnot \tavailable \ton \tthe \talias \t\"%s\"' % (format_str, action_alias_db.name))) \n\tresult = extract_parameters(format_str=format_str, param_stream=param_stream) \n\treturn result\n", 
" \tid_to_qual = dict([rec for rec in MinimalQualParser(infile, value_cast_f)]) \n\treturn id_to_qual\n", 
" \taccepted = parseAcceptHeader(accept_header) \n\tpreferred = matchTypes(accepted, have_types) \n\treturn [mtype for (mtype, _) in preferred]\n", 
" \tparts = name.rsplit('#', 1) \n\tparts[0] = ('%s_%s' % (parts[0], suffix)) \n\treturn '#'.join(parts)\n", 
" \tres = set() \n\tprefix = abspath(prefix) \n\tignore = {u'pkgs', u'envs', u'conda-bld', u'conda-meta', u'.conda_lock', u'users', u'LICENSE.txt', u'info', u'conda-recipes', u'.index', u'.unionfs', u'.nonadmin'} \n\tbinignore = {u'conda', u'activate', u'deactivate'} \n\tif (sys.platform == u'darwin'): \n\t \tignore.update({u'python.app', u'Launcher.app'}) \n\tfor fn in os.listdir(prefix): \n\t \tif (ignore_predefined_files and (fn in ignore)): \n\t \t \tcontinue \n\t \tif isfile(join(prefix, fn)): \n\t \t \tres.add(fn) \n\t \t \tcontinue \n\t \tfor (root, dirs, files) in os.walk(join(prefix, fn)): \n\t \t \tshould_ignore = (ignore_predefined_files and (root == join(prefix, u'bin'))) \n\t \t \tfor fn2 in files: \n\t \t \t \tif (should_ignore and (fn2 in binignore)): \n\t \t \t \t \tcontinue \n\t \t \t \tres.add(relpath(join(root, fn2), prefix)) \n\t \t \tfor dn in dirs: \n\t \t \t \tpath = join(root, dn) \n\t \t \t \tif islink(path): \n\t \t \t \t \tres.add(relpath(path, prefix)) \n\tif (on_win and windows_forward_slashes): \n\t \treturn {path.replace(u'\\\\', u'/') for path in res} \n\telse: \n\t \treturn res\n", 
" \thtml = ustr(html) \n\tif (not html): \n\t \treturn '' \n\ttree = etree.fromstring(html, parser=etree.HTMLParser()) \n\tif (body_id is not None): \n\t \tsource = tree.xpath(('//*[@id=%s]' % (body_id,))) \n\telse: \n\t \tsource = tree.xpath('//body') \n\tif len(source): \n\t \ttree = source[0] \n\turl_index = [] \n\ti = 0 \n\tfor link in tree.findall('.//a'): \n\t \turl = link.get('href') \n\t \tif url: \n\t \t \ti += 1 \n\t \t \tlink.tag = 'span' \n\t \t \tlink.text = ('%s \t[%s]' % (link.text, i)) \n\t \t \turl_index.append(url) \n\thtml = ustr(etree.tostring(tree, encoding=encoding)) \n\thtml = html.replace('&#13;', '') \n\thtml = html.replace('<strong>', '*').replace('</strong>', '*') \n\thtml = html.replace('<b>', '*').replace('</b>', '*') \n\thtml = html.replace('<h3>', '*').replace('</h3>', '*') \n\thtml = html.replace('<h2>', '**').replace('</h2>', '**') \n\thtml = html.replace('<h1>', '**').replace('</h1>', '**') \n\thtml = html.replace('<em>', '/').replace('</em>', '/') \n\thtml = html.replace('<tr>', '\\n') \n\thtml = html.replace('</p>', '\\n') \n\thtml = re.sub('<br\\\\s*/?>', '\\n', html) \n\thtml = re.sub('<.*?>', ' \t', html) \n\thtml = html.replace((' \t' * 2), ' \t') \n\thtml = html.replace('&gt;', '>') \n\thtml = html.replace('&lt;', '<') \n\thtml = html.replace('&amp;', '&') \n\thtml = '\\n'.join([x.strip() for x in html.splitlines()]) \n\thtml = html.replace(('\\n' * 2), '\\n') \n\tfor (i, url) in enumerate(url_index): \n\t \tif (i == 0): \n\t \t \thtml += '\\n\\n' \n\t \thtml += (ustr('[%s] \t%s\\n') % ((i + 1), url)) \n\treturn html\n", 
" \treturn json.dumps(data, indent=indent, cls=JSONEncoderForHTML)\n", 
" \tif isinstance(value, six.string_types): \n\t \tif (value.lower() in ['on', 'true', 'yes']): \n\t \t \tvalue = True \n\t \telif (value.lower() in ['off', 'false', 'no']): \n\t \t \tvalue = False \n\t \ttry: \n\t \t \treturn ast.literal_eval(value) \n\t \texcept (TypeError, ValueError, SyntaxError): \n\t \t \tpass \n\treturn value\n", 
" \tif isinstance(value, six.string_types): \n\t \tif (value.lower() in ['on', 'true', 'yes']): \n\t \t \tvalue = True \n\t \telif (value.lower() in ['off', 'false', 'no']): \n\t \t \tvalue = False \n\t \ttry: \n\t \t \treturn ast.literal_eval(value) \n\t \texcept (TypeError, ValueError, SyntaxError): \n\t \t \tpass \n\treturn value\n", 
" \tif isinstance(value, six.string_types): \n\t \tif (value.lower() in ['on', 'true', 'yes']): \n\t \t \tvalue = True \n\t \telif (value.lower() in ['off', 'false', 'no']): \n\t \t \tvalue = False \n\t \ttry: \n\t \t \treturn ast.literal_eval(value) \n\t \texcept (TypeError, ValueError, SyntaxError): \n\t \t \tpass \n\treturn value\n", 
" \tdef test_roundtrip(self): \n\t \tif (with_polymorphic == 'unions'): \n\t \t \tif include_base: \n\t \t \t \tperson_join = polymorphic_union({'engineer': people.join(engineers), 'manager': people.join(managers), 'person': people.select((people.c.type == 'person'))}, None, 'pjoin') \n\t \t \telse: \n\t \t \t \tperson_join = polymorphic_union({'engineer': people.join(engineers), 'manager': people.join(managers)}, None, 'pjoin') \n\t \t \tmanager_join = people.join(managers).outerjoin(boss) \n\t \t \tperson_with_polymorphic = ['*', person_join] \n\t \t \tmanager_with_polymorphic = ['*', manager_join] \n\t \telif (with_polymorphic == 'joins'): \n\t \t \tperson_join = people.outerjoin(engineers).outerjoin(managers).outerjoin(boss) \n\t \t \tmanager_join = people.join(managers).outerjoin(boss) \n\t \t \tperson_with_polymorphic = ['*', person_join] \n\t \t \tmanager_with_polymorphic = ['*', manager_join] \n\t \telif (with_polymorphic == 'auto'): \n\t \t \tperson_with_polymorphic = '*' \n\t \t \tmanager_with_polymorphic = '*' \n\t \telse: \n\t \t \tperson_with_polymorphic = None \n\t \t \tmanager_with_polymorphic = None \n\t \tif redefine_colprop: \n\t \t \tperson_mapper = mapper(Person, people, with_polymorphic=person_with_polymorphic, polymorphic_on=people.c.type, polymorphic_identity='person', properties={'person_name': people.c.name}) \n\t \telse: \n\t \t \tperson_mapper = mapper(Person, people, with_polymorphic=person_with_polymorphic, polymorphic_on=people.c.type, polymorphic_identity='person') \n\t \tmapper(Engineer, engineers, inherits=person_mapper, polymorphic_identity='engineer') \n\t \tmapper(Manager, managers, inherits=person_mapper, with_polymorphic=manager_with_polymorphic, polymorphic_identity='manager') \n\t \tmapper(Boss, boss, inherits=Manager, polymorphic_identity='boss') \n\t \tmapper(Company, companies, properties={'employees': relationship(Person, lazy=lazy_relationship, cascade='all, \tdelete-orphan', backref='company', order_by=people.c.person_id)}) \n\t \tif redefine_colprop: \n\t \t \tperson_attribute_name = 'person_name' \n\t \telse: \n\t \t \tperson_attribute_name = 'name' \n\t \temployees = [Manager(status='AAB', manager_name='manager1', **{person_attribute_name: 'pointy \thaired \tboss'}), Engineer(status='BBA', engineer_name='engineer1', primary_language='java', **{person_attribute_name: 'dilbert'})] \n\t \tif include_base: \n\t \t \temployees.append(Person(**{person_attribute_name: 'joesmith'})) \n\t \temployees += [Engineer(status='CGG', engineer_name='engineer2', primary_language='python', **{person_attribute_name: 'wally'}), Manager(status='ABA', manager_name='manager2', **{person_attribute_name: 'jsmith'})] \n\t \tpointy = employees[0] \n\t \tjsmith = employees[(-1)] \n\t \tdilbert = employees[1] \n\t \tsession = create_session() \n\t \tc = Company(name='company1') \n\t \tc.employees = employees \n\t \tsession.add(c) \n\t \tsession.flush() \n\t \tsession.expunge_all() \n\t \teq_(session.query(Person).get(dilbert.person_id), dilbert) \n\t \tsession.expunge_all() \n\t \teq_(session.query(Person).filter((Person.person_id == dilbert.person_id)).one(), dilbert) \n\t \tsession.expunge_all() \n\t \tdef go(): \n\t \t \tcc = session.query(Company).get(c.company_id) \n\t \t \teq_(cc.employees, employees) \n\t \tif (not lazy_relationship): \n\t \t \tif (with_polymorphic != 'none'): \n\t \t \t \tself.assert_sql_count(testing.db, go, 1) \n\t \t \telse: \n\t \t \t \tself.assert_sql_count(testing.db, go, 5) \n\t \telif (with_polymorphic != 'none'): \n\t \t \tself.assert_sql_count(testing.db, go, 2) \n\t \telse: \n\t \t \tself.assert_sql_count(testing.db, go, 6) \n\t \teq_(session.query(Person).filter((getattr(Person, person_attribute_name) == 'dilbert')).first(), dilbert) \n\t \tassert session.query(Person).filter((getattr(Person, person_attribute_name) == 'dilbert')).first().person_id \n\t \teq_(session.query(Engineer).filter((getattr(Person, person_attribute_name) == 'dilbert')).first(), dilbert) \n\t \tpalias = people.alias('palias') \n\t \tdilbert = session.query(Person).get(dilbert.person_id) \n\t \tis_(dilbert, session.query(Person).filter(((palias.c.name == 'dilbert') & (palias.c.person_id == Person.person_id))).first()) \n\t \tis_(dilbert, session.query(Engineer).filter(((palias.c.name == 'dilbert') & (palias.c.person_id == Person.person_id))).first()) \n\t \tis_(dilbert, session.query(Person).filter(((Engineer.engineer_name == 'engineer1') & (engineers.c.person_id == people.c.person_id))).first()) \n\t \tis_(dilbert, session.query(Engineer).filter((Engineer.engineer_name == 'engineer1'))[0]) \n\t \tsession.flush() \n\t \tsession.expunge_all() \n\t \tdef go(): \n\t \t \tsession.query(Person).filter((getattr(Person, person_attribute_name) == 'dilbert')).first() \n\t \tself.assert_sql_count(testing.db, go, 1) \n\t \tsession.expunge_all() \n\t \tdilbert = session.query(Person).filter((getattr(Person, person_attribute_name) == 'dilbert')).first() \n\t \tdef go(): \n\t \t \td = session.query(Person).filter((getattr(Person, person_attribute_name) == 'dilbert')).first() \n\t \tself.assert_sql_count(testing.db, go, 1) \n\t \tdaboss = Boss(status='BBB', manager_name='boss', golf_swing='fore', **{person_attribute_name: 'daboss'}) \n\t \tsession.add(daboss) \n\t \tassert_raises(sa_exc.DBAPIError, session.flush) \n\t \tc = session.query(Company).first() \n\t \tdaboss.company = c \n\t \tmanager_list = [e for e in c.employees if isinstance(e, Manager)] \n\t \tsession.flush() \n\t \tsession.expunge_all() \n\t \teq_(session.query(Manager).order_by(Manager.person_id).all(), manager_list) \n\t \tc = session.query(Company).first() \n\t \tsession.delete(c) \n\t \tsession.flush() \n\t \teq_(select([func.count('*')]).select_from(people).scalar(), 0) \n\ttest_roundtrip = function_named(test_roundtrip, ('test_%s%s%s_%s' % (((lazy_relationship and 'lazy') or 'eager'), ((include_base and '_inclbase') or ''), ((redefine_colprop and '_redefcol') or ''), with_polymorphic))) \n\tsetattr(RoundTripTest, test_roundtrip.__name__, test_roundtrip)\n", 
" \texitStatus = 0 \n\tparser.add_option('-t', '--testlist', action='callback', callback=collect_set, dest='testlist_file', help='Test \tlist \tfile, \tspecifying \ttests \t(one \tper \tline)') \n\tparser.add_option('-v', '--verbose', action='store_true', dest='verbose') \n\t(options, tests) = parser.parse_args(args=parse_args) \n\ttests = set(tests) \n\targs = ['--boxed', '--verbose'] \n\troot = 'tests' \n\tif options.coverage: \n\t \targs.append('--cov=nupic') \n\tif (options.processes is not None): \n\t \targs.extend(['-n', options.processes]) \n\tif (options.markexpresson is not None): \n\t \targs.extend(['-m', options.markexpresson]) \n\tif (options.results is not None): \n\t \tresults = options.results[:2] \n\t \tformat = results.pop(0) \n\t \tif results: \n\t \t \trunid = results.pop(0) \n\t \telse: \n\t \t \trunid = datetime.now().strftime('%Y%m%d%H%M%S') \n\t \tresults = os.path.join(root, 'results', 'xunit', str(runid)) \n\t \ttry: \n\t \t \tos.makedirs(results) \n\t \texcept os.error: \n\t \t \tpass \n\t \targs.append(('--junitxml=' + os.path.join(results, 'results.xml'))) \n\tif (options.tests is not None): \n\t \ttests.update(options.tests) \n\tif (options.unit or options.all): \n\t \ttests.add(os.path.join(root, 'unit')) \n\tif (options.integration or options.all): \n\t \ttests.add(os.path.join(root, 'integration')) \n\tif (options.swarming or options.all): \n\t \ttests.add(os.path.join(root, 'swarming')) \n\tif options.verbose: \n\t \targs.append('-v') \n\tif options.failfast: \n\t \targs.append('-x') \n\tif ((not tests) or options.all): \n\t \ttests.add(os.path.join(root, 'external')) \n\t \ttests.add(os.path.join(root, 'unit')) \n\tif (options.testlist_file is not None): \n\t \tif options.testlist_file: \n\t \t \ttestlist = options.testlist_file.pop() \n\t \t \tif testlist.endswith('.testlist'): \n\t \t \t \ttestlist = [test.strip() for test in open(testlist).readlines()] \n\t \t \telse: \n\t \t \t \ttestlist = options.testlist_file \n\t \t \t \ttestlist.add(testlist) \n\t \tfor test in testlist: \n\t \t \tspecific_args = [(arg.replace('results.xml', (test.replace('/', '_') + '.xml')) if arg.startswith('--junitxml=') else arg) for arg in args] \n\t \t \ttestStatus = call(((['py.test'] + specific_args) + [test])) \n\t \t \tif (testStatus is not 0): \n\t \t \t \texitStatus = testStatus \n\telse: \n\t \texitStatus = call(((['py.test'] + args) + list(tests))) \n\treturn exitStatus\n", 
" \tdef deprecationDecorator(function): \n\t \t'\\n \t \t \t \t \t \t \t \tDecorator \tthat \tmarks \tC{function} \tas \tdeprecated.\\n \t \t \t \t \t \t \t \t' \n\t \twarningString = getDeprecationWarningString(function, version, None, replacement) \n\t \tdef deprecatedFunction(*args, **kwargs): \n\t \t \twarn(warningString, DeprecationWarning, stacklevel=2) \n\t \t \treturn function(*args, **kwargs) \n\t \tdeprecatedFunction = mergeFunctionMetadata(function, deprecatedFunction) \n\t \t_appendToDocstring(deprecatedFunction, _getDeprecationDocstring(version, replacement)) \n\t \tdeprecatedFunction.deprecatedVersion = version \n\t \treturn deprecatedFunction \n\treturn deprecationDecorator\n", 
" \tsuiteNames = ['OneNodeTests', 'MultiNodeTests', 'ModelMaturityTests', 'SwarmTerminatorTests'] \n\ttestNames = [] \n\tfor suite in suiteNames: \n\t \tfor f in dir(eval(suite)): \n\t \t \tif f.startswith('test'): \n\t \t \t \ttestNames.append(('%s.%s' % (suite, f))) \n\treturn testNames\n", 
" \tjscsrc_path = os.path.join(os.getcwd(), '.jscsrc') \n\tparsed_args = _PARSER.parse_args() \n\tif parsed_args.path: \n\t \tinput_path = os.path.join(os.getcwd(), parsed_args.path) \n\t \tif (not os.path.exists(input_path)): \n\t \t \tprint ('Could \tnot \tlocate \tfile \tor \tdirectory \t%s. \tExiting.' % input_path) \n\t \t \tprint '----------------------------------------' \n\t \t \tsys.exit(1) \n\t \tif os.path.isfile(input_path): \n\t \t \tall_files = [input_path] \n\t \telse: \n\t \t \texcluded_glob_patterns = _get_glob_patterns_excluded_from_jscsrc(jscsrc_path) \n\t \t \tall_files = _get_all_files_in_directory(input_path, excluded_glob_patterns) \n\telif parsed_args.files: \n\t \tvalid_filepaths = [] \n\t \tinvalid_filepaths = [] \n\t \tfor f in parsed_args.files: \n\t \t \tif os.path.isfile(f): \n\t \t \t \tvalid_filepaths.append(f) \n\t \t \telse: \n\t \t \t \tinvalid_filepaths.append(f) \n\t \tif invalid_filepaths: \n\t \t \tprint ('The \tfollowing \tfile(s) \tdo \tnot \texist: \t%s\\nExiting.' % invalid_filepaths) \n\t \t \tsys.exit(1) \n\t \tall_files = valid_filepaths \n\telse: \n\t \tall_files = _get_changed_filenames() \n\treturn all_files\n", 
" \tif (not name): \n\t \treturn blocks.CURRENT \n\ts_switch(name) \n\tif (not blocks.REQUESTS.has_key(name)): \n\t \traise sex.SullyRuntimeError(('blocks.REQUESTS \tNOT \tFOUND: \t%s' % name)) \n\treturn blocks.REQUESTS[name]\n", 
" \thttp = build_http() \n\t(response, content) = http.request(FLAGS.discovery_uri) \n\tdiscovery = json.loads(content) \n\tservice = build_from_document(discovery) \n\tname = discovery['version'] \n\tversion = safe_version(discovery['version']) \n\tdocument_collection_recursive(service, ('%s_%s.' % (name, version)), discovery, discovery)\n", 
" \tfrom .models import AnonymousUser \n\tuser = None \n\ttry: \n\t \tuser_id = _get_user_session_key(request) \n\t \tbackend_path = request.session[BACKEND_SESSION_KEY] \n\texcept KeyError: \n\t \tpass \n\telse: \n\t \tif (backend_path in settings.AUTHENTICATION_BACKENDS): \n\t \t \tbackend = load_backend(backend_path) \n\t \t \tuser = backend.get_user(user_id) \n\t \t \tif hasattr(user, 'get_session_auth_hash'): \n\t \t \t \tsession_hash = request.session.get(HASH_SESSION_KEY) \n\t \t \t \tsession_hash_verified = (session_hash and constant_time_compare(session_hash, user.get_session_auth_hash())) \n\t \t \t \tif (not session_hash_verified): \n\t \t \t \t \trequest.session.flush() \n\t \t \t \t \tuser = None \n\treturn (user or AnonymousUser())\n", 
" \ttry: \n\t \trecs = psutil.users() \n\t \treturn [dict(x._asdict()) for x in recs] \n\texcept AttributeError: \n\t \ttry: \n\t \t \timport utmp \n\t \t \tresult = [] \n\t \t \twhile True: \n\t \t \t \trec = utmp.utmpaccess.getutent() \n\t \t \t \tif (rec is None): \n\t \t \t \t \treturn result \n\t \t \t \telif (rec[0] == 7): \n\t \t \t \t \tstarted = rec[8] \n\t \t \t \t \tif isinstance(started, tuple): \n\t \t \t \t \t \tstarted = started[0] \n\t \t \t \t \tresult.append({'name': rec[4], 'terminal': rec[2], 'started': started, 'host': rec[5]}) \n\t \texcept ImportError: \n\t \t \treturn False\n", 
" \tdef wraps(f): \n\t \tf._permission_name = name \n\t \treturn f \n\treturn wraps\n", 
" \tdef wraps(f): \n\t \tf._permission_name = name \n\t \treturn f \n\treturn wraps\n", 
" \tdef decorator(func): \n\t \tadd_event_handler(name, func, priority) \n\t \treturn func \n\treturn decorator\n", 
" \treturn _is_interactive\n", 
" \tdef _authenticated(method): \n\t \t@functools.wraps(method) \n\t \tdef wrapper(self, *args, **kwargs): \n\t \t \tif (not self.current_user): \n\t \t \t \tif (self.request.method in ('GET', 'HEAD')): \n\t \t \t \t \turl = self.get_login_url() \n\t \t \t \t \tif ('?' not in url): \n\t \t \t \t \t \tif urlparse.urlsplit(url).scheme: \n\t \t \t \t \t \t \tnext_url = self.request.full_url() \n\t \t \t \t \t \telse: \n\t \t \t \t \t \t \tnext_url = self.request.uri \n\t \t \t \t \t \turl += ('?' + urllib.urlencode(dict(next=next_url))) \n\t \t \t \t \tself.redirect(url) \n\t \t \t \t \treturn \n\t \t \t \traise web.HTTPError(401, 'You \tare \tnot \tlogged \tin. \tOnly \tusers \tthat \thave \tlogged \tin \tcan \taccess \tthis \tpage.') \n\t \t \telif (isinstance(self.current_user, User) and (not allow_prospective) and (not self.current_user.IsRegistered())): \n\t \t \t \traise web.HTTPError(403, 'You \tare \tnot \ta \tregistered \tuser. \tSign \tup \tfor \tViewfinder \tto \tgain \taccess \tto \tthis \tpage.') \n\t \t \treturn method(self, *args, **kwargs) \n\t \treturn wrapper \n\treturn _authenticated\n", 
" \tthe_class = orm_obj._meta.object_name \n\toriginal_class = the_class \n\tpk_name = orm_obj._meta.pk.name \n\toriginal_pk_name = pk_name \n\tpk_value = getattr(orm_obj, pk_name) \n\twhile (hasattr(pk_value, '_meta') and hasattr(pk_value._meta, 'pk') and hasattr(pk_value._meta.pk, 'name')): \n\t \tthe_class = pk_value._meta.object_name \n\t \tpk_name = pk_value._meta.pk.name \n\t \tpk_value = getattr(pk_value, pk_name) \n\tclean_dict = make_clean_dict(orm_obj.__dict__) \n\tfor key in clean_dict: \n\t \tv = clean_dict[key] \n\t \tif ((v is not None) and (not isinstance(v, (six.string_types, six.integer_types, float, datetime.datetime)))): \n\t \t \tclean_dict[key] = six.u(('%s' % v)) \n\toutput = (' \timporter.locate_object(%s, \t\"%s\", \t%s, \t\"%s\", \t%s, \t%s \t) \t' % (original_class, original_pk_name, the_class, pk_name, pk_value, clean_dict)) \n\treturn output\n", 
" \treturn make_middleware_decorator(middleware_class)()\n", 
" \tresult = mapper.match(environ=environ) \n\tif (result is None): \n\t \traise webob.exc.HTTPNotFound(json_formatter=util.json_error_formatter) \n\thandler = result.pop('action') \n\tenviron['wsgiorg.routing_args'] = ((), result) \n\treturn handler(environ, start_response)\n", 
" \tlog.info('installing \tapplication \troutes:') \n\tmethods = inspect.getmembers(application) \n\tmethods = filter((lambda n: (not n[0].startswith('_'))), methods) \n\tfor (method, func) in dict(methods).iteritems(): \n\t \tpieces = method.split('_') \n\t \t(verb, path) = (pieces[0], pieces[1:]) \n\t \targs = inspect.getargspec(func).args[1:] \n\t \targs = [('<%s>' % arg) for arg in args] \n\t \targs = '/'.join(args) \n\t \targs = ('' if (len(args) == 0) else ('/' + args)) \n\t \tpath.insert(0, application._namespace) \n\t \tpath = ('/'.join(path) + args) \n\t \tlog.info(('%6s: \t%s' % (verb, path))) \n\t \tregister.route(path, method=verb, name=method)(func)\n", 
" \tkwds = kwargs.copy() \n\t_check_for_invalid_keys(fname, kwargs, compat_args) \n\t_check_for_default_values(fname, kwds, compat_args)\n", 
" \t_check_challenge(signature) \n\tmd5 = hashlib.md5() \n\tmd5.update(sample_file.data) \n\twith open(os.path.join(config['MobSF']['samples'], md5.hexdigest()), 'wb') as handle: \n\t \thandle.write(sample_file.data) \n\treturn md5.hexdigest()\n", 
" \tdata = np.asarray(data) \n\tif ((not (data.ndim == 3)) and (data.shape[(-1)] in (3, 4))): \n\t \traise ValueError('data \tmust \tbe \ta \t3D \tarray \twith \tlast \tdimension \t3 \tor \t4') \n\twith open(filename, 'wb') as f: \n\t \tf.write(_make_png(data))\n", 
" \t(existing_username, existing_key_name, existing_zone, instance_ids) = _read_server_list(zone) \n\tcount = int(count) \n\tif ((existing_username == username) and (existing_key_name == key_name) and (existing_zone == zone)): \n\t \tec2_connection = boto.ec2.connect_to_region(_get_region(zone)) \n\t \texisting_reservations = ec2_connection.get_all_instances(instance_ids=instance_ids) \n\t \texisting_instances = filter((lambda i: (i.state == 'running')), [r.instances[0] for r in existing_reservations]) \n\t \tif (count <= len(existing_instances)): \n\t \t \tprint 'Bees \tare \talready \tassembled \tand \tawaiting \torders.' \n\t \t \treturn \n\t \telse: \n\t \t \tcount -= len(existing_instances) \n\telif instance_ids: \n\t \tprint 'Taking \tdown \t{} \tunusable \tbees.'.format(len(instance_ids)) \n\t \twith _redirect_stdout(): \n\t \t \tdown() \n\t \t(existing_username, existing_key_name, existing_zone, instance_ids) = _read_server_list(zone) \n\tpem_path = _get_pem_path(key_name) \n\tif (not os.path.isfile(pem_path)): \n\t \tprint ('Warning. \tNo \tkey \tfile \tfound \tfor \t%s. \tYou \twill \tneed \tto \tadd \tthis \tkey \tto \tyour \tSSH \tagent \tto \tconnect.' % pem_path) \n\tprint 'Connecting \tto \tthe \thive.' \n\ttry: \n\t \tec2_connection = boto.ec2.connect_to_region(_get_region(zone)) \n\texcept boto.exception.NoAuthHandlerFound as e: \n\t \tprint 'Authenciation \tconfig \terror, \tperhaps \tyou \tdo \tnot \thave \ta \t~/.boto \tfile \twith \tcorrect \tpermissions?' \n\t \tprint e.message \n\t \treturn e \n\texcept Exception as e: \n\t \tprint 'Unknown \terror \toccured:' \n\t \tprint e.message \n\t \treturn e \n\tif (ec2_connection == None): \n\t \traise Exception('Invalid \tzone \tspecified? \tUnable \tto \tconnect \tto \tregion \tusing \tzone \tname') \n\tgroupId = (group if (subnet is None) else _get_security_group_id(ec2_connection, group, subnet)) \n\tprint ('GroupId \tfound: \t%s' % groupId) \n\tplacement = (None if ('gov' in zone) else zone) \n\tprint ('Placement: \t%s' % placement) \n\tif bid: \n\t \tprint ('Attempting \tto \tcall \tup \t%i \tspot \tbees, \tthis \tcan \ttake \ta \twhile...' % count) \n\t \tspot_requests = ec2_connection.request_spot_instances(image_id=image_id, price=bid, count=count, key_name=key_name, security_group_ids=[groupId], instance_type=instance_type, placement=placement, subnet_id=subnet) \n\t \ttime.sleep(5) \n\t \tinstances = _wait_for_spot_request_fulfillment(ec2_connection, spot_requests) \n\telse: \n\t \tprint ('Attempting \tto \tcall \tup \t%i \tbees.' % count) \n\t \ttry: \n\t \t \treservation = ec2_connection.run_instances(image_id=image_id, min_count=count, max_count=count, key_name=key_name, security_group_ids=[groupId], instance_type=instance_type, placement=placement, subnet_id=subnet) \n\t \texcept boto.exception.EC2ResponseError as e: \n\t \t \tprint ('Unable \tto \tcall \tbees:', e.message) \n\t \t \tprint 'Is \tyour \tsec \tgroup \tavailable \tin \tthis \tregion?' \n\t \t \treturn e \n\t \tinstances = reservation.instances \n\tif instance_ids: \n\t \texisting_reservations = ec2_connection.get_all_instances(instance_ids=instance_ids) \n\t \texisting_instances = filter((lambda i: (i.state == 'running')), [r.instances[0] for r in existing_reservations]) \n\t \tmap(instances.append, existing_instances) \n\t \tdead_instances = filter((lambda i: (i not in [j.id for j in existing_instances])), instance_ids) \n\t \tmap(instance_ids.pop, [instance_ids.index(i) for i in dead_instances]) \n\tprint 'Waiting \tfor \tbees \tto \tload \ttheir \tmachine \tguns...' \n\tinstance_ids = (instance_ids or []) \n\tfor instance in [i for i in instances if (i.state == 'pending')]: \n\t \tinstance.update() \n\t \twhile (instance.state != 'running'): \n\t \t \tprint '.' \n\t \t \ttime.sleep(5) \n\t \t \tinstance.update() \n\t \tinstance_ids.append(instance.id) \n\t \tprint ('Bee \t%s \tis \tready \tfor \tthe \tattack.' % instance.id) \n\tec2_connection.create_tags(instance_ids, {'Name': 'a \tbee!'}) \n\t_write_server_list(username, key_name, zone, instances) \n\tprint ('The \tswarm \thas \tassembled \t%i \tbees.' % len(instances))\n", 
" \t(existing_username, existing_key_name, existing_zone, instance_ids) = _read_server_list(zone) \n\tcount = int(count) \n\tif ((existing_username == username) and (existing_key_name == key_name) and (existing_zone == zone)): \n\t \tec2_connection = boto.ec2.connect_to_region(_get_region(zone)) \n\t \texisting_reservations = ec2_connection.get_all_instances(instance_ids=instance_ids) \n\t \texisting_instances = filter((lambda i: (i.state == 'running')), [r.instances[0] for r in existing_reservations]) \n\t \tif (count <= len(existing_instances)): \n\t \t \tprint 'Bees \tare \talready \tassembled \tand \tawaiting \torders.' \n\t \t \treturn \n\t \telse: \n\t \t \tcount -= len(existing_instances) \n\telif instance_ids: \n\t \tprint 'Taking \tdown \t{} \tunusable \tbees.'.format(len(instance_ids)) \n\t \twith _redirect_stdout(): \n\t \t \tdown() \n\t \t(existing_username, existing_key_name, existing_zone, instance_ids) = _read_server_list(zone) \n\tpem_path = _get_pem_path(key_name) \n\tif (not os.path.isfile(pem_path)): \n\t \tprint ('Warning. \tNo \tkey \tfile \tfound \tfor \t%s. \tYou \twill \tneed \tto \tadd \tthis \tkey \tto \tyour \tSSH \tagent \tto \tconnect.' % pem_path) \n\tprint 'Connecting \tto \tthe \thive.' \n\ttry: \n\t \tec2_connection = boto.ec2.connect_to_region(_get_region(zone)) \n\texcept boto.exception.NoAuthHandlerFound as e: \n\t \tprint 'Authenciation \tconfig \terror, \tperhaps \tyou \tdo \tnot \thave \ta \t~/.boto \tfile \twith \tcorrect \tpermissions?' \n\t \tprint e.message \n\t \treturn e \n\texcept Exception as e: \n\t \tprint 'Unknown \terror \toccured:' \n\t \tprint e.message \n\t \treturn e \n\tif (ec2_connection == None): \n\t \traise Exception('Invalid \tzone \tspecified? \tUnable \tto \tconnect \tto \tregion \tusing \tzone \tname') \n\tgroupId = (group if (subnet is None) else _get_security_group_id(ec2_connection, group, subnet)) \n\tprint ('GroupId \tfound: \t%s' % groupId) \n\tplacement = (None if ('gov' in zone) else zone) \n\tprint ('Placement: \t%s' % placement) \n\tif bid: \n\t \tprint ('Attempting \tto \tcall \tup \t%i \tspot \tbees, \tthis \tcan \ttake \ta \twhile...' % count) \n\t \tspot_requests = ec2_connection.request_spot_instances(image_id=image_id, price=bid, count=count, key_name=key_name, security_group_ids=[groupId], instance_type=instance_type, placement=placement, subnet_id=subnet) \n\t \ttime.sleep(5) \n\t \tinstances = _wait_for_spot_request_fulfillment(ec2_connection, spot_requests) \n\telse: \n\t \tprint ('Attempting \tto \tcall \tup \t%i \tbees.' % count) \n\t \ttry: \n\t \t \treservation = ec2_connection.run_instances(image_id=image_id, min_count=count, max_count=count, key_name=key_name, security_group_ids=[groupId], instance_type=instance_type, placement=placement, subnet_id=subnet) \n\t \texcept boto.exception.EC2ResponseError as e: \n\t \t \tprint ('Unable \tto \tcall \tbees:', e.message) \n\t \t \tprint 'Is \tyour \tsec \tgroup \tavailable \tin \tthis \tregion?' \n\t \t \treturn e \n\t \tinstances = reservation.instances \n\tif instance_ids: \n\t \texisting_reservations = ec2_connection.get_all_instances(instance_ids=instance_ids) \n\t \texisting_instances = filter((lambda i: (i.state == 'running')), [r.instances[0] for r in existing_reservations]) \n\t \tmap(instances.append, existing_instances) \n\t \tdead_instances = filter((lambda i: (i not in [j.id for j in existing_instances])), instance_ids) \n\t \tmap(instance_ids.pop, [instance_ids.index(i) for i in dead_instances]) \n\tprint 'Waiting \tfor \tbees \tto \tload \ttheir \tmachine \tguns...' \n\tinstance_ids = (instance_ids or []) \n\tfor instance in [i for i in instances if (i.state == 'pending')]: \n\t \tinstance.update() \n\t \twhile (instance.state != 'running'): \n\t \t \tprint '.' \n\t \t \ttime.sleep(5) \n\t \t \tinstance.update() \n\t \tinstance_ids.append(instance.id) \n\t \tprint ('Bee \t%s \tis \tready \tfor \tthe \tattack.' % instance.id) \n\tec2_connection.create_tags(instance_ids, {'Name': 'a \tbee!'}) \n\t_write_server_list(username, key_name, zone, instances) \n\tprint ('The \tswarm \thas \tassembled \t%i \tbees.' % len(instances))\n", 
" \treturn _stores.all()\n", 
" \tclass CIFAR10Record(object, ): \n\t \tpass \n\tresult = CIFAR10Record() \n\tlabel_bytes = 1 \n\tresult.height = 32 \n\tresult.width = 32 \n\tresult.depth = 3 \n\timage_bytes = ((result.height * result.width) * result.depth) \n\trecord_bytes = (label_bytes + image_bytes) \n\treader = tf.FixedLengthRecordReader(record_bytes=record_bytes) \n\t(result.key, value) = reader.read(filename_queue) \n\trecord_bytes = tf.decode_raw(value, tf.uint8) \n\tresult.label = tf.cast(tf.strided_slice(record_bytes, [0], [label_bytes]), tf.int32) \n\tdepth_major = tf.reshape(tf.strided_slice(record_bytes, [label_bytes], [(label_bytes + image_bytes)]), [result.depth, result.height, result.width]) \n\tresult.uint8image = tf.transpose(depth_major, [1, 2, 0]) \n\treturn result\n", 
" \tfreqOrder = getFrequencyOrder(message) \n\tmatchScore = 0 \n\tfor commonLetter in ETAOIN[:6]: \n\t \tif (commonLetter in freqOrder[:6]): \n\t \t \tmatchScore += 1 \n\tfor uncommonLetter in ETAOIN[(-6):]: \n\t \tif (uncommonLetter in freqOrder[(-6):]): \n\t \t \tmatchScore += 1 \n\treturn matchScore\n", 
" \tret = [] \n\tnotifier = _get_notifier(config) \n\twm = notifier._watch_manager \n\tif notifier.check_events(1): \n\t \tnotifier.read_events() \n\t \tnotifier.process_events() \n\t \tqueue = __context__['inotify.queue'] \n\t \twhile queue: \n\t \t \tevent = queue.popleft() \n\t \t \t_append = True \n\t \t \tpath = event.path \n\t \t \twhile (path != '/'): \n\t \t \t \tif (path in config): \n\t \t \t \t \tbreak \n\t \t \t \tpath = os.path.dirname(path) \n\t \t \texcludes = config[path].get('exclude', '') \n\t \t \tif (excludes and isinstance(excludes, list)): \n\t \t \t \tfor exclude in excludes: \n\t \t \t \t \tif isinstance(exclude, dict): \n\t \t \t \t \t \tif exclude.values()[0].get('regex', False): \n\t \t \t \t \t \t \ttry: \n\t \t \t \t \t \t \t \tif re.search(exclude.keys()[0], event.pathname): \n\t \t \t \t \t \t \t \t \t_append = False \n\t \t \t \t \t \t \texcept Exception: \n\t \t \t \t \t \t \t \tlog.warning('Failed \tto \tcompile \tregex: \t{0}'.format(exclude.keys()[0])) \n\t \t \t \t \t \telse: \n\t \t \t \t \t \t \texclude = exclude.keys()[0] \n\t \t \t \t \telif ('*' in exclude): \n\t \t \t \t \t \tif fnmatch.fnmatch(event.pathname, exclude): \n\t \t \t \t \t \t \t_append = False \n\t \t \t \t \telif event.pathname.startswith(exclude): \n\t \t \t \t \t \t_append = False \n\t \t \tif _append: \n\t \t \t \tsub = {'tag': event.path, 'path': event.pathname, 'change': event.maskname} \n\t \t \t \tret.append(sub) \n\t \t \telse: \n\t \t \t \tlog.info('Excluding \t{0} \tfrom \tevent \tfor \t{1}'.format(event.pathname, path)) \n\tcurrent = set() \n\tfor wd in wm.watches: \n\t \tcurrent.add(wm.watches[wd].path) \n\tfor path in config: \n\t \tif isinstance(config[path], dict): \n\t \t \tmask = config[path].get('mask', DEFAULT_MASK) \n\t \t \tif isinstance(mask, list): \n\t \t \t \tr_mask = 0 \n\t \t \t \tfor sub in mask: \n\t \t \t \t \tr_mask |= _get_mask(sub) \n\t \t \telif isinstance(mask, salt.ext.six.binary_type): \n\t \t \t \tr_mask = _get_mask(mask) \n\t \t \telse: \n\t \t \t \tr_mask = mask \n\t \t \tmask = r_mask \n\t \t \trec = config[path].get('recurse', False) \n\t \t \tauto_add = config[path].get('auto_add', False) \n\t \telse: \n\t \t \tmask = DEFAULT_MASK \n\t \t \trec = False \n\t \t \tauto_add = False \n\t \tif (path in current): \n\t \t \tfor wd in wm.watches: \n\t \t \t \tif (path == wm.watches[wd].path): \n\t \t \t \t \tupdate = False \n\t \t \t \t \tif (wm.watches[wd].mask != mask): \n\t \t \t \t \t \tupdate = True \n\t \t \t \t \tif (wm.watches[wd].auto_add != auto_add): \n\t \t \t \t \t \tupdate = True \n\t \t \t \t \tif update: \n\t \t \t \t \t \twm.update_watch(wd, mask=mask, rec=rec, auto_add=auto_add) \n\t \telif os.path.exists(path): \n\t \t \texcludes = config[path].get('exclude', '') \n\t \t \texcl = None \n\t \t \tif isinstance(excludes, list): \n\t \t \t \texcl = [] \n\t \t \t \tfor exclude in excludes: \n\t \t \t \t \tif isinstance(exclude, dict): \n\t \t \t \t \t \texcl.append(exclude.keys()[0]) \n\t \t \t \t \telse: \n\t \t \t \t \t \texcl.append(exclude) \n\t \t \t \texcl = pyinotify.ExcludeFilter(excl) \n\t \t \twm.add_watch(path, mask, rec=rec, auto_add=auto_add, exclude_filter=excl) \n\treturn ret\n", 
" \t(opts, args) = parse_args(argv) \n\tsetup_logging(opts.log, opts.log_facility, opts.log_level) \n\tparameters = {'vhost': '/', 'username': opts.username, 'password': opts.password, 'metric_group': 'rabbitmq', 'zero_rates_when_idle': 'yes', 'host': opts.admin_host, 'port': opts.admin_port, 'stats': opts.stats.split(','), 'vhosts': opts.vhosts.split(',')} \n\tdescriptors = metric_init(parameters) \n\tresult = refreshStats(stats=parameters['stats'], vhosts=parameters['vhosts']) \n\tprint ('***' * 20) \n\tif (opts.list_only is True): \n\t \tprint 'nodes:' \n\t \tpprint.pprint(list_nodes()) \n\t \tprint 'exchanges:' \n\t \tfor vhost in parameters['vhosts']: \n\t \t \tprint ('vhost: \t%s' % vhost) \n\t \t \tpprint.pprint(list_exchanges(vhost)) \n\t \tprint 'queues:' \n\t \tfor vhost in parameters['vhosts']: \n\t \t \tprint ('vhost: \t%s' % vhost) \n\t \t \tpprint.pprint(list_queues(vhost)) \n\t \treturn \n\ttry: \n\t \twhile True: \n\t \t \tfor d in descriptors: \n\t \t \t \tv = d['call_back'](d['name']) \n\t \t \t \tif (v is None): \n\t \t \t \t \tprint ('got \tNone \tfor \t%s' % d['name']) \n\t \t \t \telse: \n\t \t \t \t \tprint ('value \tfor \t%s \tis \t%r' % (d['name'], v)) \n\t \t \ttime.sleep(5) \n\t \t \tprint '----------------------------' \n\texcept KeyboardInterrupt: \n\t \tlog.debug('KeyboardInterrupt, \tshutting \tdown...') \n\t \tmetric_cleanup()\n", 
" \t(opts, args) = parse_args(argv) \n\tsetup_logging(opts.log, opts.log_facility, opts.log_level) \n\tparameters = {'vhost': '/', 'username': opts.username, 'password': opts.password, 'metric_group': 'rabbitmq', 'zero_rates_when_idle': 'yes', 'host': opts.admin_host, 'port': opts.admin_port, 'stats': opts.stats.split(','), 'vhosts': opts.vhosts.split(',')} \n\tdescriptors = metric_init(parameters) \n\tresult = refreshStats(stats=parameters['stats'], vhosts=parameters['vhosts']) \n\tprint ('***' * 20) \n\tif (opts.list_only is True): \n\t \tprint 'nodes:' \n\t \tpprint.pprint(list_nodes()) \n\t \tprint 'exchanges:' \n\t \tfor vhost in parameters['vhosts']: \n\t \t \tprint ('vhost: \t%s' % vhost) \n\t \t \tpprint.pprint(list_exchanges(vhost)) \n\t \tprint 'queues:' \n\t \tfor vhost in parameters['vhosts']: \n\t \t \tprint ('vhost: \t%s' % vhost) \n\t \t \tpprint.pprint(list_queues(vhost)) \n\t \treturn \n\ttry: \n\t \twhile True: \n\t \t \tfor d in descriptors: \n\t \t \t \tv = d['call_back'](d['name']) \n\t \t \t \tif (v is None): \n\t \t \t \t \tprint ('got \tNone \tfor \t%s' % d['name']) \n\t \t \t \telse: \n\t \t \t \t \tprint ('value \tfor \t%s \tis \t%r' % (d['name'], v)) \n\t \t \ttime.sleep(5) \n\t \t \tprint '----------------------------' \n\texcept KeyboardInterrupt: \n\t \tlog.debug('KeyboardInterrupt, \tshutting \tdown...') \n\t \tmetric_cleanup()\n", 
" \t(opts, args) = parse_args(argv) \n\tsetup_logging(opts.log, opts.log_facility, opts.log_level) \n\tparameters = {'vhost': '/', 'username': opts.username, 'password': opts.password, 'metric_group': 'rabbitmq', 'zero_rates_when_idle': 'yes', 'host': opts.admin_host, 'port': opts.admin_port, 'stats': opts.stats.split(','), 'vhosts': opts.vhosts.split(',')} \n\tdescriptors = metric_init(parameters) \n\tresult = refreshStats(stats=parameters['stats'], vhosts=parameters['vhosts']) \n\tprint ('***' * 20) \n\tif (opts.list_only is True): \n\t \tprint 'nodes:' \n\t \tpprint.pprint(list_nodes()) \n\t \tprint 'exchanges:' \n\t \tfor vhost in parameters['vhosts']: \n\t \t \tprint ('vhost: \t%s' % vhost) \n\t \t \tpprint.pprint(list_exchanges(vhost)) \n\t \tprint 'queues:' \n\t \tfor vhost in parameters['vhosts']: \n\t \t \tprint ('vhost: \t%s' % vhost) \n\t \t \tpprint.pprint(list_queues(vhost)) \n\t \treturn \n\ttry: \n\t \twhile True: \n\t \t \tfor d in descriptors: \n\t \t \t \tv = d['call_back'](d['name']) \n\t \t \t \tif (v is None): \n\t \t \t \t \tprint ('got \tNone \tfor \t%s' % d['name']) \n\t \t \t \telse: \n\t \t \t \t \tprint ('value \tfor \t%s \tis \t%r' % (d['name'], v)) \n\t \t \ttime.sleep(5) \n\t \t \tprint '----------------------------' \n\texcept KeyboardInterrupt: \n\t \tlog.debug('KeyboardInterrupt, \tshutting \tdown...') \n\t \tmetric_cleanup()\n", 
" \t(opts, args) = parse_args(argv) \n\tsetup_logging(opts.log, opts.log_facility, opts.log_level) \n\tparameters = {'vhost': '/', 'username': opts.username, 'password': opts.password, 'metric_group': 'rabbitmq', 'zero_rates_when_idle': 'yes', 'host': opts.admin_host, 'port': opts.admin_port, 'stats': opts.stats.split(','), 'vhosts': opts.vhosts.split(',')} \n\tdescriptors = metric_init(parameters) \n\tresult = refreshStats(stats=parameters['stats'], vhosts=parameters['vhosts']) \n\tprint ('***' * 20) \n\tif (opts.list_only is True): \n\t \tprint 'nodes:' \n\t \tpprint.pprint(list_nodes()) \n\t \tprint 'exchanges:' \n\t \tfor vhost in parameters['vhosts']: \n\t \t \tprint ('vhost: \t%s' % vhost) \n\t \t \tpprint.pprint(list_exchanges(vhost)) \n\t \tprint 'queues:' \n\t \tfor vhost in parameters['vhosts']: \n\t \t \tprint ('vhost: \t%s' % vhost) \n\t \t \tpprint.pprint(list_queues(vhost)) \n\t \treturn \n\ttry: \n\t \twhile True: \n\t \t \tfor d in descriptors: \n\t \t \t \tv = d['call_back'](d['name']) \n\t \t \t \tif (v is None): \n\t \t \t \t \tprint ('got \tNone \tfor \t%s' % d['name']) \n\t \t \t \telse: \n\t \t \t \t \tprint ('value \tfor \t%s \tis \t%r' % (d['name'], v)) \n\t \t \ttime.sleep(5) \n\t \t \tprint '----------------------------' \n\texcept KeyboardInterrupt: \n\t \tlog.debug('KeyboardInterrupt, \tshutting \tdown...') \n\t \tmetric_cleanup()\n", 
" \t(opts, args) = parse_args(argv) \n\tsetup_logging(opts.log, opts.log_facility, opts.log_level) \n\tparameters = {'vhost': '/', 'username': opts.username, 'password': opts.password, 'metric_group': 'rabbitmq', 'zero_rates_when_idle': 'yes', 'host': opts.admin_host, 'port': opts.admin_port, 'stats': opts.stats.split(','), 'vhosts': opts.vhosts.split(',')} \n\tdescriptors = metric_init(parameters) \n\tresult = refreshStats(stats=parameters['stats'], vhosts=parameters['vhosts']) \n\tprint ('***' * 20) \n\tif (opts.list_only is True): \n\t \tprint 'nodes:' \n\t \tpprint.pprint(list_nodes()) \n\t \tprint 'exchanges:' \n\t \tfor vhost in parameters['vhosts']: \n\t \t \tprint ('vhost: \t%s' % vhost) \n\t \t \tpprint.pprint(list_exchanges(vhost)) \n\t \tprint 'queues:' \n\t \tfor vhost in parameters['vhosts']: \n\t \t \tprint ('vhost: \t%s' % vhost) \n\t \t \tpprint.pprint(list_queues(vhost)) \n\t \treturn \n\ttry: \n\t \twhile True: \n\t \t \tfor d in descriptors: \n\t \t \t \tv = d['call_back'](d['name']) \n\t \t \t \tif (v is None): \n\t \t \t \t \tprint ('got \tNone \tfor \t%s' % d['name']) \n\t \t \t \telse: \n\t \t \t \t \tprint ('value \tfor \t%s \tis \t%r' % (d['name'], v)) \n\t \t \ttime.sleep(5) \n\t \t \tprint '----------------------------' \n\texcept KeyboardInterrupt: \n\t \tlog.debug('KeyboardInterrupt, \tshutting \tdown...') \n\t \tmetric_cleanup()\n", 
" \t(opts, args) = parse_args(argv) \n\tsetup_logging(opts.log, opts.log_facility, opts.log_level) \n\tparameters = {'vhost': '/', 'username': opts.username, 'password': opts.password, 'metric_group': 'rabbitmq', 'zero_rates_when_idle': 'yes', 'host': opts.admin_host, 'port': opts.admin_port, 'stats': opts.stats.split(','), 'vhosts': opts.vhosts.split(',')} \n\tdescriptors = metric_init(parameters) \n\tresult = refreshStats(stats=parameters['stats'], vhosts=parameters['vhosts']) \n\tprint ('***' * 20) \n\tif (opts.list_only is True): \n\t \tprint 'nodes:' \n\t \tpprint.pprint(list_nodes()) \n\t \tprint 'exchanges:' \n\t \tfor vhost in parameters['vhosts']: \n\t \t \tprint ('vhost: \t%s' % vhost) \n\t \t \tpprint.pprint(list_exchanges(vhost)) \n\t \tprint 'queues:' \n\t \tfor vhost in parameters['vhosts']: \n\t \t \tprint ('vhost: \t%s' % vhost) \n\t \t \tpprint.pprint(list_queues(vhost)) \n\t \treturn \n\ttry: \n\t \twhile True: \n\t \t \tfor d in descriptors: \n\t \t \t \tv = d['call_back'](d['name']) \n\t \t \t \tif (v is None): \n\t \t \t \t \tprint ('got \tNone \tfor \t%s' % d['name']) \n\t \t \t \telse: \n\t \t \t \t \tprint ('value \tfor \t%s \tis \t%r' % (d['name'], v)) \n\t \t \ttime.sleep(5) \n\t \t \tprint '----------------------------' \n\texcept KeyboardInterrupt: \n\t \tlog.debug('KeyboardInterrupt, \tshutting \tdown...') \n\t \tmetric_cleanup()\n", 
" \t(opts, args) = parse_args(argv) \n\tsetup_logging(opts.log, opts.log_facility, opts.log_level) \n\tparameters = {'vhost': '/', 'username': opts.username, 'password': opts.password, 'metric_group': 'rabbitmq', 'zero_rates_when_idle': 'yes', 'host': opts.admin_host, 'port': opts.admin_port, 'stats': opts.stats.split(','), 'vhosts': opts.vhosts.split(',')} \n\tdescriptors = metric_init(parameters) \n\tresult = refreshStats(stats=parameters['stats'], vhosts=parameters['vhosts']) \n\tprint ('***' * 20) \n\tif (opts.list_only is True): \n\t \tprint 'nodes:' \n\t \tpprint.pprint(list_nodes()) \n\t \tprint 'exchanges:' \n\t \tfor vhost in parameters['vhosts']: \n\t \t \tprint ('vhost: \t%s' % vhost) \n\t \t \tpprint.pprint(list_exchanges(vhost)) \n\t \tprint 'queues:' \n\t \tfor vhost in parameters['vhosts']: \n\t \t \tprint ('vhost: \t%s' % vhost) \n\t \t \tpprint.pprint(list_queues(vhost)) \n\t \treturn \n\ttry: \n\t \twhile True: \n\t \t \tfor d in descriptors: \n\t \t \t \tv = d['call_back'](d['name']) \n\t \t \t \tif (v is None): \n\t \t \t \t \tprint ('got \tNone \tfor \t%s' % d['name']) \n\t \t \t \telse: \n\t \t \t \t \tprint ('value \tfor \t%s \tis \t%r' % (d['name'], v)) \n\t \t \ttime.sleep(5) \n\t \t \tprint '----------------------------' \n\texcept KeyboardInterrupt: \n\t \tlog.debug('KeyboardInterrupt, \tshutting \tdown...') \n\t \tmetric_cleanup()\n", 
" \t(opts, args) = parse_args(argv) \n\tsetup_logging(opts.log, opts.log_facility, opts.log_level) \n\tparameters = {'vhost': '/', 'username': opts.username, 'password': opts.password, 'metric_group': 'rabbitmq', 'zero_rates_when_idle': 'yes', 'host': opts.admin_host, 'port': opts.admin_port, 'stats': opts.stats.split(','), 'vhosts': opts.vhosts.split(',')} \n\tdescriptors = metric_init(parameters) \n\tresult = refreshStats(stats=parameters['stats'], vhosts=parameters['vhosts']) \n\tprint ('***' * 20) \n\tif (opts.list_only is True): \n\t \tprint 'nodes:' \n\t \tpprint.pprint(list_nodes()) \n\t \tprint 'exchanges:' \n\t \tfor vhost in parameters['vhosts']: \n\t \t \tprint ('vhost: \t%s' % vhost) \n\t \t \tpprint.pprint(list_exchanges(vhost)) \n\t \tprint 'queues:' \n\t \tfor vhost in parameters['vhosts']: \n\t \t \tprint ('vhost: \t%s' % vhost) \n\t \t \tpprint.pprint(list_queues(vhost)) \n\t \treturn \n\ttry: \n\t \twhile True: \n\t \t \tfor d in descriptors: \n\t \t \t \tv = d['call_back'](d['name']) \n\t \t \t \tif (v is None): \n\t \t \t \t \tprint ('got \tNone \tfor \t%s' % d['name']) \n\t \t \t \telse: \n\t \t \t \t \tprint ('value \tfor \t%s \tis \t%r' % (d['name'], v)) \n\t \t \ttime.sleep(5) \n\t \t \tprint '----------------------------' \n\texcept KeyboardInterrupt: \n\t \tlog.debug('KeyboardInterrupt, \tshutting \tdown...') \n\t \tmetric_cleanup()\n", 
" \t(opts, args) = parse_args(argv) \n\tsetup_logging(opts.log, opts.log_facility, opts.log_level) \n\tparameters = {'vhost': '/', 'username': opts.username, 'password': opts.password, 'metric_group': 'rabbitmq', 'zero_rates_when_idle': 'yes', 'host': opts.admin_host, 'port': opts.admin_port, 'stats': opts.stats.split(','), 'vhosts': opts.vhosts.split(',')} \n\tdescriptors = metric_init(parameters) \n\tresult = refreshStats(stats=parameters['stats'], vhosts=parameters['vhosts']) \n\tprint ('***' * 20) \n\tif (opts.list_only is True): \n\t \tprint 'nodes:' \n\t \tpprint.pprint(list_nodes()) \n\t \tprint 'exchanges:' \n\t \tfor vhost in parameters['vhosts']: \n\t \t \tprint ('vhost: \t%s' % vhost) \n\t \t \tpprint.pprint(list_exchanges(vhost)) \n\t \tprint 'queues:' \n\t \tfor vhost in parameters['vhosts']: \n\t \t \tprint ('vhost: \t%s' % vhost) \n\t \t \tpprint.pprint(list_queues(vhost)) \n\t \treturn \n\ttry: \n\t \twhile True: \n\t \t \tfor d in descriptors: \n\t \t \t \tv = d['call_back'](d['name']) \n\t \t \t \tif (v is None): \n\t \t \t \t \tprint ('got \tNone \tfor \t%s' % d['name']) \n\t \t \t \telse: \n\t \t \t \t \tprint ('value \tfor \t%s \tis \t%r' % (d['name'], v)) \n\t \t \ttime.sleep(5) \n\t \t \tprint '----------------------------' \n\texcept KeyboardInterrupt: \n\t \tlog.debug('KeyboardInterrupt, \tshutting \tdown...') \n\t \tmetric_cleanup()\n", 
" \t(opts, args) = parse_args(argv) \n\tsetup_logging(opts.log, opts.log_facility, opts.log_level) \n\tparameters = {'vhost': '/', 'username': opts.username, 'password': opts.password, 'metric_group': 'rabbitmq', 'zero_rates_when_idle': 'yes', 'host': opts.admin_host, 'port': opts.admin_port, 'stats': opts.stats.split(','), 'vhosts': opts.vhosts.split(',')} \n\tdescriptors = metric_init(parameters) \n\tresult = refreshStats(stats=parameters['stats'], vhosts=parameters['vhosts']) \n\tprint ('***' * 20) \n\tif (opts.list_only is True): \n\t \tprint 'nodes:' \n\t \tpprint.pprint(list_nodes()) \n\t \tprint 'exchanges:' \n\t \tfor vhost in parameters['vhosts']: \n\t \t \tprint ('vhost: \t%s' % vhost) \n\t \t \tpprint.pprint(list_exchanges(vhost)) \n\t \tprint 'queues:' \n\t \tfor vhost in parameters['vhosts']: \n\t \t \tprint ('vhost: \t%s' % vhost) \n\t \t \tpprint.pprint(list_queues(vhost)) \n\t \treturn \n\ttry: \n\t \twhile True: \n\t \t \tfor d in descriptors: \n\t \t \t \tv = d['call_back'](d['name']) \n\t \t \t \tif (v is None): \n\t \t \t \t \tprint ('got \tNone \tfor \t%s' % d['name']) \n\t \t \t \telse: \n\t \t \t \t \tprint ('value \tfor \t%s \tis \t%r' % (d['name'], v)) \n\t \t \ttime.sleep(5) \n\t \t \tprint '----------------------------' \n\texcept KeyboardInterrupt: \n\t \tlog.debug('KeyboardInterrupt, \tshutting \tdown...') \n\t \tmetric_cleanup()\n", 
" \t(opts, args) = parse_args(argv) \n\tsetup_logging(opts.log, opts.log_facility, opts.log_level) \n\tparameters = {'vhost': '/', 'username': opts.username, 'password': opts.password, 'metric_group': 'rabbitmq', 'zero_rates_when_idle': 'yes', 'host': opts.admin_host, 'port': opts.admin_port, 'stats': opts.stats.split(','), 'vhosts': opts.vhosts.split(',')} \n\tdescriptors = metric_init(parameters) \n\tresult = refreshStats(stats=parameters['stats'], vhosts=parameters['vhosts']) \n\tprint ('***' * 20) \n\tif (opts.list_only is True): \n\t \tprint 'nodes:' \n\t \tpprint.pprint(list_nodes()) \n\t \tprint 'exchanges:' \n\t \tfor vhost in parameters['vhosts']: \n\t \t \tprint ('vhost: \t%s' % vhost) \n\t \t \tpprint.pprint(list_exchanges(vhost)) \n\t \tprint 'queues:' \n\t \tfor vhost in parameters['vhosts']: \n\t \t \tprint ('vhost: \t%s' % vhost) \n\t \t \tpprint.pprint(list_queues(vhost)) \n\t \treturn \n\ttry: \n\t \twhile True: \n\t \t \tfor d in descriptors: \n\t \t \t \tv = d['call_back'](d['name']) \n\t \t \t \tif (v is None): \n\t \t \t \t \tprint ('got \tNone \tfor \t%s' % d['name']) \n\t \t \t \telse: \n\t \t \t \t \tprint ('value \tfor \t%s \tis \t%r' % (d['name'], v)) \n\t \t \ttime.sleep(5) \n\t \t \tprint '----------------------------' \n\texcept KeyboardInterrupt: \n\t \tlog.debug('KeyboardInterrupt, \tshutting \tdown...') \n\t \tmetric_cleanup()\n", 
" \treturn sys.exc_info()[1]\n", 
" \t(opts, args) = parse_args(argv) \n\tsetup_logging(opts.log, opts.log_facility, opts.log_level) \n\tparameters = {'vhost': '/', 'username': opts.username, 'password': opts.password, 'metric_group': 'rabbitmq', 'zero_rates_when_idle': 'yes', 'host': opts.admin_host, 'port': opts.admin_port, 'stats': opts.stats.split(','), 'vhosts': opts.vhosts.split(',')} \n\tdescriptors = metric_init(parameters) \n\tresult = refreshStats(stats=parameters['stats'], vhosts=parameters['vhosts']) \n\tprint ('***' * 20) \n\tif (opts.list_only is True): \n\t \tprint 'nodes:' \n\t \tpprint.pprint(list_nodes()) \n\t \tprint 'exchanges:' \n\t \tfor vhost in parameters['vhosts']: \n\t \t \tprint ('vhost: \t%s' % vhost) \n\t \t \tpprint.pprint(list_exchanges(vhost)) \n\t \tprint 'queues:' \n\t \tfor vhost in parameters['vhosts']: \n\t \t \tprint ('vhost: \t%s' % vhost) \n\t \t \tpprint.pprint(list_queues(vhost)) \n\t \treturn \n\ttry: \n\t \twhile True: \n\t \t \tfor d in descriptors: \n\t \t \t \tv = d['call_back'](d['name']) \n\t \t \t \tif (v is None): \n\t \t \t \t \tprint ('got \tNone \tfor \t%s' % d['name']) \n\t \t \t \telse: \n\t \t \t \t \tprint ('value \tfor \t%s \tis \t%r' % (d['name'], v)) \n\t \t \ttime.sleep(5) \n\t \t \tprint '----------------------------' \n\texcept KeyboardInterrupt: \n\t \tlog.debug('KeyboardInterrupt, \tshutting \tdown...') \n\t \tmetric_cleanup()\n", 
" \ttry: \n\t \titer(value) \n\texcept TypeError: \n\t \treturn False \n\treturn True\n", 
" \tif ((not user) and (not appsettings.REGISTRATION_OPEN)): \n\t \traise AuthException(backend, _(u'New \tregistrations \tare \tdisabled!'))\n", 
" \twith _IpmiCommand(**kwargs) as c: \n\t \treturn c.create_user(uid, name, password, channel, callback, link_auth, ipmi_msg, privilege_level)\n", 
" \tdict1_name = os.path.basename(dict1_path) \n\tdict2_name = os.path.basename(dict2_path) \n\tif (dict1_name == dict2_name): \n\t \tdict1_name = 'dict1' \n\t \tdict2_name = 'dict2' \n\treturn (dict1_name, dict2_name)\n", 
" \tdict1_name = os.path.basename(dict1_path) \n\tdict2_name = os.path.basename(dict2_path) \n\tif (dict1_name == dict2_name): \n\t \tdict1_name = 'dict1' \n\t \tdict2_name = 'dict2' \n\treturn (dict1_name, dict2_name)\n", 
" \tassert ((A & B) == And(A, B)) \n\tassert ((A | B) == Or(A, B)) \n\tassert (((A & B) | C) == Or(And(A, B), C)) \n\tassert ((A >> B) == Implies(A, B)) \n\tassert ((A << B) == Implies(B, A)) \n\tassert ((~ A) == Not(A)) \n\tassert ((A ^ B) == Xor(A, B))\n", 
" \tstart = datetime.datetime(2011, 1, 1, tzinfo=mdates.UTC) \n\tend = datetime.datetime(2011, 1, 2, tzinfo=mdates.UTC) \n\tdelta = datetime.timedelta(hours=1) \n\tassert_equal(24, len(mdates.drange(start, end, delta))) \n\tend = (end + datetime.timedelta(microseconds=1)) \n\tassert_equal(25, len(mdates.drange(start, end, delta))) \n\tend = datetime.datetime(2011, 1, 2, tzinfo=mdates.UTC) \n\tdelta = datetime.timedelta(hours=4) \n\tdaterange = mdates.drange(start, end, delta) \n\tassert_equal(6, len(daterange)) \n\tassert_equal(mdates.num2date(daterange[(-1)]), (end - delta))\n", 
" \treturn (300 <= status <= 399)\n", 
" \treturn (300 <= status <= 399)\n", 
" \treturn (300 <= status <= 399)\n", 
" \treturn (300 <= status <= 399)\n", 
" \tfor name in names: \n\t \tfunc = getattr(obj, name, None) \n\t \tif (func is not None): \n\t \t \tif (type(obj) == types.ModuleType): \n\t \t \t \tif isinstance(func, types.FunctionType): \n\t \t \t \t \t(args, varargs, varkw, defaults) = inspect.getargspec(func) \n\t \t \t \telse: \n\t \t \t \t \tif (hasattr(func, '__call__') and (not inspect.ismethod(func))): \n\t \t \t \t \t \tfunc = func.__call__ \n\t \t \t \t \ttry: \n\t \t \t \t \t \t(args, varargs, varkw, defaults) = inspect.getargspec(func) \n\t \t \t \t \t \targs.pop(0) \n\t \t \t \t \texcept TypeError: \n\t \t \t \t \t \traise TypeError(('Attribute \t%s \tof \t%r \tis \tnot \ta \tpython \tfunction. \tOnly \tfunctions \tor \tcallables \tmay \tbe \tused \tas \tfixtures.' % (name, obj))) \n\t \t \t \tif len(args): \n\t \t \t \t \tlog.debug('call \tfixture \t%s.%s(%s)', obj, name, obj) \n\t \t \t \t \treturn func(obj) \n\t \t \tlog.debug('call \tfixture \t%s.%s', obj, name) \n\t \t \treturn func()\n", 
" \tif (obj.type == 'tag'): \n\t \tobj = deref_tag(obj) \n\tif (obj.type != 'commit'): \n\t \traise ValueError(('Cannot \tconvert \tobject \t%r \tto \ttype \tcommit' % obj)) \n\treturn obj\n", 
" \tif (not obj): \n\t \treturn None \n\tif (not keys): \n\t \tkeys = obj.keys() \n\tfor key in keys: \n\t \ttry: \n\t \t \tobj[key] = json.loads(obj[key]) \n\t \texcept: \n\t \t \tpass \n\treturn obj\n", 
" \tdata = func(*args, **kwargs) \n\treturn to_jsonp(data)\n", 
" \tmatch = CONTENT_TYPE_RE.match(request.META['CONTENT_TYPE']) \n\tif match: \n\t \tcharset = match.group(1) \n\telse: \n\t \tcharset = settings.DEFAULT_CHARSET \n\tobj_dict = simplejson.loads(request.raw_post_data.decode(charset)) \n\tobj_json = simplejson.dumps(obj_dict, encoding=charset, cls=DjangoJSONEncoder, ensure_ascii=False) \n\tresponse = HttpResponse(smart_str(obj_json, encoding=charset), status=200, mimetype=('application/json; \tcharset=' + charset)) \n\tresponse['Content-Disposition'] = 'attachment; \tfilename=testfile.json' \n\treturn response\n", 
" \ttry: \n\t \titer(value) \n\texcept TypeError: \n\t \treturn False \n\treturn True\n", 
" \tSetOutputFile() \n\tif filename: \n\t \tSetOutputFile(open(filename, 'w'), 1)\n", 
" \ttry: \n\t \titer(value) \n\texcept TypeError: \n\t \treturn False \n\treturn True\n", 
" \t@wraps(func) \n\tdef call_and_format(self, trans, *args, **kwargs): \n\t \tjsonp_callback = kwargs.pop(JSONP_CALLBACK_KEY, None) \n\t \tif jsonp_callback: \n\t \t \ttrans.response.set_content_type(JSONP_CONTENT_TYPE) \n\t \telse: \n\t \t \ttrans.response.set_content_type(JSON_CONTENT_TYPE) \n\t \trval = func(self, trans, *args, **kwargs) \n\t \treturn _format_return_as_json(rval, jsonp_callback, pretty=(pretty or trans.debug)) \n\tif (not hasattr(func, '_orig')): \n\t \tcall_and_format._orig = func \n\treturn expose(_save_orig_fn(call_and_format, func))\n", 
" \t@wraps(func) \n\tdef call_and_format(self, trans, *args, **kwargs): \n\t \tjsonp_callback = kwargs.pop(JSONP_CALLBACK_KEY, None) \n\t \tif jsonp_callback: \n\t \t \ttrans.response.set_content_type(JSONP_CONTENT_TYPE) \n\t \telse: \n\t \t \ttrans.response.set_content_type(JSON_CONTENT_TYPE) \n\t \trval = func(self, trans, *args, **kwargs) \n\t \treturn _format_return_as_json(rval, jsonp_callback, pretty=(pretty or trans.debug)) \n\tif (not hasattr(func, '_orig')): \n\t \tcall_and_format._orig = func \n\treturn expose(_save_orig_fn(call_and_format, func))\n", 
" \taccept = parse_accept_header(request.META.get('HTTP_ACCEPT', '')) \n\treturn (media_type in [t for (t, p, q) in accept])\n", 
" \tif (suffix and string.endswith(suffix)): \n\t \treturn string[:(- len(suffix))] \n\telse: \n\t \treturn string\n", 
" \treturn Queue(('control.%s' % config.server_name), galaxy_exchange, routing_key='control')\n", 
" \tconn = _auth(profile) \n\treturn conn.create_router(name, ext_network, admin_state_up)\n", 
" \tdef Wrapped(self, *args): \n\t \treturn getattr(self, f)(*args) \n\treturn Wrapped\n", 
" \tfor (url, ofno) in group(mapping, 2): \n\t \tif isinstance(ofno, tuple): \n\t \t \t(ofn, fna) = (ofno[0], list(ofno[1:])) \n\t \telse: \n\t \t \t(ofn, fna) = (ofno, []) \n\t \t(fn, result) = re_subm((('^' + url) + '$'), ofn, ctx.path) \n\t \tif result: \n\t \t \tif (fn.split(' \t', 1)[0] == 'redirect'): \n\t \t \t \turl = fn.split(' \t', 1)[1] \n\t \t \t \tif (ctx.method == 'GET'): \n\t \t \t \t \tx = ctx.env.get('QUERY_STRING', '') \n\t \t \t \t \tif x: \n\t \t \t \t \t \turl += ('?' + x) \n\t \t \t \treturn redirect(url) \n\t \t \telif ('.' in fn): \n\t \t \t \tx = fn.split('.') \n\t \t \t \t(mod, cls) = ('.'.join(x[:(-1)]), x[(-1)]) \n\t \t \t \tmod = __import__(mod, globals(), locals(), ['']) \n\t \t \t \tcls = getattr(mod, cls) \n\t \t \telse: \n\t \t \t \tcls = fn \n\t \t \t \tmod = (fvars or upvars()) \n\t \t \t \tif isinstance(mod, types.ModuleType): \n\t \t \t \t \tmod = vars(mod) \n\t \t \t \ttry: \n\t \t \t \t \tcls = mod[cls] \n\t \t \t \texcept KeyError: \n\t \t \t \t \treturn notfound() \n\t \t \tmeth = ctx.method \n\t \t \tif (meth == 'HEAD'): \n\t \t \t \tif (not hasattr(cls, meth)): \n\t \t \t \t \tmeth = 'GET' \n\t \t \tif (not hasattr(cls, meth)): \n\t \t \t \treturn nomethod(cls) \n\t \t \ttocall = getattr(cls(), meth) \n\t \t \targs = list(result.groups()) \n\t \t \tfor d in re.findall('\\\\\\\\(\\\\d+)', ofn): \n\t \t \t \targs.pop((int(d) - 1)) \n\t \t \treturn tocall(*([urllib.unquote(x) for x in args] + fna)) \n\treturn notfound()\n", 
" \txfm = np.random.randn(4, 4).astype(np.float32) \n\tnew_xfm = xfm.dot(rotate(180, (1, 0, 0)).dot(rotate((-90), (0, 1, 0)))) \n\tnew_xfm = new_xfm.dot(rotate(90, (0, 0, 1)).dot(rotate(90, (0, 1, 0)))) \n\tnew_xfm = new_xfm.dot(rotate(90, (1, 0, 0))) \n\tassert_allclose(xfm, new_xfm) \n\tnew_xfm = translate((1, (-1), 1)).dot(translate(((-1), 1, (-1)))).dot(xfm) \n\tassert_allclose(xfm, new_xfm) \n\tnew_xfm = scale((1, 2, 3)).dot(scale((1, (1.0 / 2.0), (1.0 / 3.0)))).dot(xfm) \n\tassert_allclose(xfm, new_xfm) \n\txfm = ortho((-1), 1, (-1), 1, (-1), 1) \n\tassert_equal(xfm.shape, (4, 4)) \n\txfm = frustum((-1), 1, (-1), 1, (-1), 1) \n\tassert_equal(xfm.shape, (4, 4)) \n\txfm = perspective(1, 1, (-1), 1) \n\tassert_equal(xfm.shape, (4, 4))\n", 
" \tdef __init__(self, original): \n\t \tsetattr(self, originalAttribute, original) \n\tcontents = {'__init__': __init__} \n\tfor name in iface: \n\t \tcontents[name] = _ProxyDescriptor(name, originalAttribute) \n\tproxy = type(('(Proxy \tfor \t%s)' % (reflect.qual(iface),)), (object,), contents) \n\tdeclarations.classImplements(proxy, iface) \n\treturn proxy\n", 
" \tclassname = obj.__class__.__name__ \n\tlog.info('Checking \t%s \tfor \twebhooks', classname) \n\tfor (name, func) in getmembers(obj, ismethod): \n\t \tif getattr(func, '_err_webhook_uri_rule', False): \n\t \t \tlog.info('Webhook \trouting \t%s', func.__name__) \n\t \t \tfor verb in func._err_webhook_methods: \n\t \t \t \twv = WebView(func, func._err_webhook_form_param, func._err_webhook_raw) \n\t \t \t \tbottle_app.route(func._err_webhook_uri_rule, verb, callback=wv, name=((func.__name__ + '_') + verb))\n", 
" \tdef Decorator(f): \n\t \tfunction_labels = getattr(f, 'labels', set()) \n\t \tf.labels = function_labels.union(set(labels)) \n\t \treturn f \n\treturn Decorator\n", 
" \tdocument = opendocx(TEST_FILE) \n\tparatextlist = getdocumenttext(document) \n\tassert (len(paratextlist) > 0)\n", 
" \tcan_compile(u'(fn \t(x \t&kwargs \tkw) \t(list \tx \tkw))') \n\tcant_compile(u'(fn \t(x \t&kwargs \txs \t&kwargs \tys) \t(list \tx \txs \tys))') \n\tcan_compile(u'(fn \t(&optional \tx \t&kwargs \tkw) \t(list \tx \tkw))')\n", 
" \t_writers.update(_WRITERS_ORIGINAL) \n\ttestfile = str(tmpdir.join(u'foo.example')) \n\twith pytest.raises(io_registry.IORegistryError) as exc: \n\t \tTable().write(testfile) \n\tassert str(exc.value).startswith(u'Format \tcould \tnot \tbe \tidentified.')\n", 
" \tif (not (eq_type.lower() in ('dare', 'care'))): \n\t \traise ValueError(\"Equation \ttype \tunknown. \tOnly \t'care' \tand \t'dare' \tis \tunderstood\") \n\ta = np.atleast_2d(_asarray_validated(a, check_finite=True)) \n\tb = np.atleast_2d(_asarray_validated(b, check_finite=True)) \n\tq = np.atleast_2d(_asarray_validated(q, check_finite=True)) \n\tr = np.atleast_2d(_asarray_validated(r, check_finite=True)) \n\tr_or_c = (complex if np.iscomplexobj(b) else float) \n\tfor (ind, mat) in enumerate((a, q, r)): \n\t \tif np.iscomplexobj(mat): \n\t \t \tr_or_c = complex \n\t \tif (not np.equal(*mat.shape)): \n\t \t \traise ValueError('Matrix \t{} \tshould \tbe \tsquare.'.format('aqr'[ind])) \n\t(m, n) = b.shape \n\tif (m != a.shape[0]): \n\t \traise ValueError('Matrix \ta \tand \tb \tshould \thave \tthe \tsame \tnumber \tof \trows.') \n\tif (m != q.shape[0]): \n\t \traise ValueError('Matrix \ta \tand \tq \tshould \thave \tthe \tsame \tshape.') \n\tif (n != r.shape[0]): \n\t \traise ValueError('Matrix \tb \tand \tr \tshould \thave \tthe \tsame \tnumber \tof \tcols.') \n\tfor (ind, mat) in enumerate((q, r)): \n\t \tif (norm((mat - mat.conj().T), 1) > (np.spacing(norm(mat, 1)) * 100)): \n\t \t \traise ValueError('Matrix \t{} \tshould \tbe \tsymmetric/hermitian.'.format('qr'[ind])) \n\tif (eq_type == 'care'): \n\t \tmin_sv = svd(r, compute_uv=False)[(-1)] \n\t \tif ((min_sv == 0.0) or (min_sv < (np.spacing(1.0) * norm(r, 1)))): \n\t \t \traise ValueError('Matrix \tr \tis \tnumerically \tsingular.') \n\tgeneralized_case = ((e is not None) or (s is not None)) \n\tif generalized_case: \n\t \tif (e is not None): \n\t \t \te = np.atleast_2d(_asarray_validated(e, check_finite=True)) \n\t \t \tif (not np.equal(*e.shape)): \n\t \t \t \traise ValueError('Matrix \te \tshould \tbe \tsquare.') \n\t \t \tif (m != e.shape[0]): \n\t \t \t \traise ValueError('Matrix \ta \tand \te \tshould \thave \tthe \tsame \tshape.') \n\t \t \tmin_sv = svd(e, compute_uv=False)[(-1)] \n\t \t \tif ((min_sv == 0.0) or (min_sv < (np.spacing(1.0) * norm(e, 1)))): \n\t \t \t \traise ValueError('Matrix \te \tis \tnumerically \tsingular.') \n\t \t \tif np.iscomplexobj(e): \n\t \t \t \tr_or_c = complex \n\t \tif (s is not None): \n\t \t \ts = np.atleast_2d(_asarray_validated(s, check_finite=True)) \n\t \t \tif (s.shape != b.shape): \n\t \t \t \traise ValueError('Matrix \tb \tand \ts \tshould \thave \tthe \tsame \tshape.') \n\t \t \tif np.iscomplexobj(s): \n\t \t \t \tr_or_c = complex \n\treturn (a, b, q, r, e, s, m, n, r_or_c, generalized_case)\n", 
" \tpos_kwargs_conflicts = [] \n\tpositional_only_kwargs = [] \n\tunsatisfied_args = [] \n\tunsatisfied_kwargs = [] \n\tunmatched_args = list(args) \n\tunmatched_kwargs = list(kwargs) \n\thas_varargs = has_var_kwargs = False \n\tif signature: \n\t \ttry: \n\t \t \tsig = signature(func) \n\t \texcept ValueError: \n\t \t \treturn \n\t \tfor param in six.itervalues(sig.parameters): \n\t \t \tif (param.kind == param.POSITIONAL_OR_KEYWORD): \n\t \t \t \tif ((param.name in unmatched_kwargs) and unmatched_args): \n\t \t \t \t \tpos_kwargs_conflicts.append(param.name) \n\t \t \t \telif unmatched_args: \n\t \t \t \t \tdel unmatched_args[0] \n\t \t \t \telif (param.name in unmatched_kwargs): \n\t \t \t \t \tunmatched_kwargs.remove(param.name) \n\t \t \t \telif (param.default is param.empty): \n\t \t \t \t \tunsatisfied_args.append(param.name) \n\t \t \telif (param.kind == param.POSITIONAL_ONLY): \n\t \t \t \tif unmatched_args: \n\t \t \t \t \tdel unmatched_args[0] \n\t \t \t \telif (param.name in unmatched_kwargs): \n\t \t \t \t \tunmatched_kwargs.remove(param.name) \n\t \t \t \t \tpositional_only_kwargs.append(param.name) \n\t \t \t \telif (param.default is param.empty): \n\t \t \t \t \tunsatisfied_args.append(param.name) \n\t \t \telif (param.kind == param.KEYWORD_ONLY): \n\t \t \t \tif (param.name in unmatched_kwargs): \n\t \t \t \t \tunmatched_kwargs.remove(param.name) \n\t \t \t \telif (param.default is param.empty): \n\t \t \t \t \tunsatisfied_kwargs.append(param.name) \n\t \t \telif (param.kind == param.VAR_POSITIONAL): \n\t \t \t \thas_varargs = True \n\t \t \telif (param.kind == param.VAR_KEYWORD): \n\t \t \t \thas_var_kwargs = True \n\telse: \n\t \tif ((not isfunction(func)) and (not ismethod(func)) and hasattr(func, '__call__')): \n\t \t \tfunc = func.__call__ \n\t \ttry: \n\t \t \targspec = getargspec(func) \n\t \texcept TypeError: \n\t \t \treturn \n\t \targspec_args = (argspec.args if (not ismethod(func)) else argspec.args[1:]) \n\t \thas_varargs = bool(argspec.varargs) \n\t \thas_var_kwargs = bool(argspec.keywords) \n\t \tfor (arg, default) in six.moves.zip_longest(argspec_args, (argspec.defaults or ()), fillvalue=undefined): \n\t \t \tif ((arg in unmatched_kwargs) and unmatched_args): \n\t \t \t \tpos_kwargs_conflicts.append(arg) \n\t \t \telif unmatched_args: \n\t \t \t \tdel unmatched_args[0] \n\t \t \telif (arg in unmatched_kwargs): \n\t \t \t \tunmatched_kwargs.remove(arg) \n\t \t \telif (default is undefined): \n\t \t \t \tunsatisfied_args.append(arg) \n\tif pos_kwargs_conflicts: \n\t \traise ValueError(('The \tfollowing \targuments \tare \tsupplied \tin \tboth \targs \tand \tkwargs: \t%s' % ', \t'.join(pos_kwargs_conflicts))) \n\tif positional_only_kwargs: \n\t \traise ValueError(('The \tfollowing \targuments \tcannot \tbe \tgiven \tas \tkeyword \targuments: \t%s' % ', \t'.join(positional_only_kwargs))) \n\tif unsatisfied_args: \n\t \traise ValueError(('The \tfollowing \targuments \thave \tnot \tbeen \tsupplied: \t%s' % ', \t'.join(unsatisfied_args))) \n\tif unsatisfied_kwargs: \n\t \traise ValueError(('The \tfollowing \tkeyword-only \targuments \thave \tnot \tbeen \tsupplied \tin \tkwargs: \t%s' % ', \t'.join(unsatisfied_kwargs))) \n\tif ((not has_varargs) and unmatched_args): \n\t \traise ValueError(('The \tlist \tof \tpositional \targuments \tis \tlonger \tthan \tthe \ttarget \tcallable \tcan \thandle \t(allowed: \t%d, \tgiven \tin \targs: \t%d)' % ((len(args) - len(unmatched_args)), len(args)))) \n\tif ((not has_var_kwargs) and unmatched_kwargs): \n\t \traise ValueError(('The \ttarget \tcallable \tdoes \tnot \taccept \tthe \tfollowing \tkeyword \targuments: \t%s' % ', \t'.join(unmatched_kwargs)))\n", 
" \tmro = inspect.getmro(klass) \n\tnew = False \n\tif (mro[(-1)] is object): \n\t \tmro = mro[:(-1)] \n\t \tnew = True \n\tfor kls in mro: \n\t \tif (new and ('__dict__' in kls.__dict__)): \n\t \t \treturn False \n\t \tif (not hasattr(kls, '__slots__')): \n\t \t \treturn False \n\treturn True\n", 
" \treturn '{0}-{1}-{2}'.format(name, int(time.time()), random.randint(0, 10000))\n", 
" \tmethod.func_name = method_name \n\tmethod.__name__ = method_name \n\tif six.PY3: \n\t \tmethodtype = types.MethodType(method, cls) \n\telse: \n\t \tmethodtype = types.MethodType(method, None, cls) \n\tsetattr(cls, method_name, methodtype)\n", 
" \tmethod.func_name = method_name \n\tmethod.__name__ = method_name \n\tif six.PY3: \n\t \tmethodtype = types.MethodType(method, cls) \n\telse: \n\t \tmethodtype = types.MethodType(method, None, cls) \n\tsetattr(cls, method_name, methodtype)\n", 
" \tpass\n", 
" \tpass\n", 
" \tfunc_name = func.__name__ \n\t@functools.wraps(func) \n\tdef _cache_guard(self, *args, **kwargs): \n\t \tcache_key = (func_name, args) \n\t \tif kwargs: \n\t \t \tkwarg_items = tuple(sorted(kwargs.items())) \n\t \t \tcache_key = (func_name, args, kwarg_items) \n\t \tresult = self._instance_cache.get(cache_key) \n\t \tif (result is not None): \n\t \t \treturn result \n\t \tresult = func(self, *args, **kwargs) \n\t \tself._instance_cache[cache_key] = result \n\t \treturn result \n\treturn _cache_guard\n", 
" \ta1 = Angle(0, unit=u.radian) \n\ta2 = Angle(10, unit=u.degree) \n\ta3 = Angle(0.543, unit=u.degree) \n\ta4 = Angle(u'1d2m3.4s') \n\tassert (Angle(str(a1)).degree == a1.degree) \n\tassert (Angle(str(a2)).degree == a2.degree) \n\tassert (Angle(str(a3)).degree == a3.degree) \n\tassert (Angle(str(a4)).degree == a4.degree) \n\tra = Longitude(u'1h2m3.4s') \n\tdec = Latitude(u'1d2m3.4s') \n\tassert_allclose(Angle(str(ra)).degree, ra.degree) \n\tassert_allclose(Angle(str(dec)).degree, dec.degree)\n", 
" \tdef validate(key, data, errors, context): \n\t \tif (not (data[key] in list_possible_values())): \n\t \t \traise Invalid('\"{0}\" \tis \tnot \ta \tvalid \tparameter'.format(data[key])) \n\treturn validate\n", 
" \tif (not headervalue): \n\t \treturn None \n\tresult = [] \n\ttry: \n\t \t(bytesunit, byteranges) = headervalue.split(u'=', 1) \n\texcept Exception: \n\t \treturn None \n\tif (bytesunit.strip() != u'bytes'): \n\t \treturn None \n\tfor brange in byteranges.split(u','): \n\t \t(start, stop) = [x.strip() for x in brange.split(u'-', 1)] \n\t \tif start: \n\t \t \tif (not stop): \n\t \t \t \tstop = (content_length - 1) \n\t \t \ttry: \n\t \t \t \t(start, stop) = (int(start), int(stop)) \n\t \t \texcept Exception: \n\t \t \t \tcontinue \n\t \t \tif (start >= content_length): \n\t \t \t \tcontinue \n\t \t \tif (stop < start): \n\t \t \t \tcontinue \n\t \t \tstop = min(stop, (content_length - 1)) \n\t \t \tresult.append(Range(start, stop, ((stop - start) + 1))) \n\t \telif stop: \n\t \t \ttry: \n\t \t \t \tstop = int(stop) \n\t \t \texcept Exception: \n\t \t \t \tcontinue \n\t \t \tif (stop > content_length): \n\t \t \t \tresult.append(Range(0, (content_length - 1), content_length)) \n\t \t \telse: \n\t \t \t \tresult.append(Range((content_length - stop), (content_length - 1), stop)) \n\treturn result\n", 
" \tif (limit == 2): \n\t \treturn [2] \n\tif (limit == 3): \n\t \treturn [2, 3] \n\tif (limit == 5): \n\t \treturn [2, 3, 5] \n\tif (limit < 2): \n\t \treturn [] \n\tprimes = [2, 3, 5] \n\tis_prime = ([False] * (limit + 1)) \n\tsqrt_limit = (int(sqrt(limit)) + 1) \n\tfor x in range(1, sqrt_limit): \n\t \tfor y in range(1, sqrt_limit): \n\t \t \tn = ((4 * (x ** 2)) + (y ** 2)) \n\t \t \tif ((n <= limit) and (((n % 12) == 1) or ((n % 12) == 5))): \n\t \t \t \tis_prime[n] = (not is_prime[n]) \n\t \t \tn = ((3 * (x ** 2)) + (y ** 2)) \n\t \t \tif ((n <= limit) and ((n % 12) == 7)): \n\t \t \t \tis_prime[n] = (not is_prime[n]) \n\t \t \tn = ((3 * (x ** 2)) - (y ** 2)) \n\t \t \tif ((x > y) and (n <= limit) and ((n % 12) == 11)): \n\t \t \t \tis_prime[n] = (not is_prime[n]) \n\tfor index in range(5, sqrt_limit): \n\t \tif is_prime[index]: \n\t \t \tfor composite in range((index ** 2), limit, (index ** 2)): \n\t \t \t \tis_prime[composite] = False \n\tfor index in range(7, limit): \n\t \tif is_prime[index]: \n\t \t \tprimes.append(index) \n\treturn primes\n", 
" \treturn (len(value) > int(arg))\n", 
" \tif (ignore and isinstance(y, ignore)): \n\t \treturn False \n\ttry: \n\t \titer(y) \n\t \treturn True \n\texcept TypeError: \n\t \treturn False\n", 
" \tstart = datetime.datetime(2011, 1, 1, tzinfo=mdates.UTC) \n\tend = datetime.datetime(2011, 1, 2, tzinfo=mdates.UTC) \n\tdelta = datetime.timedelta(hours=1) \n\tassert_equal(24, len(mdates.drange(start, end, delta))) \n\tend = (end + datetime.timedelta(microseconds=1)) \n\tassert_equal(25, len(mdates.drange(start, end, delta))) \n\tend = datetime.datetime(2011, 1, 2, tzinfo=mdates.UTC) \n\tdelta = datetime.timedelta(hours=4) \n\tdaterange = mdates.drange(start, end, delta) \n\tassert_equal(6, len(daterange)) \n\tassert_equal(mdates.num2date(daterange[(-1)]), (end - delta))\n", 
" \tif (',' in argument): \n\t \tentries = argument.split(',') \n\telse: \n\t \tentries = argument.split() \n\treturn [positive_int(entry) for entry in entries]\n", 
" \t(min_val, max_val) = _is_num_param(('min', 'max'), (min, max), to_float=True) \n\tif (not isinstance(value, (int, long, float, string_type))): \n\t \traise VdtTypeError(value) \n\tif (not isinstance(value, float)): \n\t \ttry: \n\t \t \tvalue = float(value) \n\t \texcept ValueError: \n\t \t \traise VdtTypeError(value) \n\tif ((min_val is not None) and (value < min_val)): \n\t \traise VdtValueTooSmallError(value) \n\tif ((max_val is not None) and (value > max_val)): \n\t \traise VdtValueTooBigError(value) \n\treturn value\n", 
" \tt = usertypes.Timer(name='foobar') \n\tassert (t._name == 'foobar') \n\tassert (t.objectName() == 'foobar') \n\tassert (repr(t) == \"<qutebrowser.utils.usertypes.Timer \tname='foobar'>\")\n", 
" \ttry: \n\t \treturn {'false': False, 'true': True, 'f': False, 't': True, '0': False, '1': True, 'no': False, 'yes': True, 'y': False, 'n': True, 'off': False, 'on': True}[s.lower()] \n\texcept KeyError: \n\t \traise ValueError, ('Unrecognized \tboolean \tstring: \t\"%s\"' % s)\n", 
" \tstart = datetime.datetime(2011, 1, 1, tzinfo=mdates.UTC) \n\tend = datetime.datetime(2011, 1, 2, tzinfo=mdates.UTC) \n\tdelta = datetime.timedelta(hours=1) \n\tassert_equal(24, len(mdates.drange(start, end, delta))) \n\tend = (end + datetime.timedelta(microseconds=1)) \n\tassert_equal(25, len(mdates.drange(start, end, delta))) \n\tend = datetime.datetime(2011, 1, 2, tzinfo=mdates.UTC) \n\tdelta = datetime.timedelta(hours=4) \n\tdaterange = mdates.drange(start, end, delta) \n\tassert_equal(6, len(daterange)) \n\tassert_equal(mdates.num2date(daterange[(-1)]), (end - delta))\n", 
" \tstart = datetime.datetime(2011, 1, 1, tzinfo=mdates.UTC) \n\tend = datetime.datetime(2011, 1, 2, tzinfo=mdates.UTC) \n\tdelta = datetime.timedelta(hours=1) \n\tassert_equal(24, len(mdates.drange(start, end, delta))) \n\tend = (end + datetime.timedelta(microseconds=1)) \n\tassert_equal(25, len(mdates.drange(start, end, delta))) \n\tend = datetime.datetime(2011, 1, 2, tzinfo=mdates.UTC) \n\tdelta = datetime.timedelta(hours=4) \n\tdaterange = mdates.drange(start, end, delta) \n\tassert_equal(6, len(daterange)) \n\tassert_equal(mdates.num2date(daterange[(-1)]), (end - delta))\n", 
" \tdef _compile_string(s): \n\t \thy_s = HyString(s) \n\t \thy_s.start_line = hy_s.end_line = 0 \n\t \thy_s.start_column = hy_s.end_column = 0 \n\t \tcode = hy_compile([hy_s], u'__main__') \n\t \treturn code.body[0].value.s \n\tassert (_compile_string(u'test') == u'test') \n\tassert (_compile_string(u'\\u03b1\\u03b2') == u'\\u03b1\\u03b2') \n\tassert (_compile_string(u'\\xc3\\xa9') == u'\\xc3\\xa9')\n", 
" \ttry: \n\t \tuuid.UUID(value) \n\t \treturn value \n\texcept (ValueError, AttributeError): \n\t \treturn int(value)\n", 
" \tif ((start is None) != (stop is None)): \n\t \treturn False \n\telif (start is None): \n\t \treturn ((length is None) or (length >= 0)) \n\telif (length is None): \n\t \treturn (0 <= start < stop) \n\telif (start >= stop): \n\t \treturn False \n\treturn (0 <= start < length)\n", 
" \treturn (len(value) <= int(arg))\n", 
" \treturn (len(value) > int(arg))\n", 
" \tstart = datetime.datetime(2011, 1, 1, tzinfo=mdates.UTC) \n\tend = datetime.datetime(2011, 1, 2, tzinfo=mdates.UTC) \n\tdelta = datetime.timedelta(hours=1) \n\tassert_equal(24, len(mdates.drange(start, end, delta))) \n\tend = (end + datetime.timedelta(microseconds=1)) \n\tassert_equal(25, len(mdates.drange(start, end, delta))) \n\tend = datetime.datetime(2011, 1, 2, tzinfo=mdates.UTC) \n\tdelta = datetime.timedelta(hours=4) \n\tdaterange = mdates.drange(start, end, delta) \n\tassert_equal(6, len(daterange)) \n\tassert_equal(mdates.num2date(daterange[(-1)]), (end - delta))\n", 
" \tclass BadType(object, ): \n\t \tpass \n\tfor (name, objects) in grammar_types: \n\t \tfor obj in objects: \n\t \t \ttmp_obj = obj() \n\t \t \tsetattr(test_obj, name, tmp_obj) \n\t \t \tnt.assert_equal(getattr(test_obj, name), tmp_obj) \n\t \t \tbad_obj = BadType() \n\t \t \tnt.assert_raises_regexp(ValueError, ((name + '.*') + obj.__name__), setattr, test_obj, name, bad_obj) \n\t \t \tnt.assert_equal(getattr(test_obj, name), tmp_obj)\n", 
" \t(min_len, max_len) = _is_num_param(('min', 'max'), (min, max)) \n\tif isinstance(value, string_type): \n\t \traise VdtTypeError(value) \n\ttry: \n\t \tnum_members = len(value) \n\texcept TypeError: \n\t \traise VdtTypeError(value) \n\tif ((min_len is not None) and (num_members < min_len)): \n\t \traise VdtValueTooShortError(value) \n\tif ((max_len is not None) and (num_members > max_len)): \n\t \traise VdtValueTooLongError(value) \n\treturn list(value)\n", 
" \tdef check_accepts(f): \n\t \tassert (len(types) == f.__code__.co_argcount) \n\t \tdef new_f(*args, **kwds): \n\t \t \tfor (a, t) in zip(args, types): \n\t \t \t \tassert isinstance(a, t), ('arg \t%r \tdoes \tnot \tmatch \t%s' % (a, t)) \n\t \t \treturn f(*args, **kwds) \n\t \tnew_f.__name__ = f.__name__ \n\t \treturn new_f \n\treturn check_accepts\n", 
" \t[Fs, x] = audioBasicIO.readAudioFile(fileName) \n\tx = audioBasicIO.stereo2mono(x) \n\tif storeStFeatures: \n\t \t[mtF, stF] = mtFeatureExtraction(x, Fs, round((Fs * midTermSize)), round((Fs * midTermStep)), round((Fs * shortTermSize)), round((Fs * shortTermStep))) \n\telse: \n\t \t[mtF, _] = mtFeatureExtraction(x, Fs, round((Fs * midTermSize)), round((Fs * midTermStep)), round((Fs * shortTermSize)), round((Fs * shortTermStep))) \n\tnumpy.save(outPutFile, mtF) \n\tif PLOT: \n\t \tprint (('Mid-term \tnumpy \tfile: \t' + outPutFile) + '.npy \tsaved') \n\tif storeToCSV: \n\t \tnumpy.savetxt((outPutFile + '.csv'), mtF.T, delimiter=',') \n\t \tif PLOT: \n\t \t \tprint (('Mid-term \tCSV \tfile: \t' + outPutFile) + '.csv \tsaved') \n\tif storeStFeatures: \n\t \tnumpy.save((outPutFile + '_st'), stF) \n\t \tif PLOT: \n\t \t \tprint (('Short-term \tnumpy \tfile: \t' + outPutFile) + '_st.npy \tsaved') \n\t \tif storeToCSV: \n\t \t \tnumpy.savetxt((outPutFile + '_st.csv'), stF.T, delimiter=',') \n\t \t \tif PLOT: \n\t \t \t \tprint (('Short-term \tCSV \tfile: \t' + outPutFile) + '_st.csv \tsaved')\n", 
" \t[Fs, x] = audioBasicIO.readAudioFile(fileName) \n\tx = audioBasicIO.stereo2mono(x) \n\tif storeStFeatures: \n\t \t[mtF, stF] = mtFeatureExtraction(x, Fs, round((Fs * midTermSize)), round((Fs * midTermStep)), round((Fs * shortTermSize)), round((Fs * shortTermStep))) \n\telse: \n\t \t[mtF, _] = mtFeatureExtraction(x, Fs, round((Fs * midTermSize)), round((Fs * midTermStep)), round((Fs * shortTermSize)), round((Fs * shortTermStep))) \n\tnumpy.save(outPutFile, mtF) \n\tif PLOT: \n\t \tprint (('Mid-term \tnumpy \tfile: \t' + outPutFile) + '.npy \tsaved') \n\tif storeToCSV: \n\t \tnumpy.savetxt((outPutFile + '.csv'), mtF.T, delimiter=',') \n\t \tif PLOT: \n\t \t \tprint (('Mid-term \tCSV \tfile: \t' + outPutFile) + '.csv \tsaved') \n\tif storeStFeatures: \n\t \tnumpy.save((outPutFile + '_st'), stF) \n\t \tif PLOT: \n\t \t \tprint (('Short-term \tnumpy \tfile: \t' + outPutFile) + '_st.npy \tsaved') \n\t \tif storeToCSV: \n\t \t \tnumpy.savetxt((outPutFile + '_st.csv'), stF.T, delimiter=',') \n\t \t \tif PLOT: \n\t \t \t \tprint (('Short-term \tCSV \tfile: \t' + outPutFile) + '_st.csv \tsaved')\n", 
" \tdef get_key_msg(key=None): \n\t \tif (key is None): \n\t \t \treturn u'' \n\t \telse: \n\t \t \treturn u\"In \tkey \t'{}': \t\".format(key) \n\tallowable = tuple(allowable) \n\tkey_msg = get_key_msg(key_arg) \n\tval = obj \n\tif (val is None): \n\t \tif can_be_none: \n\t \t \tval = list(default) \n\t \telse: \n\t \t \traise raise_type(u'{}Expected \tan \tobject \tof \tacceptable \ttype \t{}, \treceived \tNone \tand \tcan_be_none \tis \tFalse'.format(key_msg, allowable)) \n\tif isinstance(val, allowable): \n\t \tlst = list(val) \n\t \tfor e in lst: \n\t \t \tif (not isinstance(e, expected_type)): \n\t \t \t \traise raise_type(u'{}Expected \ta \tlist \tcontaining \tvalues \tof \ttype \t{}, \tinstead \tgot \ta \tvalue \t{} \tof \t{}'.format(key_msg, expected_type, e, e.__class__)) \n\t \treturn lst \n\telse: \n\t \traise raise_type(u'{}Expected \tan \tobject \tof \tacceptable \ttype \t{}, \treceived \t{} \tinstead'.format(key_msg, allowable, val))\n", 
" \tmimepart = mime.from_string('From: \t\"\"Bob\"\" \t<bob@foocorp.com>') \n\tparsed = parse_mimepart_address_header(mimepart, 'From') \n\tassert (parsed == [[' \tBob \t', 'bob@foocorp.com']]) \n\tmimepart = mime.from_string('From: \t\"Bob\" \t<bob@foocorp.com>(through \tYahoo! \t \tStore \tOrder \tSystem)') \n\tparsed = parse_mimepart_address_header(mimepart, 'From') \n\tassert (parsed == [['Bob', 'bob@foocorp.com']]) \n\tmimepart = mime.from_string('From: \tIndiegogo \t<noreply@indiegogo.com> \t(no \treply)') \n\tparsed = parse_mimepart_address_header(mimepart, 'From') \n\tassert (parsed == [['Indiegogo', 'noreply@indiegogo.com']]) \n\tmimepart = mime.from_string('From: \tAnon \t<support@github.com> \t(GitHub \tStaff)') \n\tparsed = parse_mimepart_address_header(mimepart, 'From') \n\tassert (parsed == [['Anon', 'support@github.com']]) \n\tmimepart = mime.from_string('From: \troot@gunks \t(Cron \tDaemon)') \n\tparsed = parse_mimepart_address_header(mimepart, 'From') \n\tassert (parsed == [['Cron \tDaemon', 'root@gunks']]) \n\tmimepart = mime.from_string('From: \tBob \t<bob@foocorp.com') \n\tparsed = parse_mimepart_address_header(mimepart, 'From') \n\tassert (parsed == [['Bob', 'bob@foocorp.com']]) \n\tmimepart = mime.from_string('From: \t \t()') \n\tparsed = parse_mimepart_address_header(mimepart, 'From') \n\tassert (parsed == []) \n\tmimepart = mime.from_string('') \n\tparsed = parse_mimepart_address_header(mimepart, 'From') \n\tassert (parsed == []) \n\tmimepart = mime.from_string('From: \tbob@foocorp.com\\r\\nFrom: \tbob@foocorp.com') \n\tparsed = parse_mimepart_address_header(mimepart, 'From') \n\tassert (parsed == [['', 'bob@foocorp.com']]) \n\tmimepart = mime.from_string('From: \t=?utf-8?Q?Foo=2C=20Corp.?= \t<info@foocorp.com>') \n\tparsed = parse_mimepart_address_header(mimepart, 'From') \n\tassert (parsed == [['Foo, \tCorp.', 'info@foocorp.com']]) \n\tmimepart = mime.from_string('To: \t=?utf-8?Q?Foo=2C=20Corp.?= \t<info@foocorp.com>, \t=?utf-8?Q?Support?= \t<support@foocorp.com>') \n\tparsed = parse_mimepart_address_header(mimepart, 'To') \n\tassert (parsed == [['Foo, \tCorp.', 'info@foocorp.com'], ['Support', 'support@foocorp.com']]) \n\tmimepart = mime.from_string('To: \talice@foocorp.com\\nSubject: \tHello\\nTo: \tbob@foocorp.com') \n\tparsed = parse_mimepart_address_header(mimepart, 'To') \n\tassert (parsed == [['', 'alice@foocorp.com'], ['', 'bob@foocorp.com']])\n", 
" \trng = numpy.random.RandomState(22) \n\tx1 = rng.rand(5) \n\tx2 = rng.rand(10) \n\tt1 = cuda.shared_constructor(numpy.asarray(x1, 'float32')) \n\tt2 = cuda.shared_constructor(numpy.asarray(x2, 'float32')) \n\tt = tensor.concatenate([t1, t2], axis=(-1)) \n\tf = theano.function(inputs=[], outputs=t) \n\tassert numpy.allclose(f(), numpy.concatenate([x1, x2], axis=(-1))) \n\tx1 = rng.rand(5, 10) \n\tx2 = rng.rand(10, 10) \n\tt1 = cuda.shared_constructor(numpy.asarray(x1, 'float32')) \n\tt2 = cuda.shared_constructor(numpy.asarray(x2, 'float32')) \n\tt = tensor.concatenate([t1, t2], axis=(-2)) \n\tf = theano.function(inputs=[], outputs=t) \n\tassert numpy.allclose(f(), numpy.concatenate([x1, x2], axis=(-2))) \n\ttry: \n\t \tt = tensor.concatenate([t1, t2], axis=(-1)) \n\t \tf = theano.function(inputs=[], outputs=t) \n\t \tf() \n\t \tassert False \n\texcept ValueError: \n\t \tassert True \n\ttry: \n\t \tt = tensor.concatenate([t1, t2], axis=(-3)) \n\t \tf = theano.function(inputs=[], outputs=t) \n\t \tf() \n\t \tassert False \n\texcept IndexError: \n\t \tassert True\n", 
" \tvals = [] \n\tfor c in concepts: \n\t \tvals.append((c.prefLabel, c.extension)) \n\tif lexicon: \n\t \tread = True \n\tif read: \n\t \tfrom nltk.sem import Valuation \n\t \tval = Valuation({}) \n\t \tval.update(vals) \n\t \tval = label_indivs(val, lexicon=lexicon) \n\t \treturn val \n\telse: \n\t \treturn vals\n", 
" \tfrom .simplexml import SimpleXMLElement \n\tlocal_namespaces = {} \n\tfor (k, v) in schema[:]: \n\t \tif k.startswith(u'xmlns'): \n\t \t \tlocal_namespaces[get_local_name(k)] = v \n\t \tif (k == u'targetNamespace'): \n\t \t \tif (v == u'urn:DefaultNamespace'): \n\t \t \t \tv = global_namespaces[None] \n\t \t \tlocal_namespaces[None] = v \n\t \tif (k == u'elementFormDefault'): \n\t \t \tqualified = (v == u'qualified') \n\tfor ns in local_namespaces.values(): \n\t \tif (ns not in global_namespaces): \n\t \t \tglobal_namespaces[ns] = (u'ns%s' % len(global_namespaces)) \n\tfor element in (schema.children() or []): \n\t \tif (element.get_local_name() in (u'import', u'include')): \n\t \t \tschema_namespace = element[u'namespace'] \n\t \t \tschema_location = element[u'schemaLocation'] \n\t \t \tif (schema_location is None): \n\t \t \t \tlog.debug((u'Schema \tlocation \tnot \tprovided \tfor \t%s!' % schema_namespace)) \n\t \t \t \tcontinue \n\t \t \tif (schema_location in imported_schemas): \n\t \t \t \tlog.debug((u'Schema \t%s \talready \timported!' % schema_location)) \n\t \t \t \tcontinue \n\t \t \timported_schemas[schema_location] = schema_namespace \n\t \t \tlog.debug((u'Importing \tschema \t%s \tfrom \t%s' % (schema_namespace, schema_location))) \n\t \t \txml = fetch(schema_location, http, cache, force_download, wsdl_basedir) \n\t \t \tpath = os.path.normpath(os.path.join(wsdl_basedir, schema_location)) \n\t \t \tpath = os.path.dirname(path) \n\t \t \timported_schema = SimpleXMLElement(xml, namespace=xsd_uri) \n\t \t \tpreprocess_schema(imported_schema, imported_schemas, elements, xsd_uri, dialect, http, cache, force_download, path, global_namespaces, qualified) \n\t \telement_type = element.get_local_name() \n\t \tif (element_type in (u'element', u'complexType', u'simpleType')): \n\t \t \tnamespace = local_namespaces[None] \n\t \t \telement_ns = global_namespaces[ns] \n\t \t \telement_name = element[u'name'] \n\t \t \tlog.debug((u'Parsing \tElement \t%s: \t%s' % (element_type, element_name))) \n\t \t \tif (element.get_local_name() == u'complexType'): \n\t \t \t \tchildren = element.children() \n\t \t \telif (element.get_local_name() == u'simpleType'): \n\t \t \t \tchildren = element(u'restriction', ns=xsd_uri, error=False) \n\t \t \t \tif (not children): \n\t \t \t \t \tchildren = element.children() \n\t \t \telif ((element.get_local_name() == u'element') and element[u'type']): \n\t \t \t \tchildren = element \n\t \t \telse: \n\t \t \t \tchildren = element.children() \n\t \t \t \tif children: \n\t \t \t \t \tchildren = children.children() \n\t \t \t \telif (element.get_local_name() == u'element'): \n\t \t \t \t \tchildren = element \n\t \t \tif children: \n\t \t \t \tprocess_element(elements, element_name, children, element_type, xsd_uri, dialect, namespace, qualified)\n", 
" \tassert isinstance(schema, dict) \n\tassert (SCHEMA_KEY_TYPE in schema) \n\tassert (schema[SCHEMA_KEY_TYPE] in ALLOWED_SCHEMA_TYPES) \n\tif (schema[SCHEMA_KEY_TYPE] == SCHEMA_TYPE_CUSTOM): \n\t \t_validate_dict_keys(schema, [SCHEMA_KEY_TYPE, SCHEMA_KEY_OBJ_TYPE], []) \n\t \tassert (schema[SCHEMA_KEY_OBJ_TYPE] in ALLOWED_CUSTOM_OBJ_TYPES), schema \n\telif (schema[SCHEMA_KEY_TYPE] == SCHEMA_TYPE_LIST): \n\t \t_validate_dict_keys(schema, [SCHEMA_KEY_ITEMS, SCHEMA_KEY_TYPE], (OPTIONAL_SCHEMA_KEYS + [SCHEMA_KEY_LEN])) \n\t \tvalidate_schema(schema[SCHEMA_KEY_ITEMS]) \n\t \tif (SCHEMA_KEY_LEN in schema): \n\t \t \tassert isinstance(schema[SCHEMA_KEY_LEN], int) \n\t \t \tassert (schema[SCHEMA_KEY_LEN] > 0) \n\telif (schema[SCHEMA_KEY_TYPE] == SCHEMA_TYPE_DICT): \n\t \t_validate_dict_keys(schema, [SCHEMA_KEY_PROPERTIES, SCHEMA_KEY_TYPE], OPTIONAL_SCHEMA_KEYS) \n\t \tassert isinstance(schema[SCHEMA_KEY_PROPERTIES], list) \n\t \tfor prop in schema[SCHEMA_KEY_PROPERTIES]: \n\t \t \t_validate_dict_keys(prop, [SCHEMA_KEY_NAME, SCHEMA_KEY_SCHEMA], [SCHEMA_KEY_DESCRIPTION]) \n\t \t \tassert isinstance(prop[SCHEMA_KEY_NAME], basestring) \n\t \t \tvalidate_schema(prop[SCHEMA_KEY_SCHEMA]) \n\t \t \tif (SCHEMA_KEY_DESCRIPTION in prop): \n\t \t \t \tassert isinstance(prop[SCHEMA_KEY_DESCRIPTION], basestring) \n\telse: \n\t \t_validate_dict_keys(schema, [SCHEMA_KEY_TYPE], OPTIONAL_SCHEMA_KEYS) \n\tif (SCHEMA_KEY_UI_CONFIG in schema): \n\t \t_validate_ui_config(schema[SCHEMA_KEY_TYPE], schema[SCHEMA_KEY_UI_CONFIG]) \n\tif ((SCHEMA_KEY_CHOICES in schema) and (SCHEMA_KEY_POST_NORMALIZERS in schema)): \n\t \traise AssertionError(\"Schema \tcannot \tcontain \tboth \ta \t'choices' \tand \ta \t'post_normalizers' \tkey.\") \n\tif (SCHEMA_KEY_POST_NORMALIZERS in schema): \n\t \tassert isinstance(schema[SCHEMA_KEY_POST_NORMALIZERS], list) \n\t \tfor post_normalizer in schema[SCHEMA_KEY_POST_NORMALIZERS]: \n\t \t \tassert isinstance(post_normalizer, dict) \n\t \t \tassert ('id' in post_normalizer) \n\t \t \tschema_utils.Normalizers.get(post_normalizer['id']) \n\tif (SCHEMA_KEY_VALIDATORS in schema): \n\t \tassert isinstance(schema[SCHEMA_KEY_VALIDATORS], list) \n\t \tfor validator in schema[SCHEMA_KEY_VALIDATORS]: \n\t \t \tassert isinstance(validator, dict) \n\t \t \tassert ('id' in validator) \n\t \t \t_validate_validator(schema[SCHEMA_KEY_TYPE], validator)\n", 
" \tdef decorator(f): \n\t \tdef new_func(*args, **kwargs): \n\t \t \tctx = get_current_context() \n\t \t \tif ensure: \n\t \t \t \tobj = ctx.ensure_object(object_type) \n\t \t \telse: \n\t \t \t \tobj = ctx.find_object(object_type) \n\t \t \tif (obj is None): \n\t \t \t \traise RuntimeError(('Managed \tto \tinvoke \tcallback \twithout \ta \tcontext \tobject \tof \ttype \t%r \texisting' % object_type.__name__)) \n\t \t \treturn ctx.invoke(f, obj, *args[1:], **kwargs) \n\t \treturn update_wrapper(new_func, f) \n\treturn decorator\n", 
" \tif ('data \tscience' in status_update['text'].lower()): \n\t \tday_of_week = status_update['created_at'].weekday() \n\t \t(yield (day_of_week, 1))\n", 
" \t@wraps(f) \n\tdef wrapper(self, *args, **kwargs): \n\t \tsupported = (platform.getType() == 'posix') \n\t \tif supported: \n\t \t \treturn f(self, *args, **kwargs) \n\t \telse: \n\t \t \te = self.assertRaises((NotImplementedError, SkipTest, self.failureException), f, self, *args, **kwargs) \n\t \t \tif isinstance(e, NotImplementedError): \n\t \t \t \tself.assertTrue(str(e).startswith('isRunning \tis \tnot \timplemented \ton \t')) \n\treturn wrapper\n", 
" \treturn str(hashlib.sha224(str(random.getrandbits(128)).encode(u'utf-8')).hexdigest())\n", 
" \tcredentials = AppAssertionCredentials('https://www.googleapis.com/auth/iam') \n\thttp_auth = credentials.authorize(httplib2.Http()) \n\tservice = build(serviceName='iam', version='v1', http=http_auth) \n\tnow = int(time.time()) \n\theader_json = json.dumps({'typ': 'JWT', 'alg': 'RS256'}) \n\tpayload_json = json.dumps({'iat': now, 'exp': (now + 3600), 'iss': SERVICE_ACCOUNT_EMAIL, 'sub': SERVICE_ACCOUNT_EMAIL, 'aud': 'echo.endpoints.sample.google.com', 'email': SERVICE_ACCOUNT_EMAIL}) \n\theaderAndPayload = '{}.{}'.format(base64.urlsafe_b64encode(header_json), base64.urlsafe_b64encode(payload_json)) \n\tslist = service.projects().serviceAccounts().signBlob(name=SERVICE_ACCOUNT, body={'bytesToSign': base64.b64encode(headerAndPayload)}) \n\tres = slist.execute() \n\tsignature = base64.urlsafe_b64encode(base64.decodestring(res['signature'])) \n\tsigned_jwt = '{}.{}'.format(headerAndPayload, signature) \n\treturn signed_jwt\n", 
" \thttp('--output=/dev/null', (httpbin + '/get'))\n", 
" \te = HTTPInternalServerError() \n\te.template = 'A \t%(ping)s \tand \t<b>%(pong)s</b> \tmessage.' \n\tassert str(e).startswith('500 \tInternal \tServer \tError') \n\tassert (e.plain({'ping': 'fun', 'pong': 'happy'}) == '500 \tInternal \tServer \tError\\r\\nA \tfun \tand \thappy \tmessage.\\r\\n') \n\tassert ('<p>A \tfun \tand \t<b>happy</b> \tmessage.</p>' in e.html({'ping': 'fun', 'pong': 'happy'}))\n", 
" \ttext = '\\nA, \tB, \tC\\n, \t2, \tnan\\na, \t-999, \t-3.4\\nnan, \t5, \t-9999\\n8, \tnan, \t7.6e12\\n' \n\ttable = read_basic(text, delimiter=',', parallel=parallel) \n\tassert isinstance(table['A'], MaskedColumn) \n\tassert (table['A'][0] is ma.masked) \n\tassert_equal(table['A'].data.data[0], '0') \n\tassert (table['A'][1] is not ma.masked) \n\ttable = read_basic(text, delimiter=',', fill_values=('-999', '0'), parallel=parallel) \n\tassert isinstance(table['B'], MaskedColumn) \n\tassert (table['A'][0] is not ma.masked) \n\tassert (table['C'][2] is not ma.masked) \n\tassert (table['B'][1] is ma.masked) \n\tassert_equal(table['B'].data.data[1], 0.0) \n\tassert (table['B'][0] is not ma.masked) \n\ttable = read_basic(text, delimiter=',', fill_values=[], parallel=parallel) \n\tfor name in 'ABC': \n\t \tassert (not isinstance(table[name], MaskedColumn)) \n\ttable = read_basic(text, delimiter=',', fill_values=[('', '0', 'A'), ('nan', '999', 'A', 'C')], parallel=parallel) \n\tassert np.isnan(table['B'][3]) \n\tassert (table['B'][3] is not ma.masked) \n\tassert (table['A'][0] is ma.masked) \n\tassert (table['A'][2] is ma.masked) \n\tassert_equal(table['A'].data.data[0], '0') \n\tassert_equal(table['A'].data.data[2], '999') \n\tassert (table['C'][0] is ma.masked) \n\tassert_almost_equal(table['C'].data.data[0], 999.0) \n\tassert_almost_equal(table['C'][1], (-3.4))\n", 
" \ts1 = FIRST_CAP_RE.sub(('\\\\1%s\\\\2' % separator), name) \n\treturn ALL_CAP_RE.sub(('\\\\1%s\\\\2' % separator), s1).lower()\n", 
" \tif (not isinstance(input_str, string_types)): \n\t \tinput_str = text_type(input_str) \n\treturn quote(text_type(input_str).encode('utf-8'), safe=safe)\n", 
" \tif (boundary.startswith('\"') and boundary.endswith('\"')): \n\t \tboundary = boundary[1:(-1)] \n\tfinal_boundary_index = data.rfind((('--' + boundary) + '--')) \n\tif (final_boundary_index == (-1)): \n\t \tgen_log.warning('Invalid \tmultipart/form-data: \tno \tfinal \tboundary') \n\t \treturn \n\tparts = data[:final_boundary_index].split((('--' + boundary) + '\\r\\n')) \n\tfor part in parts: \n\t \tif (not part): \n\t \t \tcontinue \n\t \teoh = part.find('\\r\\n\\r\\n') \n\t \tif (eoh == (-1)): \n\t \t \tgen_log.warning('multipart/form-data \tmissing \theaders') \n\t \t \tcontinue \n\t \theaders = HTTPHeaders.parse(part[:eoh].decode('utf-8')) \n\t \tdisp_header = headers.get('Content-Disposition', '') \n\t \t(disposition, disp_params) = _parse_header(disp_header) \n\t \tif ((disposition != 'form-data') or (not part.endswith('\\r\\n'))): \n\t \t \tgen_log.warning('Invalid \tmultipart/form-data') \n\t \t \tcontinue \n\t \tvalue = part[(eoh + 4):(-2)] \n\t \tif (not disp_params.get('name')): \n\t \t \tgen_log.warning('multipart/form-data \tvalue \tmissing \tname') \n\t \t \tcontinue \n\t \tname = disp_params['name'] \n\t \tif disp_params.get('filename'): \n\t \t \tctype = headers.get('Content-Type', 'application/unknown') \n\t \t \tfiles.setdefault(name, []).append(HTTPFile(filename=disp_params['filename'], body=value, content_type=ctype)) \n\t \telse: \n\t \t \targuments.setdefault(name, []).append(value)\n", 
" \tstart = datetime.datetime(2011, 1, 1, tzinfo=mdates.UTC) \n\tend = datetime.datetime(2011, 1, 2, tzinfo=mdates.UTC) \n\tdelta = datetime.timedelta(hours=1) \n\tassert_equal(24, len(mdates.drange(start, end, delta))) \n\tend = (end + datetime.timedelta(microseconds=1)) \n\tassert_equal(25, len(mdates.drange(start, end, delta))) \n\tend = datetime.datetime(2011, 1, 2, tzinfo=mdates.UTC) \n\tdelta = datetime.timedelta(hours=4) \n\tdaterange = mdates.drange(start, end, delta) \n\tassert_equal(6, len(daterange)) \n\tassert_equal(mdates.num2date(daterange[(-1)]), (end - delta))\n", 
" \tstart = datetime.datetime(2011, 1, 1, tzinfo=mdates.UTC) \n\tend = datetime.datetime(2011, 1, 2, tzinfo=mdates.UTC) \n\tdelta = datetime.timedelta(hours=1) \n\tassert_equal(24, len(mdates.drange(start, end, delta))) \n\tend = (end + datetime.timedelta(microseconds=1)) \n\tassert_equal(25, len(mdates.drange(start, end, delta))) \n\tend = datetime.datetime(2011, 1, 2, tzinfo=mdates.UTC) \n\tdelta = datetime.timedelta(hours=4) \n\tdaterange = mdates.drange(start, end, delta) \n\tassert_equal(6, len(daterange)) \n\tassert_equal(mdates.num2date(daterange[(-1)]), (end - delta))\n", 
" \tif present: \n\t \tif (not expected): \n\t \t \traise exc_unexpected(*exc_args) \n\telif (expected and (expected is not Argument.ignore)): \n\t \traise exc_missing(*exc_args)\n", 
" \tfrom .singleton import S \n\tfrom .basic import Basic \n\tfrom .sympify import sympify, SympifyError \n\tfrom .compatibility import iterable \n\tif isinstance(item, Basic): \n\t \treturn item.sort_key(order=order) \n\tif iterable(item, exclude=string_types): \n\t \tif isinstance(item, dict): \n\t \t \targs = item.items() \n\t \t \tunordered = True \n\t \telif isinstance(item, set): \n\t \t \targs = item \n\t \t \tunordered = True \n\t \telse: \n\t \t \targs = list(item) \n\t \t \tunordered = False \n\t \targs = [default_sort_key(arg, order=order) for arg in args] \n\t \tif unordered: \n\t \t \targs = sorted(args) \n\t \t(cls_index, args) = (10, (len(args), tuple(args))) \n\telse: \n\t \tif (not isinstance(item, string_types)): \n\t \t \ttry: \n\t \t \t \titem = sympify(item) \n\t \t \texcept SympifyError: \n\t \t \t \tpass \n\t \t \telse: \n\t \t \t \tif isinstance(item, Basic): \n\t \t \t \t \treturn default_sort_key(item) \n\t \t(cls_index, args) = (0, (1, (str(item),))) \n\treturn ((cls_index, 0, item.__class__.__name__), args, S.One.sort_key(), S.One)\n", 
" \tif context.is_admin: \n\t \treturn True \n\tif (context.owner is None): \n\t \treturn False \n\treturn (namespace_property.namespace.owner == context.owner)\n", 
" \tout = git.diff_tree(name_only=True, no_commit_id=True, r=True, z=True, *args)[STDOUT] \n\treturn _parse_diff_filenames(out)\n", 
" \tsizes = set(settings.AVATAR_AUTO_GENERATE_SIZES) \n\tif (size is not None): \n\t \tsizes.add(size) \n\tfor prefix in cached_funcs: \n\t \tfor size in sizes: \n\t \t \tcache.delete(get_cache_key(user, size, prefix))\n", 
" \tif context.is_admin: \n\t \treturn True \n\tif (context.owner is None): \n\t \treturn False \n\treturn (tag.namespace.owner == context.owner)\n", 
" \tkwargs = salt.utils.clean_kwargs(**kwargs) \n\thex_ = kwargs.pop('hex', False) \n\tif kwargs: \n\t \tsalt.utils.invalid_kwargs(kwargs) \n\tcmd = ['xattr', '-w'] \n\tif hex_: \n\t \tcmd.append('-x') \n\tcmd.extend([attribute, value, path]) \n\ttry: \n\t \tsalt.utils.mac_utils.execute_return_success(cmd) \n\texcept CommandExecutionError as exc: \n\t \tif ('No \tsuch \tfile' in exc.strerror): \n\t \t \traise CommandExecutionError('File \tnot \tfound: \t{0}'.format(path)) \n\t \traise CommandExecutionError('Unknown \tError: \t{0}'.format(exc.strerror)) \n\treturn (read(path, attribute, **{'hex': hex_}) == value)\n", 
" \ts1 = _sig(os.stat(f1)) \n\ts2 = _sig(os.stat(f2)) \n\tif ((s1[0] != stat.S_IFREG) or (s2[0] != stat.S_IFREG)): \n\t \treturn False \n\tif (shallow and (s1 == s2)): \n\t \treturn True \n\tif (s1[1] != s2[1]): \n\t \treturn False \n\toutcome = _cache.get((f1, f2, s1, s2)) \n\tif (outcome is None): \n\t \toutcome = _do_cmp(f1, f2) \n\t \tif (len(_cache) > 100): \n\t \t \tclear_cache() \n\t \t_cache[(f1, f2, s1, s2)] = outcome \n\treturn outcome\n", 
" \tdef Modified(self): \n\t \t'Sets \tthe \t_cached_byte_size_dirty \tbit \tto \ttrue,\\n \t \t \t \tand \tpropagates \tthis \tto \tour \tlistener \tiff \tthis \twas \ta \tstate \tchange.\\n \t \t \t \t' \n\t \tif (not self._cached_byte_size_dirty): \n\t \t \tself._cached_byte_size_dirty = True \n\t \t \tself._listener_for_children.dirty = True \n\t \t \tself._is_present_in_parent = True \n\t \t \tself._listener.Modified() \n\tcls._Modified = Modified \n\tcls.SetInParent = Modified\n", 
" \tclass Dummy(object, ): \n\t \tattr = field \n\t \tattr._name = u'attr' \n\treturn Dummy()\n", 
" \tt = Template('Username \tis \t{{ \tuser \t}}.') \n\tc = RequestContext(request, {}) \n\treturn HttpResponse(t.render(c))\n", 
" \traise ValueError\n", 
" \tfor name in ['allow_nan', 'separators', 'sort_keys']: \n\t \tif (name in kwargs): \n\t \t \traise ValueError(('The \tvalue \tof \t%r \tis \tcomputed \tinternally, \toverriding \tis \tnot \tpermissable.' % name)) \n\tpretty = settings.pretty(pretty) \n\tif pretty: \n\t \tseparators = (',', ': \t') \n\telse: \n\t \tseparators = (',', ':') \n\tif (pretty and (indent is None)): \n\t \tindent = 2 \n\treturn json.dumps(obj, cls=BokehJSONEncoder, allow_nan=False, indent=indent, separators=separators, sort_keys=True, **kwargs)\n", 
" \treturn ' \t'.join(s.split())\n", 
" \tif (method not in ['MNE', 'dSPM', 'sLORETA']): \n\t \traise ValueError('method \tparameter \tshould \tbe \t\"MNE\" \tor \t\"dSPM\" \tor \t\"sLORETA\".') \n\treturn method\n", 
" \tdef _ugettext(text): \n\t \treturn translations.get(text, text) \n\treturn _ugettext\n", 
" \traise exception\n", 
" \tprivate_name = ('_' + name) \n\t@deprecated(since, name=name, obj_type='attribute') \n\tdef get(self): \n\t \treturn getattr(self, private_name) \n\t@deprecated(since, name=name, obj_type='attribute') \n\tdef set(self, val): \n\t \tsetattr(self, private_name, val) \n\t@deprecated(since, name=name, obj_type='attribute') \n\tdef delete(self): \n\t \tdelattr(self, private_name) \n\treturn property(get, set, delete)\n", 
" \tresult = JsonDumpForScriptContext(dump_object) \n\tif xssi_protection: \n\t \tresult = (')]}\\n' + result) \n\tresponse = http.HttpResponse(result, content_type='application/json; \tcharset=utf-8') \n\tresponse['Content-Disposition'] = 'attachment; \tfilename=response.json' \n\tresponse['X-Content-Type-Options'] = 'nosniff' \n\treturn response\n", 
" \tassert isinstance(formset, BaseSimpleFormSet) \n\tres = django.http.QueryDict('', mutable=True) \n\tfor (i, data_dict) in enumerate(data_dict_list): \n\t \tprefix = formset.make_prefix(i) \n\t \tform = formset.form(prefix=prefix) \n\t \tres.update(denormalize_form_dict(data_dict, form, attr_list)) \n\t \tres[(prefix + '-_exists')] = 'True' \n\tres[str(formset.management_form.add_prefix('next_form_id'))] = str(len(data_dict_list)) \n\treturn res \n\tdef __str__(self): \n\t \treturn ('%s: \t%s' % (self.__class__, self.query))\n", 
" \tif (getattr(response, 'content_type', None) == 'application/json'): \n\t \tresponse_data = json.loads(response.body) \n\t \tresponse_data.update(data) \n\t \tresponse.body = json.dumps(response_data) \n\treturn response\n", 
" \tglobal _global_data \n\tif (_global_data is None): \n\t \tdirname = os.path.join(os.path.dirname(__file__)) \n\t \tfilename = os.path.join(dirname, 'global.dat') \n\t \tfileobj = open(filename, 'rb') \n\t \ttry: \n\t \t \t_global_data = pickle.load(fileobj) \n\t \tfinally: \n\t \t \tfileobj.close() \n\treturn _global_data.get(key, {})\n", 
" \tif hasattr(random, 'SystemRandom'): \n\t \tlogging.info('Generating \ta \tsecure \trandom \tkey \tusing \tSystemRandom.') \n\t \tchoice = random.SystemRandom().choice \n\telse: \n\t \tmsg = 'WARNING: \tSystemRandom \tnot \tpresent. \tGenerating \ta \trandom \tkey \tusing \trandom.choice \t(NOT \tCRYPTOGRAPHICALLY \tSECURE).' \n\t \tlogging.warning(msg) \n\t \tchoice = random.choice \n\treturn ''.join(map((lambda x: choice((string.digits + string.ascii_letters))), range(key_length)))\n", 
" \tabspath = os.path.abspath(key_file) \n\tif os.path.exists(key_file): \n\t \tkey = read_from_file(key_file) \n\t \treturn key \n\tlock = lockutils.external_lock((key_file + '.lock'), lock_path=os.path.dirname(abspath)) \n\twith lock: \n\t \tif (not os.path.exists(key_file)): \n\t \t \tkey = generate_key(key_length) \n\t \t \told_umask = os.umask(127) \n\t \t \twith open(key_file, 'w') as f: \n\t \t \t \tf.write(key) \n\t \t \tos.umask(old_umask) \n\t \telse: \n\t \t \tkey = read_from_file(key_file) \n\t \treturn key\n", 
" \td = _translate_attachment_summary_view(volume_id, instance_uuid, mountpoint) \n\treturn d\n", 
" \td = {} \n\td['id'] = volume_id \n\td['volumeId'] = volume_id \n\td['serverId'] = instance_uuid \n\tif mountpoint: \n\t \td['device'] = mountpoint \n\treturn d\n", 
" \td = _translate_volume_summary_view(context, vol) \n\treturn d\n", 
" \td = {} \n\td['id'] = volume_id \n\td['volumeId'] = volume_id \n\td['serverId'] = instance_uuid \n\tif mountpoint: \n\t \td['device'] = mountpoint \n\treturn d\n", 
" \tif context.is_admin: \n\t \tfor key in ('sort_key', 'sort_dir', 'limit', 'marker'): \n\t \t \tsearch_options.pop(key, None) \n\t \treturn \n\tunknown_options = [opt for opt in search_options if (opt not in allowed_search_options)] \n\tif unknown_options: \n\t \tLOG.debug(\"Removing \toptions \t'%s' \tfrom \tquery\", ', \t'.join(unknown_options)) \n\t \tfor opt in unknown_options: \n\t \t \tsearch_options.pop(opt, None)\n", 
" \tdef decorator(func): \n\t \tif (not hasattr(func, 'wsgi_serializers')): \n\t \t \tfunc.wsgi_serializers = {} \n\t \tfunc.wsgi_serializers.update(serializers) \n\t \treturn func \n\treturn decorator\n", 
" \tdef decorator(func): \n\t \tif (not hasattr(func, 'wsgi_deserializers')): \n\t \t \tfunc.wsgi_deserializers = {} \n\t \tfunc.wsgi_deserializers.update(deserializers) \n\t \treturn func \n\treturn decorator\n", 
" \tdef decorator(func): \n\t \tfunc.wsgi_code = code \n\t \treturn func \n\treturn decorator\n", 
" \ttry: \n\t \tdecoded = jsonutils.loads(body) \n\texcept ValueError: \n\t \tmsg = _('cannot \tunderstand \tJSON') \n\t \traise exception.MalformedRequestBody(reason=msg) \n\tif (len(decoded) != 1): \n\t \tmsg = _('too \tmany \tbody \tkeys') \n\t \traise exception.MalformedRequestBody(reason=msg) \n\treturn list(decoded.keys())[0]\n", 
" \ttry: \n\t \tdecoded = jsonutils.loads(body) \n\texcept ValueError: \n\t \tmsg = _('cannot \tunderstand \tJSON') \n\t \traise exception.MalformedRequestBody(reason=msg) \n\tif (len(decoded) != 1): \n\t \tmsg = _('too \tmany \tbody \tkeys') \n\t \traise exception.MalformedRequestBody(reason=msg) \n\treturn list(decoded.keys())[0]\n", 
" \tdef decorator(func): \n\t \tfunc.wsgi_action = name \n\t \treturn func \n\treturn decorator\n", 
" \tdef decorator(func): \n\t \tfunc.wsgi_extends = (func.__name__, kwargs.get('action')) \n\t \treturn func \n\tif args: \n\t \treturn decorator(*args) \n\treturn decorator\n", 
" \tcurr_time = timeutils.utcnow(with_timezone=True) \n\tcontext = req.environ['cinder.context'] \n\tfilters = {'disabled': False} \n\tservices = objects.ServiceList.get_all(context, filters) \n\tzone = '' \n\tif ('zone' in req.GET): \n\t \tzone = req.GET['zone'] \n\tif zone: \n\t \tservices = [s for s in services if (s['availability_zone'] == zone)] \n\thosts = [] \n\tfor host in services: \n\t \tdelta = (curr_time - (host.updated_at or host.created_at)) \n\t \talive = (abs(delta.total_seconds()) <= CONF.service_down_time) \n\t \tstatus = ((alive and 'available') or 'unavailable') \n\t \tactive = 'enabled' \n\t \tif host.disabled: \n\t \t \tactive = 'disabled' \n\t \tLOG.debug('status, \tactive \tand \tupdate: \t%s, \t%s, \t%s', status, active, host.updated_at) \n\t \tupdated_at = host.updated_at \n\t \tif updated_at: \n\t \t \tupdated_at = timeutils.normalize_time(updated_at) \n\t \thosts.append({'host_name': host.host, 'service': host.topic, 'zone': host.availability_zone, 'service-status': status, 'service-state': active, 'last-update': updated_at}) \n\tif service: \n\t \thosts = [host for host in hosts if (host['service'] == service)] \n\treturn hosts\n", 
" \tdef wrapped(self, req, id, service=None, *args, **kwargs): \n\t \tlisted_hosts = _list_hosts(req, service) \n\t \thosts = [h['host_name'] for h in listed_hosts] \n\t \tif (id in hosts): \n\t \t \treturn fn(self, req, id, *args, **kwargs) \n\t \traise exception.HostNotFound(host=id) \n\treturn wrapped\n", 
" \treturn _load_pipeline(loader, local_conf[CONF.api.auth_strategy].split())\n", 
" \tif (not isinstance(default, int)): \n\t \tmsg = (\"'%s' \tobject \tcannot \tbe \tinterpreted \tas \tan \tinteger\" % type(default).__name__) \n\t \traise TypeError(msg) \n\ttry: \n\t \treturn len(obj) \n\texcept TypeError: \n\t \tpass \n\ttry: \n\t \thint = type(obj).__length_hint__ \n\texcept AttributeError: \n\t \treturn default \n\ttry: \n\t \tval = hint(obj) \n\texcept TypeError: \n\t \treturn default \n\tif (val is NotImplemented): \n\t \treturn default \n\tif (not isinstance(val, int)): \n\t \tmsg = ('__length_hint__ \tmust \tbe \tinteger, \tnot \t%s' % type(val).__name__) \n\t \traise TypeError(msg) \n\tif (val < 0): \n\t \tmsg = '__length_hint__() \tshould \treturn \t>= \t0' \n\t \traise ValueError(msg) \n\treturn val\n", 
" \treturn itertools.chain(element.iterfind((_OLD_NAMESPACE_PREFIX + tag)), element.iterfind((_NEW_NAMESPACE_PREFIX + tag)))\n", 
" \tfor (key, value) in sub_dict.items(): \n\t \tif isinstance(value, list): \n\t \t \tfor repeated_element in value: \n\t \t \t \tsub_element = ET.SubElement(parent, key) \n\t \t \t \t_add_element_attrs(sub_element, repeated_element.get('attrs', {})) \n\t \t \t \tchildren = repeated_element.get('children', None) \n\t \t \t \tif isinstance(children, dict): \n\t \t \t \t \t_add_sub_elements_from_dict(sub_element, children) \n\t \t \t \telif isinstance(children, str): \n\t \t \t \t \tsub_element.text = children \n\t \telse: \n\t \t \tsub_element = ET.SubElement(parent, key) \n\t \t \t_add_element_attrs(sub_element, value.get('attrs', {})) \n\t \t \tchildren = value.get('children', None) \n\t \t \tif isinstance(children, dict): \n\t \t \t \t_add_sub_elements_from_dict(sub_element, children) \n\t \t \telif isinstance(children, str): \n\t \t \t \tsub_element.text = children\n", 
" \treturn ''.join([t for t in educate_tokens(tokenize(text), attr, language)])\n", 
" \tour_dir = path[0] \n\tfor (dirpath, dirnames, filenames) in os.walk(our_dir): \n\t \trelpath = os.path.relpath(dirpath, our_dir) \n\t \tif (relpath == '.'): \n\t \t \trelpkg = '' \n\t \telse: \n\t \t \trelpkg = ('.%s' % '.'.join(relpath.split(os.sep))) \n\t \tfor fname in filenames: \n\t \t \t(root, ext) = os.path.splitext(fname) \n\t \t \tif ((ext not in ('.py', '.pyc')) or (root == '__init__') or (fname in FILES_TO_SKIP)): \n\t \t \t \tcontinue \n\t \t \tif ((ext == '.pyc') and ((root + '.py') in filenames)): \n\t \t \t \tcontinue \n\t \t \tclassname = ('%s%s' % (root[0].upper(), root[1:])) \n\t \t \tclasspath = ('%s%s.%s.%s' % (package, relpkg, root, classname)) \n\t \t \tif ((ext_list is not None) and (classname not in ext_list)): \n\t \t \t \tlogger.debug(('Skipping \textension: \t%s' % classpath)) \n\t \t \t \tcontinue \n\t \t \ttry: \n\t \t \t \text_mgr.load_extension(classpath) \n\t \t \texcept Exception as exc: \n\t \t \t \tlogger.warning(_LW('Failed \tto \tload \textension \t%(classpath)s: \t%(exc)s'), {'classpath': classpath, 'exc': exc}) \n\t \tsubdirs = [] \n\t \tfor dname in dirnames: \n\t \t \tif (not os.path.exists(os.path.join(dirpath, dname, '__init__.py'))): \n\t \t \t \tcontinue \n\t \t \text_name = ('%s%s.%s.extension' % (package, relpkg, dname)) \n\t \t \ttry: \n\t \t \t \text = importutils.import_class(ext_name) \n\t \t \texcept ImportError: \n\t \t \t \tsubdirs.append(dname) \n\t \t \telse: \n\t \t \t \ttry: \n\t \t \t \t \text(ext_mgr) \n\t \t \t \texcept Exception as exc: \n\t \t \t \t \tlogger.warning(_LW('Failed \tto \tload \textension \t%(ext_name)s: \t%(exc)s'), {'ext_name': ext_name, 'exc': exc}) \n\t \tdirnames[:] = subdirs\n", 
" \tmax_limit = _get_pagination_max_limit() \n\tlimit = _get_limit_param(request) \n\tif (max_limit > 0): \n\t \tlimit = (min(max_limit, limit) or max_limit) \n\tif (not limit): \n\t \treturn (None, None) \n\tmarker = request.GET.get('marker', None) \n\treturn (limit, marker)\n", 
" \tlimit = request.GET.get('limit', 0) \n\ttry: \n\t \tlimit = int(limit) \n\t \tif (limit >= 0): \n\t \t \treturn limit \n\texcept ValueError: \n\t \tpass \n\tmsg = (_(\"Limit \tmust \tbe \tan \tinteger \t0 \tor \tgreater \tand \tnot \t'%s'\") % limit) \n\traise exceptions.BadRequest(resource='limit', msg=msg)\n", 
" \treturn request.GET['marker']\n", 
" \tparams = get_pagination_params(request) \n\toffset = params.get('offset', 0) \n\tlimit = CONF.api.max_limit \n\tlimit = min(limit, (params.get('limit') or limit)) \n\treturn items[offset:(offset + limit)]\n", 
" \tmax_limit = (max_limit or CONF.osapi_max_limit) \n\t(marker, limit, __) = get_pagination_params(request.GET.copy(), max_limit) \n\tstart_index = 0 \n\tif marker: \n\t \tstart_index = (-1) \n\t \tfor (i, item) in enumerate(items): \n\t \t \tif ('flavorid' in item): \n\t \t \t \tif (item['flavorid'] == marker): \n\t \t \t \t \tstart_index = (i + 1) \n\t \t \t \t \tbreak \n\t \t \telif ((item['id'] == marker) or (item.get('uuid') == marker)): \n\t \t \t \tstart_index = (i + 1) \n\t \t \t \tbreak \n\t \tif (start_index < 0): \n\t \t \tmsg = (_('marker \t[%s] \tnot \tfound') % marker) \n\t \t \traise webob.exc.HTTPBadRequest(explanation=msg) \n\trange_end = (start_index + limit) \n\treturn items[start_index:range_end]\n", 
" \tparsed_url = urllib.parse.urlsplit(href) \n\turl_parts = parsed_url.path.split('/', 2) \n\texpression = re.compile('^v([0-9]+|[0-9]+\\\\.[0-9]+)(/.*|$)') \n\tfor x in range(len(url_parts)): \n\t \tif expression.match(url_parts[x]): \n\t \t \tdel url_parts[x] \n\t \t \tbreak \n\tnew_path = '/'.join(url_parts) \n\tif (new_path == parsed_url.path): \n\t \tmsg = ('href \t%s \tdoes \tnot \tcontain \tversion' % href) \n\t \tLOG.debug(msg) \n\t \traise ValueError(msg) \n\tparsed_url = list(parsed_url) \n\tparsed_url[2] = new_path \n\treturn urllib.parse.urlunsplit(parsed_url)\n", 
" \tif (value and (value[0] == value[(-1)] == '\"')): \n\t \tvalue = value[1:(-1)] \n\treturn value\n", 
" \tresult = [] \n\tfor item in _parse_list_header(value): \n\t \tif (item[:1] == item[(-1):] == '\"'): \n\t \t \titem = unquote_header_value(item[1:(-1)]) \n\t \tresult.append(item) \n\treturn result\n", 
" \tif (not value): \n\t \treturn ('', {}) \n\tresult = [] \n\tvalue = (',' + value.replace('\\n', ',')) \n\twhile value: \n\t \tmatch = _option_header_start_mime_type.match(value) \n\t \tif (not match): \n\t \t \tbreak \n\t \tresult.append(match.group(1)) \n\t \toptions = {} \n\t \trest = match.group(2) \n\t \twhile rest: \n\t \t \toptmatch = _option_header_piece_re.match(rest) \n\t \t \tif (not optmatch): \n\t \t \t \tbreak \n\t \t \t(option, encoding, _, option_value) = optmatch.groups() \n\t \t \toption = unquote_header_value(option) \n\t \t \tif (option_value is not None): \n\t \t \t \toption_value = unquote_header_value(option_value, (option == 'filename')) \n\t \t \t \tif (encoding is not None): \n\t \t \t \t \toption_value = _unquote(option_value).decode(encoding) \n\t \t \toptions[option] = option_value \n\t \t \trest = rest[optmatch.end():] \n\t \tresult.append(options) \n\t \tif (multiple is False): \n\t \t \treturn tuple(result) \n\t \tvalue = rest \n\treturn (tuple(result) if result else ('', {}))\n", 
" \ttry: \n\t \tuuid.UUID(value) \n\t \treturn value \n\texcept (ValueError, AttributeError): \n\t \treturn int(value)\n", 
" \t(orig_exc_type, orig_exc_value, orig_exc_traceback) = sys.exc_info() \n\tif isinstance(new_exc, six.string_types): \n\t \tnew_exc = orig_exc_type(new_exc) \n\tif hasattr(new_exc, 'args'): \n\t \tif (len(new_exc.args) > 0): \n\t \t \tnew_message = ', \t'.join((str(arg) for arg in new_exc.args)) \n\t \telse: \n\t \t \tnew_message = '' \n\t \tnew_message += ('\\n\\nOriginal \texception:\\n DCTB ' + orig_exc_type.__name__) \n\t \tif (hasattr(orig_exc_value, 'args') and (len(orig_exc_value.args) > 0)): \n\t \t \tif getattr(orig_exc_value, 'reraised', False): \n\t \t \t \tnew_message += (': \t' + str(orig_exc_value.args[0])) \n\t \t \telse: \n\t \t \t \tnew_message += (': \t' + ', \t'.join((str(arg) for arg in orig_exc_value.args))) \n\t \tnew_exc.args = ((new_message,) + new_exc.args[1:]) \n\tnew_exc.__cause__ = orig_exc_value \n\tnew_exc.reraised = True \n\tsix.reraise(type(new_exc), new_exc, orig_exc_traceback)\n", 
" \tret = _ConvertToList(arg) \n\tfor element in ret: \n\t \tif (not isinstance(element, element_type)): \n\t \t \traise TypeError(('%s \tshould \tbe \tsingle \telement \tor \tlist \tof \ttype \t%s' % (arg_name, element_type))) \n\treturn ret\n", 
" \tlogger = logging.getLogger() \n\tloglevel = get_loglevel((loglevel or u'ERROR')) \n\tlogfile = (logfile if logfile else sys.__stderr__) \n\tif (not logger.handlers): \n\t \tif hasattr(logfile, u'write'): \n\t \t \thandler = logging.StreamHandler(logfile) \n\t \telse: \n\t \t \thandler = WatchedFileHandler(logfile) \n\t \tlogger.addHandler(handler) \n\t \tlogger.setLevel(loglevel) \n\treturn logger\n", 
" \treturn ((module in sys.modules) and isinstance(obj, getattr(import_module(module), class_name)))\n", 
" \tvalidate_config_version(config_details.config_files) \n\tprocessed_files = [process_config_file(config_file, config_details.environment) for config_file in config_details.config_files] \n\tconfig_details = config_details._replace(config_files=processed_files) \n\tmain_file = config_details.config_files[0] \n\tvolumes = load_mapping(config_details.config_files, u'get_volumes', u'Volume') \n\tnetworks = load_mapping(config_details.config_files, u'get_networks', u'Network') \n\tservice_dicts = load_services(config_details, main_file) \n\tif (main_file.version != V1): \n\t \tfor service_dict in service_dicts: \n\t \t \tmatch_named_volumes(service_dict, volumes) \n\tservices_using_deploy = [s for s in service_dicts if s.get(u'deploy')] \n\tif services_using_deploy: \n\t \tlog.warn(u\"Some \tservices \t({}) \tuse \tthe \t'deploy' \tkey, \twhich \twill \tbe \tignored. \tCompose \tdoes \tnot \tsupport \tdeploy \tconfiguration \t- \tuse \t`docker \tstack \tdeploy` \tto \tdeploy \tto \ta \tswarm.\".format(u', \t'.join(sorted((s[u'name'] for s in services_using_deploy))))) \n\treturn Config(main_file.version, service_dicts, volumes, networks)\n", 
" \ttry: \n\t \tvalue = args_list.pop(0) \n\texcept IndexError: \n\t \traise BadCommandUsage(msg) \n\tif ((expected_size_after is not None) and (len(args_list) > expected_size_after)): \n\t \traise BadCommandUsage('too \tmany \targuments') \n\treturn value\n", 
" \treturn datetime.datetime.strptime(d['isostr'], _DATETIME_FORMAT)\n", 
" \treturn _aliases.get(alias.lower(), alias)\n", 
" \twith open(CHANGELOG) as f: \n\t \tcur_index = 0 \n\t \tfor line in f: \n\t \t \tmatch = re.search('^\\\\d+\\\\.\\\\d+\\\\.\\\\d+', line) \n\t \t \tif match: \n\t \t \t \tif (cur_index == index): \n\t \t \t \t \treturn match.group(0) \n\t \t \t \telse: \n\t \t \t \t \tcur_index += 1\n", 
" \tgitstr = _git_str() \n\tif (gitstr is None): \n\t \tgitstr = '' \n\tpath = os.path.join(BASEDIR, 'qutebrowser', 'git-commit-id') \n\twith _open(path, 'w', encoding='ascii') as f: \n\t \tf.write(gitstr)\n", 
" \tif (len(sys.argv) < 2): \n\t \treturn True \n\tinfo_commands = ['--help-commands', '--name', '--version', '-V', '--fullname', '--author', '--author-email', '--maintainer', '--maintainer-email', '--contact', '--contact-email', '--url', '--license', '--description', '--long-description', '--platforms', '--classifiers', '--keywords', '--provides', '--requires', '--obsoletes'] \n\tinfo_commands.extend(['egg_info', 'install_egg_info', 'rotate']) \n\tfor command in info_commands: \n\t \tif (command in sys.argv[1:]): \n\t \t \treturn False \n\treturn True\n", 
" \trefs = list_refs(refnames=[refname], repo_dir=repo_dir, limit_to_heads=True) \n\tl = tuple(islice(refs, 2)) \n\tif l: \n\t \tassert (len(l) == 1) \n\t \treturn l[0][1] \n\telse: \n\t \treturn None\n", 
" \tif isinstance(fileIshObject, TextIOBase): \n\t \treturn unicode \n\tif isinstance(fileIshObject, IOBase): \n\t \treturn bytes \n\tencoding = getattr(fileIshObject, 'encoding', None) \n\timport codecs \n\tif isinstance(fileIshObject, (codecs.StreamReader, codecs.StreamWriter)): \n\t \tif encoding: \n\t \t \treturn unicode \n\t \telse: \n\t \t \treturn bytes \n\tif (not _PY3): \n\t \tif isinstance(fileIshObject, file): \n\t \t \tif (encoding is not None): \n\t \t \t \treturn basestring \n\t \t \telse: \n\t \t \t \treturn bytes \n\t \tfrom cStringIO import InputType, OutputType \n\t \tfrom StringIO import StringIO \n\t \tif isinstance(fileIshObject, (StringIO, InputType, OutputType)): \n\t \t \treturn bytes \n\treturn default\n", 
" \tcmd = (_pkg(jail, chroot, root) + ['--version']) \n\treturn __salt__['cmd.run'](cmd).strip()\n", 
" \tcmd = (_pkg(jail, chroot, root) + ['--version']) \n\treturn __salt__['cmd.run'](cmd).strip()\n", 
" \thost = node \n\tport = 27017 \n\tidx = node.rfind(':') \n\tif (idx != (-1)): \n\t \t(host, port) = (node[:idx], int(node[(idx + 1):])) \n\tif host.startswith('['): \n\t \thost = host[1:(-1)] \n\treturn (host, port)\n", 
" \tif isinstance(mode, six.string_types): \n\t \tif (mode.lower() == 'enforcing'): \n\t \t \tmode = '1' \n\t \t \tmodestring = 'Enforcing' \n\t \telif (mode.lower() == 'permissive'): \n\t \t \tmode = '0' \n\t \t \tmodestring = 'Permissive' \n\t \telif (mode.lower() == 'disabled'): \n\t \t \tmode = '0' \n\t \t \tmodestring = 'Disabled' \n\t \telse: \n\t \t \treturn 'Invalid \tmode \t{0}'.format(mode) \n\telif isinstance(mode, int): \n\t \tif mode: \n\t \t \tmode = '1' \n\t \telse: \n\t \t \tmode = '0' \n\telse: \n\t \treturn 'Invalid \tmode \t{0}'.format(mode) \n\tif (getenforce() != 'Disabled'): \n\t \tenforce = os.path.join(selinux_fs_path(), 'enforce') \n\t \ttry: \n\t \t \twith salt.utils.fopen(enforce, 'w') as _fp: \n\t \t \t \t_fp.write(mode) \n\t \texcept (IOError, OSError) as exc: \n\t \t \tmsg = 'Could \tnot \twrite \tSELinux \tenforce \tfile: \t{0}' \n\t \t \traise CommandExecutionError(msg.format(str(exc))) \n\tconfig = '/etc/selinux/config' \n\ttry: \n\t \twith salt.utils.fopen(config, 'r') as _cf: \n\t \t \tconf = _cf.read() \n\t \ttry: \n\t \t \twith salt.utils.fopen(config, 'w') as _cf: \n\t \t \t \tconf = re.sub('\\\\nSELINUX=.*\\\\n', (('\\nSELINUX=' + modestring) + '\\n'), conf) \n\t \t \t \t_cf.write(conf) \n\t \texcept (IOError, OSError) as exc: \n\t \t \tmsg = 'Could \tnot \twrite \tSELinux \tconfig \tfile: \t{0}' \n\t \t \traise CommandExecutionError(msg.format(str(exc))) \n\texcept (IOError, OSError) as exc: \n\t \tmsg = 'Could \tnot \tread \tSELinux \tconfig \tfile: \t{0}' \n\t \traise CommandExecutionError(msg.format(str(exc))) \n\treturn getenforce()\n", 
" \t'\\n \t \t \t \tInitialize \tthe \tworkflow\\n \t \t \t \t' \n\tgetmask = pe.Workflow(name=name) \n\t'\\n \t \t \t \tDefine \tthe \tinputs \tto \tthe \tworkflow.\\n \t \t \t \t' \n\tinputnode = pe.Node(niu.IdentityInterface(fields=['source_file', 'subject_id', 'subjects_dir', 'contrast_type']), name='inputspec') \n\t'\\n \t \t \t \tDefine \tall \tthe \tnodes \tof \tthe \tworkflow:\\n\\n \t \t \t \tfssource: \tused \tto \tretrieve \taseg.mgz\\n \t \t \t \tthreshold \t: \tbinarize \taseg\\n \t \t \t \tregister \t: \tcoregister \tsource \tfile \tto \tfreesurfer \tspace\\n \t \t \t \tvoltransform: \tconvert \tbinarized \taseg \tto \tsource \tfile \tspace\\n \t \t \t \t' \n\tfssource = pe.Node(nio.FreeSurferSource(), name='fssource') \n\tthreshold = pe.Node(fs.Binarize(min=0.5, out_type='nii'), name='threshold') \n\tregister = pe.MapNode(fs.BBRegister(init='fsl'), iterfield=['source_file'], name='register') \n\tvoltransform = pe.MapNode(fs.ApplyVolTransform(inverse=True), iterfield=['source_file', 'reg_file'], name='transform') \n\t'\\n \t \t \t \tConnect \tthe \tnodes\\n \t \t \t \t' \n\tgetmask.connect([(inputnode, fssource, [('subject_id', 'subject_id'), ('subjects_dir', 'subjects_dir')]), (inputnode, register, [('source_file', 'source_file'), ('subject_id', 'subject_id'), ('subjects_dir', 'subjects_dir'), ('contrast_type', 'contrast_type')]), (inputnode, voltransform, [('subjects_dir', 'subjects_dir'), ('source_file', 'source_file')]), (fssource, threshold, [(('aparc_aseg', get_aparc_aseg), 'in_file')]), (register, voltransform, [('out_reg_file', 'reg_file')]), (threshold, voltransform, [('binary_file', 'target_file')])]) \n\t'\\n \t \t \t \tAdd \tremaining \tnodes \tand \tconnections\\n\\n \t \t \t \tdilate \t: \tdilate \tthe \ttransformed \tfile \tin \tsource \tspace\\n \t \t \t \tthreshold2 \t: \tbinarize \ttransformed \tfile\\n \t \t \t \t' \n\tthreshold2 = pe.MapNode(fs.Binarize(min=0.5, out_type='nii'), iterfield=['in_file'], name='threshold2') \n\tif dilate_mask: \n\t \tthreshold2.inputs.dilate = 1 \n\tgetmask.connect([(voltransform, threshold2, [('transformed_file', 'in_file')])]) \n\t'\\n \t \t \t \tSetup \tan \toutputnode \tthat \tdefines \trelevant \tinputs \tof \tthe \tworkflow.\\n \t \t \t \t' \n\toutputnode = pe.Node(niu.IdentityInterface(fields=['mask_file', 'reg_file', 'reg_cost']), name='outputspec') \n\tgetmask.connect([(register, outputnode, [('out_reg_file', 'reg_file')]), (register, outputnode, [('min_cost_file', 'reg_cost')]), (threshold2, outputnode, [('binary_file', 'mask_file')])]) \n\treturn getmask\n", 
" \trule_method = ('telemetry:' + policy_name) \n\theaders = request.headers \n\tpolicy_dict = dict() \n\tpolicy_dict['roles'] = headers.get('X-Roles', '').split(',') \n\tpolicy_dict['user_id'] = headers.get('X-User-Id') \n\tpolicy_dict['project_id'] = headers.get('X-Project-Id') \n\tif ((_has_rule('default') or _has_rule(rule_method)) and (not pecan.request.enforcer.enforce(rule_method, {}, policy_dict))): \n\t \tpecan.core.abort(status_code=403, detail='RBAC \tAuthorization \tFailed')\n", 
" \tdef wrapped(f): \n\t \tviews[name] = f \n\t \treturn f \n\treturn wrapped\n", 
" \tif (target is None): \n\t \ttarget = {} \n\tmatch_rule = _build_match_rule(action, target, pluralized) \n\tcredentials = context.to_policy_values() \n\treturn (match_rule, target, credentials)\n", 
" \ttry: \n\t \tlength = len(value) \n\texcept TypeError: \n\t \traise VdtTypeError(value) \n\tif (length < len(args)): \n\t \traise VdtValueTooShortError(value) \n\telif (length > len(args)): \n\t \traise VdtValueTooLongError(value) \n\ttry: \n\t \treturn [fun_dict[arg](val) for (arg, val) in zip(args, value)] \n\texcept KeyError as e: \n\t \traise VdtParamError('mixed_list', e)\n", 
" \treq = get('http://{host}:{port}'.format(host=host, port=port), timeout=SOCKET_TIMEOUT_FOR_POLLING, persistent=False) \n\tdef failed(failure): \n\t \treturn False \n\tdef succeeded(result): \n\t \treturn True \n\treq.addCallbacks(succeeded, failed) \n\treturn req\n", 
" \tfeasible_ind = numpy.array(individual) \n\tfeasible_ind = numpy.maximum(MIN_BOUND, feasible_ind) \n\tfeasible_ind = numpy.minimum(MAX_BOUND, feasible_ind) \n\treturn feasible_ind\n", 
" \tif (value in (u'1', u'0')): \n\t \treturn bool(int(value)) \n\traise ValueError((u'%r \tis \tnot \t0 \tor \t1' % value))\n", 
" \tif isinstance(obj, str): \n\t \tobj = obj.strip().lower() \n\t \tif (obj in ('true', 'yes', 'on', 'y', 't', '1')): \n\t \t \treturn True \n\t \tif (obj in ('false', 'no', 'off', 'n', 'f', '0')): \n\t \t \treturn False \n\t \traise ValueError(('Unable \tto \tinterpret \tvalue \t\"%s\" \tas \tboolean' % obj)) \n\treturn bool(obj)\n", 
" \tglobal _PARSING_CACHE \n\tif (string in _PARSING_CACHE): \n\t \tstack = _PARSING_CACHE[string] \n\telse: \n\t \tif (not _RE_STARTTOKEN.search(string)): \n\t \t \treturn string \n\t \tstack = ParseStack() \n\t \tncallable = 0 \n\t \tfor match in _RE_TOKEN.finditer(string): \n\t \t \tgdict = match.groupdict() \n\t \t \tif gdict['singlequote']: \n\t \t \t \tstack.append(gdict['singlequote']) \n\t \t \telif gdict['doublequote']: \n\t \t \t \tstack.append(gdict['doublequote']) \n\t \t \telif gdict['end']: \n\t \t \t \tif (ncallable <= 0): \n\t \t \t \t \tstack.append(')') \n\t \t \t \t \tcontinue \n\t \t \t \targs = [] \n\t \t \t \twhile stack: \n\t \t \t \t \toperation = stack.pop() \n\t \t \t \t \tif callable(operation): \n\t \t \t \t \t \tif (not strip): \n\t \t \t \t \t \t \tstack.append((operation, [arg for arg in reversed(args)])) \n\t \t \t \t \t \tncallable -= 1 \n\t \t \t \t \t \tbreak \n\t \t \t \t \telse: \n\t \t \t \t \t \targs.append(operation) \n\t \t \telif gdict['start']: \n\t \t \t \tfuncname = _RE_STARTTOKEN.match(gdict['start']).group(1) \n\t \t \t \ttry: \n\t \t \t \t \tstack.append(_INLINE_FUNCS[funcname]) \n\t \t \t \texcept KeyError: \n\t \t \t \t \tstack.append(_INLINE_FUNCS['nomatch']) \n\t \t \t \t \tstack.append(funcname) \n\t \t \t \tncallable += 1 \n\t \t \telif gdict['escaped']: \n\t \t \t \ttoken = gdict['escaped'].lstrip('\\\\') \n\t \t \t \tstack.append(token) \n\t \t \telif gdict['comma']: \n\t \t \t \tif (ncallable > 0): \n\t \t \t \t \tstack.append(None) \n\t \t \t \telse: \n\t \t \t \t \tstack.append(',') \n\t \t \telse: \n\t \t \t \tstack.append(gdict['rest']) \n\t \tif (ncallable > 0): \n\t \t \treturn string \n\t \tif ((_STACK_MAXSIZE > 0) and (_STACK_MAXSIZE < len(stack))): \n\t \t \treturn (string + gdict['stackfull'](*args, **kwargs)) \n\t \telse: \n\t \t \t_PARSING_CACHE[string] = stack \n\tdef _run_stack(item, depth=0): \n\t \tretval = item \n\t \tif isinstance(item, tuple): \n\t \t \tif strip: \n\t \t \t \treturn '' \n\t \t \telse: \n\t \t \t \t(func, arglist) = item \n\t \t \t \targs = [''] \n\t \t \t \tfor arg in arglist: \n\t \t \t \t \tif (arg is None): \n\t \t \t \t \t \targs.append('') \n\t \t \t \t \telse: \n\t \t \t \t \t \targs[(-1)] += _run_stack(arg, depth=(depth + 1)) \n\t \t \t \tkwargs['inlinefunc_stack_depth'] = depth \n\t \t \t \tretval = ('' if strip else func(*args, **kwargs)) \n\t \treturn utils.to_str(retval, force_string=True) \n\treturn ''.join((_run_stack(item) for item in _PARSING_CACHE[string]))\n", 
" \tif (type(s) is str): \n\t \treturn s \n\telse: \n\t \treturn s3_unicode(s).encode('utf-8', 'strict')\n", 
" \tdef wrapper(func): \n\t \t@functools.wraps(func) \n\t \tdef inner(*args, **kwds): \n\t \t \tlock.acquire() \n\t \t \ttry: \n\t \t \t \treturn func(*args, **kwds) \n\t \t \tfinally: \n\t \t \t \tlock.release() \n\t \treturn inner \n\treturn wrapper\n", 
" \treturn auth_is_loggedin_user()\n", 
" \tif ((r.representation == 'html') and (r.name == 'shelter') and r.id and (not r.component)): \n\t \tT = current.T \n\t \tmsg = current.msg \n\t \ts3db = current.s3db \n\t \trecord = r.record \n\t \tctable = s3db.pr_contact \n\t \tstable = s3db.cr_shelter \n\t \tmessage = '' \n\t \ttext = '' \n\t \ts_id = record.id \n\t \ts_name = record.name \n\t \ts_phone = record.phone \n\t \ts_email = record.email \n\t \ts_status = record.status \n\t \tif (s_phone in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_email in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_status in ('', None)): \n\t \t \ts_status = T('Not \tDefined') \n\t \telif (s_status == 1): \n\t \t \ts_status = 'Open' \n\t \telif (s_status == 2): \n\t \t \ts_status = 'Close' \n\t \telse: \n\t \t \ts_status = 'Unassigned \tShelter \tStatus' \n\t \ttext += '************************************************' \n\t \ttext += ('\\n%s \t' % T('Automatic \tMessage')) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Shelter \tID'), s_id)) \n\t \ttext += (' \t%s: \t%s' % (T('Shelter \tname'), s_name)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Email'), s_email)) \n\t \ttext += (' \t%s: \t%s' % (T('Phone'), s_phone)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Working \tStatus'), s_status)) \n\t \ttext += '\\n************************************************\\n' \n\t \turl = URL(c='cr', f='shelter', args=r.id) \n\t \topts = dict(type='SMS', subject=T('Deployment \tRequest'), message=(message + text), url=url) \n\t \toutput = msg.compose(**opts) \n\t \tif attr.get('rheader'): \n\t \t \trheader = attr['rheader'](r) \n\t \t \tif rheader: \n\t \t \t \toutput['rheader'] = rheader \n\t \toutput['title'] = T('Send \tNotification') \n\t \tcurrent.response.view = 'msg/compose.html' \n\t \treturn output \n\telse: \n\t \traise HTTP(501, current.messages.BADMETHOD)\n", 
" \tcan_users_receive_email = email_manager.can_users_receive_thread_email(recipient_list, exploration_id, has_suggestion) \n\tfor (index, recipient_id) in enumerate(recipient_list): \n\t \tif can_users_receive_email[index]: \n\t \t \ttransaction_services.run_in_transaction(_enqueue_feedback_thread_status_change_email_task, recipient_id, feedback_message_reference, old_status, new_status)\n", 
" \tglobal _notifier \n\tif (_notifier is None): \n\t \thost = (CONF.default_publisher_id or socket.gethostname()) \n\t \ttry: \n\t \t \ttransport = oslo_messaging.get_notification_transport(CONF) \n\t \t \t_notifier = oslo_messaging.Notifier(transport, ('identity.%s' % host)) \n\t \texcept Exception: \n\t \t \tLOG.exception(_LE('Failed \tto \tconstruct \tnotifier')) \n\t \t \t_notifier = False \n\treturn _notifier\n", 
" \tcan_users_receive_email = email_manager.can_users_receive_thread_email(recipient_list, exploration_id, has_suggestion) \n\tfor (index, recipient_id) in enumerate(recipient_list): \n\t \tif can_users_receive_email[index]: \n\t \t \ttransaction_services.run_in_transaction(_enqueue_feedback_thread_status_change_email_task, recipient_id, feedback_message_reference, old_status, new_status)\n", 
" \tdef wrapped_func(*args, **kwarg): \n\t \tCALLED_FUNCTION.append(name) \n\t \treturn function(*args, **kwarg) \n\treturn wrapped_func\n", 
" \tlevel = notification.get_default_level() \n\tif (stack.status == stack.IN_PROGRESS): \n\t \tsuffix = 'start' \n\telif (stack.status == stack.COMPLETE): \n\t \tsuffix = 'end' \n\telse: \n\t \tsuffix = 'error' \n\t \tlevel = notification.ERROR \n\tevent_type = ('%s.%s.%s' % ('stack', stack.action.lower(), suffix)) \n\tnotification.notify(stack.context, event_type, level, engine_api.format_notification_body(stack))\n", 
" \ttry: \n\t \tdriver = _DRIVERS[driver_name] \n\texcept KeyError: \n\t \traise DriverNotFoundError(('No \tdriver \tfor \t%s' % driver_name)) \n\treturn driver(*args, **kwargs)\n", 
" \tlevel = notification.get_default_level() \n\tif (stack.status == stack.IN_PROGRESS): \n\t \tsuffix = 'start' \n\telif (stack.status == stack.COMPLETE): \n\t \tsuffix = 'end' \n\telse: \n\t \tsuffix = 'error' \n\t \tlevel = notification.ERROR \n\tevent_type = ('%s.%s.%s' % ('stack', stack.action.lower(), suffix)) \n\tnotification.notify(stack.context, event_type, level, engine_api.format_notification_body(stack))\n", 
" \tseed = (pseed or config.unittests.rseed) \n\tif (seed == 'random'): \n\t \tseed = None \n\ttry: \n\t \tif seed: \n\t \t \tseed = int(seed) \n\t \telse: \n\t \t \tseed = None \n\texcept ValueError: \n\t \tprint('Error: \tconfig.unittests.rseed \tcontains \tinvalid \tseed, \tusing \tNone \tinstead', file=sys.stderr) \n\t \tseed = None \n\treturn seed\n", 
" \tif ((r.representation == 'html') and (r.name == 'shelter') and r.id and (not r.component)): \n\t \tT = current.T \n\t \tmsg = current.msg \n\t \ts3db = current.s3db \n\t \trecord = r.record \n\t \tctable = s3db.pr_contact \n\t \tstable = s3db.cr_shelter \n\t \tmessage = '' \n\t \ttext = '' \n\t \ts_id = record.id \n\t \ts_name = record.name \n\t \ts_phone = record.phone \n\t \ts_email = record.email \n\t \ts_status = record.status \n\t \tif (s_phone in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_email in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_status in ('', None)): \n\t \t \ts_status = T('Not \tDefined') \n\t \telif (s_status == 1): \n\t \t \ts_status = 'Open' \n\t \telif (s_status == 2): \n\t \t \ts_status = 'Close' \n\t \telse: \n\t \t \ts_status = 'Unassigned \tShelter \tStatus' \n\t \ttext += '************************************************' \n\t \ttext += ('\\n%s \t' % T('Automatic \tMessage')) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Shelter \tID'), s_id)) \n\t \ttext += (' \t%s: \t%s' % (T('Shelter \tname'), s_name)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Email'), s_email)) \n\t \ttext += (' \t%s: \t%s' % (T('Phone'), s_phone)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Working \tStatus'), s_status)) \n\t \ttext += '\\n************************************************\\n' \n\t \turl = URL(c='cr', f='shelter', args=r.id) \n\t \topts = dict(type='SMS', subject=T('Deployment \tRequest'), message=(message + text), url=url) \n\t \toutput = msg.compose(**opts) \n\t \tif attr.get('rheader'): \n\t \t \trheader = attr['rheader'](r) \n\t \t \tif rheader: \n\t \t \t \toutput['rheader'] = rheader \n\t \toutput['title'] = T('Send \tNotification') \n\t \tcurrent.response.view = 'msg/compose.html' \n\t \treturn output \n\telse: \n\t \traise HTTP(501, current.messages.BADMETHOD)\n", 
" \ttrunc = 20 \n\targspec = inspect.getargspec(fobj) \n\targ_list = [] \n\tif argspec.args: \n\t \tfor arg in argspec.args: \n\t \t \targ_list.append(str(arg)) \n\targ_list.reverse() \n\tif argspec.defaults: \n\t \tfor i in range(len(argspec.defaults)): \n\t \t \targ_list[i] = ((str(arg_list[i]) + '=') + str(argspec.defaults[(- i)])) \n\targ_list.reverse() \n\tif argspec.varargs: \n\t \targ_list.append(argspec.varargs) \n\tif argspec.keywords: \n\t \targ_list.append(argspec.keywords) \n\targ_list = [x[:trunc] for x in arg_list] \n\tstr_param = ('%s(%s)' % (name, ', \t'.join(arg_list))) \n\treturn str_param\n", 
" \treturn salt.utils.etcd_util.get_conn(profile)\n", 
" \treturn RetryWithBackoff(callable_func, retry_notify_func, delay, 1, delay, max_tries)\n", 
" \tmsg_tested_versions = ['xp', 'vista', '2008', '2003'] \n\tmsg_args = ['/c', '%SystemRoot%\\\\System32\\\\msg.exe', '*', '/TIME:0'] \n\thost_version = platform.platform().lower() \n\tif (not msg): \n\t \treturn ('Command \tnot \tran.', 'Empty \tmessage.', (-1)) \n\telse: \n\t \tmsg_args.extend([msg]) \n\tfor version in msg_tested_versions: \n\t \tif (host_version.find(version) != (-1)): \n\t \t \tres = client_utils_common.Execute('cmd', msg_args, time_limit=(-1), bypass_whitelist=True) \n\t \t \treturn res \n\treturn ('', 'Command \tnot \tavailable \tfor \tthis \tversion.', (-1))\n", 
" \tmsg_tested_versions = ['xp', 'vista', '2008', '2003'] \n\tmsg_args = ['/c', '%SystemRoot%\\\\System32\\\\msg.exe', '*', '/TIME:0'] \n\thost_version = platform.platform().lower() \n\tif (not msg): \n\t \treturn ('Command \tnot \tran.', 'Empty \tmessage.', (-1)) \n\telse: \n\t \tmsg_args.extend([msg]) \n\tfor version in msg_tested_versions: \n\t \tif (host_version.find(version) != (-1)): \n\t \t \tres = client_utils_common.Execute('cmd', msg_args, time_limit=(-1), bypass_whitelist=True) \n\t \t \treturn res \n\treturn ('', 'Command \tnot \tavailable \tfor \tthis \tversion.', (-1))\n", 
" \tex = copy(event_exchange) \n\tif (conn.transport.driver_type == u'redis'): \n\t \tex.type = u'fanout' \n\treturn ex\n", 
" \tmsg_tested_versions = ['xp', 'vista', '2008', '2003'] \n\tmsg_args = ['/c', '%SystemRoot%\\\\System32\\\\msg.exe', '*', '/TIME:0'] \n\thost_version = platform.platform().lower() \n\tif (not msg): \n\t \treturn ('Command \tnot \tran.', 'Empty \tmessage.', (-1)) \n\telse: \n\t \tmsg_args.extend([msg]) \n\tfor version in msg_tested_versions: \n\t \tif (host_version.find(version) != (-1)): \n\t \t \tres = client_utils_common.Execute('cmd', msg_args, time_limit=(-1), bypass_whitelist=True) \n\t \t \treturn res \n\treturn ('', 'Command \tnot \tavailable \tfor \tthis \tversion.', (-1))\n", 
" \tex = copy(event_exchange) \n\tif (conn.transport.driver_type == u'redis'): \n\t \tex.type = u'fanout' \n\treturn ex\n", 
" \tif ((r.representation == 'html') and (r.name == 'shelter') and r.id and (not r.component)): \n\t \tT = current.T \n\t \tmsg = current.msg \n\t \ts3db = current.s3db \n\t \trecord = r.record \n\t \tctable = s3db.pr_contact \n\t \tstable = s3db.cr_shelter \n\t \tmessage = '' \n\t \ttext = '' \n\t \ts_id = record.id \n\t \ts_name = record.name \n\t \ts_phone = record.phone \n\t \ts_email = record.email \n\t \ts_status = record.status \n\t \tif (s_phone in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_email in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_status in ('', None)): \n\t \t \ts_status = T('Not \tDefined') \n\t \telif (s_status == 1): \n\t \t \ts_status = 'Open' \n\t \telif (s_status == 2): \n\t \t \ts_status = 'Close' \n\t \telse: \n\t \t \ts_status = 'Unassigned \tShelter \tStatus' \n\t \ttext += '************************************************' \n\t \ttext += ('\\n%s \t' % T('Automatic \tMessage')) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Shelter \tID'), s_id)) \n\t \ttext += (' \t%s: \t%s' % (T('Shelter \tname'), s_name)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Email'), s_email)) \n\t \ttext += (' \t%s: \t%s' % (T('Phone'), s_phone)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Working \tStatus'), s_status)) \n\t \ttext += '\\n************************************************\\n' \n\t \turl = URL(c='cr', f='shelter', args=r.id) \n\t \topts = dict(type='SMS', subject=T('Deployment \tRequest'), message=(message + text), url=url) \n\t \toutput = msg.compose(**opts) \n\t \tif attr.get('rheader'): \n\t \t \trheader = attr['rheader'](r) \n\t \t \tif rheader: \n\t \t \t \toutput['rheader'] = rheader \n\t \toutput['title'] = T('Send \tNotification') \n\t \tcurrent.response.view = 'msg/compose.html' \n\t \treturn output \n\telse: \n\t \traise HTTP(501, current.messages.BADMETHOD)\n", 
" \treturn salt.utils.etcd_util.get_conn(profile)\n", 
" \t@functools.wraps(f) \n\tdef Wrapper(self, request): \n\t \t'Wrap \tthe \tfunction \tcan \tcatch \texceptions, \tconverting \tthem \tto \tstatus.' \n\t \tfailed = True \n\t \tresponse = rdf_data_store.DataStoreResponse() \n\t \tresponse.status = rdf_data_store.DataStoreResponse.Status.OK \n\t \ttry: \n\t \t \tf(self, request, response) \n\t \t \tfailed = False \n\t \texcept access_control.UnauthorizedAccess as e: \n\t \t \tresponse.Clear() \n\t \t \tresponse.request = request \n\t \t \tresponse.status = rdf_data_store.DataStoreResponse.Status.AUTHORIZATION_DENIED \n\t \t \tif e.subject: \n\t \t \t \tresponse.failed_subject = utils.SmartUnicode(e.subject) \n\t \t \tresponse.status_desc = utils.SmartUnicode(e) \n\t \texcept data_store.Error as e: \n\t \t \tresponse.Clear() \n\t \t \tresponse.request = request \n\t \t \tresponse.status = rdf_data_store.DataStoreResponse.Status.DATA_STORE_ERROR \n\t \t \tresponse.status_desc = utils.SmartUnicode(e) \n\t \texcept access_control.ExpiryError as e: \n\t \t \tresponse.Clear() \n\t \t \tresponse.request = request \n\t \t \tresponse.status = rdf_data_store.DataStoreResponse.Status.TIMEOUT_ERROR \n\t \t \tresponse.status_desc = utils.SmartUnicode(e) \n\t \tif failed: \n\t \t \tlogging.info('Failed: \t%s', utils.SmartStr(response)[:1000]) \n\t \tserialized_response = response.SerializeToString() \n\t \treturn serialized_response \n\treturn Wrapper\n", 
" \treturn RetryWithBackoff(callable_func, retry_notify_func, delay, 1, delay, max_tries)\n", 
" \tmsg_tested_versions = ['xp', 'vista', '2008', '2003'] \n\tmsg_args = ['/c', '%SystemRoot%\\\\System32\\\\msg.exe', '*', '/TIME:0'] \n\thost_version = platform.platform().lower() \n\tif (not msg): \n\t \treturn ('Command \tnot \tran.', 'Empty \tmessage.', (-1)) \n\telse: \n\t \tmsg_args.extend([msg]) \n\tfor version in msg_tested_versions: \n\t \tif (host_version.find(version) != (-1)): \n\t \t \tres = client_utils_common.Execute('cmd', msg_args, time_limit=(-1), bypass_whitelist=True) \n\t \t \treturn res \n\treturn ('', 'Command \tnot \tavailable \tfor \tthis \tversion.', (-1))\n", 
" \tcache_key = _cache_get_key() \n\ttry: \n\t \treturn __context__[cache_key] \n\texcept KeyError: \n\t \tpass \n\tconn = _get_conn(region=region, key=key, keyid=keyid, profile=profile) \n\t__context__[cache_key] = {} \n\ttopics = conn.get_all_topics() \n\tfor t in topics['ListTopicsResponse']['ListTopicsResult']['Topics']: \n\t \tshort_name = t['TopicArn'].split(':')[(-1)] \n\t \t__context__[cache_key][short_name] = t['TopicArn'] \n\treturn __context__[cache_key]\n", 
" \treturn salt.utils.etcd_util.get_conn(profile)\n", 
" \treturn RetryWithBackoff(callable_func, retry_notify_func, delay, 1, delay, max_tries)\n", 
" \tmsg_tested_versions = ['xp', 'vista', '2008', '2003'] \n\tmsg_args = ['/c', '%SystemRoot%\\\\System32\\\\msg.exe', '*', '/TIME:0'] \n\thost_version = platform.platform().lower() \n\tif (not msg): \n\t \treturn ('Command \tnot \tran.', 'Empty \tmessage.', (-1)) \n\telse: \n\t \tmsg_args.extend([msg]) \n\tfor version in msg_tested_versions: \n\t \tif (host_version.find(version) != (-1)): \n\t \t \tres = client_utils_common.Execute('cmd', msg_args, time_limit=(-1), bypass_whitelist=True) \n\t \t \treturn res \n\treturn ('', 'Command \tnot \tavailable \tfor \tthis \tversion.', (-1))\n", 
" \tmsg_tested_versions = ['xp', 'vista', '2008', '2003'] \n\tmsg_args = ['/c', '%SystemRoot%\\\\System32\\\\msg.exe', '*', '/TIME:0'] \n\thost_version = platform.platform().lower() \n\tif (not msg): \n\t \treturn ('Command \tnot \tran.', 'Empty \tmessage.', (-1)) \n\telse: \n\t \tmsg_args.extend([msg]) \n\tfor version in msg_tested_versions: \n\t \tif (host_version.find(version) != (-1)): \n\t \t \tres = client_utils_common.Execute('cmd', msg_args, time_limit=(-1), bypass_whitelist=True) \n\t \t \treturn res \n\treturn ('', 'Command \tnot \tavailable \tfor \tthis \tversion.', (-1))\n", 
" \tex = copy(event_exchange) \n\tif (conn.transport.driver_type == u'redis'): \n\t \tex.type = u'fanout' \n\treturn ex\n", 
" \tmsg_tested_versions = ['xp', 'vista', '2008', '2003'] \n\tmsg_args = ['/c', '%SystemRoot%\\\\System32\\\\msg.exe', '*', '/TIME:0'] \n\thost_version = platform.platform().lower() \n\tif (not msg): \n\t \treturn ('Command \tnot \tran.', 'Empty \tmessage.', (-1)) \n\telse: \n\t \tmsg_args.extend([msg]) \n\tfor version in msg_tested_versions: \n\t \tif (host_version.find(version) != (-1)): \n\t \t \tres = client_utils_common.Execute('cmd', msg_args, time_limit=(-1), bypass_whitelist=True) \n\t \t \treturn res \n\treturn ('', 'Command \tnot \tavailable \tfor \tthis \tversion.', (-1))\n", 
" \tex = copy(event_exchange) \n\tif (conn.transport.driver_type == u'redis'): \n\t \tex.type = u'fanout' \n\treturn ex\n", 
" \tif ((r.representation == 'html') and (r.name == 'shelter') and r.id and (not r.component)): \n\t \tT = current.T \n\t \tmsg = current.msg \n\t \ts3db = current.s3db \n\t \trecord = r.record \n\t \tctable = s3db.pr_contact \n\t \tstable = s3db.cr_shelter \n\t \tmessage = '' \n\t \ttext = '' \n\t \ts_id = record.id \n\t \ts_name = record.name \n\t \ts_phone = record.phone \n\t \ts_email = record.email \n\t \ts_status = record.status \n\t \tif (s_phone in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_email in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_status in ('', None)): \n\t \t \ts_status = T('Not \tDefined') \n\t \telif (s_status == 1): \n\t \t \ts_status = 'Open' \n\t \telif (s_status == 2): \n\t \t \ts_status = 'Close' \n\t \telse: \n\t \t \ts_status = 'Unassigned \tShelter \tStatus' \n\t \ttext += '************************************************' \n\t \ttext += ('\\n%s \t' % T('Automatic \tMessage')) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Shelter \tID'), s_id)) \n\t \ttext += (' \t%s: \t%s' % (T('Shelter \tname'), s_name)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Email'), s_email)) \n\t \ttext += (' \t%s: \t%s' % (T('Phone'), s_phone)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Working \tStatus'), s_status)) \n\t \ttext += '\\n************************************************\\n' \n\t \turl = URL(c='cr', f='shelter', args=r.id) \n\t \topts = dict(type='SMS', subject=T('Deployment \tRequest'), message=(message + text), url=url) \n\t \toutput = msg.compose(**opts) \n\t \tif attr.get('rheader'): \n\t \t \trheader = attr['rheader'](r) \n\t \t \tif rheader: \n\t \t \t \toutput['rheader'] = rheader \n\t \toutput['title'] = T('Send \tNotification') \n\t \tcurrent.response.view = 'msg/compose.html' \n\t \treturn output \n\telse: \n\t \traise HTTP(501, current.messages.BADMETHOD)\n", 
" \tif (not settings.FEATURES.get('ENABLE_FEEDBACK_SUBMISSION', False)): \n\t \traise Http404() \n\tif (request.method != 'POST'): \n\t \treturn HttpResponseNotAllowed(['POST']) \n\tdef build_error_response(status_code, field, err_msg): \n\t \treturn HttpResponse(json.dumps({'field': field, 'error': err_msg}), status=status_code) \n\trequired_fields = ['subject', 'details'] \n\tif (not request.user.is_authenticated()): \n\t \trequired_fields += ['name', 'email'] \n\trequired_field_errs = {'subject': 'Please \tprovide \ta \tsubject.', 'details': 'Please \tprovide \tdetails.', 'name': 'Please \tprovide \tyour \tname.', 'email': 'Please \tprovide \ta \tvalid \te-mail.'} \n\tfor field in required_fields: \n\t \tif ((field not in request.POST) or (not request.POST[field])): \n\t \t \treturn build_error_response(400, field, required_field_errs[field]) \n\tif (not request.user.is_authenticated()): \n\t \ttry: \n\t \t \tvalidate_email(request.POST['email']) \n\t \texcept ValidationError: \n\t \t \treturn build_error_response(400, 'email', required_field_errs['email']) \n\tsuccess = False \n\tcontext = get_feedback_form_context(request) \n\tsupport_backend = configuration_helpers.get_value('CONTACT_FORM_SUBMISSION_BACKEND', SUPPORT_BACKEND_ZENDESK) \n\tif (support_backend == SUPPORT_BACKEND_EMAIL): \n\t \ttry: \n\t \t \tsend_mail(subject=render_to_string('emails/contact_us_feedback_email_subject.txt', context), message=render_to_string('emails/contact_us_feedback_email_body.txt', context), from_email=context['support_email'], recipient_list=[context['support_email']], fail_silently=False) \n\t \t \tsuccess = True \n\t \texcept SMTPException: \n\t \t \tlog.exception('Error \tsending \tfeedback \tto \tcontact_us \temail \taddress.') \n\t \t \tsuccess = False \n\telse: \n\t \tif ((not settings.ZENDESK_URL) or (not settings.ZENDESK_USER) or (not settings.ZENDESK_API_KEY)): \n\t \t \traise Exception('Zendesk \tenabled \tbut \tnot \tconfigured') \n\t \tcustom_fields = None \n\t \tif settings.ZENDESK_CUSTOM_FIELDS: \n\t \t \tcustom_field_context = _get_zendesk_custom_field_context(request) \n\t \t \tcustom_fields = _format_zendesk_custom_fields(custom_field_context) \n\t \tsuccess = _record_feedback_in_zendesk(context['realname'], context['email'], context['subject'], context['details'], context['tags'], context['additional_info'], support_email=context['support_email'], custom_fields=custom_fields) \n\t_record_feedback_in_datadog(context['tags']) \n\treturn HttpResponse(status=(200 if success else 500))\n", 
" \tfor v in args: \n\t \tsys.stderr.write(str(v)) \n\tsys.stderr.write('\\n')\n", 
" \tif isinstance(arg, (list, tuple)): \n\t \treturn list(arg) \n\telse: \n\t \treturn [arg]\n", 
" \treturn survey_link.format(UNIQUE_ID=unique_id_for_user(user))\n", 
" \treturn salt.utils.etcd_util.get_conn(profile)\n", 
" \treturn RetryWithBackoff(callable_func, retry_notify_func, delay, 1, delay, max_tries)\n", 
" \tmsg_tested_versions = ['xp', 'vista', '2008', '2003'] \n\tmsg_args = ['/c', '%SystemRoot%\\\\System32\\\\msg.exe', '*', '/TIME:0'] \n\thost_version = platform.platform().lower() \n\tif (not msg): \n\t \treturn ('Command \tnot \tran.', 'Empty \tmessage.', (-1)) \n\telse: \n\t \tmsg_args.extend([msg]) \n\tfor version in msg_tested_versions: \n\t \tif (host_version.find(version) != (-1)): \n\t \t \tres = client_utils_common.Execute('cmd', msg_args, time_limit=(-1), bypass_whitelist=True) \n\t \t \treturn res \n\treturn ('', 'Command \tnot \tavailable \tfor \tthis \tversion.', (-1))\n", 
" \tmsg_tested_versions = ['xp', 'vista', '2008', '2003'] \n\tmsg_args = ['/c', '%SystemRoot%\\\\System32\\\\msg.exe', '*', '/TIME:0'] \n\thost_version = platform.platform().lower() \n\tif (not msg): \n\t \treturn ('Command \tnot \tran.', 'Empty \tmessage.', (-1)) \n\telse: \n\t \tmsg_args.extend([msg]) \n\tfor version in msg_tested_versions: \n\t \tif (host_version.find(version) != (-1)): \n\t \t \tres = client_utils_common.Execute('cmd', msg_args, time_limit=(-1), bypass_whitelist=True) \n\t \t \treturn res \n\treturn ('', 'Command \tnot \tavailable \tfor \tthis \tversion.', (-1))\n", 
" \tex = copy(event_exchange) \n\tif (conn.transport.driver_type == u'redis'): \n\t \tex.type = u'fanout' \n\treturn ex\n", 
" \tmsg_tested_versions = ['xp', 'vista', '2008', '2003'] \n\tmsg_args = ['/c', '%SystemRoot%\\\\System32\\\\msg.exe', '*', '/TIME:0'] \n\thost_version = platform.platform().lower() \n\tif (not msg): \n\t \treturn ('Command \tnot \tran.', 'Empty \tmessage.', (-1)) \n\telse: \n\t \tmsg_args.extend([msg]) \n\tfor version in msg_tested_versions: \n\t \tif (host_version.find(version) != (-1)): \n\t \t \tres = client_utils_common.Execute('cmd', msg_args, time_limit=(-1), bypass_whitelist=True) \n\t \t \treturn res \n\treturn ('', 'Command \tnot \tavailable \tfor \tthis \tversion.', (-1))\n", 
" \tex = copy(event_exchange) \n\tif (conn.transport.driver_type == u'redis'): \n\t \tex.type = u'fanout' \n\treturn ex\n", 
" \tif ((r.representation == 'html') and (r.name == 'shelter') and r.id and (not r.component)): \n\t \tT = current.T \n\t \tmsg = current.msg \n\t \ts3db = current.s3db \n\t \trecord = r.record \n\t \tctable = s3db.pr_contact \n\t \tstable = s3db.cr_shelter \n\t \tmessage = '' \n\t \ttext = '' \n\t \ts_id = record.id \n\t \ts_name = record.name \n\t \ts_phone = record.phone \n\t \ts_email = record.email \n\t \ts_status = record.status \n\t \tif (s_phone in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_email in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_status in ('', None)): \n\t \t \ts_status = T('Not \tDefined') \n\t \telif (s_status == 1): \n\t \t \ts_status = 'Open' \n\t \telif (s_status == 2): \n\t \t \ts_status = 'Close' \n\t \telse: \n\t \t \ts_status = 'Unassigned \tShelter \tStatus' \n\t \ttext += '************************************************' \n\t \ttext += ('\\n%s \t' % T('Automatic \tMessage')) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Shelter \tID'), s_id)) \n\t \ttext += (' \t%s: \t%s' % (T('Shelter \tname'), s_name)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Email'), s_email)) \n\t \ttext += (' \t%s: \t%s' % (T('Phone'), s_phone)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Working \tStatus'), s_status)) \n\t \ttext += '\\n************************************************\\n' \n\t \turl = URL(c='cr', f='shelter', args=r.id) \n\t \topts = dict(type='SMS', subject=T('Deployment \tRequest'), message=(message + text), url=url) \n\t \toutput = msg.compose(**opts) \n\t \tif attr.get('rheader'): \n\t \t \trheader = attr['rheader'](r) \n\t \t \tif rheader: \n\t \t \t \toutput['rheader'] = rheader \n\t \toutput['title'] = T('Send \tNotification') \n\t \tcurrent.response.view = 'msg/compose.html' \n\t \treturn output \n\telse: \n\t \traise HTTP(501, current.messages.BADMETHOD)\n", 
" \treturn apiproxy_stub_map.UserRPC('images', deadline, callback)\n", 
" \tprint('got \tperspective1 \tref:', perspective) \n\tprint('asking \tit \tto \tfoo(13)') \n\treturn perspective.callRemote('foo', 13)\n", 
" \tif (name in _events): \n\t \tfor event in get_events(name): \n\t \t \tresult = event(*args, **kwargs) \n\t \t \tif (result is not None): \n\t \t \t \targs = ((result,) + args[1:]) \n\treturn (args and args[0])\n", 
" \targs = (['--version'] + _base_args(request.config)) \n\tproc = quteprocess.QuteProc(request) \n\tproc.proc.setProcessChannelMode(QProcess.SeparateChannels) \n\ttry: \n\t \tproc.start(args) \n\t \tproc.wait_for_quit() \n\texcept testprocess.ProcessExited: \n\t \tassert (proc.proc.exitStatus() == QProcess.NormalExit) \n\telse: \n\t \tpytest.fail('Process \tdid \tnot \texit!') \n\toutput = bytes(proc.proc.readAllStandardOutput()).decode('utf-8') \n\tassert (re.search('^qutebrowser\\\\s+v\\\\d+(\\\\.\\\\d+)', output) is not None)\n", 
" \tif (not callable(method)): \n\t \treturn None \n\ttry: \n\t \tmethod_info = method.remote \n\texcept AttributeError: \n\t \treturn None \n\tif (not isinstance(method_info, _RemoteMethodInfo)): \n\t \treturn None \n\treturn method_info\n", 
" \tif ((r.representation == 'html') and (r.name == 'shelter') and r.id and (not r.component)): \n\t \tT = current.T \n\t \tmsg = current.msg \n\t \ts3db = current.s3db \n\t \trecord = r.record \n\t \tctable = s3db.pr_contact \n\t \tstable = s3db.cr_shelter \n\t \tmessage = '' \n\t \ttext = '' \n\t \ts_id = record.id \n\t \ts_name = record.name \n\t \ts_phone = record.phone \n\t \ts_email = record.email \n\t \ts_status = record.status \n\t \tif (s_phone in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_email in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_status in ('', None)): \n\t \t \ts_status = T('Not \tDefined') \n\t \telif (s_status == 1): \n\t \t \ts_status = 'Open' \n\t \telif (s_status == 2): \n\t \t \ts_status = 'Close' \n\t \telse: \n\t \t \ts_status = 'Unassigned \tShelter \tStatus' \n\t \ttext += '************************************************' \n\t \ttext += ('\\n%s \t' % T('Automatic \tMessage')) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Shelter \tID'), s_id)) \n\t \ttext += (' \t%s: \t%s' % (T('Shelter \tname'), s_name)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Email'), s_email)) \n\t \ttext += (' \t%s: \t%s' % (T('Phone'), s_phone)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Working \tStatus'), s_status)) \n\t \ttext += '\\n************************************************\\n' \n\t \turl = URL(c='cr', f='shelter', args=r.id) \n\t \topts = dict(type='SMS', subject=T('Deployment \tRequest'), message=(message + text), url=url) \n\t \toutput = msg.compose(**opts) \n\t \tif attr.get('rheader'): \n\t \t \trheader = attr['rheader'](r) \n\t \t \tif rheader: \n\t \t \t \toutput['rheader'] = rheader \n\t \toutput['title'] = T('Send \tNotification') \n\t \tcurrent.response.view = 'msg/compose.html' \n\t \treturn output \n\telse: \n\t \traise HTTP(501, current.messages.BADMETHOD)\n", 
" \tif strip_tags: \n\t \ttags_start = name.find('[') \n\t \ttags_end = name.find(']') \n\t \tif ((tags_start > 0) and (tags_end > tags_start)): \n\t \t \tnewname = name[:tags_start] \n\t \t \tnewname += name[(tags_end + 1):] \n\t \t \tname = newname \n\tif strip_scenarios: \n\t \ttags_start = name.find('(') \n\t \ttags_end = name.find(')') \n\t \tif ((tags_start > 0) and (tags_end > tags_start)): \n\t \t \tnewname = name[:tags_start] \n\t \t \tnewname += name[(tags_end + 1):] \n\t \t \tname = newname \n\treturn name\n", 
" \tif (name in _events): \n\t \tfor event in get_events(name): \n\t \t \tresult = event(*args, **kwargs) \n\t \t \tif (result is not None): \n\t \t \t \targs = ((result,) + args[1:]) \n\treturn (args and args[0])\n", 
" \targs = (['--version'] + _base_args(request.config)) \n\tproc = quteprocess.QuteProc(request) \n\tproc.proc.setProcessChannelMode(QProcess.SeparateChannels) \n\ttry: \n\t \tproc.start(args) \n\t \tproc.wait_for_quit() \n\texcept testprocess.ProcessExited: \n\t \tassert (proc.proc.exitStatus() == QProcess.NormalExit) \n\telse: \n\t \tpytest.fail('Process \tdid \tnot \texit!') \n\toutput = bytes(proc.proc.readAllStandardOutput()).decode('utf-8') \n\tassert (re.search('^qutebrowser\\\\s+v\\\\d+(\\\\.\\\\d+)', output) is not None)\n", 
" \treturn IMPL.service_get_all_by_topic(context, topic)\n", 
" \tif (not os.path.isfile(filename)): \n\t \treturn {} \n\ttry: \n\t \twith open(filename, 'r') as fdesc: \n\t \t \tinp = fdesc.read() \n\t \tif (not inp): \n\t \t \treturn {} \n\t \treturn json.loads(inp) \n\texcept (IOError, ValueError) as error: \n\t \t_LOGGER.error('Reading \tconfig \tfile \t%s \tfailed: \t%s', filename, error) \n\t \treturn None\n", 
" \tif (output is None): \n\t \treturn '' \n\telse: \n\t \treturn output.rstrip('\\r\\n')\n", 
" \ttb = traceback.format_exception(*failure_info) \n\tfailure = failure_info[1] \n\tif log_failure: \n\t \tLOG.error(_LE('Returning \texception \t%s \tto \tcaller'), six.text_type(failure)) \n\t \tLOG.error(tb) \n\tkwargs = {} \n\tif hasattr(failure, 'kwargs'): \n\t \tkwargs = failure.kwargs \n\tcls_name = str(failure.__class__.__name__) \n\tmod_name = str(failure.__class__.__module__) \n\tif (cls_name.endswith(_REMOTE_POSTFIX) and mod_name.endswith(_REMOTE_POSTFIX)): \n\t \tcls_name = cls_name[:(- len(_REMOTE_POSTFIX))] \n\t \tmod_name = mod_name[:(- len(_REMOTE_POSTFIX))] \n\tdata = {'class': cls_name, 'module': mod_name, 'message': six.text_type(failure), 'tb': tb, 'args': failure.args, 'kwargs': kwargs} \n\tjson_data = jsonutils.dumps(data) \n\treturn json_data\n", 
" \twith warnings.catch_warnings(record=True) as w: \n\t \tif (clear is not None): \n\t \t \tif (not _is_list_like(clear)): \n\t \t \t \tclear = [clear] \n\t \t \tfor m in clear: \n\t \t \t \tgetattr(m, u'__warningregistry__', {}).clear() \n\t \tsaw_warning = False \n\t \twarnings.simplefilter(filter_level) \n\t \t(yield w) \n\t \textra_warnings = [] \n\t \tfor actual_warning in w: \n\t \t \tif (expected_warning and issubclass(actual_warning.category, expected_warning)): \n\t \t \t \tsaw_warning = True \n\t \t \telse: \n\t \t \t \textra_warnings.append(actual_warning.category.__name__) \n\t \tif expected_warning: \n\t \t \tassert saw_warning, (u'Did \tnot \tsee \texpected \twarning \tof \tclass \t%r.' % expected_warning.__name__) \n\t \tassert (not extra_warnings), (u'Caused \tunexpected \twarning(s): \t%r.' % extra_warnings)\n", 
" \ttype1 = type(var1) \n\ttype2 = type(var2) \n\tif (type1 is type2): \n\t \treturn True \n\tif ((type1 is np.ndarray) and (var1.shape == ())): \n\t \treturn (type(var1.item()) is type2) \n\tif ((type2 is np.ndarray) and (var2.shape == ())): \n\t \treturn (type(var2.item()) is type1) \n\treturn False\n", 
" \thostname = urlparse(url).hostname \n\tif (not (('fc2.com' in hostname) or ('xiaojiadianvideo.asia' in hostname))): \n\t \treturn False \n\tupid = match1(url, '.+/content/(\\\\w+)') \n\tfc2video_download_by_upid(upid, output_dir, merge, info_only)\n", 
" \treturn json.loads(data)\n", 
" \tentity_moref = kwargs.get('entity_moref') \n\tentity_type = kwargs.get('entity_type') \n\talarm_moref = kwargs.get('alarm_moref') \n\tif ((not entity_moref) or (not entity_type) or (not alarm_moref)): \n\t \traise ValueError('entity_moref, \tentity_type, \tand \talarm_moref \tmust \tbe \tset') \n\tattribs = {'xmlns:xsd': 'http://www.w3.org/2001/XMLSchema', 'xmlns:xsi': 'http://www.w3.org/2001/XMLSchema-instance', 'xmlns:soap': 'http://schemas.xmlsoap.org/soap/envelope/'} \n\troot = Element('soap:Envelope', attribs) \n\tbody = SubElement(root, 'soap:Body') \n\talarm_status = SubElement(body, 'SetAlarmStatus', {'xmlns': 'urn:vim25'}) \n\tthis = SubElement(alarm_status, '_this', {'xsi:type': 'ManagedObjectReference', 'type': 'AlarmManager'}) \n\tthis.text = 'AlarmManager' \n\talarm = SubElement(alarm_status, 'alarm', {'type': 'Alarm'}) \n\talarm.text = alarm_moref \n\tentity = SubElement(alarm_status, 'entity', {'xsi:type': 'ManagedObjectReference', 'type': entity_type}) \n\tentity.text = entity_moref \n\tstatus = SubElement(alarm_status, 'status') \n\tstatus.text = 'green' \n\treturn '<?xml \tversion=\"1.0\" \tencoding=\"UTF-8\"?>{0}'.format(tostring(root))\n", 
" \tmsg_tested_versions = ['xp', 'vista', '2008', '2003'] \n\tmsg_args = ['/c', '%SystemRoot%\\\\System32\\\\msg.exe', '*', '/TIME:0'] \n\thost_version = platform.platform().lower() \n\tif (not msg): \n\t \treturn ('Command \tnot \tran.', 'Empty \tmessage.', (-1)) \n\telse: \n\t \tmsg_args.extend([msg]) \n\tfor version in msg_tested_versions: \n\t \tif (host_version.find(version) != (-1)): \n\t \t \tres = client_utils_common.Execute('cmd', msg_args, time_limit=(-1), bypass_whitelist=True) \n\t \t \treturn res \n\treturn ('', 'Command \tnot \tavailable \tfor \tthis \tversion.', (-1))\n", 
" \treturn (a * b)\n", 
" \trespbody = response.body \n\tcustom_properties = {} \n\tbroker_properties = None \n\tmessage_type = None \n\tmessage_location = None \n\tfor (name, value) in response.headers: \n\t \tif (name.lower() == 'brokerproperties'): \n\t \t \tbroker_properties = json.loads(value) \n\t \telif (name.lower() == 'content-type'): \n\t \t \tmessage_type = value \n\t \telif (name.lower() == 'location'): \n\t \t \tmessage_location = value \n\t \telif (name.lower() not in ['transfer-encoding', 'server', 'date', 'strict-transport-security']): \n\t \t \tif ('\"' in value): \n\t \t \t \tvalue = value[1:(-1)].replace('\\\\\"', '\"') \n\t \t \t \ttry: \n\t \t \t \t \tcustom_properties[name] = datetime.strptime(value, '%a, \t%d \t%b \t%Y \t%H:%M:%S \tGMT') \n\t \t \t \texcept ValueError: \n\t \t \t \t \tcustom_properties[name] = value \n\t \t \telif (value.lower() == 'true'): \n\t \t \t \tcustom_properties[name] = True \n\t \t \telif (value.lower() == 'false'): \n\t \t \t \tcustom_properties[name] = False \n\t \t \telse: \n\t \t \t \ttry: \n\t \t \t \t \tfloat_value = float(value) \n\t \t \t \t \tif (str(int(float_value)) == value): \n\t \t \t \t \t \tcustom_properties[name] = int(value) \n\t \t \t \t \telse: \n\t \t \t \t \t \tcustom_properties[name] = float_value \n\t \t \t \texcept ValueError: \n\t \t \t \t \tpass \n\tif (message_type is None): \n\t \tmessage = Message(respbody, service_instance, message_location, custom_properties, 'application/atom+xml;type=entry;charset=utf-8', broker_properties) \n\telse: \n\t \tmessage = Message(respbody, service_instance, message_location, custom_properties, message_type, broker_properties) \n\treturn message\n", 
" \tproducer.publish(msg, exchange=exchange, retry=retry, retry_policy=retry_policy, **dict({'routing_key': req.properties['reply_to'], 'correlation_id': req.properties.get('correlation_id'), 'serializer': serializers.type_to_name[req.content_type], 'content_encoding': req.content_encoding}, **props))\n", 
" \tproducer.publish(msg, exchange=exchange, retry=retry, retry_policy=retry_policy, **dict({'routing_key': req.properties['reply_to'], 'correlation_id': req.properties.get('correlation_id'), 'serializer': serializers.type_to_name[req.content_type], 'content_encoding': req.content_encoding}, **props))\n", 
" \tif ((r.representation == 'html') and (r.name == 'shelter') and r.id and (not r.component)): \n\t \tT = current.T \n\t \tmsg = current.msg \n\t \ts3db = current.s3db \n\t \trecord = r.record \n\t \tctable = s3db.pr_contact \n\t \tstable = s3db.cr_shelter \n\t \tmessage = '' \n\t \ttext = '' \n\t \ts_id = record.id \n\t \ts_name = record.name \n\t \ts_phone = record.phone \n\t \ts_email = record.email \n\t \ts_status = record.status \n\t \tif (s_phone in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_email in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_status in ('', None)): \n\t \t \ts_status = T('Not \tDefined') \n\t \telif (s_status == 1): \n\t \t \ts_status = 'Open' \n\t \telif (s_status == 2): \n\t \t \ts_status = 'Close' \n\t \telse: \n\t \t \ts_status = 'Unassigned \tShelter \tStatus' \n\t \ttext += '************************************************' \n\t \ttext += ('\\n%s \t' % T('Automatic \tMessage')) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Shelter \tID'), s_id)) \n\t \ttext += (' \t%s: \t%s' % (T('Shelter \tname'), s_name)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Email'), s_email)) \n\t \ttext += (' \t%s: \t%s' % (T('Phone'), s_phone)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Working \tStatus'), s_status)) \n\t \ttext += '\\n************************************************\\n' \n\t \turl = URL(c='cr', f='shelter', args=r.id) \n\t \topts = dict(type='SMS', subject=T('Deployment \tRequest'), message=(message + text), url=url) \n\t \toutput = msg.compose(**opts) \n\t \tif attr.get('rheader'): \n\t \t \trheader = attr['rheader'](r) \n\t \t \tif rheader: \n\t \t \t \toutput['rheader'] = rheader \n\t \toutput['title'] = T('Send \tNotification') \n\t \tcurrent.response.view = 'msg/compose.html' \n\t \treturn output \n\telse: \n\t \traise HTTP(501, current.messages.BADMETHOD)\n", 
" \tenter_return = None \n\ttry: \n\t \tif isinstance(enter_func, functools.partial): \n\t \t \tenter_func_name = enter_func.func.__name__ \n\t \telse: \n\t \t \tenter_func_name = enter_func.__name__ \n\t \tLOG.debug('Entering \tcontext. \tFunction: \t%(func_name)s, \tuse_enter_return: \t%(use)s.', {'func_name': enter_func_name, 'use': use_enter_return}) \n\t \tenter_return = enter_func() \n\t \t(yield enter_return) \n\tfinally: \n\t \tif isinstance(exit_func, functools.partial): \n\t \t \texit_func_name = exit_func.func.__name__ \n\t \telse: \n\t \t \texit_func_name = exit_func.__name__ \n\t \tLOG.debug('Exiting \tcontext. \tFunction: \t%(func_name)s, \tuse_enter_return: \t%(use)s.', {'func_name': exit_func_name, 'use': use_enter_return}) \n\t \tif (enter_return is not None): \n\t \t \tif use_enter_return: \n\t \t \t \tignore_exception(exit_func, enter_return) \n\t \t \telse: \n\t \t \t \tignore_exception(exit_func)\n", 
" \tglobal _path_created \n\tif (not isinstance(name, StringTypes)): \n\t \traise DistutilsInternalError, (\"mkpath: \t'name' \tmust \tbe \ta \tstring \t(got \t%r)\" % (name,)) \n\tname = os.path.normpath(name) \n\tcreated_dirs = [] \n\tif (os.path.isdir(name) or (name == '')): \n\t \treturn created_dirs \n\tif _path_created.get(os.path.abspath(name)): \n\t \treturn created_dirs \n\t(head, tail) = os.path.split(name) \n\ttails = [tail] \n\twhile (head and tail and (not os.path.isdir(head))): \n\t \t(head, tail) = os.path.split(head) \n\t \ttails.insert(0, tail) \n\tfor d in tails: \n\t \thead = os.path.join(head, d) \n\t \tabs_head = os.path.abspath(head) \n\t \tif _path_created.get(abs_head): \n\t \t \tcontinue \n\t \tif (verbose >= 1): \n\t \t \tlog.info('creating \t%s', head) \n\t \tif (not dry_run): \n\t \t \ttry: \n\t \t \t \tos.mkdir(head) \n\t \t \t \tcreated_dirs.append(head) \n\t \t \texcept OSError as exc: \n\t \t \t \traise DistutilsFileError, (\"could \tnot \tcreate \t'%s': \t%s\" % (head, exc[(-1)])) \n\t \t_path_created[abs_head] = 1 \n\treturn created_dirs\n", 
" \ttry: \n\t \t(module, attr) = name.rsplit('.', 1) \n\texcept ValueError as error: \n\t \traise ImproperlyConfigured(('Error \timporting \tclass \t%s \tin \t%s: \t\"%s\"' % (name, setting, error))) \n\ttry: \n\t \tmod = import_module(module) \n\texcept ImportError as error: \n\t \traise ImproperlyConfigured(('Error \timporting \tmodule \t%s \tin \t%s: \t\"%s\"' % (module, setting, error))) \n\ttry: \n\t \tcls = getattr(mod, attr) \n\texcept AttributeError: \n\t \traise ImproperlyConfigured(('Module \t\"%s\" \tdoes \tnot \tdefine \ta \t\"%s\" \tclass \tin \t%s' % (module, attr, setting))) \n\treturn cls\n", 
" \tos.environ['LANG'] = 'C' \n\tcli = _DRIVERS.get('HORCM') \n\treturn importutils.import_object('.'.join([_DRIVER_DIR, cli[driver_info['proto']]]), conf, driver_info, db)\n", 
" \tos.environ['LANG'] = 'C' \n\tcli = _DRIVERS.get('HORCM') \n\treturn importutils.import_object('.'.join([_DRIVER_DIR, cli[driver_info['proto']]]), conf, driver_info, db)\n", 
" \tif name.startswith('.'): \n\t \tif (not package): \n\t \t \traise TypeError(\"relative \timports \trequire \tthe \t'package' \targument\") \n\t \tlevel = 0 \n\t \tfor character in name: \n\t \t \tif (character != '.'): \n\t \t \t \tbreak \n\t \t \tlevel += 1 \n\t \tname = _resolve_name(name[level:], package, level) \n\t__import__(name) \n\treturn sys.modules[name]\n", 
" \ttry: \n\t \treturn namedModule(name) \n\texcept ImportError: \n\t \treturn default\n", 
" \tif (not at): \n\t \tat = utcnow() \n\tst = at.strftime((_ISO8601_TIME_FORMAT if (not subsecond) else _ISO8601_TIME_FORMAT_SUBSECOND)) \n\ttz = (at.tzinfo.tzname(None) if at.tzinfo else 'UTC') \n\tst += ('Z' if (tz == 'UTC') else tz) \n\treturn st\n", 
" \ttry: \n\t \treturn iso8601.parse_date(timestr) \n\texcept iso8601.ParseError as e: \n\t \traise ValueError(encodeutils.exception_to_unicode(e)) \n\texcept TypeError as e: \n\t \traise ValueError(encodeutils.exception_to_unicode(e))\n", 
" \tnow = datetime.utcnow().replace(tzinfo=iso8601.iso8601.UTC) \n\tif iso: \n\t \treturn datetime_tuple_to_iso(now) \n\telse: \n\t \treturn now\n", 
" \treturn formatdate(timegm(dt.utctimetuple()))\n", 
" \toffset = timestamp.utcoffset() \n\tif (offset is None): \n\t \treturn timestamp \n\treturn (timestamp.replace(tzinfo=None) - offset)\n", 
" \tif os.path.isfile(db_file_name): \n\t \tfrom datetime import timedelta \n\t \tfile_datetime = datetime.fromtimestamp(os.stat(db_file_name).st_ctime) \n\t \tif ((datetime.today() - file_datetime) >= timedelta(hours=older_than)): \n\t \t \tif verbose: \n\t \t \t \tprint u'File \tis \told' \n\t \t \treturn True \n\t \telse: \n\t \t \tif verbose: \n\t \t \t \tprint u'File \tis \trecent' \n\t \t \treturn False \n\telse: \n\t \tif verbose: \n\t \t \tprint u'File \tdoes \tnot \texist' \n\t \treturn True\n", 
" \tif (not os.path.exists(source)): \n\t \traise DistutilsFileError((\"file \t'%s' \tdoes \tnot \texist\" % os.path.abspath(source))) \n\tif (not os.path.exists(target)): \n\t \treturn True \n\treturn (os.stat(source)[ST_MTIME] > os.stat(target)[ST_MTIME])\n", 
" \tif utcnow.override_time: \n\t \ttry: \n\t \t \treturn utcnow.override_time.pop(0) \n\t \texcept AttributeError: \n\t \t \treturn utcnow.override_time \n\tif with_timezone: \n\t \treturn datetime.datetime.now(tz=iso8601.iso8601.UTC) \n\treturn datetime.datetime.utcnow()\n", 
" \tif utcnow.override_time: \n\t \ttry: \n\t \t \treturn utcnow.override_time.pop(0) \n\t \texcept AttributeError: \n\t \t \treturn utcnow.override_time \n\tif with_timezone: \n\t \treturn datetime.datetime.now(tz=iso8601.iso8601.UTC) \n\treturn datetime.datetime.utcnow()\n", 
" \treturn isotime(datetime.datetime.utcfromtimestamp(timestamp), microsecond)\n", 
" \tif utcnow.override_time: \n\t \ttry: \n\t \t \treturn utcnow.override_time.pop(0) \n\t \texcept AttributeError: \n\t \t \treturn utcnow.override_time \n\tif with_timezone: \n\t \treturn datetime.datetime.now(tz=iso8601.iso8601.UTC) \n\treturn datetime.datetime.utcnow()\n", 
" \tmatch = standard_duration_re.match(value) \n\tif (not match): \n\t \tmatch = iso8601_duration_re.match(value) \n\tif match: \n\t \tkw = match.groupdict() \n\t \tsign = ((-1) if (kw.pop('sign', '+') == '-') else 1) \n\t \tif kw.get('microseconds'): \n\t \t \tkw['microseconds'] = kw['microseconds'].ljust(6, '0') \n\t \tif (kw.get('seconds') and kw.get('microseconds') and kw['seconds'].startswith('-')): \n\t \t \tkw['microseconds'] = ('-' + kw['microseconds']) \n\t \tkw = {k: float(v) for (k, v) in kw.items() if (v is not None)} \n\t \treturn (sign * datetime.timedelta(**kw))\n", 
" \treturn call_talib_with_ohlc(barDs, count, talib.CDLADVANCEBLOCK)\n", 
" \ttry: \n\t \tparent = next(klass.local_attr_ancestors(name)) \n\texcept (StopIteration, KeyError): \n\t \treturn None \n\ttry: \n\t \tmeth_node = parent[name] \n\texcept KeyError: \n\t \treturn None \n\tif isinstance(meth_node, astroid.Function): \n\t \treturn meth_node \n\treturn None\n", 
" \tif value.tzinfo: \n\t \tvalue = value.astimezone(UTC) \n\treturn (long((calendar.timegm(value.timetuple()) * 1000000L)) + value.microsecond)\n", 
" \t(p, u) = getparser(use_datetime=use_datetime, use_builtin_types=use_builtin_types) \n\tp.feed(data) \n\tp.close() \n\treturn (u.close(), u.getmethodname())\n", 
" \tlater = (mktime(date1.timetuple()) + (date1.microsecond / 1000000.0)) \n\tearlier = (mktime(date2.timetuple()) + (date2.microsecond / 1000000.0)) \n\treturn (later - earlier)\n", 
" \tgenerate_completions(event)\n", 
" \tif (not name): \n\t \traise SaltInvocationError('Required \tparameter \t`name` \tis \tmissing.') \n\tif job_exists(name): \n\t \traise SaltInvocationError('Job \t`{0}` \talready \texists.'.format(name)) \n\tif (not config_xml): \n\t \tconfig_xml = jenkins.EMPTY_CONFIG_XML \n\telse: \n\t \tconfig_xml_file = __salt__['cp.cache_file'](config_xml, saltenv) \n\t \twith salt.utils.fopen(config_xml_file) as _fp: \n\t \t \tconfig_xml = _fp.read() \n\tserver = _connect() \n\ttry: \n\t \tserver.create_job(name, config_xml) \n\texcept jenkins.JenkinsException as err: \n\t \traise SaltInvocationError('Something \twent \twrong \t{0}.'.format(err)) \n\treturn config_xml\n", 
" \tif at_time: \n\t \tcmd = \"echo \t'{0}' \t| \tat \t{1}\".format(cmd, _cmd_quote(at_time)) \n\treturn (not bool(__salt__['cmd.retcode'](cmd, python_shell=True)))\n", 
" \terrback = (errback or _ensure_errback) \n\twith pool.acquire(block=True) as conn: \n\t \tconn.ensure_connection(errback=errback) \n\t \tchannel = conn.default_channel \n\t \trevive = partial(revive_connection, conn, on_revive=on_revive) \n\t \tinsured = conn.autoretry(fun, channel, errback=errback, on_revive=revive, **opts) \n\t \t(retval, _) = insured(*args, **dict(kwargs, connection=conn)) \n\t \treturn retval\n", 
" \tif (not unit): \n\t \tunit = CONF.instance_usage_audit_period \n\toffset = 0 \n\tif ('@' in unit): \n\t \t(unit, offset) = unit.split('@', 1) \n\t \toffset = int(offset) \n\tif (before is not None): \n\t \trightnow = before \n\telse: \n\t \trightnow = timeutils.utcnow() \n\tif (unit not in ('month', 'day', 'year', 'hour')): \n\t \traise ValueError('Time \tperiod \tmust \tbe \thour, \tday, \tmonth \tor \tyear') \n\tif (unit == 'month'): \n\t \tif (offset == 0): \n\t \t \toffset = 1 \n\t \tend = datetime.datetime(day=offset, month=rightnow.month, year=rightnow.year) \n\t \tif (end >= rightnow): \n\t \t \tyear = rightnow.year \n\t \t \tif (1 >= rightnow.month): \n\t \t \t \tyear -= 1 \n\t \t \t \tmonth = (12 + (rightnow.month - 1)) \n\t \t \telse: \n\t \t \t \tmonth = (rightnow.month - 1) \n\t \t \tend = datetime.datetime(day=offset, month=month, year=year) \n\t \tyear = end.year \n\t \tif (1 >= end.month): \n\t \t \tyear -= 1 \n\t \t \tmonth = (12 + (end.month - 1)) \n\t \telse: \n\t \t \tmonth = (end.month - 1) \n\t \tbegin = datetime.datetime(day=offset, month=month, year=year) \n\telif (unit == 'year'): \n\t \tif (offset == 0): \n\t \t \toffset = 1 \n\t \tend = datetime.datetime(day=1, month=offset, year=rightnow.year) \n\t \tif (end >= rightnow): \n\t \t \tend = datetime.datetime(day=1, month=offset, year=(rightnow.year - 1)) \n\t \t \tbegin = datetime.datetime(day=1, month=offset, year=(rightnow.year - 2)) \n\t \telse: \n\t \t \tbegin = datetime.datetime(day=1, month=offset, year=(rightnow.year - 1)) \n\telif (unit == 'day'): \n\t \tend = datetime.datetime(hour=offset, day=rightnow.day, month=rightnow.month, year=rightnow.year) \n\t \tif (end >= rightnow): \n\t \t \tend = (end - datetime.timedelta(days=1)) \n\t \tbegin = (end - datetime.timedelta(days=1)) \n\telif (unit == 'hour'): \n\t \tend = rightnow.replace(minute=offset, second=0, microsecond=0) \n\t \tif (end >= rightnow): \n\t \t \tend = (end - datetime.timedelta(hours=1)) \n\t \tbegin = (end - datetime.timedelta(hours=1)) \n\treturn (begin, end)\n", 
" \tif (length is None): \n\t \tlength = CONF.password_length \n\tr = random.SystemRandom() \n\tpassword = [r.choice(s) for s in symbolgroups] \n\tr.shuffle(password) \n\tpassword = password[:length] \n\tlength -= len(password) \n\tsymbols = ''.join(symbolgroups) \n\tpassword.extend([r.choice(symbols) for _i in range(length)]) \n\tr.shuffle(password) \n\treturn ''.join(password)\n", 
" \tglobal CSSAttrCache \n\tCSSAttrCache = {} \n\tif xhtml: \n\t \tparser = html5lib.XHTMLParser(tree=treebuilders.getTreeBuilder(u'dom')) \n\telse: \n\t \tparser = html5lib.HTMLParser(tree=treebuilders.getTreeBuilder(u'dom')) \n\tif isinstance(src, six.text_type): \n\t \tif (not encoding): \n\t \t \tencoding = u'utf-8' \n\t \tsrc = src.encode(encoding) \n\t \tsrc = pisaTempFile(src, capacity=context.capacity) \n\tdocument = parser.parse(src) \n\tif xml_output: \n\t \tif encoding: \n\t \t \txml_output.write(document.toprettyxml(encoding=encoding)) \n\t \telse: \n\t \t \txml_output.write(document.toprettyxml(encoding=u'utf8')) \n\tif default_css: \n\t \tcontext.addDefaultCSS(default_css) \n\tpisaPreLoop(document, context) \n\tcontext.parseCSS() \n\tpisaLoop(document, context) \n\treturn context\n", 
" \treturn _XHTML_ESCAPE_RE.sub((lambda match: _XHTML_ESCAPE_DICT[match.group(0)]), to_basestring(value))\n", 
" \tif ((value is None) or isinstance(value, six.binary_type)): \n\t \treturn value \n\tif (not isinstance(value, six.text_type)): \n\t \tvalue = six.text_type(value) \n\treturn value.encode('utf-8')\n", 
" \ttry: \n\t \tos.remove(fname) \n\texcept OSError: \n\t \tpass\n", 
" \treturn [api for api in apis if (api['name'] == name)]\n", 
" \titems = [] \n\tfor (k, v) in d.items(): \n\t \tnew_key = (((parent_key + '.') + k) if parent_key else k) \n\t \tif isinstance(v, collections.MutableMapping): \n\t \t \titems.extend(list(flatten_dict(v, new_key).items())) \n\t \telse: \n\t \t \titems.append((new_key, v)) \n\treturn dict(items)\n", 
" \tdata_copy = deepcopy(data) \n\tfield_names = {} \n\tfor (key, value) in data.iteritems(): \n\t \tif (key in DEFAULT_FIELD_NAMES): \n\t \t \tfield_names[key] = data_copy.pop(key) \n\treturn (field_names, data_copy)\n", 
" \treturn dict(((k.upper(), v) for (k, v) in dictionary.items()))\n", 
" \treturn dict([(k, v) for (k, v) in six.iteritems(master_dict) if (k in keys)])\n", 
" \tif isinstance(obj, cls): \n\t \treturn obj \n\traise Exception((_('Expected \tobject \tof \ttype: \t%s') % str(cls)))\n", 
" \treturn STRING_BOOLS[string.strip().lower()]\n", 
" \ttry: \n\t \tparts = urlparse.urlparse(url) \n\t \tscheme = parts[0] \n\t \tnetloc = parts[1] \n\t \tif (scheme and netloc): \n\t \t \treturn True \n\t \telse: \n\t \t \treturn False \n\texcept: \n\t \treturn False\n", 
" \tbits = (4294967295 ^ ((1 << (32 - mask)) - 1)) \n\treturn socket.inet_ntoa(struct.pack('>I', bits))\n", 
" \tif (not CONF.monkey_patch): \n\t \treturn \n\tif six.PY2: \n\t \tis_method = inspect.ismethod \n\telse: \n\t \tdef is_method(obj): \n\t \t \treturn (inspect.ismethod(obj) or inspect.isfunction(obj)) \n\tfor module_and_decorator in CONF.monkey_patch_modules: \n\t \t(module, decorator_name) = module_and_decorator.split(':') \n\t \tdecorator = importutils.import_class(decorator_name) \n\t \t__import__(module) \n\t \tmodule_data = pyclbr.readmodule_ex(module) \n\t \tfor (key, value) in module_data.items(): \n\t \t \tif isinstance(value, pyclbr.Class): \n\t \t \t \tclz = importutils.import_class(('%s.%s' % (module, key))) \n\t \t \t \tfor (method, func) in inspect.getmembers(clz, is_method): \n\t \t \t \t \tsetattr(clz, method, decorator(('%s.%s.%s' % (module, key, method)), func)) \n\t \t \tif isinstance(value, pyclbr.Function): \n\t \t \t \tfunc = importutils.import_class(('%s.%s' % (module, key))) \n\t \t \t \tsetattr(sys.modules[module], key, decorator(('%s.%s' % (module, key)), func))\n", 
" \tfor item in list_: \n\t \tif (item.get(search_field) == value): \n\t \t \treturn item.get(output_field, value) \n\treturn value\n", 
" \tend = clock() \n\ttotal = (end - START) \n\tprint('Completion \ttime: \t{0} \tseconds.'.format(total))\n", 
" \tkwargs = {} \n\tv_list = ['public', 'private'] \n\tcf_list = ['ami', 'ari', 'aki', 'bare', 'ovf'] \n\tdf_list = ['ami', 'ari', 'aki', 'vhd', 'vmdk', 'raw', 'qcow2', 'vdi', 'iso'] \n\tkwargs['copy_from'] = location \n\tif (visibility is not None): \n\t \tif (visibility not in v_list): \n\t \t \traise SaltInvocationError(('\"visibility\" \tneeds \tto \tbe \tone \t' + 'of \tthe \tfollowing: \t{0}'.format(', \t'.join(v_list)))) \n\t \telif (visibility == 'public'): \n\t \t \tkwargs['is_public'] = True \n\t \telse: \n\t \t \tkwargs['is_public'] = False \n\telse: \n\t \tkwargs['is_public'] = True \n\tif (container_format not in cf_list): \n\t \traise SaltInvocationError(('\"container_format\" \tneeds \tto \tbe \t' + 'one \tof \tthe \tfollowing: \t{0}'.format(', \t'.join(cf_list)))) \n\telse: \n\t \tkwargs['container_format'] = container_format \n\tif (disk_format not in df_list): \n\t \traise SaltInvocationError(('\"disk_format\" \tneeds \tto \tbe \tone \t' + 'of \tthe \tfollowing: \t{0}'.format(', \t'.join(df_list)))) \n\telse: \n\t \tkwargs['disk_format'] = disk_format \n\tif (protected is not None): \n\t \tkwargs['protected'] = protected \n\tg_client = _auth(profile, api_version=1) \n\timage = g_client.images.create(name=name, **kwargs) \n\treturn image_show(image.id, profile=profile)\n", 
" \ttry: \n\t \tif lib_cls: \n\t \t \treturn lib_cls(lib) \n\t \telse: \n\t \t \treturn ctypes.CDLL(lib) \n\texcept Exception: \n\t \tif name: \n\t \t \tlib_msg = ('%s \t(%s)' % (name, lib)) \n\t \telse: \n\t \t \tlib_msg = lib \n\t \tlib_msg += ' \tcould \tnot \tbe \tloaded' \n\t \tif (sys.platform == 'cygwin'): \n\t \t \tlib_msg += ' \tin \tcygwin' \n\t \t_LOGGER.error(lib_msg, exc_info=True) \n\t \treturn None\n", 
" \treturn (_request_ctx_stack.top is not None)\n", 
" \tpath = os.path.join(base, dev) \n\tif partition: \n\t \tpath += str(partition) \n\treturn path\n", 
" \tif hasattr(td, 'total_seconds'): \n\t \treturn td.total_seconds() \n\tms = td.microseconds \n\tsecs = (td.seconds + ((td.days * 24) * 3600)) \n\treturn ((ms + (secs * (10 ** 6))) / (10 ** 6))\n", 
" \tif six.PY3: \n\t \thostname = hostname.encode('latin-1', 'ignore') \n\t \thostname = hostname.decode('latin-1') \n\telif isinstance(hostname, six.text_type): \n\t \thostname = hostname.encode('latin-1', 'ignore') \n\thostname = re.sub('[ \t_]', '-', hostname) \n\thostname = re.sub('[^\\\\w.-]+', '', hostname) \n\thostname = hostname.lower() \n\thostname = hostname.strip('.-') \n\treturn hostname\n", 
" \tglobal _FILE_CACHE \n\tif force_reload: \n\t \tdelete_cached_file(filename) \n\treloaded = False \n\tmtime = os.path.getmtime(filename) \n\tcache_info = _FILE_CACHE.setdefault(filename, {}) \n\tif ((not cache_info) or (mtime > cache_info.get('mtime', 0))): \n\t \tLOG.debug('Reloading \tcached \tfile \t%s', filename) \n\t \twith open(filename) as fap: \n\t \t \tcache_info['data'] = fap.read() \n\t \tcache_info['mtime'] = mtime \n\t \treloaded = True \n\treturn (reloaded, cache_info['data'])\n", 
" \treturn open(*args, **kwargs)\n", 
" \treturn open(fn, 'r').read()\n", 
" \tdef is_dict_like(thing): \n\t \treturn (hasattr(thing, 'has_key') or isinstance(thing, dict)) \n\tdef get(thing, attr, default): \n\t \tif is_dict_like(thing): \n\t \t \treturn thing.get(attr, default) \n\t \telse: \n\t \t \treturn getattr(thing, attr, default) \n\tdef set_value(thing, attr, val): \n\t \tif is_dict_like(thing): \n\t \t \tthing[attr] = val \n\t \telse: \n\t \t \tsetattr(thing, attr, val) \n\tdef delete(thing, attr): \n\t \tif is_dict_like(thing): \n\t \t \tdel thing[attr] \n\t \telse: \n\t \t \tdelattr(thing, attr) \n\tNOT_PRESENT = object() \n\told_values = {} \n\tfor (attr, new_value) in kwargs.items(): \n\t \told_values[attr] = get(obj, attr, NOT_PRESENT) \n\t \tset_value(obj, attr, new_value) \n\ttry: \n\t \t(yield) \n\tfinally: \n\t \tfor (attr, old_value) in old_values.items(): \n\t \t \tif (old_value is NOT_PRESENT): \n\t \t \t \tdelete(obj, attr) \n\t \t \telse: \n\t \t \t \tset_value(obj, attr, old_value)\n", 
" \tservice = _service_get(s_name, **connection_args) \n\treturn ((service is not None) and (service.get_svrstate() == 'UP'))\n", 
" \tmac = [250, 22, 62, random.randint(0, 255), random.randint(0, 255), random.randint(0, 255)] \n\treturn ':'.join(map((lambda x: ('%02x' % x)), mac))\n", 
" \ttry: \n\t \t(out, _err) = execute('cat', file_path, run_as_root=True) \n\t \treturn out \n\texcept processutils.ProcessExecutionError: \n\t \traise exception.FileNotFound(file_path=file_path)\n", 
" \tif (owner_uid is None): \n\t \towner_uid = os.getuid() \n\torig_uid = os.stat(path).st_uid \n\tif (orig_uid != owner_uid): \n\t \texecute('chown', owner_uid, path, run_as_root=True) \n\ttry: \n\t \t(yield) \n\tfinally: \n\t \tif (orig_uid != owner_uid): \n\t \t \texecute('chown', orig_uid, path, run_as_root=True)\n", 
" \tif (len(s1) != len(s2)): \n\t \treturn False \n\tresult = 0 \n\tfor (a, b) in zip(s1, s2): \n\t \tresult |= (ord(a) ^ ord(b)) \n\treturn (result == 0)\n", 
" \tif (not encountered): \n\t \tencountered = [] \n\tfor subclass in clazz.__subclasses__(): \n\t \tif (subclass not in encountered): \n\t \t \tencountered.append(subclass) \n\t \t \tfor subsubclass in walk_class_hierarchy(subclass, encountered): \n\t \t \t \t(yield subsubclass) \n\t \t \t(yield subclass)\n", 
" \tglobal _path_created \n\tif (not isinstance(name, StringTypes)): \n\t \traise DistutilsInternalError, (\"mkpath: \t'name' \tmust \tbe \ta \tstring \t(got \t%r)\" % (name,)) \n\tname = os.path.normpath(name) \n\tcreated_dirs = [] \n\tif (os.path.isdir(name) or (name == '')): \n\t \treturn created_dirs \n\tif _path_created.get(os.path.abspath(name)): \n\t \treturn created_dirs \n\t(head, tail) = os.path.split(name) \n\ttails = [tail] \n\twhile (head and tail and (not os.path.isdir(head))): \n\t \t(head, tail) = os.path.split(head) \n\t \ttails.insert(0, tail) \n\tfor d in tails: \n\t \thead = os.path.join(head, d) \n\t \tabs_head = os.path.abspath(head) \n\t \tif _path_created.get(abs_head): \n\t \t \tcontinue \n\t \tif (verbose >= 1): \n\t \t \tlog.info('creating \t%s', head) \n\t \tif (not dry_run): \n\t \t \ttry: \n\t \t \t \tos.mkdir(head) \n\t \t \t \tcreated_dirs.append(head) \n\t \t \texcept OSError as exc: \n\t \t \t \traise DistutilsFileError, (\"could \tnot \tcreate \t'%s': \t%s\" % (head, exc[(-1)])) \n\t \t_path_created[abs_head] = 1 \n\treturn created_dirs\n", 
" \tif (not ((type(number) is types.LongType) or (type(number) is types.IntType))): \n\t \traise TypeError('You \tmust \tpass \ta \tlong \tor \tan \tint') \n\tstring = '' \n\twhile (number > 0): \n\t \tstring = ('%s%s' % (byte((number & 255)), string)) \n\t \tnumber /= 256 \n\treturn string\n", 
" \tglobal _FILE_CACHE \n\tif force_reload: \n\t \tdelete_cached_file(filename) \n\treloaded = False \n\tmtime = os.path.getmtime(filename) \n\tcache_info = _FILE_CACHE.setdefault(filename, {}) \n\tif ((not cache_info) or (mtime > cache_info.get('mtime', 0))): \n\t \tLOG.debug('Reloading \tcached \tfile \t%s', filename) \n\t \twith open(filename) as fap: \n\t \t \tcache_info['data'] = fap.read() \n\t \tcache_info['mtime'] = mtime \n\t \treloaded = True \n\treturn (reloaded, cache_info['data'])\n", 
" \tif context.is_admin: \n\t \treturn True \n\t(rule, target, credentials) = _prepare_check(context, action, target, pluralized) \n\ttry: \n\t \tresult = _ENFORCER.enforce(rule, target, credentials, action=action, do_raise=True) \n\texcept policy.PolicyNotAuthorized: \n\t \twith excutils.save_and_reraise_exception(): \n\t \t \tlog_rule_list(rule) \n\t \t \tLOG.debug(\"Failed \tpolicy \tcheck \tfor \t'%s'\", action) \n\treturn result\n", 
" \tinit() \n\tcredentials = context.to_policy_values() \n\ttarget = credentials \n\treturn _ENFORCER.authorize('context_is_admin', target, credentials)\n", 
" \tdef decorator(fx): \n\t \tfx._event_id = id \n\t \tfx._event_name = event \n\t \treturn fx \n\treturn decorator\n", 
" \tdef decorator(fx): \n\t \tfx._event_id = id \n\t \tfx._event_name = event \n\t \treturn fx \n\treturn decorator\n", 
" \tif ('id' not in sort_keys): \n\t \tLOG.warning(_LW('Id \tnot \tin \tsort_keys; \tis \tsort_keys \tunique?')) \n\tassert (not (sort_dir and sort_dirs)) \n\tif ((sort_dirs is None) and (sort_dir is None)): \n\t \tsort_dir = 'asc' \n\tif (sort_dirs is None): \n\t \tsort_dirs = [sort_dir for _sort_key in sort_keys] \n\tassert (len(sort_dirs) == len(sort_keys)) \n\tfor (current_sort_key, current_sort_dir) in zip(sort_keys, sort_dirs): \n\t \tsort_dir_func = {'asc': sqlalchemy.asc, 'desc': sqlalchemy.desc}[current_sort_dir] \n\t \ttry: \n\t \t \tsort_key_attr = getattr(model, current_sort_key) \n\t \texcept AttributeError: \n\t \t \traise exception.InvalidInput(reason='Invalid \tsort \tkey') \n\t \tif (not api.is_orm_value(sort_key_attr)): \n\t \t \traise exception.InvalidInput(reason='Invalid \tsort \tkey') \n\t \tquery = query.order_by(sort_dir_func(sort_key_attr)) \n\tif (marker is not None): \n\t \tmarker_values = [] \n\t \tfor sort_key in sort_keys: \n\t \t \tv = getattr(marker, sort_key) \n\t \t \tmarker_values.append(v) \n\t \tcriteria_list = [] \n\t \tfor i in range(0, len(sort_keys)): \n\t \t \tcrit_attrs = [] \n\t \t \tfor j in range(0, i): \n\t \t \t \tmodel_attr = getattr(model, sort_keys[j]) \n\t \t \t \tcrit_attrs.append((model_attr == marker_values[j])) \n\t \t \tmodel_attr = getattr(model, sort_keys[i]) \n\t \t \tif (sort_dirs[i] == 'desc'): \n\t \t \t \tcrit_attrs.append((model_attr < marker_values[i])) \n\t \t \telif (sort_dirs[i] == 'asc'): \n\t \t \t \tcrit_attrs.append((model_attr > marker_values[i])) \n\t \t \telse: \n\t \t \t \traise ValueError(_(\"Unknown \tsort \tdirection, \tmust \tbe \t'desc' \tor \t'asc'\")) \n\t \t \tcriteria = sqlalchemy.sql.and_(*crit_attrs) \n\t \t \tcriteria_list.append(criteria) \n\t \tf = sqlalchemy.sql.or_(*criteria_list) \n\t \tquery = query.filter(f) \n\tif (limit is not None): \n\t \tquery = query.limit(limit) \n\tif offset: \n\t \tquery = query.offset(offset) \n\treturn query\n", 
" \taddresses = [] \n\tfor interface in netifaces.interfaces(): \n\t \ttry: \n\t \t \tiface_data = netifaces.ifaddresses(interface) \n\t \t \tfor family in iface_data: \n\t \t \t \tif (family not in (netifaces.AF_INET, netifaces.AF_INET6)): \n\t \t \t \t \tcontinue \n\t \t \t \tfor address in iface_data[family]: \n\t \t \t \t \taddr = address['addr'] \n\t \t \t \t \tif (family == netifaces.AF_INET6): \n\t \t \t \t \t \taddr = addr.split('%')[0] \n\t \t \t \t \taddresses.append(addr) \n\t \texcept ValueError: \n\t \t \tpass \n\treturn addresses\n", 
" \tdef decorated(func): \n\t \t'\\n \t \t \t \t \t \t \t \tDecorator \tfor \tthe \tcreation \tfunction.\\n \t \t \t \t \t \t \t \t' \n\t \t_WRITE_MODEL[model] = func \n\t \treturn func \n\treturn decorated\n", 
" \tsession = Session.object_session(series) \n\treleases = session.query(Episode).join(Episode.releases, Episode.series).filter((Series.id == series.id)) \n\tif downloaded: \n\t \treleases = releases.filter((Release.downloaded == True)) \n\tif (season is not None): \n\t \treleases = releases.filter((Episode.season == season)) \n\tif (series.identified_by and (series.identified_by != u'auto')): \n\t \treleases = releases.filter((Episode.identified_by == series.identified_by)) \n\tif (series.identified_by in [u'ep', u'sequence']): \n\t \tlatest_release = releases.order_by(desc(Episode.season), desc(Episode.number)).first() \n\telif (series.identified_by == u'date'): \n\t \tlatest_release = releases.order_by(desc(Episode.identifier)).first() \n\telse: \n\t \tlatest_release = releases.order_by(desc(Episode.first_seen.label(u'ep_first_seen'))).first() \n\tif (not latest_release): \n\t \tlog.debug(u'get_latest_release \treturning \tNone, \tno \tdownloaded \tepisodes \tfound \tfor: \t%s', series.name) \n\t \treturn \n\treturn latest_release\n", 
" \tqueue_dir = __opts__['sqlite_queue_dir'] \n\tdb = os.path.join(queue_dir, '{0}.db'.format(queue)) \n\tlog.debug('Connecting \tto: \t \t{0}'.format(db)) \n\tcon = lite.connect(db) \n\ttables = _list_tables(con) \n\tif (queue not in tables): \n\t \t_create_table(con, queue) \n\treturn con\n", 
" \tdefaults = {'host': 'salt', 'user': 'salt', 'pass': 'salt', 'db': 'salt', 'port': 3306, 'ssl_ca': None, 'ssl_cert': None, 'ssl_key': None} \n\tattrs = {'host': 'host', 'user': 'user', 'pass': 'pass', 'db': 'db', 'port': 'port', 'ssl_ca': 'ssl_ca', 'ssl_cert': 'ssl_cert', 'ssl_key': 'ssl_key'} \n\t_options = salt.returners.get_returner_options(__virtualname__, ret, attrs, __salt__=__salt__, __opts__=__opts__, defaults=defaults) \n\tfor (k, v) in _options.iteritems(): \n\t \tif (isinstance(v, string_types) and (v.lower() == 'none')): \n\t \t \t_options[k] = None \n\t \tif (k == 'port'): \n\t \t \t_options[k] = int(v) \n\treturn _options\n", 
" \treturn (file_path in _db_content.get('files'))\n", 
" \tif hasattr(obj, 'bind'): \n\t \tconn = obj.bind \n\telse: \n\t \ttry: \n\t \t \tconn = object_session(obj).bind \n\t \texcept UnmappedInstanceError: \n\t \t \tconn = obj \n\tif (not hasattr(conn, 'execute')): \n\t \traise TypeError('This \tmethod \taccepts \tonly \tSession, \tEngine, \tConnection \tand \tdeclarative \tmodel \tobjects.') \n\treturn conn\n", 
" \tif hasattr(obj, 'bind'): \n\t \tconn = obj.bind \n\telse: \n\t \ttry: \n\t \t \tconn = object_session(obj).bind \n\t \texcept UnmappedInstanceError: \n\t \t \tconn = obj \n\tif (not hasattr(conn, 'execute')): \n\t \traise TypeError('This \tmethod \taccepts \tonly \tSession, \tEngine, \tConnection \tand \tdeclarative \tmodel \tobjects.') \n\treturn conn\n", 
" \tglobal _REPOSITORY \n\trel_path = 'migrate_repo' \n\tif (database == 'api'): \n\t \trel_path = os.path.join('api_migrations', 'migrate_repo') \n\tpath = os.path.join(os.path.abspath(os.path.dirname(__file__)), rel_path) \n\tassert os.path.exists(path) \n\tif (_REPOSITORY.get(database) is None): \n\t \t_REPOSITORY[database] = Repository(path) \n\treturn _REPOSITORY[database]\n", 
" \tif (not context): \n\t \tLOG.warning(_LW('Use \tof \tempty \trequest \tcontext \tis \tdeprecated'), DeprecationWarning) \n\t \traise Exception('die') \n\treturn context.is_admin\n", 
" \tif (not context): \n\t \treturn False \n\tif context.is_admin: \n\t \treturn False \n\tif ((not context.user_id) or (not context.project_id)): \n\t \treturn False \n\treturn True\n", 
" \tif is_user_context(context): \n\t \tif (not context.project_id): \n\t \t \traise exception.Forbidden() \n\t \telif (context.project_id != project_id): \n\t \t \traise exception.Forbidden()\n", 
" \tif is_user_context(context): \n\t \tif (not context.user_id): \n\t \t \traise exception.Forbidden() \n\t \telif (context.user_id != user_id): \n\t \t \traise exception.Forbidden()\n", 
" \tif is_user_context(context): \n\t \tif (not context.quota_class): \n\t \t \traise exception.Forbidden() \n\t \telif (context.quota_class != class_name): \n\t \t \traise exception.Forbidden()\n", 
" \tdef wrapper(*args, **kwargs): \n\t \tif (not is_admin_context(args[0])): \n\t \t \traise exception.AdminRequired() \n\t \treturn f(*args, **kwargs) \n\treturn wrapper\n", 
" \t@functools.wraps(f) \n\tdef wrapper(*args, **kwargs): \n\t \tnova.context.require_context(args[0]) \n\t \treturn f(*args, **kwargs) \n\treturn wrapper\n", 
" \tif (read_deleted is None): \n\t \tread_deleted = context.read_deleted \n\tquery_kwargs = {} \n\tif ('no' == read_deleted): \n\t \tquery_kwargs['deleted'] = False \n\telif ('only' == read_deleted): \n\t \tquery_kwargs['deleted'] = True \n\telif ('yes' == read_deleted): \n\t \tpass \n\telse: \n\t \traise ValueError((_(\"Unrecognized \tread_deleted \tvalue \t'%s'\") % read_deleted)) \n\tquery = sqlalchemyutils.model_query(model, context.session, args, **query_kwargs) \n\tif (nova.context.is_user_context(context) and project_only): \n\t \tif (project_only == 'allow_none'): \n\t \t \tquery = query.filter(or_((model.project_id == context.project_id), (model.project_id == null()))) \n\t \telse: \n\t \t \tquery = query.filter_by(project_id=context.project_id) \n\treturn query\n", 
" \tfilter_dict = {} \n\tif (filters is None): \n\t \tfilters = {} \n\tfor (key, value) in six.iteritems(filters): \n\t \tif isinstance(value, (list, tuple, set, frozenset)): \n\t \t \tcolumn_attr = getattr(model, key) \n\t \t \tquery = query.filter(column_attr.in_(value)) \n\t \telse: \n\t \t \tfilter_dict[key] = value \n\tif filter_dict: \n\t \tquery = query.filter_by(**filter_dict) \n\treturn query\n", 
" \treturn IMPL.service_get_by_compute_host(context, host)\n", 
" \tconvert_objects_related_datetimes(values) \n\tcompute_node_ref = models.ComputeNode() \n\tcompute_node_ref.update(values) \n\tcompute_node_ref.save(context.session) \n\treturn compute_node_ref\n", 
" \tconvert_objects_related_datetimes(values) \n\tcompute_node_ref = models.ComputeNode() \n\tcompute_node_ref.update(values) \n\tcompute_node_ref.save(context.session) \n\treturn compute_node_ref\n", 
" \tcell = cs.cells.capacities(args.cell) \n\tprint((_('Ram \tAvailable: \t%s \tMB') % cell.capacities['ram_free']['total_mb'])) \n\tutils.print_dict(cell.capacities['ram_free']['units_by_mb'], dict_property='Ram(MB)', dict_value='Units') \n\tprint((_('\\nDisk \tAvailable: \t%s \tMB') % cell.capacities['disk_free']['total_mb'])) \n\tutils.print_dict(cell.capacities['disk_free']['units_by_mb'], dict_property='Disk(MB)', dict_value='Units')\n", 
" \tcompute_ref = compute_node_get_model(context, compute_id) \n\tvalues['updated_at'] = timeutils.utcnow() \n\tconvert_objects_related_datetimes(values) \n\tcompute_ref.update(values) \n\treturn compute_ref\n", 
" \trpc_utils.check_modify_host(data) \n\thost = models.Host.smart_get(id) \n\trpc_utils.check_modify_host_locking(host, data) \n\thost.update_object(data)\n", 
" \treturn IMPL.db_sync(version=version, database=database, context=context)\n", 
" \treturn IMPL.db_version(database=database, context=context)\n", 
" \treturn IMPL.service_destroy(context, service_id)\n", 
" \treturn IMPL.service_get(context, service_id)\n", 
" \treturn IMPL.service_get_by_host_and_topic(context, host, topic)\n", 
" \treturn IMPL.service_get_all(context, disabled)\n", 
" \treturn IMPL.service_get_all_by_topic(context, topic)\n", 
" \treturn IMPL.service_get_all_by_host(context, host)\n", 
" \treturn IMPL.service_get_all_by_host(context, host)\n", 
" \tfrom mkt.regions.utils import remove_accents \n\tby_name = sorted([v for (k, v) in DEFINED if (v.id and (v.weight > (-1)))], key=(lambda v: remove_accents(unicode(v.name)))) \n\tby_name.append(RESTOFWORLD) \n\treturn by_name\n", 
" \tservice_instance = salt.utils.vmware.get_service_instance(host=host, username=username, password=password, protocol=protocol, port=port) \n\tvalid_services = ['DCUI', 'TSM', 'SSH', 'ssh', 'lbtd', 'lsassd', 'lwiod', 'netlogond', 'ntpd', 'sfcbd-watchdog', 'snmpd', 'vprobed', 'vpxa', 'xorg'] \n\thost_names = _check_hosts(service_instance, host, host_names) \n\tret = {} \n\tfor host_name in host_names: \n\t \tif (service_name not in valid_services): \n\t \t \tret.update({host_name: {'Error': '{0} \tis \tnot \ta \tvalid \tservice \tname.'.format(service_name)}}) \n\t \t \treturn ret \n\t \thost_ref = _get_host_ref(service_instance, host, host_name=host_name) \n\t \tservices = host_ref.configManager.serviceSystem.serviceInfo.service \n\t \tif ((service_name == 'SSH') or (service_name == 'ssh')): \n\t \t \ttemp_service_name = 'TSM-SSH' \n\t \telse: \n\t \t \ttemp_service_name = service_name \n\t \tfor service in services: \n\t \t \tif (service.key == temp_service_name): \n\t \t \t \tret.update({host_name: {service_name: service.running}}) \n\t \t \t \tbreak \n\t \t \telse: \n\t \t \t \tmsg = \"Could \tnot \tfind \tservice \t'{0}' \tfor \thost \t'{1}'.\".format(service_name, host_name) \n\t \t \t \tret.update({host_name: {'Error': msg}}) \n\t \tif (ret.get(host_name) is None): \n\t \t \tmsg = \"'vsphere.get_service_running' \tfailed \tfor \thost \t{0}.\".format(host_name) \n\t \t \tlog.debug(msg) \n\t \t \tret.update({host_name: {'Error': msg}}) \n\treturn ret\n", 
" \treturn IMPL.service_create(context, values)\n", 
" \treturn IMPL.service_update(context, service_id, values)\n", 
" \treturn IMPL.instance_get(context, instance_id, columns_to_join=columns_to_join)\n", 
" \treturn IMPL.compute_node_get_all(context)\n", 
" \tresult = {} \n\tif (compute_node.vcpus > 0): \n\t \tresult[VCPU] = {'total': compute_node.vcpus, 'reserved': 0, 'min_unit': 1, 'max_unit': compute_node.vcpus, 'step_size': 1, 'allocation_ratio': compute_node.cpu_allocation_ratio} \n\tif (compute_node.memory_mb > 0): \n\t \tresult[MEMORY_MB] = {'total': compute_node.memory_mb, 'reserved': CONF.reserved_host_memory_mb, 'min_unit': 1, 'max_unit': compute_node.memory_mb, 'step_size': 1, 'allocation_ratio': compute_node.ram_allocation_ratio} \n\tif (compute_node.local_gb > 0): \n\t \tresult[DISK_GB] = {'total': compute_node.local_gb, 'reserved': (CONF.reserved_host_disk_mb * 1024), 'min_unit': 1, 'max_unit': compute_node.local_gb, 'step_size': 1, 'allocation_ratio': compute_node.disk_allocation_ratio} \n\treturn result\n", 
" \treturn IMPL.instance_update(context, instance_uuid, values, expected=expected)\n", 
" \timport cPickle \n\tdata = [1, 1.0, 2j, 2L, System.Int64(1), System.UInt64(1), System.UInt32(1), System.Int16(1), System.UInt16(1), System.Byte(1), System.SByte(1), System.Decimal(1), System.Char.MaxValue, System.DBNull.Value, System.Single(1.0), System.DateTime.Now, None, {}, (), [], {'a': 2}, (42,), [42], System.StringSplitOptions.RemoveEmptyEntries] \n\tdata.append(list(data)) \n\tdata.append(tuple(data)) \n\tclass X: \n\t \tdef __init__(self): \n\t \t \tself.abc = 3 \n\tclass Y(object, ): \n\t \tdef __init__(self): \n\t \t \tself.abc = 3 \n\tdata.append(X().__dict__) \n\tdata.append(Y().__dict__) \n\tl = [] \n\tl.append(l) \n\tdata.append(l) \n\td = {} \n\tcnt = 100 \n\tfor x in data: \n\t \td[cnt] = x \n\t \tcnt += 1 \n\tdata.append(d) \n\td1 = {} \n\td2 = {} \n\td1['abc'] = d2 \n\td1['foo'] = 'baz' \n\td2['abc'] = d1 \n\tdata.append(d1) \n\tdata.append(d2) \n\tfor value in data: \n\t \tfor newVal in (cPickle.loads(cPickle.dumps(value)), clr.Deserialize(*clr.Serialize(value))): \n\t \t \tAreEqual(type(newVal), type(value)) \n\t \t \ttry: \n\t \t \t \tAreEqual(newVal, value) \n\t \t \texcept RuntimeError as e: \n\t \t \t \tAreEqual(e.message, 'maximum \trecursion \tdepth \texceeded \tin \tcmp') \n\t \t \t \tAssert(((type(newVal) is list) or (type(newVal) is dict))) \n\tAssertError(ValueError, clr.Deserialize, 'unknown', 'foo') \n\tal = System.Collections.ArrayList() \n\tal.Add(2) \n\tgl = System.Collections.Generic.List[int]() \n\tgl.Add(2) \n\tfor value in (al, gl): \n\t \tfor newX in (cPickle.loads(cPickle.dumps(value)), clr.Deserialize(*clr.Serialize(value))): \n\t \t \tAreEqual(value.Count, newX.Count) \n\t \t \tfor i in xrange(value.Count): \n\t \t \t \tAreEqual(value[i], newX[i]) \n\tht = System.Collections.Hashtable() \n\tht['foo'] = 'bar' \n\tgd = System.Collections.Generic.Dictionary[(str, str)]() \n\tgd['foo'] = 'bar' \n\tfor value in (ht, gd): \n\t \tfor newX in (cPickle.loads(cPickle.dumps(value)), clr.Deserialize(*clr.Serialize(value))): \n\t \t \tAreEqual(value.Count, newX.Count) \n\t \t \tfor key in value.Keys: \n\t \t \t \tAreEqual(value[key], newX[key]) \n\tfor tempX in [System.Exception('some \tmessage')]: \n\t \tfor newX in (cPickle.loads(cPickle.dumps(tempX)), clr.Deserialize(*clr.Serialize(tempX))): \n\t \t \tAreEqual(newX.Message, tempX.Message) \n\ttry: \n\t \texec ' \tprint \t1' \n\texcept Exception as tempX: \n\t \tpass \n\tnewX = cPickle.loads(cPickle.dumps(tempX)) \n\tfor attr in ['args', 'filename', 'text', 'lineno', 'msg', 'offset', 'print_file_and_line', 'message']: \n\t \tAreEqual(eval(('newX.%s' % attr)), eval(('tempX.%s' % attr))) \n\tclass K(System.Exception, ): \n\t \tother = 'something \telse' \n\ttempX = K() \n\ttempX = System.Exception\n", 
" \ttry: \n\t \tfunc(*args) \n\texcept exc: \n\t \treturn True \n\telse: \n\t \treturn False\n", 
" \ttry: \n\t \tfn(*args, **kwargs) \n\texcept Exception as e: \n\t \tassert (e.__class__ == cls), ('got \t%s, \texpected \t%s' % (e.__class__.__name__, cls.__name__)) \n\telse: \n\t \traise AssertionError(('%s \tnot \traised' % cls))\n", 
" \treturn s3_rest_controller()\n", 
" \ttry: \n\t \t__import__(module_name) \n\texcept ImportError: \n\t \treturn False \n\telse: \n\t \treturn True\n", 
" \tresult = [] \n\tchecks = get_qualitychecks() \n\tfor (check, cat) in checks.items(): \n\t \tresult.append({'code': check, 'is_critical': (cat == Category.CRITICAL), 'title': (u'%s' % check_names.get(check, check)), 'url': path_obj.get_translate_url(check=check)}) \n\tdef alphabetical_critical_first(item): \n\t \tcritical_first = (0 if item['is_critical'] else 1) \n\t \treturn (critical_first, item['title'].lower()) \n\tresult = sorted(result, key=alphabetical_critical_first) \n\treturn result\n", 
" \tcount_failed = count_all = 0 \n\treport = BaseReport(options) \n\tcounters = report.counters \n\tchecks = (options.physical_checks + options.logical_checks) \n\tfor (name, check, argument_names) in checks: \n\t \tfor line in check.__doc__.splitlines(): \n\t \t \tline = line.lstrip() \n\t \t \tmatch = SELFTEST_REGEX.match(line) \n\t \t \tif (match is None): \n\t \t \t \tcontinue \n\t \t \t(code, source) = match.groups() \n\t \t \tlines = [(part.replace('\\\\t', ' DCTB ') + '\\n') for part in source.split('\\\\n')] \n\t \t \tchecker = Checker(lines=lines, options=options, report=report) \n\t \t \tchecker.check_all() \n\t \t \terror = None \n\t \t \tif (code == 'Okay'): \n\t \t \t \tif (len(counters) > len(options.benchmark_keys)): \n\t \t \t \t \tcodes = [key for key in counters if (key not in options.benchmark_keys)] \n\t \t \t \t \terror = ('incorrectly \tfound \t%s' % ', \t'.join(codes)) \n\t \t \telif (not counters.get(code)): \n\t \t \t \terror = ('failed \tto \tfind \t%s' % code) \n\t \t \tfor key in (set(counters) - set(options.benchmark_keys)): \n\t \t \t \tdel counters[key] \n\t \t \tcount_all += 1 \n\t \t \tif (not error): \n\t \t \t \tif options.verbose: \n\t \t \t \t \tprint ('%s: \t%s' % (code, source)) \n\t \t \telse: \n\t \t \t \tcount_failed += 1 \n\t \t \t \tprint ('pycodestyle.py: \t%s:' % error) \n\t \t \t \tfor line in checker.lines: \n\t \t \t \t \tprint line.rstrip() \n\treturn (count_failed, count_all)\n", 
" \tif (sys.platform == 'win32'): \n\t \traise SkipTest('Skipping \tline \tendings \tcheck \ton \tWindows') \n\treport = list() \n\tgood_exts = ('.py', '.dat', '.sel', '.lout', '.css', '.js', '.lay', '.txt', '.elc', '.csd', '.sfp', '.json', '.hpts', '.vmrk', '.vhdr', '.head', '.eve', '.ave', '.cov', '.label') \n\tfor (dirpath, dirnames, filenames) in os.walk(dir_): \n\t \tfor fname in filenames: \n\t \t \tif ((op.splitext(fname)[1] not in good_exts) or (fname in skip_files)): \n\t \t \t \tcontinue \n\t \t \tfilename = op.join(dirpath, fname) \n\t \t \trelfilename = op.relpath(filename, dir_) \n\t \t \ttry: \n\t \t \t \twith open(filename, 'rb') as fid: \n\t \t \t \t \ttext = fid.read().decode('utf-8') \n\t \t \texcept UnicodeDecodeError: \n\t \t \t \treport.append(('In \t%s \tfound \tnon-decodable \tbytes' % relfilename)) \n\t \t \telse: \n\t \t \t \tcrcount = text.count('\\r') \n\t \t \t \tif crcount: \n\t \t \t \t \treport.append(('In \t%s \tfound \t%i/%i \tCR/LF' % (relfilename, crcount, text.count('\\n')))) \n\tif (len(report) > 0): \n\t \traise AssertionError(('Found \t%s \tfiles \twith \tincorrect \tendings:\\n%s' % (len(report), '\\n'.join(report))))\n", 
" \tglobal fr, st \n\tfr = inspect.currentframe() \n\tst = inspect.stack() \n\tp = x \n\tq = (y / 0)\n", 
" \turl = api_url(host, port, '/Users/AuthenticateByName') \n\tr = requests.post(url, headers=headers, data=auth_data) \n\treturn r.json().get('AccessToken')\n", 
" \treturn (not re.match('[a-z]+://', file_location))\n", 
" \tif (not _state): \n\t \traise RuntimeError('no \tactive \tinput()') \n\treturn _state.filename()\n", 
" \tattribute = getattr(module, attribute_name) \n\tsetattr(attribute_replacement, __BACKUP_ATTRIBUTE_NAME, attribute) \n\tsetattr(module, attribute_name, attribute_replacement) \n\treturn is_patched(module, attribute_name)\n", 
" \treturn getattr(settings, 'CMS_CACHE_DURATIONS', {'menus': (60 * 60), 'content': 60, 'permissions': (60 * 60)})\n", 
" \tdef decorator(func): \n\t \tif (not hasattr(func, 'wsgi_serializers')): \n\t \t \tfunc.wsgi_serializers = {} \n\t \tfunc.wsgi_serializers.update(serializers) \n\t \treturn func \n\treturn decorator\n", 
" \tdef decorator(func): \n\t \tif (not hasattr(func, 'wsgi_deserializers')): \n\t \t \tfunc.wsgi_deserializers = {} \n\t \tfunc.wsgi_deserializers.update(deserializers) \n\t \treturn func \n\treturn decorator\n", 
" \tdef decorator(func): \n\t \tfunc.wsgi_code = code \n\t \treturn func \n\treturn decorator\n", 
" \ttry: \n\t \tdecoded = jsonutils.loads(body) \n\texcept ValueError: \n\t \tmsg = _('cannot \tunderstand \tJSON') \n\t \traise exception.MalformedRequestBody(reason=msg) \n\tif (len(decoded) != 1): \n\t \tmsg = _('too \tmany \tbody \tkeys') \n\t \traise exception.MalformedRequestBody(reason=msg) \n\treturn list(decoded.keys())[0]\n", 
" \ttry: \n\t \tdecoded = jsonutils.loads(body) \n\texcept ValueError: \n\t \tmsg = _('cannot \tunderstand \tJSON') \n\t \traise exception.MalformedRequestBody(reason=msg) \n\tif (len(decoded) != 1): \n\t \tmsg = _('too \tmany \tbody \tkeys') \n\t \traise exception.MalformedRequestBody(reason=msg) \n\treturn list(decoded.keys())[0]\n", 
" \tdef decorator(func): \n\t \tfunc.wsgi_action = name \n\t \treturn func \n\treturn decorator\n", 
" \tdef decorator(func): \n\t \tfunc.wsgi_extends = (func.__name__, kwargs.get('action')) \n\t \treturn func \n\tif args: \n\t \treturn decorator(*args) \n\treturn decorator\n", 
" \tcurr_time = timeutils.utcnow(with_timezone=True) \n\tcontext = req.environ['cinder.context'] \n\tfilters = {'disabled': False} \n\tservices = objects.ServiceList.get_all(context, filters) \n\tzone = '' \n\tif ('zone' in req.GET): \n\t \tzone = req.GET['zone'] \n\tif zone: \n\t \tservices = [s for s in services if (s['availability_zone'] == zone)] \n\thosts = [] \n\tfor host in services: \n\t \tdelta = (curr_time - (host.updated_at or host.created_at)) \n\t \talive = (abs(delta.total_seconds()) <= CONF.service_down_time) \n\t \tstatus = ((alive and 'available') or 'unavailable') \n\t \tactive = 'enabled' \n\t \tif host.disabled: \n\t \t \tactive = 'disabled' \n\t \tLOG.debug('status, \tactive \tand \tupdate: \t%s, \t%s, \t%s', status, active, host.updated_at) \n\t \tupdated_at = host.updated_at \n\t \tif updated_at: \n\t \t \tupdated_at = timeutils.normalize_time(updated_at) \n\t \thosts.append({'host_name': host.host, 'service': host.topic, 'zone': host.availability_zone, 'service-status': status, 'service-state': active, 'last-update': updated_at}) \n\tif service: \n\t \thosts = [host for host in hosts if (host['service'] == service)] \n\treturn hosts\n", 
" \tdef wrapped(self, req, id, service=None, *args, **kwargs): \n\t \tlisted_hosts = _list_hosts(req, service) \n\t \thosts = [h['host_name'] for h in listed_hosts] \n\t \tif (id in hosts): \n\t \t \treturn fn(self, req, id, *args, **kwargs) \n\t \traise exception.HostNotFound(host=id) \n\treturn wrapped\n", 
" \treturn _load_pipeline(loader, local_conf[CONF.api.auth_strategy].split())\n", 
" \tif (not isinstance(default, int)): \n\t \tmsg = (\"'%s' \tobject \tcannot \tbe \tinterpreted \tas \tan \tinteger\" % type(default).__name__) \n\t \traise TypeError(msg) \n\ttry: \n\t \treturn len(obj) \n\texcept TypeError: \n\t \tpass \n\ttry: \n\t \thint = type(obj).__length_hint__ \n\texcept AttributeError: \n\t \treturn default \n\ttry: \n\t \tval = hint(obj) \n\texcept TypeError: \n\t \treturn default \n\tif (val is NotImplemented): \n\t \treturn default \n\tif (not isinstance(val, int)): \n\t \tmsg = ('__length_hint__ \tmust \tbe \tinteger, \tnot \t%s' % type(val).__name__) \n\t \traise TypeError(msg) \n\tif (val < 0): \n\t \tmsg = '__length_hint__() \tshould \treturn \t>= \t0' \n\t \traise ValueError(msg) \n\treturn val\n", 
" \treturn itertools.chain(element.iterfind((_OLD_NAMESPACE_PREFIX + tag)), element.iterfind((_NEW_NAMESPACE_PREFIX + tag)))\n", 
" \tfor (key, value) in sub_dict.items(): \n\t \tif isinstance(value, list): \n\t \t \tfor repeated_element in value: \n\t \t \t \tsub_element = ET.SubElement(parent, key) \n\t \t \t \t_add_element_attrs(sub_element, repeated_element.get('attrs', {})) \n\t \t \t \tchildren = repeated_element.get('children', None) \n\t \t \t \tif isinstance(children, dict): \n\t \t \t \t \t_add_sub_elements_from_dict(sub_element, children) \n\t \t \t \telif isinstance(children, str): \n\t \t \t \t \tsub_element.text = children \n\t \telse: \n\t \t \tsub_element = ET.SubElement(parent, key) \n\t \t \t_add_element_attrs(sub_element, value.get('attrs', {})) \n\t \t \tchildren = value.get('children', None) \n\t \t \tif isinstance(children, dict): \n\t \t \t \t_add_sub_elements_from_dict(sub_element, children) \n\t \t \telif isinstance(children, str): \n\t \t \t \tsub_element.text = children\n", 
" \treturn ''.join([t for t in educate_tokens(tokenize(text), attr, language)])\n", 
" \tour_dir = path[0] \n\tfor (dirpath, dirnames, filenames) in os.walk(our_dir): \n\t \trelpath = os.path.relpath(dirpath, our_dir) \n\t \tif (relpath == '.'): \n\t \t \trelpkg = '' \n\t \telse: \n\t \t \trelpkg = ('.%s' % '.'.join(relpath.split(os.sep))) \n\t \tfor fname in filenames: \n\t \t \t(root, ext) = os.path.splitext(fname) \n\t \t \tif ((ext not in ('.py', '.pyc')) or (root == '__init__') or (fname in FILES_TO_SKIP)): \n\t \t \t \tcontinue \n\t \t \tif ((ext == '.pyc') and ((root + '.py') in filenames)): \n\t \t \t \tcontinue \n\t \t \tclassname = ('%s%s' % (root[0].upper(), root[1:])) \n\t \t \tclasspath = ('%s%s.%s.%s' % (package, relpkg, root, classname)) \n\t \t \tif ((ext_list is not None) and (classname not in ext_list)): \n\t \t \t \tlogger.debug(('Skipping \textension: \t%s' % classpath)) \n\t \t \t \tcontinue \n\t \t \ttry: \n\t \t \t \text_mgr.load_extension(classpath) \n\t \t \texcept Exception as exc: \n\t \t \t \tlogger.warning(_LW('Failed \tto \tload \textension \t%(classpath)s: \t%(exc)s'), {'classpath': classpath, 'exc': exc}) \n\t \tsubdirs = [] \n\t \tfor dname in dirnames: \n\t \t \tif (not os.path.exists(os.path.join(dirpath, dname, '__init__.py'))): \n\t \t \t \tcontinue \n\t \t \text_name = ('%s%s.%s.extension' % (package, relpkg, dname)) \n\t \t \ttry: \n\t \t \t \text = importutils.import_class(ext_name) \n\t \t \texcept ImportError: \n\t \t \t \tsubdirs.append(dname) \n\t \t \telse: \n\t \t \t \ttry: \n\t \t \t \t \text(ext_mgr) \n\t \t \t \texcept Exception as exc: \n\t \t \t \t \tlogger.warning(_LW('Failed \tto \tload \textension \t%(ext_name)s: \t%(exc)s'), {'ext_name': ext_name, 'exc': exc}) \n\t \tdirnames[:] = subdirs\n", 
" \tmax_limit = _get_pagination_max_limit() \n\tlimit = _get_limit_param(request) \n\tif (max_limit > 0): \n\t \tlimit = (min(max_limit, limit) or max_limit) \n\tif (not limit): \n\t \treturn (None, None) \n\tmarker = request.GET.get('marker', None) \n\treturn (limit, marker)\n", 
" \tlimit = request.GET.get('limit', 0) \n\ttry: \n\t \tlimit = int(limit) \n\t \tif (limit >= 0): \n\t \t \treturn limit \n\texcept ValueError: \n\t \tpass \n\tmsg = (_(\"Limit \tmust \tbe \tan \tinteger \t0 \tor \tgreater \tand \tnot \t'%s'\") % limit) \n\traise exceptions.BadRequest(resource='limit', msg=msg)\n", 
" \treturn request.GET['marker']\n", 
" \tparams = get_pagination_params(request) \n\toffset = params.get('offset', 0) \n\tlimit = CONF.api.max_limit \n\tlimit = min(limit, (params.get('limit') or limit)) \n\treturn items[offset:(offset + limit)]\n", 
" \tmax_limit = (max_limit or CONF.osapi_max_limit) \n\t(marker, limit, __) = get_pagination_params(request.GET.copy(), max_limit) \n\tstart_index = 0 \n\tif marker: \n\t \tstart_index = (-1) \n\t \tfor (i, item) in enumerate(items): \n\t \t \tif ('flavorid' in item): \n\t \t \t \tif (item['flavorid'] == marker): \n\t \t \t \t \tstart_index = (i + 1) \n\t \t \t \t \tbreak \n\t \t \telif ((item['id'] == marker) or (item.get('uuid') == marker)): \n\t \t \t \tstart_index = (i + 1) \n\t \t \t \tbreak \n\t \tif (start_index < 0): \n\t \t \tmsg = (_('marker \t[%s] \tnot \tfound') % marker) \n\t \t \traise webob.exc.HTTPBadRequest(explanation=msg) \n\trange_end = (start_index + limit) \n\treturn items[start_index:range_end]\n", 
" \tparsed_url = urllib.parse.urlsplit(href) \n\turl_parts = parsed_url.path.split('/', 2) \n\texpression = re.compile('^v([0-9]+|[0-9]+\\\\.[0-9]+)(/.*|$)') \n\tfor x in range(len(url_parts)): \n\t \tif expression.match(url_parts[x]): \n\t \t \tdel url_parts[x] \n\t \t \tbreak \n\tnew_path = '/'.join(url_parts) \n\tif (new_path == parsed_url.path): \n\t \tmsg = ('href \t%s \tdoes \tnot \tcontain \tversion' % href) \n\t \tLOG.debug(msg) \n\t \traise ValueError(msg) \n\tparsed_url = list(parsed_url) \n\tparsed_url[2] = new_path \n\treturn urllib.parse.urlunsplit(parsed_url)\n", 
" \tif (value and (value[0] == value[(-1)] == '\"')): \n\t \tvalue = value[1:(-1)] \n\treturn value\n", 
" \tresult = [] \n\tfor item in _parse_list_header(value): \n\t \tif (item[:1] == item[(-1):] == '\"'): \n\t \t \titem = unquote_header_value(item[1:(-1)]) \n\t \tresult.append(item) \n\treturn result\n", 
" \tif (not value): \n\t \treturn ('', {}) \n\tresult = [] \n\tvalue = (',' + value.replace('\\n', ',')) \n\twhile value: \n\t \tmatch = _option_header_start_mime_type.match(value) \n\t \tif (not match): \n\t \t \tbreak \n\t \tresult.append(match.group(1)) \n\t \toptions = {} \n\t \trest = match.group(2) \n\t \twhile rest: \n\t \t \toptmatch = _option_header_piece_re.match(rest) \n\t \t \tif (not optmatch): \n\t \t \t \tbreak \n\t \t \t(option, encoding, _, option_value) = optmatch.groups() \n\t \t \toption = unquote_header_value(option) \n\t \t \tif (option_value is not None): \n\t \t \t \toption_value = unquote_header_value(option_value, (option == 'filename')) \n\t \t \t \tif (encoding is not None): \n\t \t \t \t \toption_value = _unquote(option_value).decode(encoding) \n\t \t \toptions[option] = option_value \n\t \t \trest = rest[optmatch.end():] \n\t \tresult.append(options) \n\t \tif (multiple is False): \n\t \t \treturn tuple(result) \n\t \tvalue = rest \n\treturn (tuple(result) if result else ('', {}))\n", 
" \ttry: \n\t \tuuid.UUID(value) \n\t \treturn value \n\texcept (ValueError, AttributeError): \n\t \treturn int(value)\n", 
" \t(orig_exc_type, orig_exc_value, orig_exc_traceback) = sys.exc_info() \n\tif isinstance(new_exc, six.string_types): \n\t \tnew_exc = orig_exc_type(new_exc) \n\tif hasattr(new_exc, 'args'): \n\t \tif (len(new_exc.args) > 0): \n\t \t \tnew_message = ', \t'.join((str(arg) for arg in new_exc.args)) \n\t \telse: \n\t \t \tnew_message = '' \n\t \tnew_message += ('\\n\\nOriginal \texception:\\n DCTB ' + orig_exc_type.__name__) \n\t \tif (hasattr(orig_exc_value, 'args') and (len(orig_exc_value.args) > 0)): \n\t \t \tif getattr(orig_exc_value, 'reraised', False): \n\t \t \t \tnew_message += (': \t' + str(orig_exc_value.args[0])) \n\t \t \telse: \n\t \t \t \tnew_message += (': \t' + ', \t'.join((str(arg) for arg in orig_exc_value.args))) \n\t \tnew_exc.args = ((new_message,) + new_exc.args[1:]) \n\tnew_exc.__cause__ = orig_exc_value \n\tnew_exc.reraised = True \n\tsix.reraise(type(new_exc), new_exc, orig_exc_traceback)\n", 
" \tret = _ConvertToList(arg) \n\tfor element in ret: \n\t \tif (not isinstance(element, element_type)): \n\t \t \traise TypeError(('%s \tshould \tbe \tsingle \telement \tor \tlist \tof \ttype \t%s' % (arg_name, element_type))) \n\treturn ret\n", 
" \tlogger = logging.getLogger() \n\tloglevel = get_loglevel((loglevel or u'ERROR')) \n\tlogfile = (logfile if logfile else sys.__stderr__) \n\tif (not logger.handlers): \n\t \tif hasattr(logfile, u'write'): \n\t \t \thandler = logging.StreamHandler(logfile) \n\t \telse: \n\t \t \thandler = WatchedFileHandler(logfile) \n\t \tlogger.addHandler(handler) \n\t \tlogger.setLevel(loglevel) \n\treturn logger\n", 
" \treturn ((module in sys.modules) and isinstance(obj, getattr(import_module(module), class_name)))\n", 
" \tvalidate_config_version(config_details.config_files) \n\tprocessed_files = [process_config_file(config_file, config_details.environment) for config_file in config_details.config_files] \n\tconfig_details = config_details._replace(config_files=processed_files) \n\tmain_file = config_details.config_files[0] \n\tvolumes = load_mapping(config_details.config_files, u'get_volumes', u'Volume') \n\tnetworks = load_mapping(config_details.config_files, u'get_networks', u'Network') \n\tservice_dicts = load_services(config_details, main_file) \n\tif (main_file.version != V1): \n\t \tfor service_dict in service_dicts: \n\t \t \tmatch_named_volumes(service_dict, volumes) \n\tservices_using_deploy = [s for s in service_dicts if s.get(u'deploy')] \n\tif services_using_deploy: \n\t \tlog.warn(u\"Some \tservices \t({}) \tuse \tthe \t'deploy' \tkey, \twhich \twill \tbe \tignored. \tCompose \tdoes \tnot \tsupport \tdeploy \tconfiguration \t- \tuse \t`docker \tstack \tdeploy` \tto \tdeploy \tto \ta \tswarm.\".format(u', \t'.join(sorted((s[u'name'] for s in services_using_deploy))))) \n\treturn Config(main_file.version, service_dicts, volumes, networks)\n", 
" \ttry: \n\t \tvalue = args_list.pop(0) \n\texcept IndexError: \n\t \traise BadCommandUsage(msg) \n\tif ((expected_size_after is not None) and (len(args_list) > expected_size_after)): \n\t \traise BadCommandUsage('too \tmany \targuments') \n\treturn value\n", 
" \treturn datetime.datetime.strptime(d['isostr'], _DATETIME_FORMAT)\n", 
" \treturn _aliases.get(alias.lower(), alias)\n", 
" \twith open(CHANGELOG) as f: \n\t \tcur_index = 0 \n\t \tfor line in f: \n\t \t \tmatch = re.search('^\\\\d+\\\\.\\\\d+\\\\.\\\\d+', line) \n\t \t \tif match: \n\t \t \t \tif (cur_index == index): \n\t \t \t \t \treturn match.group(0) \n\t \t \t \telse: \n\t \t \t \t \tcur_index += 1\n", 
" \tgitstr = _git_str() \n\tif (gitstr is None): \n\t \tgitstr = '' \n\tpath = os.path.join(BASEDIR, 'qutebrowser', 'git-commit-id') \n\twith _open(path, 'w', encoding='ascii') as f: \n\t \tf.write(gitstr)\n", 
" \tif (len(sys.argv) < 2): \n\t \treturn True \n\tinfo_commands = ['--help-commands', '--name', '--version', '-V', '--fullname', '--author', '--author-email', '--maintainer', '--maintainer-email', '--contact', '--contact-email', '--url', '--license', '--description', '--long-description', '--platforms', '--classifiers', '--keywords', '--provides', '--requires', '--obsoletes'] \n\tinfo_commands.extend(['egg_info', 'install_egg_info', 'rotate']) \n\tfor command in info_commands: \n\t \tif (command in sys.argv[1:]): \n\t \t \treturn False \n\treturn True\n", 
" \trefs = list_refs(refnames=[refname], repo_dir=repo_dir, limit_to_heads=True) \n\tl = tuple(islice(refs, 2)) \n\tif l: \n\t \tassert (len(l) == 1) \n\t \treturn l[0][1] \n\telse: \n\t \treturn None\n", 
" \tif isinstance(fileIshObject, TextIOBase): \n\t \treturn unicode \n\tif isinstance(fileIshObject, IOBase): \n\t \treturn bytes \n\tencoding = getattr(fileIshObject, 'encoding', None) \n\timport codecs \n\tif isinstance(fileIshObject, (codecs.StreamReader, codecs.StreamWriter)): \n\t \tif encoding: \n\t \t \treturn unicode \n\t \telse: \n\t \t \treturn bytes \n\tif (not _PY3): \n\t \tif isinstance(fileIshObject, file): \n\t \t \tif (encoding is not None): \n\t \t \t \treturn basestring \n\t \t \telse: \n\t \t \t \treturn bytes \n\t \tfrom cStringIO import InputType, OutputType \n\t \tfrom StringIO import StringIO \n\t \tif isinstance(fileIshObject, (StringIO, InputType, OutputType)): \n\t \t \treturn bytes \n\treturn default\n", 
" \tcmd = (_pkg(jail, chroot, root) + ['--version']) \n\treturn __salt__['cmd.run'](cmd).strip()\n", 
" \tcmd = (_pkg(jail, chroot, root) + ['--version']) \n\treturn __salt__['cmd.run'](cmd).strip()\n", 
" \thost = node \n\tport = 27017 \n\tidx = node.rfind(':') \n\tif (idx != (-1)): \n\t \t(host, port) = (node[:idx], int(node[(idx + 1):])) \n\tif host.startswith('['): \n\t \thost = host[1:(-1)] \n\treturn (host, port)\n", 
" \tif isinstance(mode, six.string_types): \n\t \tif (mode.lower() == 'enforcing'): \n\t \t \tmode = '1' \n\t \t \tmodestring = 'Enforcing' \n\t \telif (mode.lower() == 'permissive'): \n\t \t \tmode = '0' \n\t \t \tmodestring = 'Permissive' \n\t \telif (mode.lower() == 'disabled'): \n\t \t \tmode = '0' \n\t \t \tmodestring = 'Disabled' \n\t \telse: \n\t \t \treturn 'Invalid \tmode \t{0}'.format(mode) \n\telif isinstance(mode, int): \n\t \tif mode: \n\t \t \tmode = '1' \n\t \telse: \n\t \t \tmode = '0' \n\telse: \n\t \treturn 'Invalid \tmode \t{0}'.format(mode) \n\tif (getenforce() != 'Disabled'): \n\t \tenforce = os.path.join(selinux_fs_path(), 'enforce') \n\t \ttry: \n\t \t \twith salt.utils.fopen(enforce, 'w') as _fp: \n\t \t \t \t_fp.write(mode) \n\t \texcept (IOError, OSError) as exc: \n\t \t \tmsg = 'Could \tnot \twrite \tSELinux \tenforce \tfile: \t{0}' \n\t \t \traise CommandExecutionError(msg.format(str(exc))) \n\tconfig = '/etc/selinux/config' \n\ttry: \n\t \twith salt.utils.fopen(config, 'r') as _cf: \n\t \t \tconf = _cf.read() \n\t \ttry: \n\t \t \twith salt.utils.fopen(config, 'w') as _cf: \n\t \t \t \tconf = re.sub('\\\\nSELINUX=.*\\\\n', (('\\nSELINUX=' + modestring) + '\\n'), conf) \n\t \t \t \t_cf.write(conf) \n\t \texcept (IOError, OSError) as exc: \n\t \t \tmsg = 'Could \tnot \twrite \tSELinux \tconfig \tfile: \t{0}' \n\t \t \traise CommandExecutionError(msg.format(str(exc))) \n\texcept (IOError, OSError) as exc: \n\t \tmsg = 'Could \tnot \tread \tSELinux \tconfig \tfile: \t{0}' \n\t \traise CommandExecutionError(msg.format(str(exc))) \n\treturn getenforce()\n", 
" \t'\\n \t \t \t \tInitialize \tthe \tworkflow\\n \t \t \t \t' \n\tgetmask = pe.Workflow(name=name) \n\t'\\n \t \t \t \tDefine \tthe \tinputs \tto \tthe \tworkflow.\\n \t \t \t \t' \n\tinputnode = pe.Node(niu.IdentityInterface(fields=['source_file', 'subject_id', 'subjects_dir', 'contrast_type']), name='inputspec') \n\t'\\n \t \t \t \tDefine \tall \tthe \tnodes \tof \tthe \tworkflow:\\n\\n \t \t \t \tfssource: \tused \tto \tretrieve \taseg.mgz\\n \t \t \t \tthreshold \t: \tbinarize \taseg\\n \t \t \t \tregister \t: \tcoregister \tsource \tfile \tto \tfreesurfer \tspace\\n \t \t \t \tvoltransform: \tconvert \tbinarized \taseg \tto \tsource \tfile \tspace\\n \t \t \t \t' \n\tfssource = pe.Node(nio.FreeSurferSource(), name='fssource') \n\tthreshold = pe.Node(fs.Binarize(min=0.5, out_type='nii'), name='threshold') \n\tregister = pe.MapNode(fs.BBRegister(init='fsl'), iterfield=['source_file'], name='register') \n\tvoltransform = pe.MapNode(fs.ApplyVolTransform(inverse=True), iterfield=['source_file', 'reg_file'], name='transform') \n\t'\\n \t \t \t \tConnect \tthe \tnodes\\n \t \t \t \t' \n\tgetmask.connect([(inputnode, fssource, [('subject_id', 'subject_id'), ('subjects_dir', 'subjects_dir')]), (inputnode, register, [('source_file', 'source_file'), ('subject_id', 'subject_id'), ('subjects_dir', 'subjects_dir'), ('contrast_type', 'contrast_type')]), (inputnode, voltransform, [('subjects_dir', 'subjects_dir'), ('source_file', 'source_file')]), (fssource, threshold, [(('aparc_aseg', get_aparc_aseg), 'in_file')]), (register, voltransform, [('out_reg_file', 'reg_file')]), (threshold, voltransform, [('binary_file', 'target_file')])]) \n\t'\\n \t \t \t \tAdd \tremaining \tnodes \tand \tconnections\\n\\n \t \t \t \tdilate \t: \tdilate \tthe \ttransformed \tfile \tin \tsource \tspace\\n \t \t \t \tthreshold2 \t: \tbinarize \ttransformed \tfile\\n \t \t \t \t' \n\tthreshold2 = pe.MapNode(fs.Binarize(min=0.5, out_type='nii'), iterfield=['in_file'], name='threshold2') \n\tif dilate_mask: \n\t \tthreshold2.inputs.dilate = 1 \n\tgetmask.connect([(voltransform, threshold2, [('transformed_file', 'in_file')])]) \n\t'\\n \t \t \t \tSetup \tan \toutputnode \tthat \tdefines \trelevant \tinputs \tof \tthe \tworkflow.\\n \t \t \t \t' \n\toutputnode = pe.Node(niu.IdentityInterface(fields=['mask_file', 'reg_file', 'reg_cost']), name='outputspec') \n\tgetmask.connect([(register, outputnode, [('out_reg_file', 'reg_file')]), (register, outputnode, [('min_cost_file', 'reg_cost')]), (threshold2, outputnode, [('binary_file', 'mask_file')])]) \n\treturn getmask\n", 
" \trule_method = ('telemetry:' + policy_name) \n\theaders = request.headers \n\tpolicy_dict = dict() \n\tpolicy_dict['roles'] = headers.get('X-Roles', '').split(',') \n\tpolicy_dict['user_id'] = headers.get('X-User-Id') \n\tpolicy_dict['project_id'] = headers.get('X-Project-Id') \n\tif ((_has_rule('default') or _has_rule(rule_method)) and (not pecan.request.enforcer.enforce(rule_method, {}, policy_dict))): \n\t \tpecan.core.abort(status_code=403, detail='RBAC \tAuthorization \tFailed')\n", 
" \tdef wrapped(f): \n\t \tviews[name] = f \n\t \treturn f \n\treturn wrapped\n", 
" \tif (target is None): \n\t \ttarget = {} \n\tmatch_rule = _build_match_rule(action, target, pluralized) \n\tcredentials = context.to_policy_values() \n\treturn (match_rule, target, credentials)\n", 
" \ttry: \n\t \tlength = len(value) \n\texcept TypeError: \n\t \traise VdtTypeError(value) \n\tif (length < len(args)): \n\t \traise VdtValueTooShortError(value) \n\telif (length > len(args)): \n\t \traise VdtValueTooLongError(value) \n\ttry: \n\t \treturn [fun_dict[arg](val) for (arg, val) in zip(args, value)] \n\texcept KeyError as e: \n\t \traise VdtParamError('mixed_list', e)\n", 
" \treq = get('http://{host}:{port}'.format(host=host, port=port), timeout=SOCKET_TIMEOUT_FOR_POLLING, persistent=False) \n\tdef failed(failure): \n\t \treturn False \n\tdef succeeded(result): \n\t \treturn True \n\treq.addCallbacks(succeeded, failed) \n\treturn req\n", 
" \tfeasible_ind = numpy.array(individual) \n\tfeasible_ind = numpy.maximum(MIN_BOUND, feasible_ind) \n\tfeasible_ind = numpy.minimum(MAX_BOUND, feasible_ind) \n\treturn feasible_ind\n", 
" \tif (value in (u'1', u'0')): \n\t \treturn bool(int(value)) \n\traise ValueError((u'%r \tis \tnot \t0 \tor \t1' % value))\n", 
" \tif isinstance(obj, str): \n\t \tobj = obj.strip().lower() \n\t \tif (obj in ('true', 'yes', 'on', 'y', 't', '1')): \n\t \t \treturn True \n\t \tif (obj in ('false', 'no', 'off', 'n', 'f', '0')): \n\t \t \treturn False \n\t \traise ValueError(('Unable \tto \tinterpret \tvalue \t\"%s\" \tas \tboolean' % obj)) \n\treturn bool(obj)\n", 
" \tglobal _PARSING_CACHE \n\tif (string in _PARSING_CACHE): \n\t \tstack = _PARSING_CACHE[string] \n\telse: \n\t \tif (not _RE_STARTTOKEN.search(string)): \n\t \t \treturn string \n\t \tstack = ParseStack() \n\t \tncallable = 0 \n\t \tfor match in _RE_TOKEN.finditer(string): \n\t \t \tgdict = match.groupdict() \n\t \t \tif gdict['singlequote']: \n\t \t \t \tstack.append(gdict['singlequote']) \n\t \t \telif gdict['doublequote']: \n\t \t \t \tstack.append(gdict['doublequote']) \n\t \t \telif gdict['end']: \n\t \t \t \tif (ncallable <= 0): \n\t \t \t \t \tstack.append(')') \n\t \t \t \t \tcontinue \n\t \t \t \targs = [] \n\t \t \t \twhile stack: \n\t \t \t \t \toperation = stack.pop() \n\t \t \t \t \tif callable(operation): \n\t \t \t \t \t \tif (not strip): \n\t \t \t \t \t \t \tstack.append((operation, [arg for arg in reversed(args)])) \n\t \t \t \t \t \tncallable -= 1 \n\t \t \t \t \t \tbreak \n\t \t \t \t \telse: \n\t \t \t \t \t \targs.append(operation) \n\t \t \telif gdict['start']: \n\t \t \t \tfuncname = _RE_STARTTOKEN.match(gdict['start']).group(1) \n\t \t \t \ttry: \n\t \t \t \t \tstack.append(_INLINE_FUNCS[funcname]) \n\t \t \t \texcept KeyError: \n\t \t \t \t \tstack.append(_INLINE_FUNCS['nomatch']) \n\t \t \t \t \tstack.append(funcname) \n\t \t \t \tncallable += 1 \n\t \t \telif gdict['escaped']: \n\t \t \t \ttoken = gdict['escaped'].lstrip('\\\\') \n\t \t \t \tstack.append(token) \n\t \t \telif gdict['comma']: \n\t \t \t \tif (ncallable > 0): \n\t \t \t \t \tstack.append(None) \n\t \t \t \telse: \n\t \t \t \t \tstack.append(',') \n\t \t \telse: \n\t \t \t \tstack.append(gdict['rest']) \n\t \tif (ncallable > 0): \n\t \t \treturn string \n\t \tif ((_STACK_MAXSIZE > 0) and (_STACK_MAXSIZE < len(stack))): \n\t \t \treturn (string + gdict['stackfull'](*args, **kwargs)) \n\t \telse: \n\t \t \t_PARSING_CACHE[string] = stack \n\tdef _run_stack(item, depth=0): \n\t \tretval = item \n\t \tif isinstance(item, tuple): \n\t \t \tif strip: \n\t \t \t \treturn '' \n\t \t \telse: \n\t \t \t \t(func, arglist) = item \n\t \t \t \targs = [''] \n\t \t \t \tfor arg in arglist: \n\t \t \t \t \tif (arg is None): \n\t \t \t \t \t \targs.append('') \n\t \t \t \t \telse: \n\t \t \t \t \t \targs[(-1)] += _run_stack(arg, depth=(depth + 1)) \n\t \t \t \tkwargs['inlinefunc_stack_depth'] = depth \n\t \t \t \tretval = ('' if strip else func(*args, **kwargs)) \n\t \treturn utils.to_str(retval, force_string=True) \n\treturn ''.join((_run_stack(item) for item in _PARSING_CACHE[string]))\n", 
" \tif (type(s) is str): \n\t \treturn s \n\telse: \n\t \treturn s3_unicode(s).encode('utf-8', 'strict')\n", 
" \tdef wrapper(func): \n\t \t@functools.wraps(func) \n\t \tdef inner(*args, **kwds): \n\t \t \tlock.acquire() \n\t \t \ttry: \n\t \t \t \treturn func(*args, **kwds) \n\t \t \tfinally: \n\t \t \t \tlock.release() \n\t \treturn inner \n\treturn wrapper\n", 
" \treturn auth_is_loggedin_user()\n", 
" \tif ((r.representation == 'html') and (r.name == 'shelter') and r.id and (not r.component)): \n\t \tT = current.T \n\t \tmsg = current.msg \n\t \ts3db = current.s3db \n\t \trecord = r.record \n\t \tctable = s3db.pr_contact \n\t \tstable = s3db.cr_shelter \n\t \tmessage = '' \n\t \ttext = '' \n\t \ts_id = record.id \n\t \ts_name = record.name \n\t \ts_phone = record.phone \n\t \ts_email = record.email \n\t \ts_status = record.status \n\t \tif (s_phone in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_email in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_status in ('', None)): \n\t \t \ts_status = T('Not \tDefined') \n\t \telif (s_status == 1): \n\t \t \ts_status = 'Open' \n\t \telif (s_status == 2): \n\t \t \ts_status = 'Close' \n\t \telse: \n\t \t \ts_status = 'Unassigned \tShelter \tStatus' \n\t \ttext += '************************************************' \n\t \ttext += ('\\n%s \t' % T('Automatic \tMessage')) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Shelter \tID'), s_id)) \n\t \ttext += (' \t%s: \t%s' % (T('Shelter \tname'), s_name)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Email'), s_email)) \n\t \ttext += (' \t%s: \t%s' % (T('Phone'), s_phone)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Working \tStatus'), s_status)) \n\t \ttext += '\\n************************************************\\n' \n\t \turl = URL(c='cr', f='shelter', args=r.id) \n\t \topts = dict(type='SMS', subject=T('Deployment \tRequest'), message=(message + text), url=url) \n\t \toutput = msg.compose(**opts) \n\t \tif attr.get('rheader'): \n\t \t \trheader = attr['rheader'](r) \n\t \t \tif rheader: \n\t \t \t \toutput['rheader'] = rheader \n\t \toutput['title'] = T('Send \tNotification') \n\t \tcurrent.response.view = 'msg/compose.html' \n\t \treturn output \n\telse: \n\t \traise HTTP(501, current.messages.BADMETHOD)\n", 
" \tcan_users_receive_email = email_manager.can_users_receive_thread_email(recipient_list, exploration_id, has_suggestion) \n\tfor (index, recipient_id) in enumerate(recipient_list): \n\t \tif can_users_receive_email[index]: \n\t \t \ttransaction_services.run_in_transaction(_enqueue_feedback_thread_status_change_email_task, recipient_id, feedback_message_reference, old_status, new_status)\n", 
" \tglobal _notifier \n\tif (_notifier is None): \n\t \thost = (CONF.default_publisher_id or socket.gethostname()) \n\t \ttry: \n\t \t \ttransport = oslo_messaging.get_notification_transport(CONF) \n\t \t \t_notifier = oslo_messaging.Notifier(transport, ('identity.%s' % host)) \n\t \texcept Exception: \n\t \t \tLOG.exception(_LE('Failed \tto \tconstruct \tnotifier')) \n\t \t \t_notifier = False \n\treturn _notifier\n", 
" \tcan_users_receive_email = email_manager.can_users_receive_thread_email(recipient_list, exploration_id, has_suggestion) \n\tfor (index, recipient_id) in enumerate(recipient_list): \n\t \tif can_users_receive_email[index]: \n\t \t \ttransaction_services.run_in_transaction(_enqueue_feedback_thread_status_change_email_task, recipient_id, feedback_message_reference, old_status, new_status)\n", 
" \tdef wrapped_func(*args, **kwarg): \n\t \tCALLED_FUNCTION.append(name) \n\t \treturn function(*args, **kwarg) \n\treturn wrapped_func\n", 
" \tlevel = notification.get_default_level() \n\tif (stack.status == stack.IN_PROGRESS): \n\t \tsuffix = 'start' \n\telif (stack.status == stack.COMPLETE): \n\t \tsuffix = 'end' \n\telse: \n\t \tsuffix = 'error' \n\t \tlevel = notification.ERROR \n\tevent_type = ('%s.%s.%s' % ('stack', stack.action.lower(), suffix)) \n\tnotification.notify(stack.context, event_type, level, engine_api.format_notification_body(stack))\n", 
" \ttry: \n\t \tdriver = _DRIVERS[driver_name] \n\texcept KeyError: \n\t \traise DriverNotFoundError(('No \tdriver \tfor \t%s' % driver_name)) \n\treturn driver(*args, **kwargs)\n", 
" \tlevel = notification.get_default_level() \n\tif (stack.status == stack.IN_PROGRESS): \n\t \tsuffix = 'start' \n\telif (stack.status == stack.COMPLETE): \n\t \tsuffix = 'end' \n\telse: \n\t \tsuffix = 'error' \n\t \tlevel = notification.ERROR \n\tevent_type = ('%s.%s.%s' % ('stack', stack.action.lower(), suffix)) \n\tnotification.notify(stack.context, event_type, level, engine_api.format_notification_body(stack))\n", 
" \tseed = (pseed or config.unittests.rseed) \n\tif (seed == 'random'): \n\t \tseed = None \n\ttry: \n\t \tif seed: \n\t \t \tseed = int(seed) \n\t \telse: \n\t \t \tseed = None \n\texcept ValueError: \n\t \tprint('Error: \tconfig.unittests.rseed \tcontains \tinvalid \tseed, \tusing \tNone \tinstead', file=sys.stderr) \n\t \tseed = None \n\treturn seed\n", 
" \tif ((r.representation == 'html') and (r.name == 'shelter') and r.id and (not r.component)): \n\t \tT = current.T \n\t \tmsg = current.msg \n\t \ts3db = current.s3db \n\t \trecord = r.record \n\t \tctable = s3db.pr_contact \n\t \tstable = s3db.cr_shelter \n\t \tmessage = '' \n\t \ttext = '' \n\t \ts_id = record.id \n\t \ts_name = record.name \n\t \ts_phone = record.phone \n\t \ts_email = record.email \n\t \ts_status = record.status \n\t \tif (s_phone in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_email in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_status in ('', None)): \n\t \t \ts_status = T('Not \tDefined') \n\t \telif (s_status == 1): \n\t \t \ts_status = 'Open' \n\t \telif (s_status == 2): \n\t \t \ts_status = 'Close' \n\t \telse: \n\t \t \ts_status = 'Unassigned \tShelter \tStatus' \n\t \ttext += '************************************************' \n\t \ttext += ('\\n%s \t' % T('Automatic \tMessage')) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Shelter \tID'), s_id)) \n\t \ttext += (' \t%s: \t%s' % (T('Shelter \tname'), s_name)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Email'), s_email)) \n\t \ttext += (' \t%s: \t%s' % (T('Phone'), s_phone)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Working \tStatus'), s_status)) \n\t \ttext += '\\n************************************************\\n' \n\t \turl = URL(c='cr', f='shelter', args=r.id) \n\t \topts = dict(type='SMS', subject=T('Deployment \tRequest'), message=(message + text), url=url) \n\t \toutput = msg.compose(**opts) \n\t \tif attr.get('rheader'): \n\t \t \trheader = attr['rheader'](r) \n\t \t \tif rheader: \n\t \t \t \toutput['rheader'] = rheader \n\t \toutput['title'] = T('Send \tNotification') \n\t \tcurrent.response.view = 'msg/compose.html' \n\t \treturn output \n\telse: \n\t \traise HTTP(501, current.messages.BADMETHOD)\n", 
" \ttrunc = 20 \n\targspec = inspect.getargspec(fobj) \n\targ_list = [] \n\tif argspec.args: \n\t \tfor arg in argspec.args: \n\t \t \targ_list.append(str(arg)) \n\targ_list.reverse() \n\tif argspec.defaults: \n\t \tfor i in range(len(argspec.defaults)): \n\t \t \targ_list[i] = ((str(arg_list[i]) + '=') + str(argspec.defaults[(- i)])) \n\targ_list.reverse() \n\tif argspec.varargs: \n\t \targ_list.append(argspec.varargs) \n\tif argspec.keywords: \n\t \targ_list.append(argspec.keywords) \n\targ_list = [x[:trunc] for x in arg_list] \n\tstr_param = ('%s(%s)' % (name, ', \t'.join(arg_list))) \n\treturn str_param\n", 
" \treturn salt.utils.etcd_util.get_conn(profile)\n", 
" \treturn RetryWithBackoff(callable_func, retry_notify_func, delay, 1, delay, max_tries)\n", 
" \tmsg_tested_versions = ['xp', 'vista', '2008', '2003'] \n\tmsg_args = ['/c', '%SystemRoot%\\\\System32\\\\msg.exe', '*', '/TIME:0'] \n\thost_version = platform.platform().lower() \n\tif (not msg): \n\t \treturn ('Command \tnot \tran.', 'Empty \tmessage.', (-1)) \n\telse: \n\t \tmsg_args.extend([msg]) \n\tfor version in msg_tested_versions: \n\t \tif (host_version.find(version) != (-1)): \n\t \t \tres = client_utils_common.Execute('cmd', msg_args, time_limit=(-1), bypass_whitelist=True) \n\t \t \treturn res \n\treturn ('', 'Command \tnot \tavailable \tfor \tthis \tversion.', (-1))\n", 
" \tmsg_tested_versions = ['xp', 'vista', '2008', '2003'] \n\tmsg_args = ['/c', '%SystemRoot%\\\\System32\\\\msg.exe', '*', '/TIME:0'] \n\thost_version = platform.platform().lower() \n\tif (not msg): \n\t \treturn ('Command \tnot \tran.', 'Empty \tmessage.', (-1)) \n\telse: \n\t \tmsg_args.extend([msg]) \n\tfor version in msg_tested_versions: \n\t \tif (host_version.find(version) != (-1)): \n\t \t \tres = client_utils_common.Execute('cmd', msg_args, time_limit=(-1), bypass_whitelist=True) \n\t \t \treturn res \n\treturn ('', 'Command \tnot \tavailable \tfor \tthis \tversion.', (-1))\n", 
" \tex = copy(event_exchange) \n\tif (conn.transport.driver_type == u'redis'): \n\t \tex.type = u'fanout' \n\treturn ex\n", 
" \tmsg_tested_versions = ['xp', 'vista', '2008', '2003'] \n\tmsg_args = ['/c', '%SystemRoot%\\\\System32\\\\msg.exe', '*', '/TIME:0'] \n\thost_version = platform.platform().lower() \n\tif (not msg): \n\t \treturn ('Command \tnot \tran.', 'Empty \tmessage.', (-1)) \n\telse: \n\t \tmsg_args.extend([msg]) \n\tfor version in msg_tested_versions: \n\t \tif (host_version.find(version) != (-1)): \n\t \t \tres = client_utils_common.Execute('cmd', msg_args, time_limit=(-1), bypass_whitelist=True) \n\t \t \treturn res \n\treturn ('', 'Command \tnot \tavailable \tfor \tthis \tversion.', (-1))\n", 
" \tex = copy(event_exchange) \n\tif (conn.transport.driver_type == u'redis'): \n\t \tex.type = u'fanout' \n\treturn ex\n", 
" \tif ((r.representation == 'html') and (r.name == 'shelter') and r.id and (not r.component)): \n\t \tT = current.T \n\t \tmsg = current.msg \n\t \ts3db = current.s3db \n\t \trecord = r.record \n\t \tctable = s3db.pr_contact \n\t \tstable = s3db.cr_shelter \n\t \tmessage = '' \n\t \ttext = '' \n\t \ts_id = record.id \n\t \ts_name = record.name \n\t \ts_phone = record.phone \n\t \ts_email = record.email \n\t \ts_status = record.status \n\t \tif (s_phone in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_email in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_status in ('', None)): \n\t \t \ts_status = T('Not \tDefined') \n\t \telif (s_status == 1): \n\t \t \ts_status = 'Open' \n\t \telif (s_status == 2): \n\t \t \ts_status = 'Close' \n\t \telse: \n\t \t \ts_status = 'Unassigned \tShelter \tStatus' \n\t \ttext += '************************************************' \n\t \ttext += ('\\n%s \t' % T('Automatic \tMessage')) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Shelter \tID'), s_id)) \n\t \ttext += (' \t%s: \t%s' % (T('Shelter \tname'), s_name)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Email'), s_email)) \n\t \ttext += (' \t%s: \t%s' % (T('Phone'), s_phone)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Working \tStatus'), s_status)) \n\t \ttext += '\\n************************************************\\n' \n\t \turl = URL(c='cr', f='shelter', args=r.id) \n\t \topts = dict(type='SMS', subject=T('Deployment \tRequest'), message=(message + text), url=url) \n\t \toutput = msg.compose(**opts) \n\t \tif attr.get('rheader'): \n\t \t \trheader = attr['rheader'](r) \n\t \t \tif rheader: \n\t \t \t \toutput['rheader'] = rheader \n\t \toutput['title'] = T('Send \tNotification') \n\t \tcurrent.response.view = 'msg/compose.html' \n\t \treturn output \n\telse: \n\t \traise HTTP(501, current.messages.BADMETHOD)\n", 
" \treturn salt.utils.etcd_util.get_conn(profile)\n", 
" \t@functools.wraps(f) \n\tdef Wrapper(self, request): \n\t \t'Wrap \tthe \tfunction \tcan \tcatch \texceptions, \tconverting \tthem \tto \tstatus.' \n\t \tfailed = True \n\t \tresponse = rdf_data_store.DataStoreResponse() \n\t \tresponse.status = rdf_data_store.DataStoreResponse.Status.OK \n\t \ttry: \n\t \t \tf(self, request, response) \n\t \t \tfailed = False \n\t \texcept access_control.UnauthorizedAccess as e: \n\t \t \tresponse.Clear() \n\t \t \tresponse.request = request \n\t \t \tresponse.status = rdf_data_store.DataStoreResponse.Status.AUTHORIZATION_DENIED \n\t \t \tif e.subject: \n\t \t \t \tresponse.failed_subject = utils.SmartUnicode(e.subject) \n\t \t \tresponse.status_desc = utils.SmartUnicode(e) \n\t \texcept data_store.Error as e: \n\t \t \tresponse.Clear() \n\t \t \tresponse.request = request \n\t \t \tresponse.status = rdf_data_store.DataStoreResponse.Status.DATA_STORE_ERROR \n\t \t \tresponse.status_desc = utils.SmartUnicode(e) \n\t \texcept access_control.ExpiryError as e: \n\t \t \tresponse.Clear() \n\t \t \tresponse.request = request \n\t \t \tresponse.status = rdf_data_store.DataStoreResponse.Status.TIMEOUT_ERROR \n\t \t \tresponse.status_desc = utils.SmartUnicode(e) \n\t \tif failed: \n\t \t \tlogging.info('Failed: \t%s', utils.SmartStr(response)[:1000]) \n\t \tserialized_response = response.SerializeToString() \n\t \treturn serialized_response \n\treturn Wrapper\n", 
" \treturn RetryWithBackoff(callable_func, retry_notify_func, delay, 1, delay, max_tries)\n", 
" \tmsg_tested_versions = ['xp', 'vista', '2008', '2003'] \n\tmsg_args = ['/c', '%SystemRoot%\\\\System32\\\\msg.exe', '*', '/TIME:0'] \n\thost_version = platform.platform().lower() \n\tif (not msg): \n\t \treturn ('Command \tnot \tran.', 'Empty \tmessage.', (-1)) \n\telse: \n\t \tmsg_args.extend([msg]) \n\tfor version in msg_tested_versions: \n\t \tif (host_version.find(version) != (-1)): \n\t \t \tres = client_utils_common.Execute('cmd', msg_args, time_limit=(-1), bypass_whitelist=True) \n\t \t \treturn res \n\treturn ('', 'Command \tnot \tavailable \tfor \tthis \tversion.', (-1))\n", 
" \tcache_key = _cache_get_key() \n\ttry: \n\t \treturn __context__[cache_key] \n\texcept KeyError: \n\t \tpass \n\tconn = _get_conn(region=region, key=key, keyid=keyid, profile=profile) \n\t__context__[cache_key] = {} \n\ttopics = conn.get_all_topics() \n\tfor t in topics['ListTopicsResponse']['ListTopicsResult']['Topics']: \n\t \tshort_name = t['TopicArn'].split(':')[(-1)] \n\t \t__context__[cache_key][short_name] = t['TopicArn'] \n\treturn __context__[cache_key]\n", 
" \treturn salt.utils.etcd_util.get_conn(profile)\n", 
" \treturn RetryWithBackoff(callable_func, retry_notify_func, delay, 1, delay, max_tries)\n", 
" \tmsg_tested_versions = ['xp', 'vista', '2008', '2003'] \n\tmsg_args = ['/c', '%SystemRoot%\\\\System32\\\\msg.exe', '*', '/TIME:0'] \n\thost_version = platform.platform().lower() \n\tif (not msg): \n\t \treturn ('Command \tnot \tran.', 'Empty \tmessage.', (-1)) \n\telse: \n\t \tmsg_args.extend([msg]) \n\tfor version in msg_tested_versions: \n\t \tif (host_version.find(version) != (-1)): \n\t \t \tres = client_utils_common.Execute('cmd', msg_args, time_limit=(-1), bypass_whitelist=True) \n\t \t \treturn res \n\treturn ('', 'Command \tnot \tavailable \tfor \tthis \tversion.', (-1))\n", 
" \tmsg_tested_versions = ['xp', 'vista', '2008', '2003'] \n\tmsg_args = ['/c', '%SystemRoot%\\\\System32\\\\msg.exe', '*', '/TIME:0'] \n\thost_version = platform.platform().lower() \n\tif (not msg): \n\t \treturn ('Command \tnot \tran.', 'Empty \tmessage.', (-1)) \n\telse: \n\t \tmsg_args.extend([msg]) \n\tfor version in msg_tested_versions: \n\t \tif (host_version.find(version) != (-1)): \n\t \t \tres = client_utils_common.Execute('cmd', msg_args, time_limit=(-1), bypass_whitelist=True) \n\t \t \treturn res \n\treturn ('', 'Command \tnot \tavailable \tfor \tthis \tversion.', (-1))\n", 
" \tex = copy(event_exchange) \n\tif (conn.transport.driver_type == u'redis'): \n\t \tex.type = u'fanout' \n\treturn ex\n", 
" \tmsg_tested_versions = ['xp', 'vista', '2008', '2003'] \n\tmsg_args = ['/c', '%SystemRoot%\\\\System32\\\\msg.exe', '*', '/TIME:0'] \n\thost_version = platform.platform().lower() \n\tif (not msg): \n\t \treturn ('Command \tnot \tran.', 'Empty \tmessage.', (-1)) \n\telse: \n\t \tmsg_args.extend([msg]) \n\tfor version in msg_tested_versions: \n\t \tif (host_version.find(version) != (-1)): \n\t \t \tres = client_utils_common.Execute('cmd', msg_args, time_limit=(-1), bypass_whitelist=True) \n\t \t \treturn res \n\treturn ('', 'Command \tnot \tavailable \tfor \tthis \tversion.', (-1))\n", 
" \tex = copy(event_exchange) \n\tif (conn.transport.driver_type == u'redis'): \n\t \tex.type = u'fanout' \n\treturn ex\n", 
" \tif ((r.representation == 'html') and (r.name == 'shelter') and r.id and (not r.component)): \n\t \tT = current.T \n\t \tmsg = current.msg \n\t \ts3db = current.s3db \n\t \trecord = r.record \n\t \tctable = s3db.pr_contact \n\t \tstable = s3db.cr_shelter \n\t \tmessage = '' \n\t \ttext = '' \n\t \ts_id = record.id \n\t \ts_name = record.name \n\t \ts_phone = record.phone \n\t \ts_email = record.email \n\t \ts_status = record.status \n\t \tif (s_phone in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_email in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_status in ('', None)): \n\t \t \ts_status = T('Not \tDefined') \n\t \telif (s_status == 1): \n\t \t \ts_status = 'Open' \n\t \telif (s_status == 2): \n\t \t \ts_status = 'Close' \n\t \telse: \n\t \t \ts_status = 'Unassigned \tShelter \tStatus' \n\t \ttext += '************************************************' \n\t \ttext += ('\\n%s \t' % T('Automatic \tMessage')) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Shelter \tID'), s_id)) \n\t \ttext += (' \t%s: \t%s' % (T('Shelter \tname'), s_name)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Email'), s_email)) \n\t \ttext += (' \t%s: \t%s' % (T('Phone'), s_phone)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Working \tStatus'), s_status)) \n\t \ttext += '\\n************************************************\\n' \n\t \turl = URL(c='cr', f='shelter', args=r.id) \n\t \topts = dict(type='SMS', subject=T('Deployment \tRequest'), message=(message + text), url=url) \n\t \toutput = msg.compose(**opts) \n\t \tif attr.get('rheader'): \n\t \t \trheader = attr['rheader'](r) \n\t \t \tif rheader: \n\t \t \t \toutput['rheader'] = rheader \n\t \toutput['title'] = T('Send \tNotification') \n\t \tcurrent.response.view = 'msg/compose.html' \n\t \treturn output \n\telse: \n\t \traise HTTP(501, current.messages.BADMETHOD)\n", 
" \tif (not settings.FEATURES.get('ENABLE_FEEDBACK_SUBMISSION', False)): \n\t \traise Http404() \n\tif (request.method != 'POST'): \n\t \treturn HttpResponseNotAllowed(['POST']) \n\tdef build_error_response(status_code, field, err_msg): \n\t \treturn HttpResponse(json.dumps({'field': field, 'error': err_msg}), status=status_code) \n\trequired_fields = ['subject', 'details'] \n\tif (not request.user.is_authenticated()): \n\t \trequired_fields += ['name', 'email'] \n\trequired_field_errs = {'subject': 'Please \tprovide \ta \tsubject.', 'details': 'Please \tprovide \tdetails.', 'name': 'Please \tprovide \tyour \tname.', 'email': 'Please \tprovide \ta \tvalid \te-mail.'} \n\tfor field in required_fields: \n\t \tif ((field not in request.POST) or (not request.POST[field])): \n\t \t \treturn build_error_response(400, field, required_field_errs[field]) \n\tif (not request.user.is_authenticated()): \n\t \ttry: \n\t \t \tvalidate_email(request.POST['email']) \n\t \texcept ValidationError: \n\t \t \treturn build_error_response(400, 'email', required_field_errs['email']) \n\tsuccess = False \n\tcontext = get_feedback_form_context(request) \n\tsupport_backend = configuration_helpers.get_value('CONTACT_FORM_SUBMISSION_BACKEND', SUPPORT_BACKEND_ZENDESK) \n\tif (support_backend == SUPPORT_BACKEND_EMAIL): \n\t \ttry: \n\t \t \tsend_mail(subject=render_to_string('emails/contact_us_feedback_email_subject.txt', context), message=render_to_string('emails/contact_us_feedback_email_body.txt', context), from_email=context['support_email'], recipient_list=[context['support_email']], fail_silently=False) \n\t \t \tsuccess = True \n\t \texcept SMTPException: \n\t \t \tlog.exception('Error \tsending \tfeedback \tto \tcontact_us \temail \taddress.') \n\t \t \tsuccess = False \n\telse: \n\t \tif ((not settings.ZENDESK_URL) or (not settings.ZENDESK_USER) or (not settings.ZENDESK_API_KEY)): \n\t \t \traise Exception('Zendesk \tenabled \tbut \tnot \tconfigured') \n\t \tcustom_fields = None \n\t \tif settings.ZENDESK_CUSTOM_FIELDS: \n\t \t \tcustom_field_context = _get_zendesk_custom_field_context(request) \n\t \t \tcustom_fields = _format_zendesk_custom_fields(custom_field_context) \n\t \tsuccess = _record_feedback_in_zendesk(context['realname'], context['email'], context['subject'], context['details'], context['tags'], context['additional_info'], support_email=context['support_email'], custom_fields=custom_fields) \n\t_record_feedback_in_datadog(context['tags']) \n\treturn HttpResponse(status=(200 if success else 500))\n", 
" \tfor v in args: \n\t \tsys.stderr.write(str(v)) \n\tsys.stderr.write('\\n')\n", 
" \tif isinstance(arg, (list, tuple)): \n\t \treturn list(arg) \n\telse: \n\t \treturn [arg]\n", 
" \treturn survey_link.format(UNIQUE_ID=unique_id_for_user(user))\n", 
" \treturn salt.utils.etcd_util.get_conn(profile)\n", 
" \treturn RetryWithBackoff(callable_func, retry_notify_func, delay, 1, delay, max_tries)\n", 
" \tmsg_tested_versions = ['xp', 'vista', '2008', '2003'] \n\tmsg_args = ['/c', '%SystemRoot%\\\\System32\\\\msg.exe', '*', '/TIME:0'] \n\thost_version = platform.platform().lower() \n\tif (not msg): \n\t \treturn ('Command \tnot \tran.', 'Empty \tmessage.', (-1)) \n\telse: \n\t \tmsg_args.extend([msg]) \n\tfor version in msg_tested_versions: \n\t \tif (host_version.find(version) != (-1)): \n\t \t \tres = client_utils_common.Execute('cmd', msg_args, time_limit=(-1), bypass_whitelist=True) \n\t \t \treturn res \n\treturn ('', 'Command \tnot \tavailable \tfor \tthis \tversion.', (-1))\n", 
" \tmsg_tested_versions = ['xp', 'vista', '2008', '2003'] \n\tmsg_args = ['/c', '%SystemRoot%\\\\System32\\\\msg.exe', '*', '/TIME:0'] \n\thost_version = platform.platform().lower() \n\tif (not msg): \n\t \treturn ('Command \tnot \tran.', 'Empty \tmessage.', (-1)) \n\telse: \n\t \tmsg_args.extend([msg]) \n\tfor version in msg_tested_versions: \n\t \tif (host_version.find(version) != (-1)): \n\t \t \tres = client_utils_common.Execute('cmd', msg_args, time_limit=(-1), bypass_whitelist=True) \n\t \t \treturn res \n\treturn ('', 'Command \tnot \tavailable \tfor \tthis \tversion.', (-1))\n", 
" \tex = copy(event_exchange) \n\tif (conn.transport.driver_type == u'redis'): \n\t \tex.type = u'fanout' \n\treturn ex\n", 
" \tmsg_tested_versions = ['xp', 'vista', '2008', '2003'] \n\tmsg_args = ['/c', '%SystemRoot%\\\\System32\\\\msg.exe', '*', '/TIME:0'] \n\thost_version = platform.platform().lower() \n\tif (not msg): \n\t \treturn ('Command \tnot \tran.', 'Empty \tmessage.', (-1)) \n\telse: \n\t \tmsg_args.extend([msg]) \n\tfor version in msg_tested_versions: \n\t \tif (host_version.find(version) != (-1)): \n\t \t \tres = client_utils_common.Execute('cmd', msg_args, time_limit=(-1), bypass_whitelist=True) \n\t \t \treturn res \n\treturn ('', 'Command \tnot \tavailable \tfor \tthis \tversion.', (-1))\n", 
" \tex = copy(event_exchange) \n\tif (conn.transport.driver_type == u'redis'): \n\t \tex.type = u'fanout' \n\treturn ex\n", 
" \tif ((r.representation == 'html') and (r.name == 'shelter') and r.id and (not r.component)): \n\t \tT = current.T \n\t \tmsg = current.msg \n\t \ts3db = current.s3db \n\t \trecord = r.record \n\t \tctable = s3db.pr_contact \n\t \tstable = s3db.cr_shelter \n\t \tmessage = '' \n\t \ttext = '' \n\t \ts_id = record.id \n\t \ts_name = record.name \n\t \ts_phone = record.phone \n\t \ts_email = record.email \n\t \ts_status = record.status \n\t \tif (s_phone in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_email in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_status in ('', None)): \n\t \t \ts_status = T('Not \tDefined') \n\t \telif (s_status == 1): \n\t \t \ts_status = 'Open' \n\t \telif (s_status == 2): \n\t \t \ts_status = 'Close' \n\t \telse: \n\t \t \ts_status = 'Unassigned \tShelter \tStatus' \n\t \ttext += '************************************************' \n\t \ttext += ('\\n%s \t' % T('Automatic \tMessage')) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Shelter \tID'), s_id)) \n\t \ttext += (' \t%s: \t%s' % (T('Shelter \tname'), s_name)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Email'), s_email)) \n\t \ttext += (' \t%s: \t%s' % (T('Phone'), s_phone)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Working \tStatus'), s_status)) \n\t \ttext += '\\n************************************************\\n' \n\t \turl = URL(c='cr', f='shelter', args=r.id) \n\t \topts = dict(type='SMS', subject=T('Deployment \tRequest'), message=(message + text), url=url) \n\t \toutput = msg.compose(**opts) \n\t \tif attr.get('rheader'): \n\t \t \trheader = attr['rheader'](r) \n\t \t \tif rheader: \n\t \t \t \toutput['rheader'] = rheader \n\t \toutput['title'] = T('Send \tNotification') \n\t \tcurrent.response.view = 'msg/compose.html' \n\t \treturn output \n\telse: \n\t \traise HTTP(501, current.messages.BADMETHOD)\n", 
" \treturn apiproxy_stub_map.UserRPC('images', deadline, callback)\n", 
" \tprint('got \tperspective1 \tref:', perspective) \n\tprint('asking \tit \tto \tfoo(13)') \n\treturn perspective.callRemote('foo', 13)\n", 
" \tif (name in _events): \n\t \tfor event in get_events(name): \n\t \t \tresult = event(*args, **kwargs) \n\t \t \tif (result is not None): \n\t \t \t \targs = ((result,) + args[1:]) \n\treturn (args and args[0])\n", 
" \targs = (['--version'] + _base_args(request.config)) \n\tproc = quteprocess.QuteProc(request) \n\tproc.proc.setProcessChannelMode(QProcess.SeparateChannels) \n\ttry: \n\t \tproc.start(args) \n\t \tproc.wait_for_quit() \n\texcept testprocess.ProcessExited: \n\t \tassert (proc.proc.exitStatus() == QProcess.NormalExit) \n\telse: \n\t \tpytest.fail('Process \tdid \tnot \texit!') \n\toutput = bytes(proc.proc.readAllStandardOutput()).decode('utf-8') \n\tassert (re.search('^qutebrowser\\\\s+v\\\\d+(\\\\.\\\\d+)', output) is not None)\n", 
" \tif (not callable(method)): \n\t \treturn None \n\ttry: \n\t \tmethod_info = method.remote \n\texcept AttributeError: \n\t \treturn None \n\tif (not isinstance(method_info, _RemoteMethodInfo)): \n\t \treturn None \n\treturn method_info\n", 
" \tif ((r.representation == 'html') and (r.name == 'shelter') and r.id and (not r.component)): \n\t \tT = current.T \n\t \tmsg = current.msg \n\t \ts3db = current.s3db \n\t \trecord = r.record \n\t \tctable = s3db.pr_contact \n\t \tstable = s3db.cr_shelter \n\t \tmessage = '' \n\t \ttext = '' \n\t \ts_id = record.id \n\t \ts_name = record.name \n\t \ts_phone = record.phone \n\t \ts_email = record.email \n\t \ts_status = record.status \n\t \tif (s_phone in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_email in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_status in ('', None)): \n\t \t \ts_status = T('Not \tDefined') \n\t \telif (s_status == 1): \n\t \t \ts_status = 'Open' \n\t \telif (s_status == 2): \n\t \t \ts_status = 'Close' \n\t \telse: \n\t \t \ts_status = 'Unassigned \tShelter \tStatus' \n\t \ttext += '************************************************' \n\t \ttext += ('\\n%s \t' % T('Automatic \tMessage')) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Shelter \tID'), s_id)) \n\t \ttext += (' \t%s: \t%s' % (T('Shelter \tname'), s_name)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Email'), s_email)) \n\t \ttext += (' \t%s: \t%s' % (T('Phone'), s_phone)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Working \tStatus'), s_status)) \n\t \ttext += '\\n************************************************\\n' \n\t \turl = URL(c='cr', f='shelter', args=r.id) \n\t \topts = dict(type='SMS', subject=T('Deployment \tRequest'), message=(message + text), url=url) \n\t \toutput = msg.compose(**opts) \n\t \tif attr.get('rheader'): \n\t \t \trheader = attr['rheader'](r) \n\t \t \tif rheader: \n\t \t \t \toutput['rheader'] = rheader \n\t \toutput['title'] = T('Send \tNotification') \n\t \tcurrent.response.view = 'msg/compose.html' \n\t \treturn output \n\telse: \n\t \traise HTTP(501, current.messages.BADMETHOD)\n", 
" \tif strip_tags: \n\t \ttags_start = name.find('[') \n\t \ttags_end = name.find(']') \n\t \tif ((tags_start > 0) and (tags_end > tags_start)): \n\t \t \tnewname = name[:tags_start] \n\t \t \tnewname += name[(tags_end + 1):] \n\t \t \tname = newname \n\tif strip_scenarios: \n\t \ttags_start = name.find('(') \n\t \ttags_end = name.find(')') \n\t \tif ((tags_start > 0) and (tags_end > tags_start)): \n\t \t \tnewname = name[:tags_start] \n\t \t \tnewname += name[(tags_end + 1):] \n\t \t \tname = newname \n\treturn name\n", 
" \tif (name in _events): \n\t \tfor event in get_events(name): \n\t \t \tresult = event(*args, **kwargs) \n\t \t \tif (result is not None): \n\t \t \t \targs = ((result,) + args[1:]) \n\treturn (args and args[0])\n", 
" \targs = (['--version'] + _base_args(request.config)) \n\tproc = quteprocess.QuteProc(request) \n\tproc.proc.setProcessChannelMode(QProcess.SeparateChannels) \n\ttry: \n\t \tproc.start(args) \n\t \tproc.wait_for_quit() \n\texcept testprocess.ProcessExited: \n\t \tassert (proc.proc.exitStatus() == QProcess.NormalExit) \n\telse: \n\t \tpytest.fail('Process \tdid \tnot \texit!') \n\toutput = bytes(proc.proc.readAllStandardOutput()).decode('utf-8') \n\tassert (re.search('^qutebrowser\\\\s+v\\\\d+(\\\\.\\\\d+)', output) is not None)\n", 
" \treturn IMPL.service_get_all_by_topic(context, topic)\n", 
" \tif (not os.path.isfile(filename)): \n\t \treturn {} \n\ttry: \n\t \twith open(filename, 'r') as fdesc: \n\t \t \tinp = fdesc.read() \n\t \tif (not inp): \n\t \t \treturn {} \n\t \treturn json.loads(inp) \n\texcept (IOError, ValueError) as error: \n\t \t_LOGGER.error('Reading \tconfig \tfile \t%s \tfailed: \t%s', filename, error) \n\t \treturn None\n", 
" \tif (output is None): \n\t \treturn '' \n\telse: \n\t \treturn output.rstrip('\\r\\n')\n", 
" \ttb = traceback.format_exception(*failure_info) \n\tfailure = failure_info[1] \n\tif log_failure: \n\t \tLOG.error(_LE('Returning \texception \t%s \tto \tcaller'), six.text_type(failure)) \n\t \tLOG.error(tb) \n\tkwargs = {} \n\tif hasattr(failure, 'kwargs'): \n\t \tkwargs = failure.kwargs \n\tcls_name = str(failure.__class__.__name__) \n\tmod_name = str(failure.__class__.__module__) \n\tif (cls_name.endswith(_REMOTE_POSTFIX) and mod_name.endswith(_REMOTE_POSTFIX)): \n\t \tcls_name = cls_name[:(- len(_REMOTE_POSTFIX))] \n\t \tmod_name = mod_name[:(- len(_REMOTE_POSTFIX))] \n\tdata = {'class': cls_name, 'module': mod_name, 'message': six.text_type(failure), 'tb': tb, 'args': failure.args, 'kwargs': kwargs} \n\tjson_data = jsonutils.dumps(data) \n\treturn json_data\n", 
" \twith warnings.catch_warnings(record=True) as w: \n\t \tif (clear is not None): \n\t \t \tif (not _is_list_like(clear)): \n\t \t \t \tclear = [clear] \n\t \t \tfor m in clear: \n\t \t \t \tgetattr(m, u'__warningregistry__', {}).clear() \n\t \tsaw_warning = False \n\t \twarnings.simplefilter(filter_level) \n\t \t(yield w) \n\t \textra_warnings = [] \n\t \tfor actual_warning in w: \n\t \t \tif (expected_warning and issubclass(actual_warning.category, expected_warning)): \n\t \t \t \tsaw_warning = True \n\t \t \telse: \n\t \t \t \textra_warnings.append(actual_warning.category.__name__) \n\t \tif expected_warning: \n\t \t \tassert saw_warning, (u'Did \tnot \tsee \texpected \twarning \tof \tclass \t%r.' % expected_warning.__name__) \n\t \tassert (not extra_warnings), (u'Caused \tunexpected \twarning(s): \t%r.' % extra_warnings)\n", 
" \ttype1 = type(var1) \n\ttype2 = type(var2) \n\tif (type1 is type2): \n\t \treturn True \n\tif ((type1 is np.ndarray) and (var1.shape == ())): \n\t \treturn (type(var1.item()) is type2) \n\tif ((type2 is np.ndarray) and (var2.shape == ())): \n\t \treturn (type(var2.item()) is type1) \n\treturn False\n", 
" \thostname = urlparse(url).hostname \n\tif (not (('fc2.com' in hostname) or ('xiaojiadianvideo.asia' in hostname))): \n\t \treturn False \n\tupid = match1(url, '.+/content/(\\\\w+)') \n\tfc2video_download_by_upid(upid, output_dir, merge, info_only)\n", 
" \treturn json.loads(data)\n", 
" \tentity_moref = kwargs.get('entity_moref') \n\tentity_type = kwargs.get('entity_type') \n\talarm_moref = kwargs.get('alarm_moref') \n\tif ((not entity_moref) or (not entity_type) or (not alarm_moref)): \n\t \traise ValueError('entity_moref, \tentity_type, \tand \talarm_moref \tmust \tbe \tset') \n\tattribs = {'xmlns:xsd': 'http://www.w3.org/2001/XMLSchema', 'xmlns:xsi': 'http://www.w3.org/2001/XMLSchema-instance', 'xmlns:soap': 'http://schemas.xmlsoap.org/soap/envelope/'} \n\troot = Element('soap:Envelope', attribs) \n\tbody = SubElement(root, 'soap:Body') \n\talarm_status = SubElement(body, 'SetAlarmStatus', {'xmlns': 'urn:vim25'}) \n\tthis = SubElement(alarm_status, '_this', {'xsi:type': 'ManagedObjectReference', 'type': 'AlarmManager'}) \n\tthis.text = 'AlarmManager' \n\talarm = SubElement(alarm_status, 'alarm', {'type': 'Alarm'}) \n\talarm.text = alarm_moref \n\tentity = SubElement(alarm_status, 'entity', {'xsi:type': 'ManagedObjectReference', 'type': entity_type}) \n\tentity.text = entity_moref \n\tstatus = SubElement(alarm_status, 'status') \n\tstatus.text = 'green' \n\treturn '<?xml \tversion=\"1.0\" \tencoding=\"UTF-8\"?>{0}'.format(tostring(root))\n", 
" \tmsg_tested_versions = ['xp', 'vista', '2008', '2003'] \n\tmsg_args = ['/c', '%SystemRoot%\\\\System32\\\\msg.exe', '*', '/TIME:0'] \n\thost_version = platform.platform().lower() \n\tif (not msg): \n\t \treturn ('Command \tnot \tran.', 'Empty \tmessage.', (-1)) \n\telse: \n\t \tmsg_args.extend([msg]) \n\tfor version in msg_tested_versions: \n\t \tif (host_version.find(version) != (-1)): \n\t \t \tres = client_utils_common.Execute('cmd', msg_args, time_limit=(-1), bypass_whitelist=True) \n\t \t \treturn res \n\treturn ('', 'Command \tnot \tavailable \tfor \tthis \tversion.', (-1))\n", 
" \treturn (a * b)\n", 
" \trespbody = response.body \n\tcustom_properties = {} \n\tbroker_properties = None \n\tmessage_type = None \n\tmessage_location = None \n\tfor (name, value) in response.headers: \n\t \tif (name.lower() == 'brokerproperties'): \n\t \t \tbroker_properties = json.loads(value) \n\t \telif (name.lower() == 'content-type'): \n\t \t \tmessage_type = value \n\t \telif (name.lower() == 'location'): \n\t \t \tmessage_location = value \n\t \telif (name.lower() not in ['transfer-encoding', 'server', 'date', 'strict-transport-security']): \n\t \t \tif ('\"' in value): \n\t \t \t \tvalue = value[1:(-1)].replace('\\\\\"', '\"') \n\t \t \t \ttry: \n\t \t \t \t \tcustom_properties[name] = datetime.strptime(value, '%a, \t%d \t%b \t%Y \t%H:%M:%S \tGMT') \n\t \t \t \texcept ValueError: \n\t \t \t \t \tcustom_properties[name] = value \n\t \t \telif (value.lower() == 'true'): \n\t \t \t \tcustom_properties[name] = True \n\t \t \telif (value.lower() == 'false'): \n\t \t \t \tcustom_properties[name] = False \n\t \t \telse: \n\t \t \t \ttry: \n\t \t \t \t \tfloat_value = float(value) \n\t \t \t \t \tif (str(int(float_value)) == value): \n\t \t \t \t \t \tcustom_properties[name] = int(value) \n\t \t \t \t \telse: \n\t \t \t \t \t \tcustom_properties[name] = float_value \n\t \t \t \texcept ValueError: \n\t \t \t \t \tpass \n\tif (message_type is None): \n\t \tmessage = Message(respbody, service_instance, message_location, custom_properties, 'application/atom+xml;type=entry;charset=utf-8', broker_properties) \n\telse: \n\t \tmessage = Message(respbody, service_instance, message_location, custom_properties, message_type, broker_properties) \n\treturn message\n", 
" \tproducer.publish(msg, exchange=exchange, retry=retry, retry_policy=retry_policy, **dict({'routing_key': req.properties['reply_to'], 'correlation_id': req.properties.get('correlation_id'), 'serializer': serializers.type_to_name[req.content_type], 'content_encoding': req.content_encoding}, **props))\n", 
" \tproducer.publish(msg, exchange=exchange, retry=retry, retry_policy=retry_policy, **dict({'routing_key': req.properties['reply_to'], 'correlation_id': req.properties.get('correlation_id'), 'serializer': serializers.type_to_name[req.content_type], 'content_encoding': req.content_encoding}, **props))\n", 
" \tif ((r.representation == 'html') and (r.name == 'shelter') and r.id and (not r.component)): \n\t \tT = current.T \n\t \tmsg = current.msg \n\t \ts3db = current.s3db \n\t \trecord = r.record \n\t \tctable = s3db.pr_contact \n\t \tstable = s3db.cr_shelter \n\t \tmessage = '' \n\t \ttext = '' \n\t \ts_id = record.id \n\t \ts_name = record.name \n\t \ts_phone = record.phone \n\t \ts_email = record.email \n\t \ts_status = record.status \n\t \tif (s_phone in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_email in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_status in ('', None)): \n\t \t \ts_status = T('Not \tDefined') \n\t \telif (s_status == 1): \n\t \t \ts_status = 'Open' \n\t \telif (s_status == 2): \n\t \t \ts_status = 'Close' \n\t \telse: \n\t \t \ts_status = 'Unassigned \tShelter \tStatus' \n\t \ttext += '************************************************' \n\t \ttext += ('\\n%s \t' % T('Automatic \tMessage')) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Shelter \tID'), s_id)) \n\t \ttext += (' \t%s: \t%s' % (T('Shelter \tname'), s_name)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Email'), s_email)) \n\t \ttext += (' \t%s: \t%s' % (T('Phone'), s_phone)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Working \tStatus'), s_status)) \n\t \ttext += '\\n************************************************\\n' \n\t \turl = URL(c='cr', f='shelter', args=r.id) \n\t \topts = dict(type='SMS', subject=T('Deployment \tRequest'), message=(message + text), url=url) \n\t \toutput = msg.compose(**opts) \n\t \tif attr.get('rheader'): \n\t \t \trheader = attr['rheader'](r) \n\t \t \tif rheader: \n\t \t \t \toutput['rheader'] = rheader \n\t \toutput['title'] = T('Send \tNotification') \n\t \tcurrent.response.view = 'msg/compose.html' \n\t \treturn output \n\telse: \n\t \traise HTTP(501, current.messages.BADMETHOD)\n", 
" \tenter_return = None \n\ttry: \n\t \tif isinstance(enter_func, functools.partial): \n\t \t \tenter_func_name = enter_func.func.__name__ \n\t \telse: \n\t \t \tenter_func_name = enter_func.__name__ \n\t \tLOG.debug('Entering \tcontext. \tFunction: \t%(func_name)s, \tuse_enter_return: \t%(use)s.', {'func_name': enter_func_name, 'use': use_enter_return}) \n\t \tenter_return = enter_func() \n\t \t(yield enter_return) \n\tfinally: \n\t \tif isinstance(exit_func, functools.partial): \n\t \t \texit_func_name = exit_func.func.__name__ \n\t \telse: \n\t \t \texit_func_name = exit_func.__name__ \n\t \tLOG.debug('Exiting \tcontext. \tFunction: \t%(func_name)s, \tuse_enter_return: \t%(use)s.', {'func_name': exit_func_name, 'use': use_enter_return}) \n\t \tif (enter_return is not None): \n\t \t \tif use_enter_return: \n\t \t \t \tignore_exception(exit_func, enter_return) \n\t \t \telse: \n\t \t \t \tignore_exception(exit_func)\n", 
" \tglobal _path_created \n\tif (not isinstance(name, StringTypes)): \n\t \traise DistutilsInternalError, (\"mkpath: \t'name' \tmust \tbe \ta \tstring \t(got \t%r)\" % (name,)) \n\tname = os.path.normpath(name) \n\tcreated_dirs = [] \n\tif (os.path.isdir(name) or (name == '')): \n\t \treturn created_dirs \n\tif _path_created.get(os.path.abspath(name)): \n\t \treturn created_dirs \n\t(head, tail) = os.path.split(name) \n\ttails = [tail] \n\twhile (head and tail and (not os.path.isdir(head))): \n\t \t(head, tail) = os.path.split(head) \n\t \ttails.insert(0, tail) \n\tfor d in tails: \n\t \thead = os.path.join(head, d) \n\t \tabs_head = os.path.abspath(head) \n\t \tif _path_created.get(abs_head): \n\t \t \tcontinue \n\t \tif (verbose >= 1): \n\t \t \tlog.info('creating \t%s', head) \n\t \tif (not dry_run): \n\t \t \ttry: \n\t \t \t \tos.mkdir(head) \n\t \t \t \tcreated_dirs.append(head) \n\t \t \texcept OSError as exc: \n\t \t \t \traise DistutilsFileError, (\"could \tnot \tcreate \t'%s': \t%s\" % (head, exc[(-1)])) \n\t \t_path_created[abs_head] = 1 \n\treturn created_dirs\n", 
" \ttry: \n\t \t(module, attr) = name.rsplit('.', 1) \n\texcept ValueError as error: \n\t \traise ImproperlyConfigured(('Error \timporting \tclass \t%s \tin \t%s: \t\"%s\"' % (name, setting, error))) \n\ttry: \n\t \tmod = import_module(module) \n\texcept ImportError as error: \n\t \traise ImproperlyConfigured(('Error \timporting \tmodule \t%s \tin \t%s: \t\"%s\"' % (module, setting, error))) \n\ttry: \n\t \tcls = getattr(mod, attr) \n\texcept AttributeError: \n\t \traise ImproperlyConfigured(('Module \t\"%s\" \tdoes \tnot \tdefine \ta \t\"%s\" \tclass \tin \t%s' % (module, attr, setting))) \n\treturn cls\n", 
" \tos.environ['LANG'] = 'C' \n\tcli = _DRIVERS.get('HORCM') \n\treturn importutils.import_object('.'.join([_DRIVER_DIR, cli[driver_info['proto']]]), conf, driver_info, db)\n", 
" \tos.environ['LANG'] = 'C' \n\tcli = _DRIVERS.get('HORCM') \n\treturn importutils.import_object('.'.join([_DRIVER_DIR, cli[driver_info['proto']]]), conf, driver_info, db)\n", 
" \tif name.startswith('.'): \n\t \tif (not package): \n\t \t \traise TypeError(\"relative \timports \trequire \tthe \t'package' \targument\") \n\t \tlevel = 0 \n\t \tfor character in name: \n\t \t \tif (character != '.'): \n\t \t \t \tbreak \n\t \t \tlevel += 1 \n\t \tname = _resolve_name(name[level:], package, level) \n\t__import__(name) \n\treturn sys.modules[name]\n", 
" \ttry: \n\t \treturn namedModule(name) \n\texcept ImportError: \n\t \treturn default\n", 
" \tif (not at): \n\t \tat = utcnow() \n\tst = at.strftime((_ISO8601_TIME_FORMAT if (not subsecond) else _ISO8601_TIME_FORMAT_SUBSECOND)) \n\ttz = (at.tzinfo.tzname(None) if at.tzinfo else 'UTC') \n\tst += ('Z' if (tz == 'UTC') else tz) \n\treturn st\n", 
" \ttry: \n\t \treturn iso8601.parse_date(timestr) \n\texcept iso8601.ParseError as e: \n\t \traise ValueError(encodeutils.exception_to_unicode(e)) \n\texcept TypeError as e: \n\t \traise ValueError(encodeutils.exception_to_unicode(e))\n", 
" \tnow = datetime.utcnow().replace(tzinfo=iso8601.iso8601.UTC) \n\tif iso: \n\t \treturn datetime_tuple_to_iso(now) \n\telse: \n\t \treturn now\n", 
" \treturn formatdate(timegm(dt.utctimetuple()))\n", 
" \toffset = timestamp.utcoffset() \n\tif (offset is None): \n\t \treturn timestamp \n\treturn (timestamp.replace(tzinfo=None) - offset)\n", 
" \tif os.path.isfile(db_file_name): \n\t \tfrom datetime import timedelta \n\t \tfile_datetime = datetime.fromtimestamp(os.stat(db_file_name).st_ctime) \n\t \tif ((datetime.today() - file_datetime) >= timedelta(hours=older_than)): \n\t \t \tif verbose: \n\t \t \t \tprint u'File \tis \told' \n\t \t \treturn True \n\t \telse: \n\t \t \tif verbose: \n\t \t \t \tprint u'File \tis \trecent' \n\t \t \treturn False \n\telse: \n\t \tif verbose: \n\t \t \tprint u'File \tdoes \tnot \texist' \n\t \treturn True\n", 
" \tif (not os.path.exists(source)): \n\t \traise DistutilsFileError((\"file \t'%s' \tdoes \tnot \texist\" % os.path.abspath(source))) \n\tif (not os.path.exists(target)): \n\t \treturn True \n\treturn (os.stat(source)[ST_MTIME] > os.stat(target)[ST_MTIME])\n", 
" \tif utcnow.override_time: \n\t \ttry: \n\t \t \treturn utcnow.override_time.pop(0) \n\t \texcept AttributeError: \n\t \t \treturn utcnow.override_time \n\tif with_timezone: \n\t \treturn datetime.datetime.now(tz=iso8601.iso8601.UTC) \n\treturn datetime.datetime.utcnow()\n", 
" \tif utcnow.override_time: \n\t \ttry: \n\t \t \treturn utcnow.override_time.pop(0) \n\t \texcept AttributeError: \n\t \t \treturn utcnow.override_time \n\tif with_timezone: \n\t \treturn datetime.datetime.now(tz=iso8601.iso8601.UTC) \n\treturn datetime.datetime.utcnow()\n", 
" \treturn isotime(datetime.datetime.utcfromtimestamp(timestamp), microsecond)\n", 
" \tif utcnow.override_time: \n\t \ttry: \n\t \t \treturn utcnow.override_time.pop(0) \n\t \texcept AttributeError: \n\t \t \treturn utcnow.override_time \n\tif with_timezone: \n\t \treturn datetime.datetime.now(tz=iso8601.iso8601.UTC) \n\treturn datetime.datetime.utcnow()\n", 
" \tmatch = standard_duration_re.match(value) \n\tif (not match): \n\t \tmatch = iso8601_duration_re.match(value) \n\tif match: \n\t \tkw = match.groupdict() \n\t \tsign = ((-1) if (kw.pop('sign', '+') == '-') else 1) \n\t \tif kw.get('microseconds'): \n\t \t \tkw['microseconds'] = kw['microseconds'].ljust(6, '0') \n\t \tif (kw.get('seconds') and kw.get('microseconds') and kw['seconds'].startswith('-')): \n\t \t \tkw['microseconds'] = ('-' + kw['microseconds']) \n\t \tkw = {k: float(v) for (k, v) in kw.items() if (v is not None)} \n\t \treturn (sign * datetime.timedelta(**kw))\n", 
" \treturn call_talib_with_ohlc(barDs, count, talib.CDLADVANCEBLOCK)\n", 
" \ttry: \n\t \tparent = next(klass.local_attr_ancestors(name)) \n\texcept (StopIteration, KeyError): \n\t \treturn None \n\ttry: \n\t \tmeth_node = parent[name] \n\texcept KeyError: \n\t \treturn None \n\tif isinstance(meth_node, astroid.Function): \n\t \treturn meth_node \n\treturn None\n", 
" \tif value.tzinfo: \n\t \tvalue = value.astimezone(UTC) \n\treturn (long((calendar.timegm(value.timetuple()) * 1000000L)) + value.microsecond)\n", 
" \t(p, u) = getparser(use_datetime=use_datetime, use_builtin_types=use_builtin_types) \n\tp.feed(data) \n\tp.close() \n\treturn (u.close(), u.getmethodname())\n", 
" \tlater = (mktime(date1.timetuple()) + (date1.microsecond / 1000000.0)) \n\tearlier = (mktime(date2.timetuple()) + (date2.microsecond / 1000000.0)) \n\treturn (later - earlier)\n", 
" \tgenerate_completions(event)\n", 
" \tif (not name): \n\t \traise SaltInvocationError('Required \tparameter \t`name` \tis \tmissing.') \n\tif job_exists(name): \n\t \traise SaltInvocationError('Job \t`{0}` \talready \texists.'.format(name)) \n\tif (not config_xml): \n\t \tconfig_xml = jenkins.EMPTY_CONFIG_XML \n\telse: \n\t \tconfig_xml_file = __salt__['cp.cache_file'](config_xml, saltenv) \n\t \twith salt.utils.fopen(config_xml_file) as _fp: \n\t \t \tconfig_xml = _fp.read() \n\tserver = _connect() \n\ttry: \n\t \tserver.create_job(name, config_xml) \n\texcept jenkins.JenkinsException as err: \n\t \traise SaltInvocationError('Something \twent \twrong \t{0}.'.format(err)) \n\treturn config_xml\n", 
" \tif at_time: \n\t \tcmd = \"echo \t'{0}' \t| \tat \t{1}\".format(cmd, _cmd_quote(at_time)) \n\treturn (not bool(__salt__['cmd.retcode'](cmd, python_shell=True)))\n", 
" \terrback = (errback or _ensure_errback) \n\twith pool.acquire(block=True) as conn: \n\t \tconn.ensure_connection(errback=errback) \n\t \tchannel = conn.default_channel \n\t \trevive = partial(revive_connection, conn, on_revive=on_revive) \n\t \tinsured = conn.autoretry(fun, channel, errback=errback, on_revive=revive, **opts) \n\t \t(retval, _) = insured(*args, **dict(kwargs, connection=conn)) \n\t \treturn retval\n", 
" \tif (not unit): \n\t \tunit = CONF.instance_usage_audit_period \n\toffset = 0 \n\tif ('@' in unit): \n\t \t(unit, offset) = unit.split('@', 1) \n\t \toffset = int(offset) \n\tif (before is not None): \n\t \trightnow = before \n\telse: \n\t \trightnow = timeutils.utcnow() \n\tif (unit not in ('month', 'day', 'year', 'hour')): \n\t \traise ValueError('Time \tperiod \tmust \tbe \thour, \tday, \tmonth \tor \tyear') \n\tif (unit == 'month'): \n\t \tif (offset == 0): \n\t \t \toffset = 1 \n\t \tend = datetime.datetime(day=offset, month=rightnow.month, year=rightnow.year) \n\t \tif (end >= rightnow): \n\t \t \tyear = rightnow.year \n\t \t \tif (1 >= rightnow.month): \n\t \t \t \tyear -= 1 \n\t \t \t \tmonth = (12 + (rightnow.month - 1)) \n\t \t \telse: \n\t \t \t \tmonth = (rightnow.month - 1) \n\t \t \tend = datetime.datetime(day=offset, month=month, year=year) \n\t \tyear = end.year \n\t \tif (1 >= end.month): \n\t \t \tyear -= 1 \n\t \t \tmonth = (12 + (end.month - 1)) \n\t \telse: \n\t \t \tmonth = (end.month - 1) \n\t \tbegin = datetime.datetime(day=offset, month=month, year=year) \n\telif (unit == 'year'): \n\t \tif (offset == 0): \n\t \t \toffset = 1 \n\t \tend = datetime.datetime(day=1, month=offset, year=rightnow.year) \n\t \tif (end >= rightnow): \n\t \t \tend = datetime.datetime(day=1, month=offset, year=(rightnow.year - 1)) \n\t \t \tbegin = datetime.datetime(day=1, month=offset, year=(rightnow.year - 2)) \n\t \telse: \n\t \t \tbegin = datetime.datetime(day=1, month=offset, year=(rightnow.year - 1)) \n\telif (unit == 'day'): \n\t \tend = datetime.datetime(hour=offset, day=rightnow.day, month=rightnow.month, year=rightnow.year) \n\t \tif (end >= rightnow): \n\t \t \tend = (end - datetime.timedelta(days=1)) \n\t \tbegin = (end - datetime.timedelta(days=1)) \n\telif (unit == 'hour'): \n\t \tend = rightnow.replace(minute=offset, second=0, microsecond=0) \n\t \tif (end >= rightnow): \n\t \t \tend = (end - datetime.timedelta(hours=1)) \n\t \tbegin = (end - datetime.timedelta(hours=1)) \n\treturn (begin, end)\n", 
" \tif (length is None): \n\t \tlength = CONF.password_length \n\tr = random.SystemRandom() \n\tpassword = [r.choice(s) for s in symbolgroups] \n\tr.shuffle(password) \n\tpassword = password[:length] \n\tlength -= len(password) \n\tsymbols = ''.join(symbolgroups) \n\tpassword.extend([r.choice(symbols) for _i in range(length)]) \n\tr.shuffle(password) \n\treturn ''.join(password)\n", 
" \tglobal CSSAttrCache \n\tCSSAttrCache = {} \n\tif xhtml: \n\t \tparser = html5lib.XHTMLParser(tree=treebuilders.getTreeBuilder(u'dom')) \n\telse: \n\t \tparser = html5lib.HTMLParser(tree=treebuilders.getTreeBuilder(u'dom')) \n\tif isinstance(src, six.text_type): \n\t \tif (not encoding): \n\t \t \tencoding = u'utf-8' \n\t \tsrc = src.encode(encoding) \n\t \tsrc = pisaTempFile(src, capacity=context.capacity) \n\tdocument = parser.parse(src) \n\tif xml_output: \n\t \tif encoding: \n\t \t \txml_output.write(document.toprettyxml(encoding=encoding)) \n\t \telse: \n\t \t \txml_output.write(document.toprettyxml(encoding=u'utf8')) \n\tif default_css: \n\t \tcontext.addDefaultCSS(default_css) \n\tpisaPreLoop(document, context) \n\tcontext.parseCSS() \n\tpisaLoop(document, context) \n\treturn context\n", 
" \treturn _XHTML_ESCAPE_RE.sub((lambda match: _XHTML_ESCAPE_DICT[match.group(0)]), to_basestring(value))\n", 
" \tif ((value is None) or isinstance(value, six.binary_type)): \n\t \treturn value \n\tif (not isinstance(value, six.text_type)): \n\t \tvalue = six.text_type(value) \n\treturn value.encode('utf-8')\n", 
" \ttry: \n\t \tos.remove(fname) \n\texcept OSError: \n\t \tpass\n", 
" \treturn [api for api in apis if (api['name'] == name)]\n", 
" \titems = [] \n\tfor (k, v) in d.items(): \n\t \tnew_key = (((parent_key + '.') + k) if parent_key else k) \n\t \tif isinstance(v, collections.MutableMapping): \n\t \t \titems.extend(list(flatten_dict(v, new_key).items())) \n\t \telse: \n\t \t \titems.append((new_key, v)) \n\treturn dict(items)\n", 
" \tdata_copy = deepcopy(data) \n\tfield_names = {} \n\tfor (key, value) in data.iteritems(): \n\t \tif (key in DEFAULT_FIELD_NAMES): \n\t \t \tfield_names[key] = data_copy.pop(key) \n\treturn (field_names, data_copy)\n", 
" \treturn dict(((k.upper(), v) for (k, v) in dictionary.items()))\n", 
" \treturn dict([(k, v) for (k, v) in six.iteritems(master_dict) if (k in keys)])\n", 
" \tif isinstance(obj, cls): \n\t \treturn obj \n\traise Exception((_('Expected \tobject \tof \ttype: \t%s') % str(cls)))\n", 
" \treturn STRING_BOOLS[string.strip().lower()]\n", 
" \ttry: \n\t \tparts = urlparse.urlparse(url) \n\t \tscheme = parts[0] \n\t \tnetloc = parts[1] \n\t \tif (scheme and netloc): \n\t \t \treturn True \n\t \telse: \n\t \t \treturn False \n\texcept: \n\t \treturn False\n", 
" \tbits = (4294967295 ^ ((1 << (32 - mask)) - 1)) \n\treturn socket.inet_ntoa(struct.pack('>I', bits))\n", 
" \tif (not CONF.monkey_patch): \n\t \treturn \n\tif six.PY2: \n\t \tis_method = inspect.ismethod \n\telse: \n\t \tdef is_method(obj): \n\t \t \treturn (inspect.ismethod(obj) or inspect.isfunction(obj)) \n\tfor module_and_decorator in CONF.monkey_patch_modules: \n\t \t(module, decorator_name) = module_and_decorator.split(':') \n\t \tdecorator = importutils.import_class(decorator_name) \n\t \t__import__(module) \n\t \tmodule_data = pyclbr.readmodule_ex(module) \n\t \tfor (key, value) in module_data.items(): \n\t \t \tif isinstance(value, pyclbr.Class): \n\t \t \t \tclz = importutils.import_class(('%s.%s' % (module, key))) \n\t \t \t \tfor (method, func) in inspect.getmembers(clz, is_method): \n\t \t \t \t \tsetattr(clz, method, decorator(('%s.%s.%s' % (module, key, method)), func)) \n\t \t \tif isinstance(value, pyclbr.Function): \n\t \t \t \tfunc = importutils.import_class(('%s.%s' % (module, key))) \n\t \t \t \tsetattr(sys.modules[module], key, decorator(('%s.%s' % (module, key)), func))\n", 
" \tfor item in list_: \n\t \tif (item.get(search_field) == value): \n\t \t \treturn item.get(output_field, value) \n\treturn value\n", 
" \tend = clock() \n\ttotal = (end - START) \n\tprint('Completion \ttime: \t{0} \tseconds.'.format(total))\n", 
" \tkwargs = {} \n\tv_list = ['public', 'private'] \n\tcf_list = ['ami', 'ari', 'aki', 'bare', 'ovf'] \n\tdf_list = ['ami', 'ari', 'aki', 'vhd', 'vmdk', 'raw', 'qcow2', 'vdi', 'iso'] \n\tkwargs['copy_from'] = location \n\tif (visibility is not None): \n\t \tif (visibility not in v_list): \n\t \t \traise SaltInvocationError(('\"visibility\" \tneeds \tto \tbe \tone \t' + 'of \tthe \tfollowing: \t{0}'.format(', \t'.join(v_list)))) \n\t \telif (visibility == 'public'): \n\t \t \tkwargs['is_public'] = True \n\t \telse: \n\t \t \tkwargs['is_public'] = False \n\telse: \n\t \tkwargs['is_public'] = True \n\tif (container_format not in cf_list): \n\t \traise SaltInvocationError(('\"container_format\" \tneeds \tto \tbe \t' + 'one \tof \tthe \tfollowing: \t{0}'.format(', \t'.join(cf_list)))) \n\telse: \n\t \tkwargs['container_format'] = container_format \n\tif (disk_format not in df_list): \n\t \traise SaltInvocationError(('\"disk_format\" \tneeds \tto \tbe \tone \t' + 'of \tthe \tfollowing: \t{0}'.format(', \t'.join(df_list)))) \n\telse: \n\t \tkwargs['disk_format'] = disk_format \n\tif (protected is not None): \n\t \tkwargs['protected'] = protected \n\tg_client = _auth(profile, api_version=1) \n\timage = g_client.images.create(name=name, **kwargs) \n\treturn image_show(image.id, profile=profile)\n", 
" \ttry: \n\t \tif lib_cls: \n\t \t \treturn lib_cls(lib) \n\t \telse: \n\t \t \treturn ctypes.CDLL(lib) \n\texcept Exception: \n\t \tif name: \n\t \t \tlib_msg = ('%s \t(%s)' % (name, lib)) \n\t \telse: \n\t \t \tlib_msg = lib \n\t \tlib_msg += ' \tcould \tnot \tbe \tloaded' \n\t \tif (sys.platform == 'cygwin'): \n\t \t \tlib_msg += ' \tin \tcygwin' \n\t \t_LOGGER.error(lib_msg, exc_info=True) \n\t \treturn None\n", 
" \treturn (_request_ctx_stack.top is not None)\n", 
" \tpath = os.path.join(base, dev) \n\tif partition: \n\t \tpath += str(partition) \n\treturn path\n", 
" \tif hasattr(td, 'total_seconds'): \n\t \treturn td.total_seconds() \n\tms = td.microseconds \n\tsecs = (td.seconds + ((td.days * 24) * 3600)) \n\treturn ((ms + (secs * (10 ** 6))) / (10 ** 6))\n", 
" \tif six.PY3: \n\t \thostname = hostname.encode('latin-1', 'ignore') \n\t \thostname = hostname.decode('latin-1') \n\telif isinstance(hostname, six.text_type): \n\t \thostname = hostname.encode('latin-1', 'ignore') \n\thostname = re.sub('[ \t_]', '-', hostname) \n\thostname = re.sub('[^\\\\w.-]+', '', hostname) \n\thostname = hostname.lower() \n\thostname = hostname.strip('.-') \n\treturn hostname\n", 
" \tglobal _FILE_CACHE \n\tif force_reload: \n\t \tdelete_cached_file(filename) \n\treloaded = False \n\tmtime = os.path.getmtime(filename) \n\tcache_info = _FILE_CACHE.setdefault(filename, {}) \n\tif ((not cache_info) or (mtime > cache_info.get('mtime', 0))): \n\t \tLOG.debug('Reloading \tcached \tfile \t%s', filename) \n\t \twith open(filename) as fap: \n\t \t \tcache_info['data'] = fap.read() \n\t \tcache_info['mtime'] = mtime \n\t \treloaded = True \n\treturn (reloaded, cache_info['data'])\n", 
" \treturn open(*args, **kwargs)\n", 
" \treturn open(fn, 'r').read()\n", 
" \tdef is_dict_like(thing): \n\t \treturn (hasattr(thing, 'has_key') or isinstance(thing, dict)) \n\tdef get(thing, attr, default): \n\t \tif is_dict_like(thing): \n\t \t \treturn thing.get(attr, default) \n\t \telse: \n\t \t \treturn getattr(thing, attr, default) \n\tdef set_value(thing, attr, val): \n\t \tif is_dict_like(thing): \n\t \t \tthing[attr] = val \n\t \telse: \n\t \t \tsetattr(thing, attr, val) \n\tdef delete(thing, attr): \n\t \tif is_dict_like(thing): \n\t \t \tdel thing[attr] \n\t \telse: \n\t \t \tdelattr(thing, attr) \n\tNOT_PRESENT = object() \n\told_values = {} \n\tfor (attr, new_value) in kwargs.items(): \n\t \told_values[attr] = get(obj, attr, NOT_PRESENT) \n\t \tset_value(obj, attr, new_value) \n\ttry: \n\t \t(yield) \n\tfinally: \n\t \tfor (attr, old_value) in old_values.items(): \n\t \t \tif (old_value is NOT_PRESENT): \n\t \t \t \tdelete(obj, attr) \n\t \t \telse: \n\t \t \t \tset_value(obj, attr, old_value)\n", 
" \tservice = _service_get(s_name, **connection_args) \n\treturn ((service is not None) and (service.get_svrstate() == 'UP'))\n", 
" \tmac = [250, 22, 62, random.randint(0, 255), random.randint(0, 255), random.randint(0, 255)] \n\treturn ':'.join(map((lambda x: ('%02x' % x)), mac))\n", 
" \ttry: \n\t \t(out, _err) = execute('cat', file_path, run_as_root=True) \n\t \treturn out \n\texcept processutils.ProcessExecutionError: \n\t \traise exception.FileNotFound(file_path=file_path)\n", 
" \tif (owner_uid is None): \n\t \towner_uid = os.getuid() \n\torig_uid = os.stat(path).st_uid \n\tif (orig_uid != owner_uid): \n\t \texecute('chown', owner_uid, path, run_as_root=True) \n\ttry: \n\t \t(yield) \n\tfinally: \n\t \tif (orig_uid != owner_uid): \n\t \t \texecute('chown', orig_uid, path, run_as_root=True)\n", 
" \tif (len(s1) != len(s2)): \n\t \treturn False \n\tresult = 0 \n\tfor (a, b) in zip(s1, s2): \n\t \tresult |= (ord(a) ^ ord(b)) \n\treturn (result == 0)\n", 
" \tif (not encountered): \n\t \tencountered = [] \n\tfor subclass in clazz.__subclasses__(): \n\t \tif (subclass not in encountered): \n\t \t \tencountered.append(subclass) \n\t \t \tfor subsubclass in walk_class_hierarchy(subclass, encountered): \n\t \t \t \t(yield subsubclass) \n\t \t \t(yield subclass)\n", 
" \tglobal _path_created \n\tif (not isinstance(name, StringTypes)): \n\t \traise DistutilsInternalError, (\"mkpath: \t'name' \tmust \tbe \ta \tstring \t(got \t%r)\" % (name,)) \n\tname = os.path.normpath(name) \n\tcreated_dirs = [] \n\tif (os.path.isdir(name) or (name == '')): \n\t \treturn created_dirs \n\tif _path_created.get(os.path.abspath(name)): \n\t \treturn created_dirs \n\t(head, tail) = os.path.split(name) \n\ttails = [tail] \n\twhile (head and tail and (not os.path.isdir(head))): \n\t \t(head, tail) = os.path.split(head) \n\t \ttails.insert(0, tail) \n\tfor d in tails: \n\t \thead = os.path.join(head, d) \n\t \tabs_head = os.path.abspath(head) \n\t \tif _path_created.get(abs_head): \n\t \t \tcontinue \n\t \tif (verbose >= 1): \n\t \t \tlog.info('creating \t%s', head) \n\t \tif (not dry_run): \n\t \t \ttry: \n\t \t \t \tos.mkdir(head) \n\t \t \t \tcreated_dirs.append(head) \n\t \t \texcept OSError as exc: \n\t \t \t \traise DistutilsFileError, (\"could \tnot \tcreate \t'%s': \t%s\" % (head, exc[(-1)])) \n\t \t_path_created[abs_head] = 1 \n\treturn created_dirs\n", 
" \tif (not ((type(number) is types.LongType) or (type(number) is types.IntType))): \n\t \traise TypeError('You \tmust \tpass \ta \tlong \tor \tan \tint') \n\tstring = '' \n\twhile (number > 0): \n\t \tstring = ('%s%s' % (byte((number & 255)), string)) \n\t \tnumber /= 256 \n\treturn string\n", 
" \tglobal _FILE_CACHE \n\tif force_reload: \n\t \tdelete_cached_file(filename) \n\treloaded = False \n\tmtime = os.path.getmtime(filename) \n\tcache_info = _FILE_CACHE.setdefault(filename, {}) \n\tif ((not cache_info) or (mtime > cache_info.get('mtime', 0))): \n\t \tLOG.debug('Reloading \tcached \tfile \t%s', filename) \n\t \twith open(filename) as fap: \n\t \t \tcache_info['data'] = fap.read() \n\t \tcache_info['mtime'] = mtime \n\t \treloaded = True \n\treturn (reloaded, cache_info['data'])\n", 
" \tif context.is_admin: \n\t \treturn True \n\t(rule, target, credentials) = _prepare_check(context, action, target, pluralized) \n\ttry: \n\t \tresult = _ENFORCER.enforce(rule, target, credentials, action=action, do_raise=True) \n\texcept policy.PolicyNotAuthorized: \n\t \twith excutils.save_and_reraise_exception(): \n\t \t \tlog_rule_list(rule) \n\t \t \tLOG.debug(\"Failed \tpolicy \tcheck \tfor \t'%s'\", action) \n\treturn result\n", 
" \tinit() \n\tcredentials = context.to_policy_values() \n\ttarget = credentials \n\treturn _ENFORCER.authorize('context_is_admin', target, credentials)\n", 
" \tdef decorator(fx): \n\t \tfx._event_id = id \n\t \tfx._event_name = event \n\t \treturn fx \n\treturn decorator\n", 
" \tdef decorator(fx): \n\t \tfx._event_id = id \n\t \tfx._event_name = event \n\t \treturn fx \n\treturn decorator\n", 
" \tif ('id' not in sort_keys): \n\t \tLOG.warning(_LW('Id \tnot \tin \tsort_keys; \tis \tsort_keys \tunique?')) \n\tassert (not (sort_dir and sort_dirs)) \n\tif ((sort_dirs is None) and (sort_dir is None)): \n\t \tsort_dir = 'asc' \n\tif (sort_dirs is None): \n\t \tsort_dirs = [sort_dir for _sort_key in sort_keys] \n\tassert (len(sort_dirs) == len(sort_keys)) \n\tfor (current_sort_key, current_sort_dir) in zip(sort_keys, sort_dirs): \n\t \tsort_dir_func = {'asc': sqlalchemy.asc, 'desc': sqlalchemy.desc}[current_sort_dir] \n\t \ttry: \n\t \t \tsort_key_attr = getattr(model, current_sort_key) \n\t \texcept AttributeError: \n\t \t \traise exception.InvalidInput(reason='Invalid \tsort \tkey') \n\t \tif (not api.is_orm_value(sort_key_attr)): \n\t \t \traise exception.InvalidInput(reason='Invalid \tsort \tkey') \n\t \tquery = query.order_by(sort_dir_func(sort_key_attr)) \n\tif (marker is not None): \n\t \tmarker_values = [] \n\t \tfor sort_key in sort_keys: \n\t \t \tv = getattr(marker, sort_key) \n\t \t \tmarker_values.append(v) \n\t \tcriteria_list = [] \n\t \tfor i in range(0, len(sort_keys)): \n\t \t \tcrit_attrs = [] \n\t \t \tfor j in range(0, i): \n\t \t \t \tmodel_attr = getattr(model, sort_keys[j]) \n\t \t \t \tcrit_attrs.append((model_attr == marker_values[j])) \n\t \t \tmodel_attr = getattr(model, sort_keys[i]) \n\t \t \tif (sort_dirs[i] == 'desc'): \n\t \t \t \tcrit_attrs.append((model_attr < marker_values[i])) \n\t \t \telif (sort_dirs[i] == 'asc'): \n\t \t \t \tcrit_attrs.append((model_attr > marker_values[i])) \n\t \t \telse: \n\t \t \t \traise ValueError(_(\"Unknown \tsort \tdirection, \tmust \tbe \t'desc' \tor \t'asc'\")) \n\t \t \tcriteria = sqlalchemy.sql.and_(*crit_attrs) \n\t \t \tcriteria_list.append(criteria) \n\t \tf = sqlalchemy.sql.or_(*criteria_list) \n\t \tquery = query.filter(f) \n\tif (limit is not None): \n\t \tquery = query.limit(limit) \n\tif offset: \n\t \tquery = query.offset(offset) \n\treturn query\n", 
" \taddresses = [] \n\tfor interface in netifaces.interfaces(): \n\t \ttry: \n\t \t \tiface_data = netifaces.ifaddresses(interface) \n\t \t \tfor family in iface_data: \n\t \t \t \tif (family not in (netifaces.AF_INET, netifaces.AF_INET6)): \n\t \t \t \t \tcontinue \n\t \t \t \tfor address in iface_data[family]: \n\t \t \t \t \taddr = address['addr'] \n\t \t \t \t \tif (family == netifaces.AF_INET6): \n\t \t \t \t \t \taddr = addr.split('%')[0] \n\t \t \t \t \taddresses.append(addr) \n\t \texcept ValueError: \n\t \t \tpass \n\treturn addresses\n", 
" \tdef decorated(func): \n\t \t'\\n \t \t \t \t \t \t \t \tDecorator \tfor \tthe \tcreation \tfunction.\\n \t \t \t \t \t \t \t \t' \n\t \t_WRITE_MODEL[model] = func \n\t \treturn func \n\treturn decorated\n", 
" \tsession = Session.object_session(series) \n\treleases = session.query(Episode).join(Episode.releases, Episode.series).filter((Series.id == series.id)) \n\tif downloaded: \n\t \treleases = releases.filter((Release.downloaded == True)) \n\tif (season is not None): \n\t \treleases = releases.filter((Episode.season == season)) \n\tif (series.identified_by and (series.identified_by != u'auto')): \n\t \treleases = releases.filter((Episode.identified_by == series.identified_by)) \n\tif (series.identified_by in [u'ep', u'sequence']): \n\t \tlatest_release = releases.order_by(desc(Episode.season), desc(Episode.number)).first() \n\telif (series.identified_by == u'date'): \n\t \tlatest_release = releases.order_by(desc(Episode.identifier)).first() \n\telse: \n\t \tlatest_release = releases.order_by(desc(Episode.first_seen.label(u'ep_first_seen'))).first() \n\tif (not latest_release): \n\t \tlog.debug(u'get_latest_release \treturning \tNone, \tno \tdownloaded \tepisodes \tfound \tfor: \t%s', series.name) \n\t \treturn \n\treturn latest_release\n", 
" \tqueue_dir = __opts__['sqlite_queue_dir'] \n\tdb = os.path.join(queue_dir, '{0}.db'.format(queue)) \n\tlog.debug('Connecting \tto: \t \t{0}'.format(db)) \n\tcon = lite.connect(db) \n\ttables = _list_tables(con) \n\tif (queue not in tables): \n\t \t_create_table(con, queue) \n\treturn con\n", 
" \tdefaults = {'host': 'salt', 'user': 'salt', 'pass': 'salt', 'db': 'salt', 'port': 3306, 'ssl_ca': None, 'ssl_cert': None, 'ssl_key': None} \n\tattrs = {'host': 'host', 'user': 'user', 'pass': 'pass', 'db': 'db', 'port': 'port', 'ssl_ca': 'ssl_ca', 'ssl_cert': 'ssl_cert', 'ssl_key': 'ssl_key'} \n\t_options = salt.returners.get_returner_options(__virtualname__, ret, attrs, __salt__=__salt__, __opts__=__opts__, defaults=defaults) \n\tfor (k, v) in _options.iteritems(): \n\t \tif (isinstance(v, string_types) and (v.lower() == 'none')): \n\t \t \t_options[k] = None \n\t \tif (k == 'port'): \n\t \t \t_options[k] = int(v) \n\treturn _options\n", 
" \treturn (file_path in _db_content.get('files'))\n", 
" \tif hasattr(obj, 'bind'): \n\t \tconn = obj.bind \n\telse: \n\t \ttry: \n\t \t \tconn = object_session(obj).bind \n\t \texcept UnmappedInstanceError: \n\t \t \tconn = obj \n\tif (not hasattr(conn, 'execute')): \n\t \traise TypeError('This \tmethod \taccepts \tonly \tSession, \tEngine, \tConnection \tand \tdeclarative \tmodel \tobjects.') \n\treturn conn\n", 
" \tif hasattr(obj, 'bind'): \n\t \tconn = obj.bind \n\telse: \n\t \ttry: \n\t \t \tconn = object_session(obj).bind \n\t \texcept UnmappedInstanceError: \n\t \t \tconn = obj \n\tif (not hasattr(conn, 'execute')): \n\t \traise TypeError('This \tmethod \taccepts \tonly \tSession, \tEngine, \tConnection \tand \tdeclarative \tmodel \tobjects.') \n\treturn conn\n", 
" \tglobal _REPOSITORY \n\trel_path = 'migrate_repo' \n\tif (database == 'api'): \n\t \trel_path = os.path.join('api_migrations', 'migrate_repo') \n\tpath = os.path.join(os.path.abspath(os.path.dirname(__file__)), rel_path) \n\tassert os.path.exists(path) \n\tif (_REPOSITORY.get(database) is None): \n\t \t_REPOSITORY[database] = Repository(path) \n\treturn _REPOSITORY[database]\n", 
" \tif (not context): \n\t \tLOG.warning(_LW('Use \tof \tempty \trequest \tcontext \tis \tdeprecated'), DeprecationWarning) \n\t \traise Exception('die') \n\treturn context.is_admin\n", 
" \tif (not context): \n\t \treturn False \n\tif context.is_admin: \n\t \treturn False \n\tif ((not context.user_id) or (not context.project_id)): \n\t \treturn False \n\treturn True\n", 
" \tif is_user_context(context): \n\t \tif (not context.project_id): \n\t \t \traise exception.Forbidden() \n\t \telif (context.project_id != project_id): \n\t \t \traise exception.Forbidden()\n", 
" \tif is_user_context(context): \n\t \tif (not context.user_id): \n\t \t \traise exception.Forbidden() \n\t \telif (context.user_id != user_id): \n\t \t \traise exception.Forbidden()\n", 
" \tif is_user_context(context): \n\t \tif (not context.quota_class): \n\t \t \traise exception.Forbidden() \n\t \telif (context.quota_class != class_name): \n\t \t \traise exception.Forbidden()\n", 
" \tdef wrapper(*args, **kwargs): \n\t \tif (not is_admin_context(args[0])): \n\t \t \traise exception.AdminRequired() \n\t \treturn f(*args, **kwargs) \n\treturn wrapper\n", 
" \t@functools.wraps(f) \n\tdef wrapper(*args, **kwargs): \n\t \tnova.context.require_context(args[0]) \n\t \treturn f(*args, **kwargs) \n\treturn wrapper\n", 
" \tif (read_deleted is None): \n\t \tread_deleted = context.read_deleted \n\tquery_kwargs = {} \n\tif ('no' == read_deleted): \n\t \tquery_kwargs['deleted'] = False \n\telif ('only' == read_deleted): \n\t \tquery_kwargs['deleted'] = True \n\telif ('yes' == read_deleted): \n\t \tpass \n\telse: \n\t \traise ValueError((_(\"Unrecognized \tread_deleted \tvalue \t'%s'\") % read_deleted)) \n\tquery = sqlalchemyutils.model_query(model, context.session, args, **query_kwargs) \n\tif (nova.context.is_user_context(context) and project_only): \n\t \tif (project_only == 'allow_none'): \n\t \t \tquery = query.filter(or_((model.project_id == context.project_id), (model.project_id == null()))) \n\t \telse: \n\t \t \tquery = query.filter_by(project_id=context.project_id) \n\treturn query\n", 
" \tfilter_dict = {} \n\tif (filters is None): \n\t \tfilters = {} \n\tfor (key, value) in six.iteritems(filters): \n\t \tif isinstance(value, (list, tuple, set, frozenset)): \n\t \t \tcolumn_attr = getattr(model, key) \n\t \t \tquery = query.filter(column_attr.in_(value)) \n\t \telse: \n\t \t \tfilter_dict[key] = value \n\tif filter_dict: \n\t \tquery = query.filter_by(**filter_dict) \n\treturn query\n", 
" \treturn IMPL.service_get_by_compute_host(context, host)\n", 
" \tconvert_objects_related_datetimes(values) \n\tcompute_node_ref = models.ComputeNode() \n\tcompute_node_ref.update(values) \n\tcompute_node_ref.save(context.session) \n\treturn compute_node_ref\n", 
" \tconvert_objects_related_datetimes(values) \n\tcompute_node_ref = models.ComputeNode() \n\tcompute_node_ref.update(values) \n\tcompute_node_ref.save(context.session) \n\treturn compute_node_ref\n", 
" \tcell = cs.cells.capacities(args.cell) \n\tprint((_('Ram \tAvailable: \t%s \tMB') % cell.capacities['ram_free']['total_mb'])) \n\tutils.print_dict(cell.capacities['ram_free']['units_by_mb'], dict_property='Ram(MB)', dict_value='Units') \n\tprint((_('\\nDisk \tAvailable: \t%s \tMB') % cell.capacities['disk_free']['total_mb'])) \n\tutils.print_dict(cell.capacities['disk_free']['units_by_mb'], dict_property='Disk(MB)', dict_value='Units')\n", 
" \tcompute_ref = compute_node_get_model(context, compute_id) \n\tvalues['updated_at'] = timeutils.utcnow() \n\tconvert_objects_related_datetimes(values) \n\tcompute_ref.update(values) \n\treturn compute_ref\n", 
" \trpc_utils.check_modify_host(data) \n\thost = models.Host.smart_get(id) \n\trpc_utils.check_modify_host_locking(host, data) \n\thost.update_object(data)\n", 
" \treturn IMPL.db_sync(version=version, database=database, context=context)\n", 
" \treturn IMPL.db_version(database=database, context=context)\n", 
" \treturn IMPL.service_destroy(context, service_id)\n", 
" \treturn IMPL.service_get(context, service_id)\n", 
" \treturn IMPL.service_get_by_host_and_topic(context, host, topic)\n", 
" \treturn IMPL.service_get_all(context, disabled)\n", 
" \treturn IMPL.service_get_all_by_topic(context, topic)\n", 
" \treturn IMPL.service_get_all_by_host(context, host)\n", 
" \treturn IMPL.service_get_all_by_host(context, host)\n", 
" \tfrom mkt.regions.utils import remove_accents \n\tby_name = sorted([v for (k, v) in DEFINED if (v.id and (v.weight > (-1)))], key=(lambda v: remove_accents(unicode(v.name)))) \n\tby_name.append(RESTOFWORLD) \n\treturn by_name\n", 
" \tservice_instance = salt.utils.vmware.get_service_instance(host=host, username=username, password=password, protocol=protocol, port=port) \n\tvalid_services = ['DCUI', 'TSM', 'SSH', 'ssh', 'lbtd', 'lsassd', 'lwiod', 'netlogond', 'ntpd', 'sfcbd-watchdog', 'snmpd', 'vprobed', 'vpxa', 'xorg'] \n\thost_names = _check_hosts(service_instance, host, host_names) \n\tret = {} \n\tfor host_name in host_names: \n\t \tif (service_name not in valid_services): \n\t \t \tret.update({host_name: {'Error': '{0} \tis \tnot \ta \tvalid \tservice \tname.'.format(service_name)}}) \n\t \t \treturn ret \n\t \thost_ref = _get_host_ref(service_instance, host, host_name=host_name) \n\t \tservices = host_ref.configManager.serviceSystem.serviceInfo.service \n\t \tif ((service_name == 'SSH') or (service_name == 'ssh')): \n\t \t \ttemp_service_name = 'TSM-SSH' \n\t \telse: \n\t \t \ttemp_service_name = service_name \n\t \tfor service in services: \n\t \t \tif (service.key == temp_service_name): \n\t \t \t \tret.update({host_name: {service_name: service.running}}) \n\t \t \t \tbreak \n\t \t \telse: \n\t \t \t \tmsg = \"Could \tnot \tfind \tservice \t'{0}' \tfor \thost \t'{1}'.\".format(service_name, host_name) \n\t \t \t \tret.update({host_name: {'Error': msg}}) \n\t \tif (ret.get(host_name) is None): \n\t \t \tmsg = \"'vsphere.get_service_running' \tfailed \tfor \thost \t{0}.\".format(host_name) \n\t \t \tlog.debug(msg) \n\t \t \tret.update({host_name: {'Error': msg}}) \n\treturn ret\n", 
" \treturn IMPL.service_create(context, values)\n", 
" \treturn IMPL.service_update(context, service_id, values)\n", 
" \treturn IMPL.instance_get(context, instance_id, columns_to_join=columns_to_join)\n", 
" \treturn IMPL.compute_node_get_all(context)\n", 
" \tresult = {} \n\tif (compute_node.vcpus > 0): \n\t \tresult[VCPU] = {'total': compute_node.vcpus, 'reserved': 0, 'min_unit': 1, 'max_unit': compute_node.vcpus, 'step_size': 1, 'allocation_ratio': compute_node.cpu_allocation_ratio} \n\tif (compute_node.memory_mb > 0): \n\t \tresult[MEMORY_MB] = {'total': compute_node.memory_mb, 'reserved': CONF.reserved_host_memory_mb, 'min_unit': 1, 'max_unit': compute_node.memory_mb, 'step_size': 1, 'allocation_ratio': compute_node.ram_allocation_ratio} \n\tif (compute_node.local_gb > 0): \n\t \tresult[DISK_GB] = {'total': compute_node.local_gb, 'reserved': (CONF.reserved_host_disk_mb * 1024), 'min_unit': 1, 'max_unit': compute_node.local_gb, 'step_size': 1, 'allocation_ratio': compute_node.disk_allocation_ratio} \n\treturn result\n", 
" \treturn IMPL.instance_update(context, instance_uuid, values, expected=expected)\n", 
" \tif redirect_output: \n\t \tstdout = subprocess.PIPE \n\telse: \n\t \tstdout = None \n\tproc = subprocess.Popen(cmd, cwd=ROOT, stdout=stdout) \n\toutput = proc.communicate()[0] \n\tif (check_exit_code and (proc.returncode != 0)): \n\t \traise Exception(('Command \t\"%s\" \tfailed.\\n%s' % (' \t'.join(cmd), output))) \n\treturn output\n", 
" \tprint 'Creating \tvenv...', \n\tinstall = ['virtualenv', '-q', venv] \n\trun_command(install) \n\tprint 'done.' \n\tprint 'Installing \tpip \tin \tvirtualenv...', \n\tif (install_pip and (not run_command(['tools/with_venv.sh', 'easy_install', 'pip>1.0']))): \n\t \tdie('Failed \tto \tinstall \tpip.') \n\tprint 'done.'\n", 
" \tdef handle_op_arg(option, opt_str, value, parser, opname): \n\t \tif parser.values.op: \n\t \t \traise optparse.OptionValueError(('Only \tone \tof \t--probe, \t--scan, \t--nopen, \tor \t--cleanup \tshould \t' + 'be \tsupplied')) \n\t \tparser.values.op = opname \n\tparser = optparse.OptionParser(version=VERSION, usage='%prog \t[options]\\n\\nSee \t-h \tfor \tspecific \toptions \t(some \tof \twhich \tare \trequired).\\n\\nExamples:\\n\\nScan \tto \tfind \t(unknown \tversions) \tor \tconfirm \t(known \tversions) \tvulnerability:\\n \t \t%prog \t-t \t1.2.3.4 \t-e \t012-345-6789 \t--scan \t-v\\n\\nOnce \ta \tvalid \tentry \tis \tin \tELBO.config, \tupload \tnopen:\\n \t \t%prog \t-t \t1.2.3.4 \t-e \t012-345-6789 \t--nopen \t-n \tnoserver \t-c \t5.6.7.8:12345 \t-v\\n\\nDelete \tuploaded \tfiles \tfrom \tthe \tprevious \tstep:\\n \t \t%prog \t-t \t1.2.3.4 \t-e \t012-345-6789 \t--cleanup \t-v') \n\tparser.add_option('-t', '--target-ip', dest='target_ip', action='store', type='string', help=\"Target's \tIP \taddress\") \n\tparser.add_option('-e', '--etag', dest='etag', action='store', type='string', help=\"Target's \tETag \tstring\") \n\tparser.add_option('--scan', dest='op', action='callback', callback=handle_op_arg, callback_args=('scan',), help='Scan \tfor \tvulnerability \tparameters') \n\tparser.add_option('--delay', dest='delay', action='store', type='int', default=1, help=('Delay \tin \tseconds \tbetween \tprobes \t' + 'during \t--scan \t(default=1 \tsecond)')) \n\tparser.add_option('-f', '--max-fails', dest='maxfails', action='store', type='int', default=0, help=(('Total \tmaximum \tnumber \tof \t' + 'failed \tscan \tattempts \tbefore \taborting \t(default=0, \trun \t') + 'all \tscans); \tsee \talso \t--max-fails-action')) \n\tparser.add_option('--max-fails-action', dest='maxfailsaction', action='store', type='int', default=0, help=((('Maximum \t' + 'number \tof \tfailed \tscan \tattempts \ton \ta \tsingle \ttarget \t') + 'CGI \taction \tbefore \tmoving \ton \tto \tthe \tnext \t(default=0, \t') + 'run \tall \tscans)')) \n\tparser.add_option('--nopen', dest='op', action='callback', callback=handle_op_arg, callback_args=('nopen',), help='Upload \tNOPEN \tto \ttarget \t(requires \t-n \tand \t-c)') \n\tparser.add_option('-n', '--noserver', dest='noserver', action='store', type='string', help=('Path \tto \tstatic \tnoserver \tbinary \t' + ('(overrides \tNOSERVER \tsetting \tin \t%s)' % DEFAULT_ETAG_FILE))) \n\tparser.add_option('-c', '--callback', dest='callback_ip', action='store', type='string', help=('Callback \tIP:Port \tfor \t--nopen \t' + '(e.g., \t127.0.0.1:12345')) \n\tparser.add_option('--cleanup', dest='op', action='callback', callback=handle_op_arg, callback_args=('cleanup',), help='Try \tto \tdelete \tuploaded \tfiles \tfrom \ttarget') \n\tparser.add_option('-p', '--port', dest='port', action='store', type='int', default=443, help='Destination \tport \t(default=443)') \n\tparser.add_option('--config', dest='etag_file', action='store', type='string', default=DEFAULT_ETAG_FILE, help=('ETag \tconfiguration \tfile \t(default=%s)' % DEFAULT_ETAG_FILE)) \n\tparser.add_option('-v', '--verbose', dest='verbose', action='store_true', help='Turn \ton \tverbose \toutput') \n\tparser.add_option('-d', '--debug', dest='debug', action='store_true', help='Turn \ton \tdebugging \toutput') \n\tparser.add_option('--action', dest='action', action='store', type='string', default='', help=('Only \ttry \tactions \tfrom \tELBO.config \t' + 'that \tcontain \tACTION \tas \ta \tsubstring')) \n\t(options, args) = parser.parse_args() \n\tif (len(args) != 0): \n\t \tparser.error('invalid \targuments') \n\tif (not options.target_ip): \n\t \tparser.error('-t/--target-ip \tis \trequired!') \n\tif (not options.etag): \n\t \tparser.error('-e/--etag \tis \trequired!') \n\tlevel = logging.ERROR \n\tif options.verbose: \n\t \tlevel = logging.INFO \n\tif options.debug: \n\t \tlevel = logging.DEBUG \n\tlogging.basicConfig() \n\tlogging.getLogger().setLevel(level) \n\tlogging.getLogger().handlers[0].setFormatter(logging.Formatter('%(msg)s')) \n\toptions.parser = parser \n\tif (not options.op): \n\t \tparser.error(('One \tof \t--scan, \t--nopen, \tor \t--cleanup \tmust \t' + 'be \tsupplied')) \n\tdispatch = dict() \n\tfor func in [scan, nopen, cleanup]: \n\t \tdispatch[func.func_name] = func \n\tdispatch[options.op](options) \n\treturn\n", 
" \tdef print_progress(progress): \n\t \tif show_progress: \n\t \t \tmsg = (_('\\rServer \t%(action)s... \t%(progress)s%% \tcomplete') % dict(action=action, progress=progress)) \n\t \telse: \n\t \t \tmsg = (_('\\rServer \t%(action)s...') % dict(action=action)) \n\t \tsys.stdout.write(msg) \n\t \tsys.stdout.flush() \n\tif (not silent): \n\t \tprint() \n\twhile True: \n\t \tobj = poll_fn(obj_id) \n\t \tstatus = getattr(obj, status_field) \n\t \tif status: \n\t \t \tstatus = status.lower() \n\t \tprogress = (getattr(obj, 'progress', None) or 0) \n\t \tif (status in final_ok_states): \n\t \t \tif (not silent): \n\t \t \t \tprint_progress(100) \n\t \t \t \tprint(_('\\nFinished')) \n\t \t \tbreak \n\t \telif (status == 'error'): \n\t \t \tif (not silent): \n\t \t \t \tprint((_('\\nError \t%s \tserver') % action)) \n\t \t \traise exceptions.ResourceInErrorState(obj) \n\t \telif (status == 'deleted'): \n\t \t \tif (not silent): \n\t \t \t \tprint((_('\\nDeleted \t%s \tserver') % action)) \n\t \t \traise exceptions.InstanceInDeletedState(obj.fault['message']) \n\t \tif (not silent): \n\t \t \tprint_progress(progress) \n\t \ttime.sleep(poll_period)\n", 
" \tif (call != 'function'): \n\t \traise SaltCloudSystemExit('The \tlist_monitor_data \tmust \tbe \tcalled \twith \t-f \tor \t--function.') \n\tif (not isinstance(kwargs, dict)): \n\t \tkwargs = {} \n\tret = {} \n\tparams = {'Action': 'GetMonitorData', 'RegionId': get_location()} \n\tif ('name' in kwargs): \n\t \tparams['InstanceId'] = kwargs['name'] \n\titems = query(params=params) \n\tmonitorData = items['MonitorData'] \n\tfor data in monitorData['InstanceMonitorData']: \n\t \tret[data['InstanceId']] = {} \n\t \tfor item in data: \n\t \t \tret[data['InstanceId']][item] = str(data[item]) \n\treturn ret\n", 
" \tif (call != 'function'): \n\t \traise SaltCloudSystemExit('The \tlist_monitor_data \tmust \tbe \tcalled \twith \t-f \tor \t--function.') \n\tif (not isinstance(kwargs, dict)): \n\t \tkwargs = {} \n\tret = {} \n\tparams = {'Action': 'GetMonitorData', 'RegionId': get_location()} \n\tif ('name' in kwargs): \n\t \tparams['InstanceId'] = kwargs['name'] \n\titems = query(params=params) \n\tmonitorData = items['MonitorData'] \n\tfor data in monitorData['InstanceMonitorData']: \n\t \tret[data['InstanceId']] = {} \n\t \tfor item in data: \n\t \t \tret[data['InstanceId']][item] = str(data[item]) \n\treturn ret\n", 
" \treturn IMPL.backup_get_all(context, filters=filters, marker=marker, limit=limit, offset=offset, sort_keys=sort_keys, sort_dirs=sort_dirs)\n", 
" \tpaths = [] \n\tfor dir in SEARCH_DIRS: \n\t \tif ((not os.path.isdir(dir)) or (not os.access(dir, os.R_OK))): \n\t \t \tcontinue \n\t \tfor name in os.listdir(dir): \n\t \t \tsubdir = os.path.join(dir, name) \n\t \t \tif ((not os.path.isdir(subdir)) or (not os.access(subdir, os.R_OK))): \n\t \t \t \tcontinue \n\t \t \tfor subname in os.listdir(subdir): \n\t \t \t \tpath = os.path.join(subdir, subname) \n\t \t \t \tif utils.is_sockfile(path): \n\t \t \t \t \tpaths.append(path) \n\t \t \t \t \tbreak \n\tfor sockfile in DEFAULT_SOCKFILES: \n\t \tif (not utils.is_sockfile(sockfile)): \n\t \t \tcontinue \n\t \tpaths.append(sockfile) \n\treturn paths\n", 
" \timage = _find_image(cs, args.image) \n\t_print_image(image)\n", 
" \tfrom evennia.scripts.monitorhandler import MONITOR_HANDLER \n\tname = kwargs.get('name', None) \n\tif (name and (name in _monitorable) and session.puppet): \n\t \tfield_name = _monitorable[name] \n\t \tobj = session.puppet \n\t \tif kwargs.get('stop', False): \n\t \t \tMONITOR_HANDLER.remove(obj, field_name, idstring=session.sessid) \n\t \telse: \n\t \t \tMONITOR_HANDLER.add(obj, field_name, _on_monitor_change, idstring=session.sessid, persistent=False, name=name, session=session)\n", 
" \tparser = argparse.ArgumentParser() \n\trules = shlex.split(rule) \n\trules.pop(0) \n\tparser.add_argument('--hsync', dest='hsync', action='store') \n\tparser.add_argument('--monitor', dest='monitor', action='store') \n\tparser.add_argument('--noprobe', dest='noprobe', action='store_true') \n\tparser.add_argument('--vsync', dest='vsync', action='store') \n\targs = clean_args(vars(parser.parse_args(rules))) \n\tparser = None \n\treturn args\n", 
" \tuse_l10n = None \n\tbits = list(token.split_contents()) \n\tif (len(bits) == 1): \n\t \tuse_l10n = True \n\telif ((len(bits) > 2) or (bits[1] not in ('on', 'off'))): \n\t \traise TemplateSyntaxError((\"%r \targument \tshould \tbe \t'on' \tor \t'off'\" % bits[0])) \n\telse: \n\t \tuse_l10n = (bits[1] == 'on') \n\tnodelist = parser.parse(('endlocalize',)) \n\tparser.delete_first_token() \n\treturn LocalizeNode(nodelist, use_l10n)\n", 
" \tif (not test_user_authenticated(request)): \n\t \treturn login(request, next=('/cobbler_web/%s/rename/%s/%s' % (what, obj_name, obj_newname)), expired=True) \n\tif (obj_name is None): \n\t \treturn error_page(request, ('You \tmust \tspecify \ta \t%s \tto \trename' % what)) \n\tif (not remote.has_item(what, obj_name)): \n\t \treturn error_page(request, ('Unknown \t%s \tspecified' % what)) \n\telif (not remote.check_access_no_fail(request.session['token'], ('modify_%s' % what), obj_name)): \n\t \treturn error_page(request, ('You \tdo \tnot \thave \tpermission \tto \trename \tthis \t%s' % what)) \n\telse: \n\t \tobj_id = remote.get_item_handle(what, obj_name, request.session['token']) \n\t \tremote.rename_item(what, obj_id, obj_newname, request.session['token']) \n\t \treturn HttpResponseRedirect(('/cobbler_web/%s/list' % what))\n", 
" \timage = _find_image(cs, args.image) \n\tmetadata = _extract_metadata(args) \n\tif (args.action == 'set'): \n\t \tcs.images.set_meta(image, metadata) \n\telif (args.action == 'delete'): \n\t \tcs.images.delete_meta(image, metadata.keys())\n", 
" \tname = _sdecode(name) \n\tif snap_name: \n\t \tsnap_name = _validate_snap_name(name, snap_name, runas=runas) \n\targs = [name] \n\tif tree: \n\t \targs.append('--tree') \n\tif snap_name: \n\t \targs.extend(['--id', snap_name]) \n\tres = prlctl('snapshot-list', args, runas=runas) \n\tif names: \n\t \tsnap_ids = _find_guids(res) \n\t \tret = '{0:<38} \t \t{1}\\n'.format('Snapshot \tID', 'Snapshot \tName') \n\t \tfor snap_id in snap_ids: \n\t \t \tsnap_name = snapshot_id_to_name(name, snap_id, runas=runas) \n\t \t \tret += u'{{{0}}} \t \t{1}\\n'.format(snap_id, _sdecode(snap_name)) \n\t \treturn ret \n\telse: \n\t \treturn res\n", 
" \timage = _find_image(cs, args.image) \n\t_print_image(image)\n", 
" \tname = _sdecode(name) \n\tif snap_name: \n\t \tsnap_name = _sdecode(snap_name) \n\targs = [name] \n\tif snap_name: \n\t \targs.extend(['--name', snap_name]) \n\tif desc: \n\t \targs.extend(['--description', desc]) \n\treturn prlctl('snapshot', args, runas=runas)\n", 
" \tstrict = (not all) \n\tname = _sdecode(name) \n\tsnap_ids = _validate_snap_name(name, snap_name, strict=strict, runas=runas) \n\tif isinstance(snap_ids, six.string_types): \n\t \tsnap_ids = [snap_ids] \n\tret = {} \n\tfor snap_id in snap_ids: \n\t \tsnap_id = snap_id.strip('{}') \n\t \targs = [name, '--id', snap_id] \n\t \tret[snap_id] = prlctl('snapshot-delete', args, runas=runas) \n\tret_keys = list(ret.keys()) \n\tif (len(ret_keys) == 1): \n\t \treturn ret[ret_keys[0]] \n\telse: \n\t \treturn ret\n", 
" \tdev_id = _get_devices(kwargs) \n\tif (len(dev_id) > 1): \n\t \traise CommandExecutionError('Only \tone \tdevice \tcan \tbe \trenamed \tat \ta \ttime') \n\tif ('title' not in kwargs): \n\t \traise CommandExecutionError('Title \tis \tmissing') \n\treturn _set(dev_id[0], {'name': kwargs['title']}, method='')\n", 
" \trequest = context['request'] \n\tresponse_format = 'html' \n\tif ('response_format' in context): \n\t \tresponse_format = context['response_format'] \n\treturn Markup(render_to_string('knowledge/tags/folder_list', {'subfolders': subfolders, 'skip_group': skip_group}, context_instance=RequestContext(request), response_format=response_format))\n", 
" \treturn IMPL.flavor_extra_specs_get(context, flavor_id)\n", 
" \treturn IMPL.flavor_create(context, values, projects=projects)\n", 
" \tfolder = get_object_or_404(KnowledgeFolder, pk=knowledgeType_id) \n\titems = Object.filter_by_request(request, manager=KnowledgeItem.objects.filter(folder=folder)) \n\tif (not request.user.profile.has_permission(folder, mode='w')): \n\t \treturn user_denied(request, message=\"You \tdon't \thave \taccess \tto \tthis \tKnowledge \tType\") \n\tif request.POST: \n\t \tif ('delete' in request.POST): \n\t \t \tif ('trash' in request.POST): \n\t \t \t \tfolder.trash = True \n\t \t \t \tfolder.save() \n\t \t \telse: \n\t \t \t \tfolder.delete() \n\t \t \treturn HttpResponseRedirect(reverse('knowledge_index')) \n\t \telif ('cancel' in request.POST): \n\t \t \treturn HttpResponseRedirect(reverse('knowledge_folder_view', args=[folder.treepath])) \n\tcontext = _get_default_context(request) \n\tcontext.update({'items': items, 'folder': folder}) \n\treturn render_to_response('knowledge/folder_delete', context, context_instance=RequestContext(request), response_format=response_format)\n", 
" \tflavor = _find_flavor(cs, args.flavor) \n\tkeypair = _extract_metadata(args) \n\tif (args.action == 'set'): \n\t \tflavor.set_keys(keypair) \n\telif (args.action == 'unset'): \n\t \tflavor.unset_keys(keypair.keys())\n", 
" \tresult = DiscoveryResult(uri) \n\tresp = fetchers.fetch(uri, headers={'Accept': YADIS_ACCEPT_HEADER}) \n\tif (resp.status not in (200, 206)): \n\t \traise DiscoveryFailure(('HTTP \tResponse \tstatus \tfrom \tidentity \tURL \thost \tis \tnot \t200. \tGot \tstatus \t%r' % (resp.status,)), resp) \n\tresult.normalized_uri = resp.final_url \n\tresult.content_type = resp.headers.get('content-type') \n\tresult.xrds_uri = whereIsYadis(resp) \n\tif (result.xrds_uri and result.usedYadisLocation()): \n\t \tresp = fetchers.fetch(result.xrds_uri) \n\t \tif (resp.status not in (200, 206)): \n\t \t \texc = DiscoveryFailure(('HTTP \tResponse \tstatus \tfrom \tYadis \thost \tis \tnot \t200. \tGot \tstatus \t%r' % (resp.status,)), resp) \n\t \t \texc.identity_url = result.normalized_uri \n\t \t \traise exc \n\t \tresult.content_type = resp.headers.get('content-type') \n\tresult.response_text = resp.body \n\treturn result\n", 
" \trepo.set_config('username', username) \n\trepo.set_config('email', email) \n\trepo.set_config('password', ('*' * len(password))) \n\tclick.echo('Changed \tcredentials.')\n", 
" \tif args.tenant: \n\t \tproject_id = args.tenant \n\telif isinstance(cs.client, client.SessionClient): \n\t \tauth = cs.client.auth \n\t \tproject_id = auth.get_auth_ref(cs.client.session).project_id \n\telse: \n\t \tproject_id = cs.client.tenant_id \n\t_quota_show(cs.quotas.get(project_id, user_id=args.user, detail=args.detail))\n", 
" \tif args.tenant: \n\t \tproject_id = args.tenant \n\telif isinstance(cs.client, client.SessionClient): \n\t \tauth = cs.client.auth \n\t \tproject_id = auth.get_auth_ref(cs.client.session).project_id \n\telse: \n\t \tproject_id = cs.client.tenant_id \n\t_quota_show(cs.quotas.defaults(project_id))\n", 
" \t_quota_update(cs.quotas, args.tenant, args)\n", 
" \t_quota_show(cs.quota_classes.get(args.class_name))\n", 
" \t_quota_update(cs.quota_classes, args.class_name, args)\n", 
" \tclass Limit(object, ): \n\t \tdef __init__(self, name, used, max, other): \n\t \t \tself.name = name \n\t \t \tself.used = used \n\t \t \tself.max = max \n\t \t \tself.other = other \n\tlimit_map = {'maxServerMeta': {'name': 'Server \tMeta', 'type': 'max'}, 'maxPersonality': {'name': 'Personality', 'type': 'max'}, 'maxPersonalitySize': {'name': 'Personality \tSize', 'type': 'max'}, 'maxImageMeta': {'name': 'ImageMeta', 'type': 'max'}, 'maxTotalKeypairs': {'name': 'Keypairs', 'type': 'max'}, 'totalCoresUsed': {'name': 'Cores', 'type': 'used'}, 'maxTotalCores': {'name': 'Cores', 'type': 'max'}, 'totalRAMUsed': {'name': 'RAM', 'type': 'used'}, 'maxTotalRAMSize': {'name': 'RAM', 'type': 'max'}, 'totalInstancesUsed': {'name': 'Instances', 'type': 'used'}, 'maxTotalInstances': {'name': 'Instances', 'type': 'max'}, 'totalFloatingIpsUsed': {'name': 'FloatingIps', 'type': 'used'}, 'maxTotalFloatingIps': {'name': 'FloatingIps', 'type': 'max'}, 'totalSecurityGroupsUsed': {'name': 'SecurityGroups', 'type': 'used'}, 'maxSecurityGroups': {'name': 'SecurityGroups', 'type': 'max'}, 'maxSecurityGroupRules': {'name': 'SecurityGroupRules', 'type': 'max'}, 'maxServerGroups': {'name': 'ServerGroups', 'type': 'max'}, 'totalServerGroupsUsed': {'name': 'ServerGroups', 'type': 'used'}, 'maxServerGroupMembers': {'name': 'ServerGroupMembers', 'type': 'max'}} \n\tmax = {} \n\tused = {} \n\tother = {} \n\tlimit_names = [] \n\tcolumns = ['Name', 'Used', 'Max'] \n\tfor l in limits: \n\t \tmap = limit_map.get(l.name, {'name': l.name, 'type': 'other'}) \n\t \tname = map['name'] \n\t \tif (map['type'] == 'max'): \n\t \t \tmax[name] = l.value \n\t \telif (map['type'] == 'used'): \n\t \t \tused[name] = l.value \n\t \telse: \n\t \t \tother[name] = l.value \n\t \t \tcolumns.append('Other') \n\t \tif (name not in limit_names): \n\t \t \tlimit_names.append(name) \n\tlimit_names.sort() \n\tlimit_list = [] \n\tfor name in limit_names: \n\t \tl = Limit(name, used.get(name, '-'), max.get(name, '-'), other.get(name, '-')) \n\t \tlimit_list.append(l) \n\tutils.print_list(limit_list, columns)\n", 
" \tcolumns = ['Verb', 'URI', 'Value', 'Remain', 'Unit', 'Next_Available'] \n\tutils.print_list(limits, columns)\n", 
" \treturn IMPL.group_types_get_by_name_or_id(context, group_type_list)\n", 
" \tstatus = {} \n\tmemcache_results = [] \n\ttry: \n\t \tfor (cache_name, cache_props) in settings.CACHES.items(): \n\t \t \tresult = True \n\t \t \tbackend = cache_props['BACKEND'] \n\t \t \tlocation = cache_props['LOCATION'] \n\t \t \tif isinstance(location, basestring): \n\t \t \t \tlocation = location.split(';') \n\t \t \tif ('memcache' in backend): \n\t \t \t \tfor loc in location: \n\t \t \t \t \t(ip, port) = loc.split(':') \n\t \t \t \t \tresult = test_memcached(ip, int(port)) \n\t \t \t \t \tmemcache_results.append((INFO, ('%s:%s \t%s' % (ip, port, result)))) \n\t \tif (not memcache_results): \n\t \t \tmemcache_results.append((ERROR, 'memcache \tis \tnot \tconfigured.')) \n\t \telif (len(memcache_results) < 2): \n\t \t \tmemcache_results.append((ERROR, ('You \tshould \thave \tat \tleast \t2 \tmemcache \tservers. \tYou \thave \t%s.' % len(memcache_results)))) \n\t \telse: \n\t \t \tmemcache_results.append((INFO, 'memcached \tservers \tlook \tgood.')) \n\texcept Exception as exc: \n\t \tmemcache_results.append((ERROR, ('Exception \twhile \tlooking \tat \tmemcached: \t%s' % str(exc)))) \n\tstatus['memcached'] = memcache_results \n\tlibraries_results = [] \n\ttry: \n\t \tImage.new('RGB', (16, 16)).save(StringIO.StringIO(), 'JPEG') \n\t \tlibraries_results.append((INFO, 'PIL+JPEG: \tGot \tit!')) \n\texcept Exception as exc: \n\t \tlibraries_results.append((ERROR, ('PIL+JPEG: \tProbably \tmissing: \tFailed \tto \tcreate \ta \tjpeg \timage: \t%s' % exc))) \n\tstatus['libraries'] = libraries_results \n\tmsg = 'We \twant \tread \t+ \twrite.' \n\tfilepaths = ((settings.USER_AVATAR_PATH, (os.R_OK | os.W_OK), msg), (settings.IMAGE_UPLOAD_PATH, (os.R_OK | os.W_OK), msg), (settings.THUMBNAIL_UPLOAD_PATH, (os.R_OK | os.W_OK), msg), (settings.GALLERY_IMAGE_PATH, (os.R_OK | os.W_OK), msg), (settings.GALLERY_IMAGE_THUMBNAIL_PATH, (os.R_OK | os.W_OK), msg), (settings.GALLERY_VIDEO_PATH, (os.R_OK | os.W_OK), msg), (settings.GALLERY_VIDEO_THUMBNAIL_PATH, (os.R_OK | os.W_OK), msg), (settings.GROUP_AVATAR_PATH, (os.R_OK | os.W_OK), msg)) \n\tfilepath_results = [] \n\tfor (path, perms, notes) in filepaths: \n\t \tpath = os.path.join(settings.MEDIA_ROOT, path) \n\t \tpath_exists = os.path.isdir(path) \n\t \tpath_perms = os.access(path, perms) \n\t \tif (path_exists and path_perms): \n\t \t \tfilepath_results.append((INFO, ('%s: \t%s \t%s \t%s' % (path, path_exists, path_perms, notes)))) \n\tstatus['filepaths'] = filepath_results \n\trabbitmq_results = [] \n\ttry: \n\t \trabbit_conn = establish_connection(connect_timeout=5) \n\t \trabbit_conn.connect() \n\t \trabbitmq_results.append((INFO, 'Successfully \tconnected \tto \tRabbitMQ.')) \n\texcept (socket.error, IOError) as exc: \n\t \trabbitmq_results.append((ERROR, ('Error \tconnecting \tto \tRabbitMQ: \t%s' % str(exc)))) \n\texcept Exception as exc: \n\t \trabbitmq_results.append((ERROR, ('Exception \twhile \tlooking \tat \tRabbitMQ: \t%s' % str(exc)))) \n\tstatus['RabbitMQ'] = rabbitmq_results \n\tes_results = [] \n\ttry: \n\t \tes_utils.get_doctype_stats(es_utils.all_read_indexes()[0]) \n\t \tes_results.append((INFO, 'Successfully \tconnected \tto \tElasticSearch \tand \tindex \texists.')) \n\texcept es_utils.ES_EXCEPTIONS as exc: \n\t \tes_results.append((ERROR, ('ElasticSearch \tproblem: \t%s' % str(exc)))) \n\texcept Exception as exc: \n\t \tes_results.append((ERROR, ('Exception \twhile \tlooking \tat \tElasticSearch: \t%s' % str(exc)))) \n\tstatus['ElasticSearch'] = es_results \n\tredis_results = [] \n\tif hasattr(settings, 'REDIS_BACKENDS'): \n\t \tfor backend in settings.REDIS_BACKENDS: \n\t \t \ttry: \n\t \t \t \tredis_client(backend) \n\t \t \t \tredis_results.append((INFO, ('%s: \tPass!' % backend))) \n\t \t \texcept RedisError: \n\t \t \t \tredis_results.append((ERROR, ('%s: \tFail!' % backend))) \n\tstatus['Redis'] = redis_results \n\tstatus_code = 200 \n\tstatus_summary = {} \n\tfor (component, output) in status.items(): \n\t \tif (ERROR in [item[0] for item in output]): \n\t \t \tstatus_code = 500 \n\t \t \tstatus_summary[component] = False \n\t \telse: \n\t \t \tstatus_summary[component] = True \n\treturn render(request, 'services/monitor.html', {'component_status': status, 'status_summary': status_summary}, status=status_code)\n", 
" \tvalues = {'user_id': (ctxt.user_id or fake.USER_ID), 'project_id': (ctxt.project_id or fake.PROJECT_ID), 'volume_id': volume_id, 'status': status, 'display_name': display_name, 'display_description': display_description, 'container': 'fake', 'availability_zone': 'fake', 'service': 'fake', 'size': ((5 * 1024) * 1024), 'object_count': 22, 'host': socket.gethostname(), 'parent_id': parent_id, 'temp_volume_id': temp_volume_id, 'temp_snapshot_id': temp_snapshot_id, 'snapshot_id': snapshot_id, 'data_timestamp': data_timestamp} \n\tvalues.update(kwargs) \n\tbackup = objects.Backup(ctxt, **values) \n\tbackup.create() \n\treturn backup\n", 
" \timage = _find_image(cs, args.image) \n\t_print_image(image)\n", 
" \treturn IMPL.backup_get_all(context, filters=filters, marker=marker, limit=limit, offset=offset, sort_keys=sort_keys, sort_dirs=sort_dirs)\n", 
" \tif (not remove(local_file)): \n\t \tlogging.warning(\"No \tlocal \tbackup \tfile \t'{0}' \tto \tdelete. \tSkipping...\".format(local_file))\n", 
" \tlogging.info('Starting \tnew \tdb \trestore.') \n\tdb_ips = appscale_info.get_db_ips() \n\tif (not db_ips): \n\t \traise BRException('Unable \tto \tfind \tany \tCassandra \tmachines.') \n\tmachines_without_restore = [] \n\tfor db_ip in db_ips: \n\t \texit_code = utils.ssh(db_ip, keyname, 'ls \t{}'.format(path), method=subprocess.call) \n\t \tif (exit_code != ExitCodes.SUCCESS): \n\t \t \tmachines_without_restore.append(db_ip) \n\tif (machines_without_restore and (not force)): \n\t \tlogging.info('The \tfollowing \tmachines \tdo \tnot \thave \ta \trestore \tfile: \t{}'.format(machines_without_restore)) \n\t \tresponse = raw_input('Would \tyou \tlike \tto \tcontinue? \t[y/N] \t') \n\t \tif (response not in ['Y', 'y']): \n\t \t \treturn \n\tfor db_ip in db_ips: \n\t \tlogging.info('Stopping \tCassandra \ton \t{}'.format(db_ip)) \n\t \tsummary = utils.ssh(db_ip, keyname, 'monit \tsummary', method=subprocess.check_output) \n\t \tstatus = utils.monit_status(summary, CASSANDRA_MONIT_WATCH_NAME) \n\t \tretries = SERVICE_STOP_RETRIES \n\t \twhile (status != MonitStates.UNMONITORED): \n\t \t \tutils.ssh(db_ip, keyname, 'monit \tstop \t{}'.format(CASSANDRA_MONIT_WATCH_NAME), method=subprocess.call) \n\t \t \ttime.sleep(3) \n\t \t \tsummary = utils.ssh(db_ip, keyname, 'monit \tsummary', method=subprocess.check_output) \n\t \t \tstatus = utils.monit_status(summary, CASSANDRA_MONIT_WATCH_NAME) \n\t \t \tretries -= 1 \n\t \t \tif (retries < 0): \n\t \t \t \traise BRException('Unable \tto \tstop \tCassandra') \n\tcassandra_dir = '{}/cassandra'.format(APPSCALE_DATA_DIR) \n\tfor db_ip in db_ips: \n\t \tlogging.info('Restoring \tCassandra \tdata \ton \t{}'.format(db_ip)) \n\t \tclear_db = 'find \t{0} \t-regex \t\".*\\\\.\\\\(db\\\\|txt\\\\|log\\\\)$\" \t-exec \trm \t{{}} \t\\\\;'.format(cassandra_dir) \n\t \tutils.ssh(db_ip, keyname, clear_db) \n\t \tif (db_ip not in machines_without_restore): \n\t \t \tutils.ssh(db_ip, keyname, 'tar \txf \t{} \t-C \t{}'.format(path, cassandra_dir)) \n\t \t \tutils.ssh(db_ip, keyname, 'chown \t-R \tcassandra \t{}'.format(cassandra_dir)) \n\t \tutils.ssh(db_ip, keyname, 'monit \tstart \t{}'.format(CASSANDRA_MONIT_WATCH_NAME)) \n\tlogging.info('Done \twith \tdb \trestore.')\n", 
" \textensions = cs.list_extensions.show_all() \n\tfields = ['Name', 'Summary', 'Alias', 'Updated'] \n\tutils.print_list(extensions, fields)\n", 
" \treturn datetime.datetime.strptime(d['isostr'], _DATETIME_FORMAT)\n", 
" \treturn _aliases.get(alias.lower(), alias)\n", 
" \twith open(CHANGELOG) as f: \n\t \tcur_index = 0 \n\t \tfor line in f: \n\t \t \tmatch = re.search('^\\\\d+\\\\.\\\\d+\\\\.\\\\d+', line) \n\t \t \tif match: \n\t \t \t \tif (cur_index == index): \n\t \t \t \t \treturn match.group(0) \n\t \t \t \telse: \n\t \t \t \t \tcur_index += 1\n", 
" \tgitstr = _git_str() \n\tif (gitstr is None): \n\t \tgitstr = '' \n\tpath = os.path.join(BASEDIR, 'qutebrowser', 'git-commit-id') \n\twith _open(path, 'w', encoding='ascii') as f: \n\t \tf.write(gitstr)\n", 
" \tif (len(sys.argv) < 2): \n\t \treturn True \n\tinfo_commands = ['--help-commands', '--name', '--version', '-V', '--fullname', '--author', '--author-email', '--maintainer', '--maintainer-email', '--contact', '--contact-email', '--url', '--license', '--description', '--long-description', '--platforms', '--classifiers', '--keywords', '--provides', '--requires', '--obsoletes'] \n\tinfo_commands.extend(['egg_info', 'install_egg_info', 'rotate']) \n\tfor command in info_commands: \n\t \tif (command in sys.argv[1:]): \n\t \t \treturn False \n\treturn True\n", 
" \trefs = list_refs(refnames=[refname], repo_dir=repo_dir, limit_to_heads=True) \n\tl = tuple(islice(refs, 2)) \n\tif l: \n\t \tassert (len(l) == 1) \n\t \treturn l[0][1] \n\telse: \n\t \treturn None\n", 
" \tif isinstance(fileIshObject, TextIOBase): \n\t \treturn unicode \n\tif isinstance(fileIshObject, IOBase): \n\t \treturn bytes \n\tencoding = getattr(fileIshObject, 'encoding', None) \n\timport codecs \n\tif isinstance(fileIshObject, (codecs.StreamReader, codecs.StreamWriter)): \n\t \tif encoding: \n\t \t \treturn unicode \n\t \telse: \n\t \t \treturn bytes \n\tif (not _PY3): \n\t \tif isinstance(fileIshObject, file): \n\t \t \tif (encoding is not None): \n\t \t \t \treturn basestring \n\t \t \telse: \n\t \t \t \treturn bytes \n\t \tfrom cStringIO import InputType, OutputType \n\t \tfrom StringIO import StringIO \n\t \tif isinstance(fileIshObject, (StringIO, InputType, OutputType)): \n\t \t \treturn bytes \n\treturn default\n", 
" \tcmd = (_pkg(jail, chroot, root) + ['--version']) \n\treturn __salt__['cmd.run'](cmd).strip()\n", 
" \tcmd = (_pkg(jail, chroot, root) + ['--version']) \n\treturn __salt__['cmd.run'](cmd).strip()\n", 
" \tif (value in (u'1', u'0')): \n\t \treturn bool(int(value)) \n\traise ValueError((u'%r \tis \tnot \t0 \tor \t1' % value))\n", 
" \tif isinstance(obj, str): \n\t \tobj = obj.strip().lower() \n\t \tif (obj in ('true', 'yes', 'on', 'y', 't', '1')): \n\t \t \treturn True \n\t \tif (obj in ('false', 'no', 'off', 'n', 'f', '0')): \n\t \t \treturn False \n\t \traise ValueError(('Unable \tto \tinterpret \tvalue \t\"%s\" \tas \tboolean' % obj)) \n\treturn bool(obj)\n", 
" \tglobal _PARSING_CACHE \n\tif (string in _PARSING_CACHE): \n\t \tstack = _PARSING_CACHE[string] \n\telse: \n\t \tif (not _RE_STARTTOKEN.search(string)): \n\t \t \treturn string \n\t \tstack = ParseStack() \n\t \tncallable = 0 \n\t \tfor match in _RE_TOKEN.finditer(string): \n\t \t \tgdict = match.groupdict() \n\t \t \tif gdict['singlequote']: \n\t \t \t \tstack.append(gdict['singlequote']) \n\t \t \telif gdict['doublequote']: \n\t \t \t \tstack.append(gdict['doublequote']) \n\t \t \telif gdict['end']: \n\t \t \t \tif (ncallable <= 0): \n\t \t \t \t \tstack.append(')') \n\t \t \t \t \tcontinue \n\t \t \t \targs = [] \n\t \t \t \twhile stack: \n\t \t \t \t \toperation = stack.pop() \n\t \t \t \t \tif callable(operation): \n\t \t \t \t \t \tif (not strip): \n\t \t \t \t \t \t \tstack.append((operation, [arg for arg in reversed(args)])) \n\t \t \t \t \t \tncallable -= 1 \n\t \t \t \t \t \tbreak \n\t \t \t \t \telse: \n\t \t \t \t \t \targs.append(operation) \n\t \t \telif gdict['start']: \n\t \t \t \tfuncname = _RE_STARTTOKEN.match(gdict['start']).group(1) \n\t \t \t \ttry: \n\t \t \t \t \tstack.append(_INLINE_FUNCS[funcname]) \n\t \t \t \texcept KeyError: \n\t \t \t \t \tstack.append(_INLINE_FUNCS['nomatch']) \n\t \t \t \t \tstack.append(funcname) \n\t \t \t \tncallable += 1 \n\t \t \telif gdict['escaped']: \n\t \t \t \ttoken = gdict['escaped'].lstrip('\\\\') \n\t \t \t \tstack.append(token) \n\t \t \telif gdict['comma']: \n\t \t \t \tif (ncallable > 0): \n\t \t \t \t \tstack.append(None) \n\t \t \t \telse: \n\t \t \t \t \tstack.append(',') \n\t \t \telse: \n\t \t \t \tstack.append(gdict['rest']) \n\t \tif (ncallable > 0): \n\t \t \treturn string \n\t \tif ((_STACK_MAXSIZE > 0) and (_STACK_MAXSIZE < len(stack))): \n\t \t \treturn (string + gdict['stackfull'](*args, **kwargs)) \n\t \telse: \n\t \t \t_PARSING_CACHE[string] = stack \n\tdef _run_stack(item, depth=0): \n\t \tretval = item \n\t \tif isinstance(item, tuple): \n\t \t \tif strip: \n\t \t \t \treturn '' \n\t \t \telse: \n\t \t \t \t(func, arglist) = item \n\t \t \t \targs = [''] \n\t \t \t \tfor arg in arglist: \n\t \t \t \t \tif (arg is None): \n\t \t \t \t \t \targs.append('') \n\t \t \t \t \telse: \n\t \t \t \t \t \targs[(-1)] += _run_stack(arg, depth=(depth + 1)) \n\t \t \t \tkwargs['inlinefunc_stack_depth'] = depth \n\t \t \t \tretval = ('' if strip else func(*args, **kwargs)) \n\t \treturn utils.to_str(retval, force_string=True) \n\treturn ''.join((_run_stack(item) for item in _PARSING_CACHE[string]))\n", 
" \tif (type(s) is str): \n\t \treturn s \n\telse: \n\t \treturn s3_unicode(s).encode('utf-8', 'strict')\n", 
" \tif (not at): \n\t \tat = utcnow() \n\tst = at.strftime((_ISO8601_TIME_FORMAT if (not subsecond) else _ISO8601_TIME_FORMAT_SUBSECOND)) \n\ttz = (at.tzinfo.tzname(None) if at.tzinfo else 'UTC') \n\tst += ('Z' if (tz == 'UTC') else tz) \n\treturn st\n", 
" \ttry: \n\t \treturn iso8601.parse_date(timestr) \n\texcept iso8601.ParseError as e: \n\t \traise ValueError(encodeutils.exception_to_unicode(e)) \n\texcept TypeError as e: \n\t \traise ValueError(encodeutils.exception_to_unicode(e))\n", 
" \tnow = datetime.utcnow().replace(tzinfo=iso8601.iso8601.UTC) \n\tif iso: \n\t \treturn datetime_tuple_to_iso(now) \n\telse: \n\t \treturn now\n", 
" \treturn formatdate(timegm(dt.utctimetuple()))\n", 
" \toffset = timestamp.utcoffset() \n\tif (offset is None): \n\t \treturn timestamp \n\treturn (timestamp.replace(tzinfo=None) - offset)\n", 
" \tif os.path.isfile(db_file_name): \n\t \tfrom datetime import timedelta \n\t \tfile_datetime = datetime.fromtimestamp(os.stat(db_file_name).st_ctime) \n\t \tif ((datetime.today() - file_datetime) >= timedelta(hours=older_than)): \n\t \t \tif verbose: \n\t \t \t \tprint u'File \tis \told' \n\t \t \treturn True \n\t \telse: \n\t \t \tif verbose: \n\t \t \t \tprint u'File \tis \trecent' \n\t \t \treturn False \n\telse: \n\t \tif verbose: \n\t \t \tprint u'File \tdoes \tnot \texist' \n\t \treturn True\n", 
" \tif (not os.path.exists(source)): \n\t \traise DistutilsFileError((\"file \t'%s' \tdoes \tnot \texist\" % os.path.abspath(source))) \n\tif (not os.path.exists(target)): \n\t \treturn True \n\treturn (os.stat(source)[ST_MTIME] > os.stat(target)[ST_MTIME])\n", 
" \tif utcnow.override_time: \n\t \ttry: \n\t \t \treturn utcnow.override_time.pop(0) \n\t \texcept AttributeError: \n\t \t \treturn utcnow.override_time \n\tif with_timezone: \n\t \treturn datetime.datetime.now(tz=iso8601.iso8601.UTC) \n\treturn datetime.datetime.utcnow()\n", 
" \tif utcnow.override_time: \n\t \ttry: \n\t \t \treturn utcnow.override_time.pop(0) \n\t \texcept AttributeError: \n\t \t \treturn utcnow.override_time \n\tif with_timezone: \n\t \treturn datetime.datetime.now(tz=iso8601.iso8601.UTC) \n\treturn datetime.datetime.utcnow()\n", 
" \treturn isotime(datetime.datetime.utcfromtimestamp(timestamp), microsecond)\n", 
" \tif utcnow.override_time: \n\t \ttry: \n\t \t \treturn utcnow.override_time.pop(0) \n\t \texcept AttributeError: \n\t \t \treturn utcnow.override_time \n\tif with_timezone: \n\t \treturn datetime.datetime.now(tz=iso8601.iso8601.UTC) \n\treturn datetime.datetime.utcnow()\n", 
" \tmatch = standard_duration_re.match(value) \n\tif (not match): \n\t \tmatch = iso8601_duration_re.match(value) \n\tif match: \n\t \tkw = match.groupdict() \n\t \tsign = ((-1) if (kw.pop('sign', '+') == '-') else 1) \n\t \tif kw.get('microseconds'): \n\t \t \tkw['microseconds'] = kw['microseconds'].ljust(6, '0') \n\t \tif (kw.get('seconds') and kw.get('microseconds') and kw['seconds'].startswith('-')): \n\t \t \tkw['microseconds'] = ('-' + kw['microseconds']) \n\t \tkw = {k: float(v) for (k, v) in kw.items() if (v is not None)} \n\t \treturn (sign * datetime.timedelta(**kw))\n", 
" \treturn call_talib_with_ohlc(barDs, count, talib.CDLADVANCEBLOCK)\n", 
" \ttry: \n\t \tparent = next(klass.local_attr_ancestors(name)) \n\texcept (StopIteration, KeyError): \n\t \treturn None \n\ttry: \n\t \tmeth_node = parent[name] \n\texcept KeyError: \n\t \treturn None \n\tif isinstance(meth_node, astroid.Function): \n\t \treturn meth_node \n\treturn None\n", 
" \tif value.tzinfo: \n\t \tvalue = value.astimezone(UTC) \n\treturn (long((calendar.timegm(value.timetuple()) * 1000000L)) + value.microsecond)\n", 
" \t(p, u) = getparser(use_datetime=use_datetime, use_builtin_types=use_builtin_types) \n\tp.feed(data) \n\tp.close() \n\treturn (u.close(), u.getmethodname())\n", 
" \tlater = (mktime(date1.timetuple()) + (date1.microsecond / 1000000.0)) \n\tearlier = (mktime(date2.timetuple()) + (date2.microsecond / 1000000.0)) \n\treturn (later - earlier)\n", 
" \tgenerate_completions(event)\n", 
" \tdef _decorator(func): \n\t \tadd_arg(func, *args, **kwargs) \n\t \treturn func \n\treturn _decorator\n", 
" \tfor arg in args: \n\t \tvalue = os.environ.get(arg) \n\t \tif value: \n\t \t \treturn value \n\treturn kwargs.get('default', '')\n", 
" \tif (not hasattr(func, 'arguments')): \n\t \tfunc.arguments = [] \n\tif ((args, kwargs) not in func.arguments): \n\t \tfunc.arguments.insert(0, (args, kwargs))\n", 
" \tif (not hasattr(f, 'resource_manager_kwargs_hooks')): \n\t \tf.resource_manager_kwargs_hooks = [] \n\tnames = [h.__name__ for h in f.resource_manager_kwargs_hooks] \n\tif (hook.__name__ not in names): \n\t \tf.resource_manager_kwargs_hooks.append(hook)\n", 
" \thooks = getattr(f, 'resource_manager_kwargs_hooks', []) \n\textra_kwargs = {} \n\tfor hook in hooks: \n\t \thook_kwargs = hook(args) \n\t \thook_name = hook.__name__ \n\t \tconflicting_keys = (set(hook_kwargs.keys()) & set(extra_kwargs.keys())) \n\t \tif (conflicting_keys and (not allow_conflicts)): \n\t \t \tmsg = (_(\"Hook \t'%(hook_name)s' \tis \tattempting \tto \tredefine \tattributes \t'%(conflicting_keys)s'\") % {'hook_name': hook_name, 'conflicting_keys': conflicting_keys}) \n\t \t \traise exceptions.NoUniqueMatch(msg) \n\t \textra_kwargs.update(hook_kwargs) \n\treturn extra_kwargs\n", 
" \tfunc.unauthenticated = True \n\treturn func\n", 
" \treturn getattr(func, 'unauthenticated', False)\n", 
" \tdef inner(f): \n\t \tf.service_type = stype \n\t \treturn f \n\treturn inner\n", 
" \treturn getattr(f, 'service_type', None)\n", 
" \tif getattr(manager, 'is_alphanum_id_allowed', False): \n\t \ttry: \n\t \t \treturn manager.get(name_or_id) \n\t \texcept exceptions.NotFound: \n\t \t \tpass \n\ttry: \n\t \ttmp_id = encodeutils.safe_encode(name_or_id) \n\t \tif six.PY3: \n\t \t \ttmp_id = tmp_id.decode() \n\t \tuuid.UUID(tmp_id) \n\t \treturn manager.get(tmp_id) \n\texcept (TypeError, ValueError, exceptions.NotFound): \n\t \tpass \n\ttry: \n\t \ttry: \n\t \t \tresource = getattr(manager, 'resource_class', None) \n\t \t \tname_attr = (resource.NAME_ATTR if resource else 'name') \n\t \t \tkwargs = {name_attr: name_or_id} \n\t \t \tkwargs.update(find_args) \n\t \t \treturn manager.find(**kwargs) \n\t \texcept exceptions.NotFound: \n\t \t \tpass \n\t \ttry: \n\t \t \treturn manager.find(human_id=name_or_id, **find_args) \n\t \texcept exceptions.NotFound: \n\t \t \tpass \n\texcept exceptions.NoUniqueMatch: \n\t \tmsg = (_(\"Multiple \t%(class)s \tmatches \tfound \tfor \t'%(name)s', \tuse \tan \tID \tto \tbe \tmore \tspecific.\") % {'class': manager.resource_class.__name__.lower(), 'name': name_or_id}) \n\t \tif wrap_exception: \n\t \t \traise exceptions.CommandError(msg) \n\t \traise exceptions.NoUniqueMatch(msg) \n\ttry: \n\t \treturn manager.get(int(name_or_id)) \n\texcept (TypeError, ValueError, exceptions.NotFound): \n\t \tmsg = (_(\"No \t%(class)s \twith \ta \tname \tor \tID \tof \t'%(name)s' \texists.\") % {'class': manager.resource_class.__name__.lower(), 'name': name_or_id}) \n\t \tif wrap_exception: \n\t \t \traise exceptions.CommandError(msg) \n\t \traise exceptions.NotFound(404, msg)\n", 
" \ttry: \n\t \tif issubclass(*args): \n\t \t \treturn True \n\texcept TypeError: \n\t \tpass \n\treturn False\n", 
" \ttry: \n\t \t(module, attr) = name.rsplit('.', 1) \n\texcept ValueError as error: \n\t \traise ImproperlyConfigured(('Error \timporting \tclass \t%s \tin \t%s: \t\"%s\"' % (name, setting, error))) \n\ttry: \n\t \tmod = import_module(module) \n\texcept ImportError as error: \n\t \traise ImproperlyConfigured(('Error \timporting \tmodule \t%s \tin \t%s: \t\"%s\"' % (module, setting, error))) \n\ttry: \n\t \tcls = getattr(mod, attr) \n\texcept AttributeError: \n\t \traise ImproperlyConfigured(('Module \t\"%s\" \tdoes \tnot \tdefine \ta \t\"%s\" \tclass \tin \t%s' % (module, attr, setting))) \n\treturn cls\n", 
" \tvalue = Markup(value).striptags() \n\timport unicodedata \n\tfrom unidecode import unidecode \n\tvalue = unidecode(value) \n\tif isinstance(value, six.binary_type): \n\t \tvalue = value.decode(u'ascii') \n\tvalue = unicodedata.normalize(u'NFKD', value).lower() \n\tnew_subs = [] \n\tfor tpl in substitutions: \n\t \ttry: \n\t \t \t(src, dst, skip) = tpl \n\t \texcept ValueError: \n\t \t \t(src, dst) = tpl \n\t \t \tskip = False \n\t \tnew_subs.append((src, dst, skip)) \n\tsubstitutions = tuple(new_subs) \n\treplace = True \n\tfor (src, dst, skip) in substitutions: \n\t \torig_value = value \n\t \tvalue = value.replace(src.lower(), dst.lower()) \n\t \tif (value != orig_value): \n\t \t \treplace = (replace and (not skip)) \n\tif replace: \n\t \tvalue = re.sub(u'[^\\\\w\\\\s-]', u'', value).strip() \n\t \tvalue = re.sub(u'[-\\\\s]+', u'-', value) \n\telse: \n\t \tvalue = value.strip() \n\tvalue = value.encode(u'ascii', u'ignore') \n\treturn value.decode(u'ascii')\n", 
" \tif refresh: \n\t \treturn True \n\tif (os.path.isfile(cache_file) and (os.path.getsize(cache_file) > 0)): \n\t \tmod_time = os.path.getmtime(cache_file) \n\t \tcurrent_time = time.time() \n\t \tif ((mod_time + cache_expiration_time) > current_time): \n\t \t \treturn False \n\treturn True\n", 
" \tretVal = payload \n\tif payload: \n\t \tretVal = '' \n\t \ti = 0 \n\t \twhile (i < len(payload)): \n\t \t \tif ((payload[i] == '%') and (i < (len(payload) - 2)) and (payload[(i + 1):(i + 2)] in string.hexdigits) and (payload[(i + 2):(i + 3)] in string.hexdigits)): \n\t \t \t \tretVal += payload[i:(i + 3)] \n\t \t \t \ti += 3 \n\t \t \telse: \n\t \t \t \tretVal += ('%%%.2X' % ord(payload[i])) \n\t \t \t \ti += 1 \n\treturn retVal\n", 
" \tconf = global_conf.copy() \n\tconf.update(local_conf) \n\tdef auth_filter(app): \n\t \treturn KeystonePasswordAuthProtocol(app, conf) \n\treturn auth_filter\n", 
" \tdir_path = os.path.expanduser(dir_path) \n\tdirectory = os.path.normpath(dir_path) \n\tif (not os.path.isdir(directory)): \n\t \tmakedirs_perms(directory, user, group, mode) \n\treturn True\n", 
" \tkey = os.urandom(32) \n\tencoded_key = base64.b64encode(key).decode('utf-8') \n\tprint 'Base \t64 \tencoded \tencryption \tkey: \t{}'.format(encoded_key)\n", 
" \tif (not s): \n\t \treturn s \n\tencvec = [] \n\tmax_unencoded = ((maxlinelen * 3) // 4) \n\tfor i in range(0, len(s), max_unencoded): \n\t \tenc = b2a_base64(s[i:(i + max_unencoded)]).decode('ascii') \n\t \tif (enc.endswith(NL) and (eol != NL)): \n\t \t \tenc = (enc[:(-1)] + eol) \n\t \tencvec.append(enc) \n\treturn EMPTYSTRING.join(encvec)\n", 
" \treturn base64.b64encode(hashlib.sha1(payload).digest())\n", 
" \ts = hashlib.sha1(t) \n\treturn s.hexdigest()\n", 
" \treturn bool(re.match((('^' + '[\\\\:\\\\-]'.join((['([0-9a-f]{2})'] * 6))) + '$'), mac.lower()))\n", 
" \tkey = bytearray(key) \n\tif (mode == AESModeOfOperation.ModeOfOperation[u'CBC']): \n\t \tdata = append_PKCS7_padding(data) \n\tkeysize = len(key) \n\tassert (keysize in AES.KeySize.values()), u'invalid \tkey \tsize: \t{0}'.format(keysize) \n\tiv = bytearray([i for i in os.urandom(16)]) \n\tmoo = AESModeOfOperation() \n\t(mode, length, ciph) = moo.encrypt(data, mode, key, keysize, iv) \n\treturn (bytes(iv) + bytes(ciph))\n", 
" \tkey = bytearray(key) \n\tkeysize = len(key) \n\tassert (keysize in AES.KeySize.values()), (u'invalid \tkey \tsize: \t%s' % keysize) \n\tiv = bytearray(data[:16]) \n\tdata = bytearray(data[16:]) \n\tmoo = AESModeOfOperation() \n\tdecr = moo.decrypt(data, None, mode, key, keysize, iv) \n\tif (mode == AESModeOfOperation.ModeOfOperation[u'CBC']): \n\t \tdecr = strip_PKCS7_padding(decr) \n\treturn bytes(decr)\n", 
" \t_ensure_subprocess() \n\tif isinstance(formatted, six.string_types): \n\t \tdata = bytearray(formatted, _encoding_for_form(inform)) \n\telse: \n\t \tdata = formatted \n\tprocess = subprocess.Popen(['openssl', 'cms', '-verify', '-certfile', signing_cert_file_name, '-CAfile', ca_file_name, '-inform', 'PEM', '-nosmimecap', '-nodetach', '-nocerts', '-noattr'], stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE, close_fds=True) \n\t(output, err, retcode) = _process_communicate_handle_oserror(process, data, (signing_cert_file_name, ca_file_name)) \n\tif (retcode == OpensslCmsExitStatus.INPUT_FILE_READ_ERROR): \n\t \tif err.startswith('Error \treading \tS/MIME \tmessage'): \n\t \t \traise exceptions.CMSError(err) \n\t \telse: \n\t \t \traise exceptions.CertificateConfigError(err) \n\telif (retcode == OpensslCmsExitStatus.COMMAND_OPTIONS_PARSING_ERROR): \n\t \tif err.startswith('cms: \tCannot \topen \tinput \tfile'): \n\t \t \traise exceptions.CertificateConfigError(err) \n\t \telse: \n\t \t \traise subprocess.CalledProcessError(retcode, 'openssl', output=err) \n\telif (retcode != OpensslCmsExitStatus.SUCCESS): \n\t \traise subprocess.CalledProcessError(retcode, 'openssl', output=err) \n\treturn output\n", 
" \tfrom .singleton import S \n\tfrom .basic import Basic \n\tfrom .sympify import sympify, SympifyError \n\tfrom .compatibility import iterable \n\tif isinstance(item, Basic): \n\t \treturn item.sort_key(order=order) \n\tif iterable(item, exclude=string_types): \n\t \tif isinstance(item, dict): \n\t \t \targs = item.items() \n\t \t \tunordered = True \n\t \telif isinstance(item, set): \n\t \t \targs = item \n\t \t \tunordered = True \n\t \telse: \n\t \t \targs = list(item) \n\t \t \tunordered = False \n\t \targs = [default_sort_key(arg, order=order) for arg in args] \n\t \tif unordered: \n\t \t \targs = sorted(args) \n\t \t(cls_index, args) = (10, (len(args), tuple(args))) \n\telse: \n\t \tif (not isinstance(item, string_types)): \n\t \t \ttry: \n\t \t \t \titem = sympify(item) \n\t \t \texcept SympifyError: \n\t \t \t \tpass \n\t \t \telse: \n\t \t \t \tif isinstance(item, Basic): \n\t \t \t \t \treturn default_sort_key(item) \n\t \t(cls_index, args) = (0, (1, (str(item),))) \n\treturn ((cls_index, 0, item.__class__.__name__), args, S.One.sort_key(), S.One)\n", 
" \t_ensure_subprocess() \n\tif isinstance(data_to_sign, six.string_types): \n\t \tdata = bytearray(data_to_sign, encoding='utf-8') \n\telse: \n\t \tdata = data_to_sign \n\tprocess = subprocess.Popen(['openssl', 'cms', '-sign', '-signer', signing_cert_file_name, '-inkey', signing_key_file_name, '-outform', 'PEM', '-nosmimecap', '-nodetach', '-nocerts', '-noattr', '-md', message_digest], stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE, close_fds=True) \n\t(output, err, retcode) = _process_communicate_handle_oserror(process, data, (signing_cert_file_name, signing_key_file_name)) \n\tif ((retcode != OpensslCmsExitStatus.SUCCESS) or ('Error' in err)): \n\t \tif (retcode == OpensslCmsExitStatus.CREATE_CMS_READ_MIME_ERROR): \n\t \t \tLOG.error(_LE('Signing \terror: \tUnable \tto \tload \tcertificate \t- \tensure \tyou \thave \tconfigured \tPKI \twith \t\"keystone-manage \tpki_setup\"')) \n\t \telse: \n\t \t \tLOG.error(_LE('Signing \terror: \t%s'), err) \n\t \traise subprocess.CalledProcessError(retcode, 'openssl') \n\tif (outform == PKI_ASN1_FORM): \n\t \treturn output.decode('utf-8') \n\telse: \n\t \treturn output\n", 
" \t(path, senv) = salt.utils.url.split_env(path) \n\tif senv: \n\t \tsaltenv = senv \n\treturn _client().hash_file(path, saltenv)\n", 
" \ttry: \n\t \treturn obj.id \n\texcept AttributeError: \n\t \treturn obj\n", 
" \tcls = _code_map.get(response.status_code, ClientException) \n\tkwargs = {'code': response.status_code, 'method': method, 'url': url, 'request_id': None} \n\tif response.headers: \n\t \tkwargs['request_id'] = response.headers.get('x-compute-request-id') \n\t \tif (issubclass(cls, RetryAfterException) and ('retry-after' in response.headers)): \n\t \t \tkwargs['retry_after'] = response.headers.get('retry-after') \n\tif body: \n\t \tmessage = 'n/a' \n\t \tdetails = 'n/a' \n\t \tif hasattr(body, 'keys'): \n\t \t \tif ('message' in body): \n\t \t \t \tmessage = body.get('message') \n\t \t \t \tdetails = body.get('details') \n\t \t \telse: \n\t \t \t \terror = body[list(body)[0]] \n\t \t \t \tmessage = error.get('message') \n\t \t \t \tdetails = error.get('details') \n\t \tkwargs['message'] = message \n\t \tkwargs['details'] = details \n\treturn cls(**kwargs)\n", 
" \tdef print_progress(progress): \n\t \tif show_progress: \n\t \t \tmsg = (_('\\rServer \t%(action)s... \t%(progress)s%% \tcomplete') % dict(action=action, progress=progress)) \n\t \telse: \n\t \t \tmsg = (_('\\rServer \t%(action)s...') % dict(action=action)) \n\t \tsys.stdout.write(msg) \n\t \tsys.stdout.flush() \n\tif (not silent): \n\t \tprint() \n\twhile True: \n\t \tobj = poll_fn(obj_id) \n\t \tstatus = getattr(obj, status_field) \n\t \tif status: \n\t \t \tstatus = status.lower() \n\t \tprogress = (getattr(obj, 'progress', None) or 0) \n\t \tif (status in final_ok_states): \n\t \t \tif (not silent): \n\t \t \t \tprint_progress(100) \n\t \t \t \tprint(_('\\nFinished')) \n\t \t \tbreak \n\t \telif (status == 'error'): \n\t \t \tif (not silent): \n\t \t \t \tprint((_('\\nError \t%s \tserver') % action)) \n\t \t \traise exceptions.ResourceInErrorState(obj) \n\t \telif (status == 'deleted'): \n\t \t \tif (not silent): \n\t \t \t \tprint((_('\\nDeleted \t%s \tserver') % action)) \n\t \t \traise exceptions.InstanceInDeletedState(obj.fault['message']) \n\t \tif (not silent): \n\t \t \tprint_progress(progress) \n\t \ttime.sleep(poll_period)\n", 
" \tif (call != 'function'): \n\t \traise SaltCloudSystemExit('The \tlist_monitor_data \tmust \tbe \tcalled \twith \t-f \tor \t--function.') \n\tif (not isinstance(kwargs, dict)): \n\t \tkwargs = {} \n\tret = {} \n\tparams = {'Action': 'GetMonitorData', 'RegionId': get_location()} \n\tif ('name' in kwargs): \n\t \tparams['InstanceId'] = kwargs['name'] \n\titems = query(params=params) \n\tmonitorData = items['MonitorData'] \n\tfor data in monitorData['InstanceMonitorData']: \n\t \tret[data['InstanceId']] = {} \n\t \tfor item in data: \n\t \t \tret[data['InstanceId']][item] = str(data[item]) \n\treturn ret\n", 
" \tif (call != 'function'): \n\t \traise SaltCloudSystemExit('The \tlist_monitor_data \tmust \tbe \tcalled \twith \t-f \tor \t--function.') \n\tif (not isinstance(kwargs, dict)): \n\t \tkwargs = {} \n\tret = {} \n\tparams = {'Action': 'GetMonitorData', 'RegionId': get_location()} \n\tif ('name' in kwargs): \n\t \tparams['InstanceId'] = kwargs['name'] \n\titems = query(params=params) \n\tmonitorData = items['MonitorData'] \n\tfor data in monitorData['InstanceMonitorData']: \n\t \tret[data['InstanceId']] = {} \n\t \tfor item in data: \n\t \t \tret[data['InstanceId']][item] = str(data[item]) \n\treturn ret\n", 
" \treturn IMPL.backup_get_all(context, filters=filters, marker=marker, limit=limit, offset=offset, sort_keys=sort_keys, sort_dirs=sort_dirs)\n", 
" \tpaths = [] \n\tfor dir in SEARCH_DIRS: \n\t \tif ((not os.path.isdir(dir)) or (not os.access(dir, os.R_OK))): \n\t \t \tcontinue \n\t \tfor name in os.listdir(dir): \n\t \t \tsubdir = os.path.join(dir, name) \n\t \t \tif ((not os.path.isdir(subdir)) or (not os.access(subdir, os.R_OK))): \n\t \t \t \tcontinue \n\t \t \tfor subname in os.listdir(subdir): \n\t \t \t \tpath = os.path.join(subdir, subname) \n\t \t \t \tif utils.is_sockfile(path): \n\t \t \t \t \tpaths.append(path) \n\t \t \t \t \tbreak \n\tfor sockfile in DEFAULT_SOCKFILES: \n\t \tif (not utils.is_sockfile(sockfile)): \n\t \t \tcontinue \n\t \tpaths.append(sockfile) \n\treturn paths\n", 
" \timage = _find_image(cs, args.image) \n\t_print_image(image)\n", 
" \tfrom evennia.scripts.monitorhandler import MONITOR_HANDLER \n\tname = kwargs.get('name', None) \n\tif (name and (name in _monitorable) and session.puppet): \n\t \tfield_name = _monitorable[name] \n\t \tobj = session.puppet \n\t \tif kwargs.get('stop', False): \n\t \t \tMONITOR_HANDLER.remove(obj, field_name, idstring=session.sessid) \n\t \telse: \n\t \t \tMONITOR_HANDLER.add(obj, field_name, _on_monitor_change, idstring=session.sessid, persistent=False, name=name, session=session)\n", 
" \tparser = argparse.ArgumentParser() \n\trules = shlex.split(rule) \n\trules.pop(0) \n\tparser.add_argument('--hsync', dest='hsync', action='store') \n\tparser.add_argument('--monitor', dest='monitor', action='store') \n\tparser.add_argument('--noprobe', dest='noprobe', action='store_true') \n\tparser.add_argument('--vsync', dest='vsync', action='store') \n\targs = clean_args(vars(parser.parse_args(rules))) \n\tparser = None \n\treturn args\n", 
" \tuse_l10n = None \n\tbits = list(token.split_contents()) \n\tif (len(bits) == 1): \n\t \tuse_l10n = True \n\telif ((len(bits) > 2) or (bits[1] not in ('on', 'off'))): \n\t \traise TemplateSyntaxError((\"%r \targument \tshould \tbe \t'on' \tor \t'off'\" % bits[0])) \n\telse: \n\t \tuse_l10n = (bits[1] == 'on') \n\tnodelist = parser.parse(('endlocalize',)) \n\tparser.delete_first_token() \n\treturn LocalizeNode(nodelist, use_l10n)\n", 
" \tif (not test_user_authenticated(request)): \n\t \treturn login(request, next=('/cobbler_web/%s/rename/%s/%s' % (what, obj_name, obj_newname)), expired=True) \n\tif (obj_name is None): \n\t \treturn error_page(request, ('You \tmust \tspecify \ta \t%s \tto \trename' % what)) \n\tif (not remote.has_item(what, obj_name)): \n\t \treturn error_page(request, ('Unknown \t%s \tspecified' % what)) \n\telif (not remote.check_access_no_fail(request.session['token'], ('modify_%s' % what), obj_name)): \n\t \treturn error_page(request, ('You \tdo \tnot \thave \tpermission \tto \trename \tthis \t%s' % what)) \n\telse: \n\t \tobj_id = remote.get_item_handle(what, obj_name, request.session['token']) \n\t \tremote.rename_item(what, obj_id, obj_newname, request.session['token']) \n\t \treturn HttpResponseRedirect(('/cobbler_web/%s/list' % what))\n", 
" \timage = _find_image(cs, args.image) \n\tmetadata = _extract_metadata(args) \n\tif (args.action == 'set'): \n\t \tcs.images.set_meta(image, metadata) \n\telif (args.action == 'delete'): \n\t \tcs.images.delete_meta(image, metadata.keys())\n", 
" \tname = _sdecode(name) \n\tif snap_name: \n\t \tsnap_name = _validate_snap_name(name, snap_name, runas=runas) \n\targs = [name] \n\tif tree: \n\t \targs.append('--tree') \n\tif snap_name: \n\t \targs.extend(['--id', snap_name]) \n\tres = prlctl('snapshot-list', args, runas=runas) \n\tif names: \n\t \tsnap_ids = _find_guids(res) \n\t \tret = '{0:<38} \t \t{1}\\n'.format('Snapshot \tID', 'Snapshot \tName') \n\t \tfor snap_id in snap_ids: \n\t \t \tsnap_name = snapshot_id_to_name(name, snap_id, runas=runas) \n\t \t \tret += u'{{{0}}} \t \t{1}\\n'.format(snap_id, _sdecode(snap_name)) \n\t \treturn ret \n\telse: \n\t \treturn res\n", 
" \timage = _find_image(cs, args.image) \n\t_print_image(image)\n", 
" \tname = _sdecode(name) \n\tif snap_name: \n\t \tsnap_name = _sdecode(snap_name) \n\targs = [name] \n\tif snap_name: \n\t \targs.extend(['--name', snap_name]) \n\tif desc: \n\t \targs.extend(['--description', desc]) \n\treturn prlctl('snapshot', args, runas=runas)\n", 
" \tstrict = (not all) \n\tname = _sdecode(name) \n\tsnap_ids = _validate_snap_name(name, snap_name, strict=strict, runas=runas) \n\tif isinstance(snap_ids, six.string_types): \n\t \tsnap_ids = [snap_ids] \n\tret = {} \n\tfor snap_id in snap_ids: \n\t \tsnap_id = snap_id.strip('{}') \n\t \targs = [name, '--id', snap_id] \n\t \tret[snap_id] = prlctl('snapshot-delete', args, runas=runas) \n\tret_keys = list(ret.keys()) \n\tif (len(ret_keys) == 1): \n\t \treturn ret[ret_keys[0]] \n\telse: \n\t \treturn ret\n", 
" \tdev_id = _get_devices(kwargs) \n\tif (len(dev_id) > 1): \n\t \traise CommandExecutionError('Only \tone \tdevice \tcan \tbe \trenamed \tat \ta \ttime') \n\tif ('title' not in kwargs): \n\t \traise CommandExecutionError('Title \tis \tmissing') \n\treturn _set(dev_id[0], {'name': kwargs['title']}, method='')\n", 
" \trequest = context['request'] \n\tresponse_format = 'html' \n\tif ('response_format' in context): \n\t \tresponse_format = context['response_format'] \n\treturn Markup(render_to_string('knowledge/tags/folder_list', {'subfolders': subfolders, 'skip_group': skip_group}, context_instance=RequestContext(request), response_format=response_format))\n", 
" \treturn IMPL.flavor_extra_specs_get(context, flavor_id)\n", 
" \treturn IMPL.flavor_create(context, values, projects=projects)\n", 
" \tfolder = get_object_or_404(KnowledgeFolder, pk=knowledgeType_id) \n\titems = Object.filter_by_request(request, manager=KnowledgeItem.objects.filter(folder=folder)) \n\tif (not request.user.profile.has_permission(folder, mode='w')): \n\t \treturn user_denied(request, message=\"You \tdon't \thave \taccess \tto \tthis \tKnowledge \tType\") \n\tif request.POST: \n\t \tif ('delete' in request.POST): \n\t \t \tif ('trash' in request.POST): \n\t \t \t \tfolder.trash = True \n\t \t \t \tfolder.save() \n\t \t \telse: \n\t \t \t \tfolder.delete() \n\t \t \treturn HttpResponseRedirect(reverse('knowledge_index')) \n\t \telif ('cancel' in request.POST): \n\t \t \treturn HttpResponseRedirect(reverse('knowledge_folder_view', args=[folder.treepath])) \n\tcontext = _get_default_context(request) \n\tcontext.update({'items': items, 'folder': folder}) \n\treturn render_to_response('knowledge/folder_delete', context, context_instance=RequestContext(request), response_format=response_format)\n", 
" \tflavor = _find_flavor(cs, args.flavor) \n\tkeypair = _extract_metadata(args) \n\tif (args.action == 'set'): \n\t \tflavor.set_keys(keypair) \n\telif (args.action == 'unset'): \n\t \tflavor.unset_keys(keypair.keys())\n", 
" \tresult = DiscoveryResult(uri) \n\tresp = fetchers.fetch(uri, headers={'Accept': YADIS_ACCEPT_HEADER}) \n\tif (resp.status not in (200, 206)): \n\t \traise DiscoveryFailure(('HTTP \tResponse \tstatus \tfrom \tidentity \tURL \thost \tis \tnot \t200. \tGot \tstatus \t%r' % (resp.status,)), resp) \n\tresult.normalized_uri = resp.final_url \n\tresult.content_type = resp.headers.get('content-type') \n\tresult.xrds_uri = whereIsYadis(resp) \n\tif (result.xrds_uri and result.usedYadisLocation()): \n\t \tresp = fetchers.fetch(result.xrds_uri) \n\t \tif (resp.status not in (200, 206)): \n\t \t \texc = DiscoveryFailure(('HTTP \tResponse \tstatus \tfrom \tYadis \thost \tis \tnot \t200. \tGot \tstatus \t%r' % (resp.status,)), resp) \n\t \t \texc.identity_url = result.normalized_uri \n\t \t \traise exc \n\t \tresult.content_type = resp.headers.get('content-type') \n\tresult.response_text = resp.body \n\treturn result\n", 
" \trepo.set_config('username', username) \n\trepo.set_config('email', email) \n\trepo.set_config('password', ('*' * len(password))) \n\tclick.echo('Changed \tcredentials.')\n", 
" \tif args.tenant: \n\t \tproject_id = args.tenant \n\telif isinstance(cs.client, client.SessionClient): \n\t \tauth = cs.client.auth \n\t \tproject_id = auth.get_auth_ref(cs.client.session).project_id \n\telse: \n\t \tproject_id = cs.client.tenant_id \n\t_quota_show(cs.quotas.get(project_id, user_id=args.user, detail=args.detail))\n", 
" \tif args.tenant: \n\t \tproject_id = args.tenant \n\telif isinstance(cs.client, client.SessionClient): \n\t \tauth = cs.client.auth \n\t \tproject_id = auth.get_auth_ref(cs.client.session).project_id \n\telse: \n\t \tproject_id = cs.client.tenant_id \n\t_quota_show(cs.quotas.defaults(project_id))\n", 
" \t_quota_update(cs.quotas, args.tenant, args)\n", 
" \t_quota_show(cs.quota_classes.get(args.class_name))\n", 
" \t_quota_update(cs.quota_classes, args.class_name, args)\n", 
" \tclass Limit(object, ): \n\t \tdef __init__(self, name, used, max, other): \n\t \t \tself.name = name \n\t \t \tself.used = used \n\t \t \tself.max = max \n\t \t \tself.other = other \n\tlimit_map = {'maxServerMeta': {'name': 'Server \tMeta', 'type': 'max'}, 'maxPersonality': {'name': 'Personality', 'type': 'max'}, 'maxPersonalitySize': {'name': 'Personality \tSize', 'type': 'max'}, 'maxImageMeta': {'name': 'ImageMeta', 'type': 'max'}, 'maxTotalKeypairs': {'name': 'Keypairs', 'type': 'max'}, 'totalCoresUsed': {'name': 'Cores', 'type': 'used'}, 'maxTotalCores': {'name': 'Cores', 'type': 'max'}, 'totalRAMUsed': {'name': 'RAM', 'type': 'used'}, 'maxTotalRAMSize': {'name': 'RAM', 'type': 'max'}, 'totalInstancesUsed': {'name': 'Instances', 'type': 'used'}, 'maxTotalInstances': {'name': 'Instances', 'type': 'max'}, 'totalFloatingIpsUsed': {'name': 'FloatingIps', 'type': 'used'}, 'maxTotalFloatingIps': {'name': 'FloatingIps', 'type': 'max'}, 'totalSecurityGroupsUsed': {'name': 'SecurityGroups', 'type': 'used'}, 'maxSecurityGroups': {'name': 'SecurityGroups', 'type': 'max'}, 'maxSecurityGroupRules': {'name': 'SecurityGroupRules', 'type': 'max'}, 'maxServerGroups': {'name': 'ServerGroups', 'type': 'max'}, 'totalServerGroupsUsed': {'name': 'ServerGroups', 'type': 'used'}, 'maxServerGroupMembers': {'name': 'ServerGroupMembers', 'type': 'max'}} \n\tmax = {} \n\tused = {} \n\tother = {} \n\tlimit_names = [] \n\tcolumns = ['Name', 'Used', 'Max'] \n\tfor l in limits: \n\t \tmap = limit_map.get(l.name, {'name': l.name, 'type': 'other'}) \n\t \tname = map['name'] \n\t \tif (map['type'] == 'max'): \n\t \t \tmax[name] = l.value \n\t \telif (map['type'] == 'used'): \n\t \t \tused[name] = l.value \n\t \telse: \n\t \t \tother[name] = l.value \n\t \t \tcolumns.append('Other') \n\t \tif (name not in limit_names): \n\t \t \tlimit_names.append(name) \n\tlimit_names.sort() \n\tlimit_list = [] \n\tfor name in limit_names: \n\t \tl = Limit(name, used.get(name, '-'), max.get(name, '-'), other.get(name, '-')) \n\t \tlimit_list.append(l) \n\tutils.print_list(limit_list, columns)\n", 
" \tcolumns = ['Verb', 'URI', 'Value', 'Remain', 'Unit', 'Next_Available'] \n\tutils.print_list(limits, columns)\n", 
" \treturn IMPL.group_types_get_by_name_or_id(context, group_type_list)\n", 
" \tstatus = {} \n\tmemcache_results = [] \n\ttry: \n\t \tfor (cache_name, cache_props) in settings.CACHES.items(): \n\t \t \tresult = True \n\t \t \tbackend = cache_props['BACKEND'] \n\t \t \tlocation = cache_props['LOCATION'] \n\t \t \tif isinstance(location, basestring): \n\t \t \t \tlocation = location.split(';') \n\t \t \tif ('memcache' in backend): \n\t \t \t \tfor loc in location: \n\t \t \t \t \t(ip, port) = loc.split(':') \n\t \t \t \t \tresult = test_memcached(ip, int(port)) \n\t \t \t \t \tmemcache_results.append((INFO, ('%s:%s \t%s' % (ip, port, result)))) \n\t \tif (not memcache_results): \n\t \t \tmemcache_results.append((ERROR, 'memcache \tis \tnot \tconfigured.')) \n\t \telif (len(memcache_results) < 2): \n\t \t \tmemcache_results.append((ERROR, ('You \tshould \thave \tat \tleast \t2 \tmemcache \tservers. \tYou \thave \t%s.' % len(memcache_results)))) \n\t \telse: \n\t \t \tmemcache_results.append((INFO, 'memcached \tservers \tlook \tgood.')) \n\texcept Exception as exc: \n\t \tmemcache_results.append((ERROR, ('Exception \twhile \tlooking \tat \tmemcached: \t%s' % str(exc)))) \n\tstatus['memcached'] = memcache_results \n\tlibraries_results = [] \n\ttry: \n\t \tImage.new('RGB', (16, 16)).save(StringIO.StringIO(), 'JPEG') \n\t \tlibraries_results.append((INFO, 'PIL+JPEG: \tGot \tit!')) \n\texcept Exception as exc: \n\t \tlibraries_results.append((ERROR, ('PIL+JPEG: \tProbably \tmissing: \tFailed \tto \tcreate \ta \tjpeg \timage: \t%s' % exc))) \n\tstatus['libraries'] = libraries_results \n\tmsg = 'We \twant \tread \t+ \twrite.' \n\tfilepaths = ((settings.USER_AVATAR_PATH, (os.R_OK | os.W_OK), msg), (settings.IMAGE_UPLOAD_PATH, (os.R_OK | os.W_OK), msg), (settings.THUMBNAIL_UPLOAD_PATH, (os.R_OK | os.W_OK), msg), (settings.GALLERY_IMAGE_PATH, (os.R_OK | os.W_OK), msg), (settings.GALLERY_IMAGE_THUMBNAIL_PATH, (os.R_OK | os.W_OK), msg), (settings.GALLERY_VIDEO_PATH, (os.R_OK | os.W_OK), msg), (settings.GALLERY_VIDEO_THUMBNAIL_PATH, (os.R_OK | os.W_OK), msg), (settings.GROUP_AVATAR_PATH, (os.R_OK | os.W_OK), msg)) \n\tfilepath_results = [] \n\tfor (path, perms, notes) in filepaths: \n\t \tpath = os.path.join(settings.MEDIA_ROOT, path) \n\t \tpath_exists = os.path.isdir(path) \n\t \tpath_perms = os.access(path, perms) \n\t \tif (path_exists and path_perms): \n\t \t \tfilepath_results.append((INFO, ('%s: \t%s \t%s \t%s' % (path, path_exists, path_perms, notes)))) \n\tstatus['filepaths'] = filepath_results \n\trabbitmq_results = [] \n\ttry: \n\t \trabbit_conn = establish_connection(connect_timeout=5) \n\t \trabbit_conn.connect() \n\t \trabbitmq_results.append((INFO, 'Successfully \tconnected \tto \tRabbitMQ.')) \n\texcept (socket.error, IOError) as exc: \n\t \trabbitmq_results.append((ERROR, ('Error \tconnecting \tto \tRabbitMQ: \t%s' % str(exc)))) \n\texcept Exception as exc: \n\t \trabbitmq_results.append((ERROR, ('Exception \twhile \tlooking \tat \tRabbitMQ: \t%s' % str(exc)))) \n\tstatus['RabbitMQ'] = rabbitmq_results \n\tes_results = [] \n\ttry: \n\t \tes_utils.get_doctype_stats(es_utils.all_read_indexes()[0]) \n\t \tes_results.append((INFO, 'Successfully \tconnected \tto \tElasticSearch \tand \tindex \texists.')) \n\texcept es_utils.ES_EXCEPTIONS as exc: \n\t \tes_results.append((ERROR, ('ElasticSearch \tproblem: \t%s' % str(exc)))) \n\texcept Exception as exc: \n\t \tes_results.append((ERROR, ('Exception \twhile \tlooking \tat \tElasticSearch: \t%s' % str(exc)))) \n\tstatus['ElasticSearch'] = es_results \n\tredis_results = [] \n\tif hasattr(settings, 'REDIS_BACKENDS'): \n\t \tfor backend in settings.REDIS_BACKENDS: \n\t \t \ttry: \n\t \t \t \tredis_client(backend) \n\t \t \t \tredis_results.append((INFO, ('%s: \tPass!' % backend))) \n\t \t \texcept RedisError: \n\t \t \t \tredis_results.append((ERROR, ('%s: \tFail!' % backend))) \n\tstatus['Redis'] = redis_results \n\tstatus_code = 200 \n\tstatus_summary = {} \n\tfor (component, output) in status.items(): \n\t \tif (ERROR in [item[0] for item in output]): \n\t \t \tstatus_code = 500 \n\t \t \tstatus_summary[component] = False \n\t \telse: \n\t \t \tstatus_summary[component] = True \n\treturn render(request, 'services/monitor.html', {'component_status': status, 'status_summary': status_summary}, status=status_code)\n", 
" \tvalues = {'user_id': (ctxt.user_id or fake.USER_ID), 'project_id': (ctxt.project_id or fake.PROJECT_ID), 'volume_id': volume_id, 'status': status, 'display_name': display_name, 'display_description': display_description, 'container': 'fake', 'availability_zone': 'fake', 'service': 'fake', 'size': ((5 * 1024) * 1024), 'object_count': 22, 'host': socket.gethostname(), 'parent_id': parent_id, 'temp_volume_id': temp_volume_id, 'temp_snapshot_id': temp_snapshot_id, 'snapshot_id': snapshot_id, 'data_timestamp': data_timestamp} \n\tvalues.update(kwargs) \n\tbackup = objects.Backup(ctxt, **values) \n\tbackup.create() \n\treturn backup\n", 
" \timage = _find_image(cs, args.image) \n\t_print_image(image)\n", 
" \treturn IMPL.backup_get_all(context, filters=filters, marker=marker, limit=limit, offset=offset, sort_keys=sort_keys, sort_dirs=sort_dirs)\n", 
" \tif (not remove(local_file)): \n\t \tlogging.warning(\"No \tlocal \tbackup \tfile \t'{0}' \tto \tdelete. \tSkipping...\".format(local_file))\n", 
" \tlogging.info('Starting \tnew \tdb \trestore.') \n\tdb_ips = appscale_info.get_db_ips() \n\tif (not db_ips): \n\t \traise BRException('Unable \tto \tfind \tany \tCassandra \tmachines.') \n\tmachines_without_restore = [] \n\tfor db_ip in db_ips: \n\t \texit_code = utils.ssh(db_ip, keyname, 'ls \t{}'.format(path), method=subprocess.call) \n\t \tif (exit_code != ExitCodes.SUCCESS): \n\t \t \tmachines_without_restore.append(db_ip) \n\tif (machines_without_restore and (not force)): \n\t \tlogging.info('The \tfollowing \tmachines \tdo \tnot \thave \ta \trestore \tfile: \t{}'.format(machines_without_restore)) \n\t \tresponse = raw_input('Would \tyou \tlike \tto \tcontinue? \t[y/N] \t') \n\t \tif (response not in ['Y', 'y']): \n\t \t \treturn \n\tfor db_ip in db_ips: \n\t \tlogging.info('Stopping \tCassandra \ton \t{}'.format(db_ip)) \n\t \tsummary = utils.ssh(db_ip, keyname, 'monit \tsummary', method=subprocess.check_output) \n\t \tstatus = utils.monit_status(summary, CASSANDRA_MONIT_WATCH_NAME) \n\t \tretries = SERVICE_STOP_RETRIES \n\t \twhile (status != MonitStates.UNMONITORED): \n\t \t \tutils.ssh(db_ip, keyname, 'monit \tstop \t{}'.format(CASSANDRA_MONIT_WATCH_NAME), method=subprocess.call) \n\t \t \ttime.sleep(3) \n\t \t \tsummary = utils.ssh(db_ip, keyname, 'monit \tsummary', method=subprocess.check_output) \n\t \t \tstatus = utils.monit_status(summary, CASSANDRA_MONIT_WATCH_NAME) \n\t \t \tretries -= 1 \n\t \t \tif (retries < 0): \n\t \t \t \traise BRException('Unable \tto \tstop \tCassandra') \n\tcassandra_dir = '{}/cassandra'.format(APPSCALE_DATA_DIR) \n\tfor db_ip in db_ips: \n\t \tlogging.info('Restoring \tCassandra \tdata \ton \t{}'.format(db_ip)) \n\t \tclear_db = 'find \t{0} \t-regex \t\".*\\\\.\\\\(db\\\\|txt\\\\|log\\\\)$\" \t-exec \trm \t{{}} \t\\\\;'.format(cassandra_dir) \n\t \tutils.ssh(db_ip, keyname, clear_db) \n\t \tif (db_ip not in machines_without_restore): \n\t \t \tutils.ssh(db_ip, keyname, 'tar \txf \t{} \t-C \t{}'.format(path, cassandra_dir)) \n\t \t \tutils.ssh(db_ip, keyname, 'chown \t-R \tcassandra \t{}'.format(cassandra_dir)) \n\t \tutils.ssh(db_ip, keyname, 'monit \tstart \t{}'.format(CASSANDRA_MONIT_WATCH_NAME)) \n\tlogging.info('Done \twith \tdb \trestore.')\n", 
" \textensions = cs.list_extensions.show_all() \n\tfields = ['Name', 'Summary', 'Alias', 'Updated'] \n\tutils.print_list(extensions, fields)\n", 
" \treturn datetime.datetime.strptime(d['isostr'], _DATETIME_FORMAT)\n", 
" \treturn _aliases.get(alias.lower(), alias)\n", 
" \twith open(CHANGELOG) as f: \n\t \tcur_index = 0 \n\t \tfor line in f: \n\t \t \tmatch = re.search('^\\\\d+\\\\.\\\\d+\\\\.\\\\d+', line) \n\t \t \tif match: \n\t \t \t \tif (cur_index == index): \n\t \t \t \t \treturn match.group(0) \n\t \t \t \telse: \n\t \t \t \t \tcur_index += 1\n", 
" \tgitstr = _git_str() \n\tif (gitstr is None): \n\t \tgitstr = '' \n\tpath = os.path.join(BASEDIR, 'qutebrowser', 'git-commit-id') \n\twith _open(path, 'w', encoding='ascii') as f: \n\t \tf.write(gitstr)\n", 
" \tif (len(sys.argv) < 2): \n\t \treturn True \n\tinfo_commands = ['--help-commands', '--name', '--version', '-V', '--fullname', '--author', '--author-email', '--maintainer', '--maintainer-email', '--contact', '--contact-email', '--url', '--license', '--description', '--long-description', '--platforms', '--classifiers', '--keywords', '--provides', '--requires', '--obsoletes'] \n\tinfo_commands.extend(['egg_info', 'install_egg_info', 'rotate']) \n\tfor command in info_commands: \n\t \tif (command in sys.argv[1:]): \n\t \t \treturn False \n\treturn True\n", 
" \trefs = list_refs(refnames=[refname], repo_dir=repo_dir, limit_to_heads=True) \n\tl = tuple(islice(refs, 2)) \n\tif l: \n\t \tassert (len(l) == 1) \n\t \treturn l[0][1] \n\telse: \n\t \treturn None\n", 
" \tif isinstance(fileIshObject, TextIOBase): \n\t \treturn unicode \n\tif isinstance(fileIshObject, IOBase): \n\t \treturn bytes \n\tencoding = getattr(fileIshObject, 'encoding', None) \n\timport codecs \n\tif isinstance(fileIshObject, (codecs.StreamReader, codecs.StreamWriter)): \n\t \tif encoding: \n\t \t \treturn unicode \n\t \telse: \n\t \t \treturn bytes \n\tif (not _PY3): \n\t \tif isinstance(fileIshObject, file): \n\t \t \tif (encoding is not None): \n\t \t \t \treturn basestring \n\t \t \telse: \n\t \t \t \treturn bytes \n\t \tfrom cStringIO import InputType, OutputType \n\t \tfrom StringIO import StringIO \n\t \tif isinstance(fileIshObject, (StringIO, InputType, OutputType)): \n\t \t \treturn bytes \n\treturn default\n", 
" \tcmd = (_pkg(jail, chroot, root) + ['--version']) \n\treturn __salt__['cmd.run'](cmd).strip()\n", 
" \tcmd = (_pkg(jail, chroot, root) + ['--version']) \n\treturn __salt__['cmd.run'](cmd).strip()\n", 
" \tif (value in (u'1', u'0')): \n\t \treturn bool(int(value)) \n\traise ValueError((u'%r \tis \tnot \t0 \tor \t1' % value))\n", 
" \tif isinstance(obj, str): \n\t \tobj = obj.strip().lower() \n\t \tif (obj in ('true', 'yes', 'on', 'y', 't', '1')): \n\t \t \treturn True \n\t \tif (obj in ('false', 'no', 'off', 'n', 'f', '0')): \n\t \t \treturn False \n\t \traise ValueError(('Unable \tto \tinterpret \tvalue \t\"%s\" \tas \tboolean' % obj)) \n\treturn bool(obj)\n", 
" \tglobal _PARSING_CACHE \n\tif (string in _PARSING_CACHE): \n\t \tstack = _PARSING_CACHE[string] \n\telse: \n\t \tif (not _RE_STARTTOKEN.search(string)): \n\t \t \treturn string \n\t \tstack = ParseStack() \n\t \tncallable = 0 \n\t \tfor match in _RE_TOKEN.finditer(string): \n\t \t \tgdict = match.groupdict() \n\t \t \tif gdict['singlequote']: \n\t \t \t \tstack.append(gdict['singlequote']) \n\t \t \telif gdict['doublequote']: \n\t \t \t \tstack.append(gdict['doublequote']) \n\t \t \telif gdict['end']: \n\t \t \t \tif (ncallable <= 0): \n\t \t \t \t \tstack.append(')') \n\t \t \t \t \tcontinue \n\t \t \t \targs = [] \n\t \t \t \twhile stack: \n\t \t \t \t \toperation = stack.pop() \n\t \t \t \t \tif callable(operation): \n\t \t \t \t \t \tif (not strip): \n\t \t \t \t \t \t \tstack.append((operation, [arg for arg in reversed(args)])) \n\t \t \t \t \t \tncallable -= 1 \n\t \t \t \t \t \tbreak \n\t \t \t \t \telse: \n\t \t \t \t \t \targs.append(operation) \n\t \t \telif gdict['start']: \n\t \t \t \tfuncname = _RE_STARTTOKEN.match(gdict['start']).group(1) \n\t \t \t \ttry: \n\t \t \t \t \tstack.append(_INLINE_FUNCS[funcname]) \n\t \t \t \texcept KeyError: \n\t \t \t \t \tstack.append(_INLINE_FUNCS['nomatch']) \n\t \t \t \t \tstack.append(funcname) \n\t \t \t \tncallable += 1 \n\t \t \telif gdict['escaped']: \n\t \t \t \ttoken = gdict['escaped'].lstrip('\\\\') \n\t \t \t \tstack.append(token) \n\t \t \telif gdict['comma']: \n\t \t \t \tif (ncallable > 0): \n\t \t \t \t \tstack.append(None) \n\t \t \t \telse: \n\t \t \t \t \tstack.append(',') \n\t \t \telse: \n\t \t \t \tstack.append(gdict['rest']) \n\t \tif (ncallable > 0): \n\t \t \treturn string \n\t \tif ((_STACK_MAXSIZE > 0) and (_STACK_MAXSIZE < len(stack))): \n\t \t \treturn (string + gdict['stackfull'](*args, **kwargs)) \n\t \telse: \n\t \t \t_PARSING_CACHE[string] = stack \n\tdef _run_stack(item, depth=0): \n\t \tretval = item \n\t \tif isinstance(item, tuple): \n\t \t \tif strip: \n\t \t \t \treturn '' \n\t \t \telse: \n\t \t \t \t(func, arglist) = item \n\t \t \t \targs = [''] \n\t \t \t \tfor arg in arglist: \n\t \t \t \t \tif (arg is None): \n\t \t \t \t \t \targs.append('') \n\t \t \t \t \telse: \n\t \t \t \t \t \targs[(-1)] += _run_stack(arg, depth=(depth + 1)) \n\t \t \t \tkwargs['inlinefunc_stack_depth'] = depth \n\t \t \t \tretval = ('' if strip else func(*args, **kwargs)) \n\t \treturn utils.to_str(retval, force_string=True) \n\treturn ''.join((_run_stack(item) for item in _PARSING_CACHE[string]))\n", 
" \tif (type(s) is str): \n\t \treturn s \n\telse: \n\t \treturn s3_unicode(s).encode('utf-8', 'strict')\n", 
" \tif (not at): \n\t \tat = utcnow() \n\tst = at.strftime((_ISO8601_TIME_FORMAT if (not subsecond) else _ISO8601_TIME_FORMAT_SUBSECOND)) \n\ttz = (at.tzinfo.tzname(None) if at.tzinfo else 'UTC') \n\tst += ('Z' if (tz == 'UTC') else tz) \n\treturn st\n", 
" \ttry: \n\t \treturn iso8601.parse_date(timestr) \n\texcept iso8601.ParseError as e: \n\t \traise ValueError(encodeutils.exception_to_unicode(e)) \n\texcept TypeError as e: \n\t \traise ValueError(encodeutils.exception_to_unicode(e))\n", 
" \tnow = datetime.utcnow().replace(tzinfo=iso8601.iso8601.UTC) \n\tif iso: \n\t \treturn datetime_tuple_to_iso(now) \n\telse: \n\t \treturn now\n", 
" \treturn formatdate(timegm(dt.utctimetuple()))\n", 
" \toffset = timestamp.utcoffset() \n\tif (offset is None): \n\t \treturn timestamp \n\treturn (timestamp.replace(tzinfo=None) - offset)\n", 
" \tif os.path.isfile(db_file_name): \n\t \tfrom datetime import timedelta \n\t \tfile_datetime = datetime.fromtimestamp(os.stat(db_file_name).st_ctime) \n\t \tif ((datetime.today() - file_datetime) >= timedelta(hours=older_than)): \n\t \t \tif verbose: \n\t \t \t \tprint u'File \tis \told' \n\t \t \treturn True \n\t \telse: \n\t \t \tif verbose: \n\t \t \t \tprint u'File \tis \trecent' \n\t \t \treturn False \n\telse: \n\t \tif verbose: \n\t \t \tprint u'File \tdoes \tnot \texist' \n\t \treturn True\n", 
" \tif (not os.path.exists(source)): \n\t \traise DistutilsFileError((\"file \t'%s' \tdoes \tnot \texist\" % os.path.abspath(source))) \n\tif (not os.path.exists(target)): \n\t \treturn True \n\treturn (os.stat(source)[ST_MTIME] > os.stat(target)[ST_MTIME])\n", 
" \tif utcnow.override_time: \n\t \ttry: \n\t \t \treturn utcnow.override_time.pop(0) \n\t \texcept AttributeError: \n\t \t \treturn utcnow.override_time \n\tif with_timezone: \n\t \treturn datetime.datetime.now(tz=iso8601.iso8601.UTC) \n\treturn datetime.datetime.utcnow()\n", 
" \tif utcnow.override_time: \n\t \ttry: \n\t \t \treturn utcnow.override_time.pop(0) \n\t \texcept AttributeError: \n\t \t \treturn utcnow.override_time \n\tif with_timezone: \n\t \treturn datetime.datetime.now(tz=iso8601.iso8601.UTC) \n\treturn datetime.datetime.utcnow()\n", 
" \treturn isotime(datetime.datetime.utcfromtimestamp(timestamp), microsecond)\n", 
" \tif utcnow.override_time: \n\t \ttry: \n\t \t \treturn utcnow.override_time.pop(0) \n\t \texcept AttributeError: \n\t \t \treturn utcnow.override_time \n\tif with_timezone: \n\t \treturn datetime.datetime.now(tz=iso8601.iso8601.UTC) \n\treturn datetime.datetime.utcnow()\n", 
" \tmatch = standard_duration_re.match(value) \n\tif (not match): \n\t \tmatch = iso8601_duration_re.match(value) \n\tif match: \n\t \tkw = match.groupdict() \n\t \tsign = ((-1) if (kw.pop('sign', '+') == '-') else 1) \n\t \tif kw.get('microseconds'): \n\t \t \tkw['microseconds'] = kw['microseconds'].ljust(6, '0') \n\t \tif (kw.get('seconds') and kw.get('microseconds') and kw['seconds'].startswith('-')): \n\t \t \tkw['microseconds'] = ('-' + kw['microseconds']) \n\t \tkw = {k: float(v) for (k, v) in kw.items() if (v is not None)} \n\t \treturn (sign * datetime.timedelta(**kw))\n", 
" \treturn call_talib_with_ohlc(barDs, count, talib.CDLADVANCEBLOCK)\n", 
" \ttry: \n\t \tparent = next(klass.local_attr_ancestors(name)) \n\texcept (StopIteration, KeyError): \n\t \treturn None \n\ttry: \n\t \tmeth_node = parent[name] \n\texcept KeyError: \n\t \treturn None \n\tif isinstance(meth_node, astroid.Function): \n\t \treturn meth_node \n\treturn None\n", 
" \tif value.tzinfo: \n\t \tvalue = value.astimezone(UTC) \n\treturn (long((calendar.timegm(value.timetuple()) * 1000000L)) + value.microsecond)\n", 
" \t(p, u) = getparser(use_datetime=use_datetime, use_builtin_types=use_builtin_types) \n\tp.feed(data) \n\tp.close() \n\treturn (u.close(), u.getmethodname())\n", 
" \tlater = (mktime(date1.timetuple()) + (date1.microsecond / 1000000.0)) \n\tearlier = (mktime(date2.timetuple()) + (date2.microsecond / 1000000.0)) \n\treturn (later - earlier)\n", 
" \tgenerate_completions(event)\n", 
" \tdef _decorator(func): \n\t \tadd_arg(func, *args, **kwargs) \n\t \treturn func \n\treturn _decorator\n", 
" \tfor arg in args: \n\t \tvalue = os.environ.get(arg) \n\t \tif value: \n\t \t \treturn value \n\treturn kwargs.get('default', '')\n", 
" \tif (not hasattr(func, 'arguments')): \n\t \tfunc.arguments = [] \n\tif ((args, kwargs) not in func.arguments): \n\t \tfunc.arguments.insert(0, (args, kwargs))\n", 
" \tif (not hasattr(f, 'resource_manager_kwargs_hooks')): \n\t \tf.resource_manager_kwargs_hooks = [] \n\tnames = [h.__name__ for h in f.resource_manager_kwargs_hooks] \n\tif (hook.__name__ not in names): \n\t \tf.resource_manager_kwargs_hooks.append(hook)\n", 
" \thooks = getattr(f, 'resource_manager_kwargs_hooks', []) \n\textra_kwargs = {} \n\tfor hook in hooks: \n\t \thook_kwargs = hook(args) \n\t \thook_name = hook.__name__ \n\t \tconflicting_keys = (set(hook_kwargs.keys()) & set(extra_kwargs.keys())) \n\t \tif (conflicting_keys and (not allow_conflicts)): \n\t \t \tmsg = (_(\"Hook \t'%(hook_name)s' \tis \tattempting \tto \tredefine \tattributes \t'%(conflicting_keys)s'\") % {'hook_name': hook_name, 'conflicting_keys': conflicting_keys}) \n\t \t \traise exceptions.NoUniqueMatch(msg) \n\t \textra_kwargs.update(hook_kwargs) \n\treturn extra_kwargs\n", 
" \tfunc.unauthenticated = True \n\treturn func\n", 
" \treturn getattr(func, 'unauthenticated', False)\n", 
" \tdef inner(f): \n\t \tf.service_type = stype \n\t \treturn f \n\treturn inner\n", 
" \treturn getattr(f, 'service_type', None)\n", 
" \tif getattr(manager, 'is_alphanum_id_allowed', False): \n\t \ttry: \n\t \t \treturn manager.get(name_or_id) \n\t \texcept exceptions.NotFound: \n\t \t \tpass \n\ttry: \n\t \ttmp_id = encodeutils.safe_encode(name_or_id) \n\t \tif six.PY3: \n\t \t \ttmp_id = tmp_id.decode() \n\t \tuuid.UUID(tmp_id) \n\t \treturn manager.get(tmp_id) \n\texcept (TypeError, ValueError, exceptions.NotFound): \n\t \tpass \n\ttry: \n\t \ttry: \n\t \t \tresource = getattr(manager, 'resource_class', None) \n\t \t \tname_attr = (resource.NAME_ATTR if resource else 'name') \n\t \t \tkwargs = {name_attr: name_or_id} \n\t \t \tkwargs.update(find_args) \n\t \t \treturn manager.find(**kwargs) \n\t \texcept exceptions.NotFound: \n\t \t \tpass \n\t \ttry: \n\t \t \treturn manager.find(human_id=name_or_id, **find_args) \n\t \texcept exceptions.NotFound: \n\t \t \tpass \n\texcept exceptions.NoUniqueMatch: \n\t \tmsg = (_(\"Multiple \t%(class)s \tmatches \tfound \tfor \t'%(name)s', \tuse \tan \tID \tto \tbe \tmore \tspecific.\") % {'class': manager.resource_class.__name__.lower(), 'name': name_or_id}) \n\t \tif wrap_exception: \n\t \t \traise exceptions.CommandError(msg) \n\t \traise exceptions.NoUniqueMatch(msg) \n\ttry: \n\t \treturn manager.get(int(name_or_id)) \n\texcept (TypeError, ValueError, exceptions.NotFound): \n\t \tmsg = (_(\"No \t%(class)s \twith \ta \tname \tor \tID \tof \t'%(name)s' \texists.\") % {'class': manager.resource_class.__name__.lower(), 'name': name_or_id}) \n\t \tif wrap_exception: \n\t \t \traise exceptions.CommandError(msg) \n\t \traise exceptions.NotFound(404, msg)\n", 
" \ttry: \n\t \tif issubclass(*args): \n\t \t \treturn True \n\texcept TypeError: \n\t \tpass \n\treturn False\n", 
" \ttry: \n\t \t(module, attr) = name.rsplit('.', 1) \n\texcept ValueError as error: \n\t \traise ImproperlyConfigured(('Error \timporting \tclass \t%s \tin \t%s: \t\"%s\"' % (name, setting, error))) \n\ttry: \n\t \tmod = import_module(module) \n\texcept ImportError as error: \n\t \traise ImproperlyConfigured(('Error \timporting \tmodule \t%s \tin \t%s: \t\"%s\"' % (module, setting, error))) \n\ttry: \n\t \tcls = getattr(mod, attr) \n\texcept AttributeError: \n\t \traise ImproperlyConfigured(('Module \t\"%s\" \tdoes \tnot \tdefine \ta \t\"%s\" \tclass \tin \t%s' % (module, attr, setting))) \n\treturn cls\n", 
" \tvalue = Markup(value).striptags() \n\timport unicodedata \n\tfrom unidecode import unidecode \n\tvalue = unidecode(value) \n\tif isinstance(value, six.binary_type): \n\t \tvalue = value.decode(u'ascii') \n\tvalue = unicodedata.normalize(u'NFKD', value).lower() \n\tnew_subs = [] \n\tfor tpl in substitutions: \n\t \ttry: \n\t \t \t(src, dst, skip) = tpl \n\t \texcept ValueError: \n\t \t \t(src, dst) = tpl \n\t \t \tskip = False \n\t \tnew_subs.append((src, dst, skip)) \n\tsubstitutions = tuple(new_subs) \n\treplace = True \n\tfor (src, dst, skip) in substitutions: \n\t \torig_value = value \n\t \tvalue = value.replace(src.lower(), dst.lower()) \n\t \tif (value != orig_value): \n\t \t \treplace = (replace and (not skip)) \n\tif replace: \n\t \tvalue = re.sub(u'[^\\\\w\\\\s-]', u'', value).strip() \n\t \tvalue = re.sub(u'[-\\\\s]+', u'-', value) \n\telse: \n\t \tvalue = value.strip() \n\tvalue = value.encode(u'ascii', u'ignore') \n\treturn value.decode(u'ascii')\n", 
" \tif refresh: \n\t \treturn True \n\tif (os.path.isfile(cache_file) and (os.path.getsize(cache_file) > 0)): \n\t \tmod_time = os.path.getmtime(cache_file) \n\t \tcurrent_time = time.time() \n\t \tif ((mod_time + cache_expiration_time) > current_time): \n\t \t \treturn False \n\treturn True\n", 
" \tretVal = payload \n\tif payload: \n\t \tretVal = '' \n\t \ti = 0 \n\t \twhile (i < len(payload)): \n\t \t \tif ((payload[i] == '%') and (i < (len(payload) - 2)) and (payload[(i + 1):(i + 2)] in string.hexdigits) and (payload[(i + 2):(i + 3)] in string.hexdigits)): \n\t \t \t \tretVal += payload[i:(i + 3)] \n\t \t \t \ti += 3 \n\t \t \telse: \n\t \t \t \tretVal += ('%%%.2X' % ord(payload[i])) \n\t \t \t \ti += 1 \n\treturn retVal\n", 
" \tconf = global_conf.copy() \n\tconf.update(local_conf) \n\tdef auth_filter(app): \n\t \treturn KeystonePasswordAuthProtocol(app, conf) \n\treturn auth_filter\n", 
" \tdir_path = os.path.expanduser(dir_path) \n\tdirectory = os.path.normpath(dir_path) \n\tif (not os.path.isdir(directory)): \n\t \tmakedirs_perms(directory, user, group, mode) \n\treturn True\n", 
" \tkey = os.urandom(32) \n\tencoded_key = base64.b64encode(key).decode('utf-8') \n\tprint 'Base \t64 \tencoded \tencryption \tkey: \t{}'.format(encoded_key)\n", 
" \tif (not s): \n\t \treturn s \n\tencvec = [] \n\tmax_unencoded = ((maxlinelen * 3) // 4) \n\tfor i in range(0, len(s), max_unencoded): \n\t \tenc = b2a_base64(s[i:(i + max_unencoded)]).decode('ascii') \n\t \tif (enc.endswith(NL) and (eol != NL)): \n\t \t \tenc = (enc[:(-1)] + eol) \n\t \tencvec.append(enc) \n\treturn EMPTYSTRING.join(encvec)\n", 
" \treturn base64.b64encode(hashlib.sha1(payload).digest())\n", 
" \ts = hashlib.sha1(t) \n\treturn s.hexdigest()\n", 
" \treturn bool(re.match((('^' + '[\\\\:\\\\-]'.join((['([0-9a-f]{2})'] * 6))) + '$'), mac.lower()))\n", 
" \tkey = bytearray(key) \n\tif (mode == AESModeOfOperation.ModeOfOperation[u'CBC']): \n\t \tdata = append_PKCS7_padding(data) \n\tkeysize = len(key) \n\tassert (keysize in AES.KeySize.values()), u'invalid \tkey \tsize: \t{0}'.format(keysize) \n\tiv = bytearray([i for i in os.urandom(16)]) \n\tmoo = AESModeOfOperation() \n\t(mode, length, ciph) = moo.encrypt(data, mode, key, keysize, iv) \n\treturn (bytes(iv) + bytes(ciph))\n", 
" \tkey = bytearray(key) \n\tkeysize = len(key) \n\tassert (keysize in AES.KeySize.values()), (u'invalid \tkey \tsize: \t%s' % keysize) \n\tiv = bytearray(data[:16]) \n\tdata = bytearray(data[16:]) \n\tmoo = AESModeOfOperation() \n\tdecr = moo.decrypt(data, None, mode, key, keysize, iv) \n\tif (mode == AESModeOfOperation.ModeOfOperation[u'CBC']): \n\t \tdecr = strip_PKCS7_padding(decr) \n\treturn bytes(decr)\n", 
" \t_ensure_subprocess() \n\tif isinstance(formatted, six.string_types): \n\t \tdata = bytearray(formatted, _encoding_for_form(inform)) \n\telse: \n\t \tdata = formatted \n\tprocess = subprocess.Popen(['openssl', 'cms', '-verify', '-certfile', signing_cert_file_name, '-CAfile', ca_file_name, '-inform', 'PEM', '-nosmimecap', '-nodetach', '-nocerts', '-noattr'], stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE, close_fds=True) \n\t(output, err, retcode) = _process_communicate_handle_oserror(process, data, (signing_cert_file_name, ca_file_name)) \n\tif (retcode == OpensslCmsExitStatus.INPUT_FILE_READ_ERROR): \n\t \tif err.startswith('Error \treading \tS/MIME \tmessage'): \n\t \t \traise exceptions.CMSError(err) \n\t \telse: \n\t \t \traise exceptions.CertificateConfigError(err) \n\telif (retcode == OpensslCmsExitStatus.COMMAND_OPTIONS_PARSING_ERROR): \n\t \tif err.startswith('cms: \tCannot \topen \tinput \tfile'): \n\t \t \traise exceptions.CertificateConfigError(err) \n\t \telse: \n\t \t \traise subprocess.CalledProcessError(retcode, 'openssl', output=err) \n\telif (retcode != OpensslCmsExitStatus.SUCCESS): \n\t \traise subprocess.CalledProcessError(retcode, 'openssl', output=err) \n\treturn output\n", 
" \tfrom .singleton import S \n\tfrom .basic import Basic \n\tfrom .sympify import sympify, SympifyError \n\tfrom .compatibility import iterable \n\tif isinstance(item, Basic): \n\t \treturn item.sort_key(order=order) \n\tif iterable(item, exclude=string_types): \n\t \tif isinstance(item, dict): \n\t \t \targs = item.items() \n\t \t \tunordered = True \n\t \telif isinstance(item, set): \n\t \t \targs = item \n\t \t \tunordered = True \n\t \telse: \n\t \t \targs = list(item) \n\t \t \tunordered = False \n\t \targs = [default_sort_key(arg, order=order) for arg in args] \n\t \tif unordered: \n\t \t \targs = sorted(args) \n\t \t(cls_index, args) = (10, (len(args), tuple(args))) \n\telse: \n\t \tif (not isinstance(item, string_types)): \n\t \t \ttry: \n\t \t \t \titem = sympify(item) \n\t \t \texcept SympifyError: \n\t \t \t \tpass \n\t \t \telse: \n\t \t \t \tif isinstance(item, Basic): \n\t \t \t \t \treturn default_sort_key(item) \n\t \t(cls_index, args) = (0, (1, (str(item),))) \n\treturn ((cls_index, 0, item.__class__.__name__), args, S.One.sort_key(), S.One)\n", 
" \t_ensure_subprocess() \n\tif isinstance(data_to_sign, six.string_types): \n\t \tdata = bytearray(data_to_sign, encoding='utf-8') \n\telse: \n\t \tdata = data_to_sign \n\tprocess = subprocess.Popen(['openssl', 'cms', '-sign', '-signer', signing_cert_file_name, '-inkey', signing_key_file_name, '-outform', 'PEM', '-nosmimecap', '-nodetach', '-nocerts', '-noattr', '-md', message_digest], stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE, close_fds=True) \n\t(output, err, retcode) = _process_communicate_handle_oserror(process, data, (signing_cert_file_name, signing_key_file_name)) \n\tif ((retcode != OpensslCmsExitStatus.SUCCESS) or ('Error' in err)): \n\t \tif (retcode == OpensslCmsExitStatus.CREATE_CMS_READ_MIME_ERROR): \n\t \t \tLOG.error(_LE('Signing \terror: \tUnable \tto \tload \tcertificate \t- \tensure \tyou \thave \tconfigured \tPKI \twith \t\"keystone-manage \tpki_setup\"')) \n\t \telse: \n\t \t \tLOG.error(_LE('Signing \terror: \t%s'), err) \n\t \traise subprocess.CalledProcessError(retcode, 'openssl') \n\tif (outform == PKI_ASN1_FORM): \n\t \treturn output.decode('utf-8') \n\telse: \n\t \treturn output\n", 
" \t(path, senv) = salt.utils.url.split_env(path) \n\tif senv: \n\t \tsaltenv = senv \n\treturn _client().hash_file(path, saltenv)\n", 
" \ttry: \n\t \treturn obj.id \n\texcept AttributeError: \n\t \treturn obj\n", 
" \tcls = _code_map.get(response.status_code, ClientException) \n\tkwargs = {'code': response.status_code, 'method': method, 'url': url, 'request_id': None} \n\tif response.headers: \n\t \tkwargs['request_id'] = response.headers.get('x-compute-request-id') \n\t \tif (issubclass(cls, RetryAfterException) and ('retry-after' in response.headers)): \n\t \t \tkwargs['retry_after'] = response.headers.get('retry-after') \n\tif body: \n\t \tmessage = 'n/a' \n\t \tdetails = 'n/a' \n\t \tif hasattr(body, 'keys'): \n\t \t \tif ('message' in body): \n\t \t \t \tmessage = body.get('message') \n\t \t \t \tdetails = body.get('details') \n\t \t \telse: \n\t \t \t \terror = body[list(body)[0]] \n\t \t \t \tmessage = error.get('message') \n\t \t \t \tdetails = error.get('details') \n\t \tkwargs['message'] = message \n\t \tkwargs['details'] = details \n\treturn cls(**kwargs)\n", 
" \treturn datetime.datetime.strptime(d['isostr'], _DATETIME_FORMAT)\n", 
" \treturn _aliases.get(alias.lower(), alias)\n", 
" \twith open(CHANGELOG) as f: \n\t \tcur_index = 0 \n\t \tfor line in f: \n\t \t \tmatch = re.search('^\\\\d+\\\\.\\\\d+\\\\.\\\\d+', line) \n\t \t \tif match: \n\t \t \t \tif (cur_index == index): \n\t \t \t \t \treturn match.group(0) \n\t \t \t \telse: \n\t \t \t \t \tcur_index += 1\n", 
" \tgitstr = _git_str() \n\tif (gitstr is None): \n\t \tgitstr = '' \n\tpath = os.path.join(BASEDIR, 'qutebrowser', 'git-commit-id') \n\twith _open(path, 'w', encoding='ascii') as f: \n\t \tf.write(gitstr)\n", 
" \tif (len(sys.argv) < 2): \n\t \treturn True \n\tinfo_commands = ['--help-commands', '--name', '--version', '-V', '--fullname', '--author', '--author-email', '--maintainer', '--maintainer-email', '--contact', '--contact-email', '--url', '--license', '--description', '--long-description', '--platforms', '--classifiers', '--keywords', '--provides', '--requires', '--obsoletes'] \n\tinfo_commands.extend(['egg_info', 'install_egg_info', 'rotate']) \n\tfor command in info_commands: \n\t \tif (command in sys.argv[1:]): \n\t \t \treturn False \n\treturn True\n", 
" \trefs = list_refs(refnames=[refname], repo_dir=repo_dir, limit_to_heads=True) \n\tl = tuple(islice(refs, 2)) \n\tif l: \n\t \tassert (len(l) == 1) \n\t \treturn l[0][1] \n\telse: \n\t \treturn None\n", 
" \tif isinstance(fileIshObject, TextIOBase): \n\t \treturn unicode \n\tif isinstance(fileIshObject, IOBase): \n\t \treturn bytes \n\tencoding = getattr(fileIshObject, 'encoding', None) \n\timport codecs \n\tif isinstance(fileIshObject, (codecs.StreamReader, codecs.StreamWriter)): \n\t \tif encoding: \n\t \t \treturn unicode \n\t \telse: \n\t \t \treturn bytes \n\tif (not _PY3): \n\t \tif isinstance(fileIshObject, file): \n\t \t \tif (encoding is not None): \n\t \t \t \treturn basestring \n\t \t \telse: \n\t \t \t \treturn bytes \n\t \tfrom cStringIO import InputType, OutputType \n\t \tfrom StringIO import StringIO \n\t \tif isinstance(fileIshObject, (StringIO, InputType, OutputType)): \n\t \t \treturn bytes \n\treturn default\n", 
" \tcmd = (_pkg(jail, chroot, root) + ['--version']) \n\treturn __salt__['cmd.run'](cmd).strip()\n", 
" \tcmd = (_pkg(jail, chroot, root) + ['--version']) \n\treturn __salt__['cmd.run'](cmd).strip()\n", 
" \tconn = _get_conn(region=region, key=key, keyid=keyid, profile=profile) \n\tpolicy_arn = _get_policy_arn(policy_name, region, key, keyid, profile) \n\ttry: \n\t \tconn.set_default_policy_version(policy_arn, version_id) \n\t \tlog.info('Set \t{0} \tpolicy \tto \tversion \t{1}.'.format(policy_name, version_id)) \n\texcept boto.exception.BotoServerError as e: \n\t \taws = __utils__['boto.get_error'](e) \n\t \tlog.debug(aws) \n\t \tmsg = 'Failed \tto \tset \t{0} \tpolicy \tto \tversion \t{1}: \t{2}' \n\t \tlog.error(msg.format(policy_name, version_id, aws.get('message'))) \n\t \treturn False \n\treturn True\n", 
" \tvalidate = attr['validate'] \n\tkey = [k for k in validate.keys() if k.startswith('type:dict')] \n\tif (not key): \n\t \tLOG.warning(_LW('Unable \tto \tfind \tdata \ttype \tdescriptor \tfor \tattribute \t%s'), attr_name) \n\t \treturn \n\tdata = validate[key[0]] \n\tif (not isinstance(data, dict)): \n\t \tLOG.debug('Attribute \ttype \tdescriptor \tis \tnot \ta \tdict. \tUnable \tto \tgenerate \tany \tsub-attr \tpolicy \trule \tfor \t%s.', attr_name) \n\t \treturn \n\tsub_attr_rules = [policy.RuleCheck('rule', ('%s:%s:%s' % (action, attr_name, sub_attr_name))) for sub_attr_name in data if (sub_attr_name in target[attr_name])] \n\treturn policy.AndCheck(sub_attr_rules)\n", 
" \trule_method = ('telemetry:' + policy_name) \n\theaders = request.headers \n\tpolicy_dict = dict() \n\tpolicy_dict['roles'] = headers.get('X-Roles', '').split(',') \n\tpolicy_dict['user_id'] = headers.get('X-User-Id') \n\tpolicy_dict['project_id'] = headers.get('X-Project-Id') \n\tif ((_has_rule('default') or _has_rule(rule_method)) and (not pecan.request.enforcer.enforce(rule_method, {}, policy_dict))): \n\t \tpecan.core.abort(status_code=403, detail='RBAC \tAuthorization \tFailed')\n", 
" \ttry: \n\t \ttree_gen = parse(file, format, **kwargs) \n\t \ttree = next(tree_gen) \n\texcept StopIteration: \n\t \traise ValueError('There \tare \tno \ttrees \tin \tthis \tfile.') \n\ttry: \n\t \tnext(tree_gen) \n\texcept StopIteration: \n\t \treturn tree \n\telse: \n\t \traise ValueError('There \tare \tmultiple \ttrees \tin \tthis \tfile; \tuse \tparse() \tinstead.')\n", 
" \tfrom django.contrib.syndication.feeds import Feed as LegacyFeed \n\timport warnings \n\twarnings.warn('The \tsyndication \tfeed() \tview \tis \tdeprecated. \tPlease \tuse \tthe \tnew \tclass \tbased \tview \tAPI.', category=PendingDeprecationWarning) \n\tif (not feed_dict): \n\t \traise Http404('No \tfeeds \tare \tregistered.') \n\ttry: \n\t \t(slug, param) = url.split('/', 1) \n\texcept ValueError: \n\t \t(slug, param) = (url, '') \n\ttry: \n\t \tf = feed_dict[slug] \n\texcept KeyError: \n\t \traise Http404((\"Slug \t%r \tisn't \tregistered.\" % slug)) \n\tif (not issubclass(f, LegacyFeed)): \n\t \tinstance = f() \n\t \tinstance.feed_url = (getattr(f, 'feed_url', None) or request.path) \n\t \tinstance.title_template = (f.title_template or ('feeds/%s_title.html' % slug)) \n\t \tinstance.description_template = (f.description_template or ('feeds/%s_description.html' % slug)) \n\t \treturn instance(request) \n\ttry: \n\t \tfeedgen = f(slug, request).get_feed(param) \n\texcept FeedDoesNotExist: \n\t \traise Http404(('Invalid \tfeed \tparameters. \tSlug \t%r \tis \tvalid, \tbut \tother \tparameters, \tor \tlack \tthereof, \tare \tnot.' % slug)) \n\tresponse = HttpResponse(mimetype=feedgen.mime_type) \n\tfeedgen.write(response, 'utf-8') \n\treturn response\n", 
" \tret = {} \n\tcmd = __execute_kadmin('list_policies') \n\tif ((cmd['retcode'] != 0) or cmd['stderr']): \n\t \tret['comment'] = cmd['stderr'].splitlines()[(-1)] \n\t \tret['result'] = False \n\t \treturn ret \n\tret = {'policies': []} \n\tfor i in cmd['stdout'].splitlines()[1:]: \n\t \tret['policies'].append(i) \n\treturn ret\n", 
" \tdef call_and_assert(arg, context=None): \n\t \tif (context is None): \n\t \t \tcontext = {} \n\t \tresult = function(arg, context=context) \n\t \tassert (result == arg), 'Should \treturn \tthe \targument \tthat \twas \tpassed \tto \tit, \tunchanged \t({arg})'.format(arg=repr(arg)) \n\t \treturn result \n\treturn call_and_assert\n", 
" \tif (not app_messages): \n\t \tapp_messages = get_messages_for_app(app) \n\tif (not app_messages): \n\t \treturn \n\ttpath = frappe.get_pymodule_path(app, u'translations') \n\tfrappe.create_folder(tpath) \n\twrite_csv_file(os.path.join(tpath, (lang + u'.csv')), app_messages, (full_dict or get_full_dict(lang)))\n", 
" \ttb = treebuilders.getTreeBuilder(treebuilder) \n\tp = HTMLParser(tb, namespaceHTMLElements=namespaceHTMLElements) \n\treturn p.parse(doc, encoding=encoding)\n", 
" \tfrom config import get_os, get_checks_places, get_valid_check_class \n\tosname = get_os() \n\tchecks_places = get_checks_places(osname, agentConfig) \n\tfor check_path_builder in checks_places: \n\t \tcheck_path = check_path_builder(check_name) \n\t \tif (not os.path.exists(check_path)): \n\t \t \tcontinue \n\t \t(check_is_valid, check_class, load_failure) = get_valid_check_class(check_name, check_path) \n\t \tif check_is_valid: \n\t \t \treturn check_class \n\tlog.warning(('Failed \tto \tload \tthe \tcheck \tclass \tfor \t%s.' % check_name)) \n\treturn None\n", 
" \ttry: \n\t \t(module, attr) = name.rsplit('.', 1) \n\texcept ValueError as error: \n\t \traise ImproperlyConfigured(('Error \timporting \tclass \t%s \tin \t%s: \t\"%s\"' % (name, setting, error))) \n\ttry: \n\t \tmod = import_module(module) \n\texcept ImportError as error: \n\t \traise ImproperlyConfigured(('Error \timporting \tmodule \t%s \tin \t%s: \t\"%s\"' % (module, setting, error))) \n\ttry: \n\t \tcls = getattr(mod, attr) \n\texcept AttributeError: \n\t \traise ImproperlyConfigured(('Module \t\"%s\" \tdoes \tnot \tdefine \ta \t\"%s\" \tclass \tin \t%s' % (module, attr, setting))) \n\treturn cls\n", 
" \tos.environ['LANG'] = 'C' \n\tcli = _DRIVERS.get('HORCM') \n\treturn importutils.import_object('.'.join([_DRIVER_DIR, cli[driver_info['proto']]]), conf, driver_info, db)\n", 
" \tos.environ['LANG'] = 'C' \n\tcli = _DRIVERS.get('HORCM') \n\treturn importutils.import_object('.'.join([_DRIVER_DIR, cli[driver_info['proto']]]), conf, driver_info, db)\n", 
" \tif name.startswith('.'): \n\t \tif (not package): \n\t \t \traise TypeError(\"relative \timports \trequire \tthe \t'package' \targument\") \n\t \tlevel = 0 \n\t \tfor character in name: \n\t \t \tif (character != '.'): \n\t \t \t \tbreak \n\t \t \tlevel += 1 \n\t \tname = _resolve_name(name[level:], package, level) \n\t__import__(name) \n\treturn sys.modules[name]\n", 
" \tif (not at): \n\t \tat = utcnow() \n\tst = at.strftime((_ISO8601_TIME_FORMAT if (not subsecond) else _ISO8601_TIME_FORMAT_SUBSECOND)) \n\ttz = (at.tzinfo.tzname(None) if at.tzinfo else 'UTC') \n\tst += ('Z' if (tz == 'UTC') else tz) \n\treturn st\n", 
" \ttry: \n\t \treturn iso8601.parse_date(timestr) \n\texcept iso8601.ParseError as e: \n\t \traise ValueError(encodeutils.exception_to_unicode(e)) \n\texcept TypeError as e: \n\t \traise ValueError(encodeutils.exception_to_unicode(e))\n", 
" \tnow = datetime.utcnow().replace(tzinfo=iso8601.iso8601.UTC) \n\tif iso: \n\t \treturn datetime_tuple_to_iso(now) \n\telse: \n\t \treturn now\n", 
" \treturn formatdate(timegm(dt.utctimetuple()))\n", 
" \toffset = timestamp.utcoffset() \n\tif (offset is None): \n\t \treturn timestamp \n\treturn (timestamp.replace(tzinfo=None) - offset)\n", 
" \tif os.path.isfile(db_file_name): \n\t \tfrom datetime import timedelta \n\t \tfile_datetime = datetime.fromtimestamp(os.stat(db_file_name).st_ctime) \n\t \tif ((datetime.today() - file_datetime) >= timedelta(hours=older_than)): \n\t \t \tif verbose: \n\t \t \t \tprint u'File \tis \told' \n\t \t \treturn True \n\t \telse: \n\t \t \tif verbose: \n\t \t \t \tprint u'File \tis \trecent' \n\t \t \treturn False \n\telse: \n\t \tif verbose: \n\t \t \tprint u'File \tdoes \tnot \texist' \n\t \treturn True\n", 
" \tif (not os.path.exists(source)): \n\t \traise DistutilsFileError((\"file \t'%s' \tdoes \tnot \texist\" % os.path.abspath(source))) \n\tif (not os.path.exists(target)): \n\t \treturn True \n\treturn (os.stat(source)[ST_MTIME] > os.stat(target)[ST_MTIME])\n", 
" \tif utcnow.override_time: \n\t \ttry: \n\t \t \treturn utcnow.override_time.pop(0) \n\t \texcept AttributeError: \n\t \t \treturn utcnow.override_time \n\tif with_timezone: \n\t \treturn datetime.datetime.now(tz=iso8601.iso8601.UTC) \n\treturn datetime.datetime.utcnow()\n", 
" \tif utcnow.override_time: \n\t \ttry: \n\t \t \treturn utcnow.override_time.pop(0) \n\t \texcept AttributeError: \n\t \t \treturn utcnow.override_time \n\tif with_timezone: \n\t \treturn datetime.datetime.now(tz=iso8601.iso8601.UTC) \n\treturn datetime.datetime.utcnow()\n", 
" \treturn isotime(datetime.datetime.utcfromtimestamp(timestamp), microsecond)\n", 
" \tif utcnow.override_time: \n\t \ttry: \n\t \t \treturn utcnow.override_time.pop(0) \n\t \texcept AttributeError: \n\t \t \treturn utcnow.override_time \n\tif with_timezone: \n\t \treturn datetime.datetime.now(tz=iso8601.iso8601.UTC) \n\treturn datetime.datetime.utcnow()\n", 
" \tmatch = standard_duration_re.match(value) \n\tif (not match): \n\t \tmatch = iso8601_duration_re.match(value) \n\tif match: \n\t \tkw = match.groupdict() \n\t \tsign = ((-1) if (kw.pop('sign', '+') == '-') else 1) \n\t \tif kw.get('microseconds'): \n\t \t \tkw['microseconds'] = kw['microseconds'].ljust(6, '0') \n\t \tif (kw.get('seconds') and kw.get('microseconds') and kw['seconds'].startswith('-')): \n\t \t \tkw['microseconds'] = ('-' + kw['microseconds']) \n\t \tkw = {k: float(v) for (k, v) in kw.items() if (v is not None)} \n\t \treturn (sign * datetime.timedelta(**kw))\n", 
" \treturn call_talib_with_ohlc(barDs, count, talib.CDLADVANCEBLOCK)\n", 
" \ttry: \n\t \tparent = next(klass.local_attr_ancestors(name)) \n\texcept (StopIteration, KeyError): \n\t \treturn None \n\ttry: \n\t \tmeth_node = parent[name] \n\texcept KeyError: \n\t \treturn None \n\tif isinstance(meth_node, astroid.Function): \n\t \treturn meth_node \n\treturn None\n", 
" \tif value.tzinfo: \n\t \tvalue = value.astimezone(UTC) \n\treturn (long((calendar.timegm(value.timetuple()) * 1000000L)) + value.microsecond)\n", 
" \t(p, u) = getparser(use_datetime=use_datetime, use_builtin_types=use_builtin_types) \n\tp.feed(data) \n\tp.close() \n\treturn (u.close(), u.getmethodname())\n", 
" \tlater = (mktime(date1.timetuple()) + (date1.microsecond / 1000000.0)) \n\tearlier = (mktime(date2.timetuple()) + (date2.microsecond / 1000000.0)) \n\treturn (later - earlier)\n", 
" \tgenerate_completions(event)\n", 
" \tunique_id = ''.join((random.choice((string.ascii_lowercase + string.digits)) for _ in range(CACHE_SLUG_PROD_LENGTH))) \n\treturn unique_id\n", 
" \texpire_delta = datetime.timedelta(seconds=CONF.token.expiration) \n\texpires_at = (timeutils.utcnow() + expire_delta) \n\treturn expires_at.replace(microsecond=0)\n", 
" \t_quota_update(cs.quotas, args.tenant, args)\n", 
" \tgroups = [name.replace('check_', '') for name in request.POST.keys() if name.startswith('check_')] \n\tindexes = [write_index(group) for group in groups] \n\trecreate_indexes(indexes=indexes) \n\tmapping_types_names = [mt.get_mapping_type_name() for mt in get_mapping_types() if (mt.get_index_group() in groups)] \n\treindex(mapping_types_names) \n\treturn HttpResponseRedirect(request.path)\n", 
" \theaders = [('X-Subject-Token', token_id)] \n\treturn wsgi.render_response(body=token_data, status=(http_client.OK, http_client.responses[http_client.OK]), headers=headers)\n", 
" \tconf = global_conf.copy() \n\tconf.update(local_conf) \n\tdef auth_filter(app): \n\t \treturn KeystonePasswordAuthProtocol(app, conf) \n\treturn auth_filter\n", 
" \tif (headers is None): \n\t \theaders = [] \n\telse: \n\t \theaders = list(headers) \n\theaders.append(('Vary', 'X-Auth-Token')) \n\tif (body is None): \n\t \tbody = '' \n\t \tstatus = (status or (http_client.NO_CONTENT, http_client.responses[http_client.NO_CONTENT])) \n\telse: \n\t \tcontent_types = [v for (h, v) in headers if (h == 'Content-Type')] \n\t \tif content_types: \n\t \t \tcontent_type = content_types[0] \n\t \telse: \n\t \t \tcontent_type = None \n\t \tif ((content_type is None) or (content_type in JSON_ENCODE_CONTENT_TYPES)): \n\t \t \tbody = jsonutils.dump_as_bytes(body, cls=utils.SmarterEncoder) \n\t \t \tif (content_type is None): \n\t \t \t \theaders.append(('Content-Type', 'application/json')) \n\t \tstatus = (status or (http_client.OK, http_client.responses[http_client.OK])) \n\tdef _convert_to_str(headers): \n\t \tstr_headers = [] \n\t \tfor header in headers: \n\t \t \tstr_header = [] \n\t \t \tfor value in header: \n\t \t \t \tif (not isinstance(value, str)): \n\t \t \t \t \tstr_header.append(str(value)) \n\t \t \t \telse: \n\t \t \t \t \tstr_header.append(value) \n\t \t \tstr_headers.append(tuple(str_header)) \n\t \treturn str_headers \n\theaders = _convert_to_str(headers) \n\tresp = webob.Response(body=body, status=('%d \t%s' % status), headerlist=headers) \n\tif (method and (method.upper() == 'HEAD')): \n\t \tstored_headers = resp.headers.copy() \n\t \tresp.body = '' \n\t \tfor (header, value) in stored_headers.items(): \n\t \t \tresp.headers[header] = value \n\treturn resp\n", 
" \terror_message = error.args[0] \n\tmessage = oslo_i18n.translate(error_message, desired_locale=user_locale) \n\tif (message is error_message): \n\t \tmessage = six.text_type(message) \n\tbody = {'error': {'code': error.code, 'title': error.title, 'message': message}} \n\theaders = [] \n\tif isinstance(error, exception.AuthPluginException): \n\t \tbody['error']['identity'] = error.authentication \n\telif isinstance(error, exception.Unauthorized): \n\t \tlocal_context = {} \n\t \tif request: \n\t \t \tlocal_context = {'environment': request.environ} \n\t \telif (context and ('environment' in context)): \n\t \t \tlocal_context = {'environment': context['environment']} \n\t \turl = Application.base_url(local_context, 'public') \n\t \theaders.append(('WWW-Authenticate', ('Keystone \turi=\"%s\"' % url))) \n\treturn render_response(status=(error.code, error.title), body=body, headers=headers)\n", 
" \tglobal _loaded_packages \n\t_init() \n\tif VERBOSE: \n\t \tprint('Load \tdependencies \tfor \tpackage', package) \n\tif (not load_recursive): \n\t \tmanifest_file = roslib.manifest.manifest_file(package, True) \n\t \tm = roslib.manifest.parse_file(manifest_file) \n\t \tdepends = [d.package for d in m.depends] \n\telse: \n\t \tdepends = rospkg.RosPack().get_depends(package, implicit=True) \n\tmsgs = [] \n\tfailures = [] \n\tfor d in depends: \n\t \tif VERBOSE: \n\t \t \tprint('Load \tdependency', d) \n\t \tif ((d in _loaded_packages) or (d == package)): \n\t \t \tcontinue \n\t \t_loaded_packages.append(d) \n\t \t(specs, failed) = get_pkg_msg_specs(d) \n\t \tmsgs.extend(specs) \n\t \tfailures.extend(failed) \n\tfor (key, spec) in msgs: \n\t \tregister(key, spec)\n", 
" \tblocks = [] \n\tlines = [] \n\tblocks.append(('global', global_registry.dump_objects())) \n\tfor win_id in window_registry: \n\t \tregistry = _get_registry('window', window=win_id) \n\t \tblocks.append(('window-{}'.format(win_id), registry.dump_objects())) \n\t \ttab_registry = get('tab-registry', scope='window', window=win_id) \n\t \tfor (tab_id, tab) in tab_registry.items(): \n\t \t \tdump = tab.registry.dump_objects() \n\t \t \tdata = [(' \t \t \t \t' + line) for line in dump] \n\t \t \tblocks.append((' \t \t \t \ttab-{}'.format(tab_id), data)) \n\tfor (name, data) in blocks: \n\t \tlines.append('') \n\t \tlines.append('{} \tobject \tregistry \t- \t{} \tobjects:'.format(name, len(data))) \n\t \tfor line in data: \n\t \t \tlines.append(' \t \t \t \t{}'.format(line)) \n\treturn lines\n", 
" \t@functools.wraps(f) \n\tdef wrapper(*args, **kw): \n\t \ttry: \n\t \t \treturn f(*args, **kw) \n\t \texcept Exception as e: \n\t \t \tLOG.debug(e, exc_info=True) \n\t \t \tLOG.critical(e) \n\t \t \tsys.exit(1) \n\treturn wrapper\n", 
" \tdef initialize(self, *args, **kwargs): \n\t \tcls = type(self) \n\t \tfor (k, v) in kwargs.items(): \n\t \t \tif hasattr(cls, k): \n\t \t \t \tattr = getattr(cls, k) \n\t \t \t \tif isinstance(attr, InstrumentedAttribute): \n\t \t \t \t \tcolumn = attr.property.columns[0] \n\t \t \t \t \tif isinstance(column.type, String): \n\t \t \t \t \t \tif (not isinstance(v, six.text_type)): \n\t \t \t \t \t \t \tv = six.text_type(v) \n\t \t \t \t \t \tif (column.type.length and (column.type.length < len(v))): \n\t \t \t \t \t \t \traise exception.StringLengthExceeded(string=v, type=k, length=column.type.length) \n\t \tinit(self, *args, **kwargs) \n\treturn initialize\n", 
" \t_conflict_msg = 'Conflict \t%(conflict_type)s: \t%(details)s' \n\tdef decorator(method): \n\t \t@functools.wraps(method) \n\t \tdef wrapper(*args, **kwargs): \n\t \t \ttry: \n\t \t \t \treturn method(*args, **kwargs) \n\t \t \texcept db_exception.DBDuplicateEntry as e: \n\t \t \t \tLOG.debug(_conflict_msg, {'conflict_type': conflict_type, 'details': six.text_type(e)}) \n\t \t \t \tname = None \n\t \t \t \tfield = None \n\t \t \t \tdomain_id = None \n\t \t \t \tparams = args[1:] \n\t \t \t \tfor arg in params: \n\t \t \t \t \tif ('name' in arg): \n\t \t \t \t \t \tfield = 'name' \n\t \t \t \t \t \tname = arg['name'] \n\t \t \t \t \telif ('id' in arg): \n\t \t \t \t \t \tfield = 'ID' \n\t \t \t \t \t \tname = arg['id'] \n\t \t \t \t \tif ('domain_id' in arg): \n\t \t \t \t \t \tdomain_id = arg['domain_id'] \n\t \t \t \tmsg = _('Duplicate \tentry') \n\t \t \t \tif (name and domain_id): \n\t \t \t \t \tmsg = (_('Duplicate \tentry \tfound \twith \t%(field)s \t%(name)s \tat \tdomain \tID \t%(domain_id)s') % {'field': field, 'name': name, 'domain_id': domain_id}) \n\t \t \t \telif name: \n\t \t \t \t \tmsg = (_('Duplicate \tentry \tfound \twith \t%(field)s \t%(name)s') % {'field': field, 'name': name}) \n\t \t \t \telif domain_id: \n\t \t \t \t \tmsg = (_('Duplicate \tentry \tat \tdomain \tID \t%s') % domain_id) \n\t \t \t \traise exception.Conflict(type=conflict_type, details=msg) \n\t \t \texcept db_exception.DBError as e: \n\t \t \t \tif isinstance(e.inner_exception, IntegrityError): \n\t \t \t \t \tLOG.debug(_conflict_msg, {'conflict_type': conflict_type, 'details': six.text_type(e)}) \n\t \t \t \t \traise exception.UnexpectedError((_('An \tunexpected \terror \toccurred \twhen \ttrying \tto \tstore \t%s') % conflict_type)) \n\t \t \t \traise \n\t \treturn wrapper \n\treturn decorator\n", 
" \tsession = get_session(conf, requests_session=requests_session, group=group) \n\treturn ks_client_v3.Client(session=session, trust_id=trust_id)\n", 
" \tparams = _get_api_params(api_url=api_url, page_id=page_id, api_key=api_key, api_version=api_version) \n\tif (not _validate_api_params(params)): \n\t \tlog.error('Invalid \tAPI \tparams.') \n\t \tlog.error(params) \n\t \treturn {'result': False, 'comment': 'Invalid \tAPI \tparams. \tSee \tlog \tfor \tdetails'} \n\theaders = _get_headers(params) \n\tretrieve_url = '{base_url}/v{version}/pages/{page_id}/{endpoint}.json'.format(base_url=params['api_url'], version=params['api_version'], page_id=params['api_page_id'], endpoint=endpoint) \n\treturn _http_request(retrieve_url, headers=headers)\n", 
" \tparams = _get_api_params(api_url=api_url, page_id=page_id, api_key=api_key, api_version=api_version) \n\tif (not _validate_api_params(params)): \n\t \tlog.error('Invalid \tAPI \tparams.') \n\t \tlog.error(params) \n\t \treturn {'result': False, 'comment': 'Invalid \tAPI \tparams. \tSee \tlog \tfor \tdetails'} \n\theaders = _get_headers(params) \n\tretrieve_url = '{base_url}/v{version}/pages/{page_id}/{endpoint}.json'.format(base_url=params['api_url'], version=params['api_version'], page_id=params['api_page_id'], endpoint=endpoint) \n\treturn _http_request(retrieve_url, headers=headers)\n", 
" \tbase = config.get_config()['plotly_api_domain'] \n\tformatter = {'base': base, 'resource': resource, 'id': id, 'route': route} \n\tif id: \n\t \tif route: \n\t \t \turl = '{base}/v2/{resource}/{id}/{route}'.format(**formatter) \n\t \telse: \n\t \t \turl = '{base}/v2/{resource}/{id}'.format(**formatter) \n\telif route: \n\t \turl = '{base}/v2/{resource}/{route}'.format(**formatter) \n\telse: \n\t \turl = '{base}/v2/{resource}'.format(**formatter) \n\treturn url\n", 
" \tdomain_id = request.session.get('domain_context', None) \n\tdomain_name = request.session.get('domain_context_name', None) \n\tif ((VERSIONS.active >= 3) and (domain_id is None)): \n\t \tdomain_id = request.user.user_domain_id \n\t \tdomain_name = request.user.user_domain_name \n\t \tif (get_name and (not request.user.is_federated)): \n\t \t \ttry: \n\t \t \t \tdomain = domain_get(request, domain_id) \n\t \t \t \tdomain_name = domain.name \n\t \t \texcept exceptions.NotAuthorized: \n\t \t \t \tLOG.debug(('Cannot \tretrieve \tdomain \tinformation \tfor \tuser \t(%s) \tthat \tdoes \tnot \thave \tan \tadmin \trole \ton \tproject \t(%s)' % (request.user.username, request.user.project_name))) \n\t \t \texcept Exception: \n\t \t \t \tLOG.warning(('Unable \tto \tretrieve \tDomain: \t%s' % domain_id)) \n\tdomain = base.APIDictWrapper({'id': domain_id, 'name': domain_name}) \n\treturn domain\n", 
" \ttry: \n\t \tconn = _get_conn(region=region, key=key, keyid=keyid, profile=profile) \n\t \tconn.delete_elasticsearch_domain(DomainName=DomainName) \n\t \treturn {'deleted': True} \n\texcept ClientError as e: \n\t \treturn {'deleted': False, 'error': salt.utils.boto3.get_error(e)}\n", 
" \tpatches = [] \n\tfilename = None \n\tdiff = [] \n\tfor line in data.splitlines(True): \n\t \tnew_filename = None \n\t \tif line.startswith('Index:'): \n\t \t \t(unused, new_filename) = line.split(':', 1) \n\t \t \tnew_filename = new_filename.strip() \n\t \telif line.startswith('Property \tchanges \ton:'): \n\t \t \t(unused, temp_filename) = line.split(':', 1) \n\t \t \ttemp_filename = to_slash(temp_filename.strip()) \n\t \t \tif (temp_filename != filename): \n\t \t \t \tnew_filename = temp_filename \n\t \tif new_filename: \n\t \t \tif (filename and diff): \n\t \t \t \tpatches.append((filename, ''.join(diff))) \n\t \t \tfilename = new_filename \n\t \t \tdiff = [line] \n\t \t \tcontinue \n\t \tif (diff is not None): \n\t \t \tdiff.append(line) \n\tif (filename and diff): \n\t \tpatches.append((filename, ''.join(diff))) \n\treturn patches\n", 
" \tid_resolver = (id_resolver or _IdentityIdResolver()) \n\treturn _EntityConverter(id_resolver)\n", 
" \tglobal _REPOSITORY \n\trel_path = 'migrate_repo' \n\tif (database == 'api'): \n\t \trel_path = os.path.join('api_migrations', 'migrate_repo') \n\tpath = os.path.join(os.path.abspath(os.path.dirname(__file__)), rel_path) \n\tassert os.path.exists(path) \n\tif (_REPOSITORY.get(database) is None): \n\t \t_REPOSITORY[database] = Repository(path) \n\treturn _REPOSITORY[database]\n", 
" \tinner = query[1:(-1)] \n\tif inner.startswith('&'): \n\t \t(l, r) = _paren_groups(inner[1:]) \n\t \treturn (_match_query(l, attrs) and _match_query(r, attrs)) \n\tif inner.startswith('|'): \n\t \t(l, r) = _paren_groups(inner[1:]) \n\t \treturn (_match_query(l, attrs) or _match_query(r, attrs)) \n\tif inner.startswith('!'): \n\t \treturn (not _match_query(query[2:(-1)], attrs)) \n\t(k, _sep, v) = inner.partition('=') \n\treturn _match(k, v, attrs)\n", 
" \tcount = 0 \n\tstart = 0 \n\tresult = [] \n\tfor pos in range(len(source)): \n\t \tif (source[pos] == '('): \n\t \t \tif (count == 0): \n\t \t \t \tstart = pos \n\t \t \tcount += 1 \n\t \tif (source[pos] == ')'): \n\t \t \tcount -= 1 \n\t \t \tif (count == 0): \n\t \t \t \tresult.append(source[start:(pos + 1)]) \n\treturn result\n", 
" \tif (key not in attrs): \n\t \treturn False \n\tif (value == '*'): \n\t \treturn True \n\tif (key != 'objectclass'): \n\t \treturn (value in attrs[key]) \n\tvalues = _subs(value) \n\tfor v in values: \n\t \tif (v in attrs[key]): \n\t \t \treturn True \n\treturn False\n", 
" \tsubs = {'groupOfNames': ['novaProject']} \n\tif (value in subs): \n\t \treturn ([value] + subs[value]) \n\treturn [value]\n", 
" \tglobal _FILE_CACHE \n\tif force_reload: \n\t \tdelete_cached_file(filename) \n\treloaded = False \n\tmtime = os.path.getmtime(filename) \n\tcache_info = _FILE_CACHE.setdefault(filename, {}) \n\tif ((not cache_info) or (mtime > cache_info.get('mtime', 0))): \n\t \tLOG.debug('Reloading \tcached \tfile \t%s', filename) \n\t \twith open(filename) as fap: \n\t \t \tcache_info['data'] = fap.read() \n\t \tcache_info['mtime'] = mtime \n\t \treloaded = True \n\treturn (reloaded, cache_info['data'])\n", 
" \tmax_length = CONF.identity.max_password_length \n\ttry: \n\t \tif (len(password) > max_length): \n\t \t \tif CONF.strict_password_check: \n\t \t \t \traise exception.PasswordVerificationError(size=max_length) \n\t \t \telse: \n\t \t \t \tmsg = _LW('Truncating \tuser \tpassword \tto \t%d \tcharacters.') \n\t \t \t \tLOG.warning(msg, max_length) \n\t \t \t \treturn password[:max_length] \n\t \telse: \n\t \t \treturn password \n\texcept TypeError: \n\t \traise exception.ValidationError(attribute='string', target='password')\n", 
" \tpassword = user.get('password') \n\tif (password is None): \n\t \treturn user \n\treturn dict(user, password=hash_password(password))\n", 
" \tpassword = user.get('password') \n\tif (password is None): \n\t \treturn user \n\treturn dict(user, password=hash_password(password))\n", 
" \tpassword_utf8 = verify_length_and_trunc_password(password).encode('utf-8') \n\treturn passlib.hash.sha512_crypt.hash(password_utf8, rounds=CONF.crypt_strength)\n", 
" \tpassword_utf8 = verify_length_and_trunc_password(password).encode('utf-8') \n\treturn passlib.hash.sha512_crypt.hash(password_utf8, rounds=CONF.crypt_strength)\n", 
" \tif ((password is None) or (hashed is None)): \n\t \treturn False \n\tpassword_utf8 = verify_length_and_trunc_password(password).encode('utf-8') \n\treturn passlib.hash.sha512_crypt.verify(password_utf8, hashed)\n", 
" \ttry: \n\t \tprocess = subprocess.Popen(stdout=stdout, *popenargs, **kwargs) \n\t \t(output, unused_err) = process.communicate() \n\t \tretcode = process.poll() \n\t \tif retcode: \n\t \t \tcmd = kwargs.get('args') \n\t \t \tif (cmd is None): \n\t \t \t \tcmd = popenargs[0] \n\t \t \terror = subprocess.CalledProcessError(retcode, cmd) \n\t \t \terror.output = output \n\t \t \traise error \n\t \treturn output \n\texcept KeyboardInterrupt: \n\t \tprocess.terminate() \n\t \traise\n", 
" \treturn calendar.timegm(dt_obj.utctimetuple())\n", 
" \tif (len(s1) != len(s2)): \n\t \treturn False \n\tresult = 0 \n\tfor (a, b) in zip(s1, s2): \n\t \tresult |= (ord(a) ^ ord(b)) \n\treturn (result == 0)\n", 
" \t_ensure_subprocess() \n\tif isinstance(formatted, six.string_types): \n\t \tdata = bytearray(formatted, _encoding_for_form(inform)) \n\telse: \n\t \tdata = formatted \n\tprocess = subprocess.Popen(['openssl', 'cms', '-verify', '-certfile', signing_cert_file_name, '-CAfile', ca_file_name, '-inform', 'PEM', '-nosmimecap', '-nodetach', '-nocerts', '-noattr'], stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE, close_fds=True) \n\t(output, err, retcode) = _process_communicate_handle_oserror(process, data, (signing_cert_file_name, ca_file_name)) \n\tif (retcode == OpensslCmsExitStatus.INPUT_FILE_READ_ERROR): \n\t \tif err.startswith('Error \treading \tS/MIME \tmessage'): \n\t \t \traise exceptions.CMSError(err) \n\t \telse: \n\t \t \traise exceptions.CertificateConfigError(err) \n\telif (retcode == OpensslCmsExitStatus.COMMAND_OPTIONS_PARSING_ERROR): \n\t \tif err.startswith('cms: \tCannot \topen \tinput \tfile'): \n\t \t \traise exceptions.CertificateConfigError(err) \n\t \telse: \n\t \t \traise subprocess.CalledProcessError(retcode, 'openssl', output=err) \n\telif (retcode != OpensslCmsExitStatus.SUCCESS): \n\t \traise subprocess.CalledProcessError(retcode, 'openssl', output=err) \n\treturn output\n", 
" \tfrom .singleton import S \n\tfrom .basic import Basic \n\tfrom .sympify import sympify, SympifyError \n\tfrom .compatibility import iterable \n\tif isinstance(item, Basic): \n\t \treturn item.sort_key(order=order) \n\tif iterable(item, exclude=string_types): \n\t \tif isinstance(item, dict): \n\t \t \targs = item.items() \n\t \t \tunordered = True \n\t \telif isinstance(item, set): \n\t \t \targs = item \n\t \t \tunordered = True \n\t \telse: \n\t \t \targs = list(item) \n\t \t \tunordered = False \n\t \targs = [default_sort_key(arg, order=order) for arg in args] \n\t \tif unordered: \n\t \t \targs = sorted(args) \n\t \t(cls_index, args) = (10, (len(args), tuple(args))) \n\telse: \n\t \tif (not isinstance(item, string_types)): \n\t \t \ttry: \n\t \t \t \titem = sympify(item) \n\t \t \texcept SympifyError: \n\t \t \t \tpass \n\t \t \telse: \n\t \t \t \tif isinstance(item, Basic): \n\t \t \t \t \treturn default_sort_key(item) \n\t \t(cls_index, args) = (0, (1, (str(item),))) \n\treturn ((cls_index, 0, item.__class__.__name__), args, S.One.sort_key(), S.One)\n", 
" \t_ensure_subprocess() \n\tif isinstance(data_to_sign, six.string_types): \n\t \tdata = bytearray(data_to_sign, encoding='utf-8') \n\telse: \n\t \tdata = data_to_sign \n\tprocess = subprocess.Popen(['openssl', 'cms', '-sign', '-signer', signing_cert_file_name, '-inkey', signing_key_file_name, '-outform', 'PEM', '-nosmimecap', '-nodetach', '-nocerts', '-noattr', '-md', message_digest], stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE, close_fds=True) \n\t(output, err, retcode) = _process_communicate_handle_oserror(process, data, (signing_cert_file_name, signing_key_file_name)) \n\tif ((retcode != OpensslCmsExitStatus.SUCCESS) or ('Error' in err)): \n\t \tif (retcode == OpensslCmsExitStatus.CREATE_CMS_READ_MIME_ERROR): \n\t \t \tLOG.error(_LE('Signing \terror: \tUnable \tto \tload \tcertificate \t- \tensure \tyou \thave \tconfigured \tPKI \twith \t\"keystone-manage \tpki_setup\"')) \n\t \telse: \n\t \t \tLOG.error(_LE('Signing \terror: \t%s'), err) \n\t \traise subprocess.CalledProcessError(retcode, 'openssl') \n\tif (outform == PKI_ASN1_FORM): \n\t \treturn output.decode('utf-8') \n\telse: \n\t \treturn output\n", 
" \t(path, senv) = salt.utils.url.split_env(path) \n\tif senv: \n\t \tsaltenv = senv \n\treturn _client().hash_file(path, saltenv)\n", 
" \tif isinstance(value, str): \n\t \tvalue = value.decode('utf-8') \n\tassert isinstance(value, unicode) \n\treturn json.loads(value, *args, **kwargs)\n", 
" \toutput = [outbuffer] \n\ttag_name = xml.tag.split(u'}', 1)[(-1)] \n\tif (u'}' in xml.tag): \n\t \ttag_xmlns = xml.tag.split(u'}', 1)[0][1:] \n\telse: \n\t \ttag_xmlns = u'' \n\tdefault_ns = u'' \n\tstream_ns = u'' \n\tuse_cdata = False \n\tif stream: \n\t \tdefault_ns = stream.default_ns \n\t \tstream_ns = stream.stream_ns \n\t \tuse_cdata = stream.use_cdata \n\tnamespace = u'' \n\tif tag_xmlns: \n\t \tif ((top_level and (tag_xmlns not in [default_ns, xmlns, stream_ns])) or ((not top_level) and (tag_xmlns != xmlns))): \n\t \t \tnamespace = (u' \txmlns=\"%s\"' % tag_xmlns) \n\tif (stream and (tag_xmlns in stream.namespace_map)): \n\t \tmapped_namespace = stream.namespace_map[tag_xmlns] \n\t \tif mapped_namespace: \n\t \t \ttag_name = (u'%s:%s' % (mapped_namespace, tag_name)) \n\toutput.append((u'<%s' % tag_name)) \n\toutput.append(namespace) \n\tnew_namespaces = set() \n\tfor (attrib, value) in xml.attrib.items(): \n\t \tvalue = escape(value, use_cdata) \n\t \tif (u'}' not in attrib): \n\t \t \toutput.append((u' \t%s=\"%s\"' % (attrib, value))) \n\t \telse: \n\t \t \tattrib_ns = attrib.split(u'}')[0][1:] \n\t \t \tattrib = attrib.split(u'}')[1] \n\t \t \tif (attrib_ns == XML_NS): \n\t \t \t \toutput.append((u' \txml:%s=\"%s\"' % (attrib, value))) \n\t \t \telif (stream and (attrib_ns in stream.namespace_map)): \n\t \t \t \tmapped_ns = stream.namespace_map[attrib_ns] \n\t \t \t \tif mapped_ns: \n\t \t \t \t \tif (namespaces is None): \n\t \t \t \t \t \tnamespaces = set() \n\t \t \t \t \tif (attrib_ns not in namespaces): \n\t \t \t \t \t \tnamespaces.add(attrib_ns) \n\t \t \t \t \t \tnew_namespaces.add(attrib_ns) \n\t \t \t \t \t \toutput.append((u' \txmlns:%s=\"%s\"' % (mapped_ns, attrib_ns))) \n\t \t \t \t \toutput.append((u' \t%s:%s=\"%s\"' % (mapped_ns, attrib, value))) \n\tif open_only: \n\t \toutput.append(u'>') \n\t \treturn u''.join(output) \n\tif (len(xml) or xml.text): \n\t \toutput.append(u'>') \n\t \tif xml.text: \n\t \t \toutput.append(escape(xml.text, use_cdata)) \n\t \tif len(xml): \n\t \t \tfor child in xml: \n\t \t \t \toutput.append(tostring(child, tag_xmlns, stream, namespaces=namespaces)) \n\t \toutput.append((u'</%s>' % tag_name)) \n\telif xml.text: \n\t \toutput.append((u'>%s</%s>' % (escape(xml.text, use_cdata), tag_name))) \n\telse: \n\t \toutput.append(u' \t/>') \n\tif xml.tail: \n\t \toutput.append(escape(xml.tail, use_cdata)) \n\tfor ns in new_namespaces: \n\t \tnamespaces.remove(ns) \n\treturn u''.join(output)\n", 
" \titems = [] \n\tfor (k, v) in d.items(): \n\t \tnew_key = (((parent_key + '.') + k) if parent_key else k) \n\t \tif isinstance(v, collections.MutableMapping): \n\t \t \titems.extend(list(flatten_dict(v, new_key).items())) \n\t \telse: \n\t \t \titems.append((new_key, v)) \n\treturn dict(items)\n", 
" \tdef wrapper(f): \n\t \t@functools.wraps(f) \n\t \tdef inner(self, request, *args, **kwargs): \n\t \t \trequest.assert_authenticated() \n\t \t \tif request.context.is_admin: \n\t \t \t \tLOG.warning(_LW('RBAC: \tBypassing \tauthorization')) \n\t \t \telif (callback is not None): \n\t \t \t \tprep_info = {'f_name': f.__name__, 'input_attr': kwargs} \n\t \t \t \tcallback(self, request, prep_info, *args, **kwargs) \n\t \t \telse: \n\t \t \t \taction = ('identity:%s' % f.__name__) \n\t \t \t \tcreds = _build_policy_check_credentials(self, action, request.context_dict, kwargs) \n\t \t \t \tpolicy_dict = {} \n\t \t \t \tif (hasattr(self, 'get_member_from_driver') and (self.get_member_from_driver is not None)): \n\t \t \t \t \tkey = ('%s_id' % self.member_name) \n\t \t \t \t \tif (key in kwargs): \n\t \t \t \t \t \tref = self.get_member_from_driver(kwargs[key]) \n\t \t \t \t \t \tpolicy_dict['target'] = {self.member_name: ref} \n\t \t \t \tif (request.context_dict.get('subject_token_id') is not None): \n\t \t \t \t \twindow_seconds = self._token_validation_window(request) \n\t \t \t \t \ttoken_ref = token_model.KeystoneToken(token_id=request.context_dict['subject_token_id'], token_data=self.token_provider_api.validate_token(request.context_dict['subject_token_id'], window_seconds=window_seconds)) \n\t \t \t \t \tpolicy_dict.setdefault('target', {}) \n\t \t \t \t \tpolicy_dict['target'].setdefault(self.member_name, {}) \n\t \t \t \t \tpolicy_dict['target'][self.member_name]['user_id'] = token_ref.user_id \n\t \t \t \t \ttry: \n\t \t \t \t \t \tuser_domain_id = token_ref.user_domain_id \n\t \t \t \t \texcept exception.UnexpectedError: \n\t \t \t \t \t \tuser_domain_id = None \n\t \t \t \t \tif user_domain_id: \n\t \t \t \t \t \tpolicy_dict['target'][self.member_name].setdefault('user', {}) \n\t \t \t \t \t \tpolicy_dict['target'][self.member_name]['user'].setdefault('domain', {}) \n\t \t \t \t \t \tpolicy_dict['target'][self.member_name]['user']['domain']['id'] = user_domain_id \n\t \t \t \tpolicy_dict.update(kwargs) \n\t \t \t \tself.policy_api.enforce(creds, action, utils.flatten_dict(policy_dict)) \n\t \t \t \tLOG.debug('RBAC: \tAuthorization \tgranted') \n\t \t \treturn f(self, request, *args, **kwargs) \n\t \treturn inner \n\treturn wrapper\n", 
" \tdef wrapper(f): \n\t \t@functools.wraps(f) \n\t \tdef inner(self, request, *args, **kwargs): \n\t \t \trequest.assert_authenticated() \n\t \t \tif request.context.is_admin: \n\t \t \t \tLOG.warning(_LW('RBAC: \tBypassing \tauthorization')) \n\t \t \telif (callback is not None): \n\t \t \t \tprep_info = {'f_name': f.__name__, 'input_attr': kwargs} \n\t \t \t \tcallback(self, request, prep_info, *args, **kwargs) \n\t \t \telse: \n\t \t \t \taction = ('identity:%s' % f.__name__) \n\t \t \t \tcreds = _build_policy_check_credentials(self, action, request.context_dict, kwargs) \n\t \t \t \tpolicy_dict = {} \n\t \t \t \tif (hasattr(self, 'get_member_from_driver') and (self.get_member_from_driver is not None)): \n\t \t \t \t \tkey = ('%s_id' % self.member_name) \n\t \t \t \t \tif (key in kwargs): \n\t \t \t \t \t \tref = self.get_member_from_driver(kwargs[key]) \n\t \t \t \t \t \tpolicy_dict['target'] = {self.member_name: ref} \n\t \t \t \tif (request.context_dict.get('subject_token_id') is not None): \n\t \t \t \t \twindow_seconds = self._token_validation_window(request) \n\t \t \t \t \ttoken_ref = token_model.KeystoneToken(token_id=request.context_dict['subject_token_id'], token_data=self.token_provider_api.validate_token(request.context_dict['subject_token_id'], window_seconds=window_seconds)) \n\t \t \t \t \tpolicy_dict.setdefault('target', {}) \n\t \t \t \t \tpolicy_dict['target'].setdefault(self.member_name, {}) \n\t \t \t \t \tpolicy_dict['target'][self.member_name]['user_id'] = token_ref.user_id \n\t \t \t \t \ttry: \n\t \t \t \t \t \tuser_domain_id = token_ref.user_domain_id \n\t \t \t \t \texcept exception.UnexpectedError: \n\t \t \t \t \t \tuser_domain_id = None \n\t \t \t \t \tif user_domain_id: \n\t \t \t \t \t \tpolicy_dict['target'][self.member_name].setdefault('user', {}) \n\t \t \t \t \t \tpolicy_dict['target'][self.member_name]['user'].setdefault('domain', {}) \n\t \t \t \t \t \tpolicy_dict['target'][self.member_name]['user']['domain']['id'] = user_domain_id \n\t \t \t \tpolicy_dict.update(kwargs) \n\t \t \t \tself.policy_api.enforce(creds, action, utils.flatten_dict(policy_dict)) \n\t \t \t \tLOG.debug('RBAC: \tAuthorization \tgranted') \n\t \t \treturn f(self, request, *args, **kwargs) \n\t \treturn inner \n\treturn wrapper\n", 
" \tif (not port): \n\t \tport = (443 if ssl else 80) \n\tif ssl: \n\t \tconn = HTTPSConnection(('%s:%s' % (ipaddr, port))) \n\telse: \n\t \tconn = BufferedHTTPConnection(('%s:%s' % (ipaddr, port))) \n\tif query_string: \n\t \tpath += ('?' + query_string) \n\tconn.path = path \n\tconn.putrequest(method, path, skip_host=(headers and ('Host' in headers))) \n\tif headers: \n\t \tfor (header, value) in headers.items(): \n\t \t \tconn.putheader(header, str(value)) \n\tconn.endheaders() \n\treturn conn\n", 
" \tif (not port): \n\t \tport = (443 if ssl else 80) \n\tif ssl: \n\t \tconn = HTTPSConnection(('%s:%s' % (ipaddr, port))) \n\telse: \n\t \tconn = BufferedHTTPConnection(('%s:%s' % (ipaddr, port))) \n\tif query_string: \n\t \tpath += ('?' + query_string) \n\tconn.path = path \n\tconn.putrequest(method, path, skip_host=(headers and ('Host' in headers))) \n\tif headers: \n\t \tfor (header, value) in headers.items(): \n\t \t \tconn.putheader(header, str(value)) \n\tconn.endheaders() \n\treturn conn\n", 
" \tproduct_name = 'neutron' \n\tlogging.set_defaults(default_log_levels=(logging.get_default_log_levels() + EXTRA_LOG_LEVEL_DEFAULTS)) \n\tlogging.setup(cfg.CONF, product_name) \n\tLOG.info(_LI('Logging \tenabled!')) \n\tLOG.info(_LI('%(prog)s \tversion \t%(version)s'), {'prog': sys.argv[0], 'version': version.version_info.release_string()}) \n\tLOG.debug('command \tline: \t%s', ' \t'.join(sys.argv))\n", 
" \tresult = util.callm(('%s/%s' % ('catalog', 'list')), {'results': results, 'start': start}) \n\tcats = [Catalog(**util.fix(d)) for d in result['response']['catalogs']] \n\tstart = result['response']['start'] \n\ttotal = result['response']['total'] \n\treturn ResultList(cats, start, total)\n", 
" \tif user_ref: \n\t \tuser_ref = user_ref.copy() \n\t \tuser_ref.pop('password', None) \n\t \tuser_ref.pop('tenants', None) \n\t \tuser_ref.pop('groups', None) \n\t \tuser_ref.pop('domains', None) \n\t \ttry: \n\t \t \tuser_ref['extra'].pop('password', None) \n\t \t \tuser_ref['extra'].pop('tenants', None) \n\t \texcept KeyError: \n\t \t \tpass \n\t \tif ('password_expires_at' not in user_ref): \n\t \t \tuser_ref['password_expires_at'] = None \n\treturn user_ref\n", 
" \tif context.is_admin: \n\t \treturn True \n\t(rule, target, credentials) = _prepare_check(context, action, target, pluralized) \n\ttry: \n\t \tresult = _ENFORCER.enforce(rule, target, credentials, action=action, do_raise=True) \n\texcept policy.PolicyNotAuthorized: \n\t \twith excutils.save_and_reraise_exception(): \n\t \t \tlog_rule_list(rule) \n\t \t \tLOG.debug(\"Failed \tpolicy \tcheck \tfor \t'%s'\", action) \n\treturn result\n", 
" \tauth_json = {} \n\tif (token is not None): \n\t \tauth_json['token'] = token \n\tif (username or password): \n\t \tauth_json['passwordCredentials'] = {} \n\tif (username is not None): \n\t \tauth_json['passwordCredentials']['username'] = username \n\tif (user_id is not None): \n\t \tauth_json['passwordCredentials']['userId'] = user_id \n\tif (password is not None): \n\t \tauth_json['passwordCredentials']['password'] = password \n\tif (tenant_name is not None): \n\t \tauth_json['tenantName'] = tenant_name \n\tif (tenant_id is not None): \n\t \tauth_json['tenantId'] = tenant_id \n\tif (trust_id is not None): \n\t \tauth_json['trust_id'] = trust_id \n\treturn auth_json\n", 
" \tglobal url, token, service_token, parsed, conn \n\tretries = kwargs.get('retries', 5) \n\t(attempts, backoff) = (0, 1) \n\tuse_account = (kwargs.pop('use_account', 1) - 1) \n\tservice_user = kwargs.pop('service_user', None) \n\tif service_user: \n\t \tservice_user -= 1 \n\turl_account = (kwargs.pop('url_account', (use_account + 1)) - 1) \n\tos_options = {'user_domain_name': swift_test_domain[use_account], 'project_domain_name': swift_test_domain[use_account]} \n\twhile (attempts <= retries): \n\t \tauth_failure = False \n\t \tattempts += 1 \n\t \ttry: \n\t \t \tif ((not url[use_account]) or (not token[use_account])): \n\t \t \t \t(url[use_account], token[use_account]) = get_url_token(use_account, os_options) \n\t \t \t \tparsed[use_account] = conn[use_account] = None \n\t \t \tif ((not parsed[use_account]) or (not conn[use_account])): \n\t \t \t \t(parsed[use_account], conn[use_account]) = connection(url[use_account]) \n\t \t \tresource = kwargs.pop('resource', '%(storage_url)s') \n\t \t \ttemplate_vars = {'storage_url': url[url_account]} \n\t \t \tparsed_result = urlparse((resource % template_vars)) \n\t \t \tif isinstance(service_user, int): \n\t \t \t \tif (not service_token[service_user]): \n\t \t \t \t \t(dummy, service_token[service_user]) = get_url_token(service_user, os_options) \n\t \t \t \tkwargs['service_token'] = service_token[service_user] \n\t \t \treturn func(url[url_account], token[use_account], parsed_result, conn[url_account], *args, **kwargs) \n\t \texcept (socket.error, HTTPException): \n\t \t \tif (attempts > retries): \n\t \t \t \traise \n\t \t \tparsed[use_account] = conn[use_account] = None \n\t \t \tif service_user: \n\t \t \t \tservice_token[service_user] = None \n\t \texcept AuthError: \n\t \t \tauth_failure = True \n\t \t \turl[use_account] = token[use_account] = None \n\t \t \tif service_user: \n\t \t \t \tservice_token[service_user] = None \n\t \texcept InternalServerError: \n\t \t \tpass \n\t \tif (attempts <= retries): \n\t \t \tif (not auth_failure): \n\t \t \t \tsleep(backoff) \n\t \t \tbackoff *= 2 \n\traise Exception(('No \tresult \tafter \t%s \tretries.' % retries))\n", 
" \tconfig = {} \n\tif (defaults is not None): \n\t \tconfig.update(defaults) \n\tconfig_file = os.environ.get('SWIFT_TEST_CONFIG_FILE', '/etc/swift/test.conf') \n\ttry: \n\t \tconfig = readconf(config_file, section_name) \n\texcept IOError: \n\t \tif (not os.path.exists(config_file)): \n\t \t \tprint(('Unable \tto \tread \ttest \tconfig \t%s \t- \tfile \tnot \tfound' % config_file), file=sys.stderr) \n\t \telif (not os.access(config_file, os.R_OK)): \n\t \t \tprint(('Unable \tto \tread \ttest \tconfig \t%s \t- \tpermission \tdenied' % config_file), file=sys.stderr) \n\texcept ValueError as e: \n\t \tprint(e) \n\treturn config\n", 
" \theaders = [a.strip() for a in headerNames.split(',') if a.strip()] \n\theaders.sort() \n\treturn ', \t'.join(headers)\n", 
" \treload_module(db_replicator)\n", 
" \tf.flush() \n\tf.seek(0) \n\toutput = f.read() \n\tf.seek(0) \n\tf.truncate() \n\treturn output\n", 
" \tif (put_timestamp is None): \n\t \tput_timestamp = Timestamp(0).internal \n\tconn.executescript(\"\\n \t \t \t \t \t \t \t \tCREATE \tTABLE \tcontainer_stat \t(\\n \t \t \t \t \t \t \t \t \t \t \t \taccount \tTEXT,\\n \t \t \t \t \t \t \t \t \t \t \t \tcontainer \tTEXT,\\n \t \t \t \t \t \t \t \t \t \t \t \tcreated_at \tTEXT,\\n \t \t \t \t \t \t \t \t \t \t \t \tput_timestamp \tTEXT \tDEFAULT \t'0',\\n \t \t \t \t \t \t \t \t \t \t \t \tdelete_timestamp \tTEXT \tDEFAULT \t'0',\\n \t \t \t \t \t \t \t \t \t \t \t \tobject_count \tINTEGER,\\n \t \t \t \t \t \t \t \t \t \t \t \tbytes_used \tINTEGER,\\n \t \t \t \t \t \t \t \t \t \t \t \treported_put_timestamp \tTEXT \tDEFAULT \t'0',\\n \t \t \t \t \t \t \t \t \t \t \t \treported_delete_timestamp \tTEXT \tDEFAULT \t'0',\\n \t \t \t \t \t \t \t \t \t \t \t \treported_object_count \tINTEGER \tDEFAULT \t0,\\n \t \t \t \t \t \t \t \t \t \t \t \treported_bytes_used \tINTEGER \tDEFAULT \t0,\\n \t \t \t \t \t \t \t \t \t \t \t \thash \tTEXT \tdefault \t'00000000000000000000000000000000',\\n \t \t \t \t \t \t \t \t \t \t \t \tid \tTEXT,\\n \t \t \t \t \t \t \t \t \t \t \t \tstatus \tTEXT \tDEFAULT \t'',\\n \t \t \t \t \t \t \t \t \t \t \t \tstatus_changed_at \tTEXT \tDEFAULT \t'0'\\n \t \t \t \t \t \t \t \t);\\n\\n \t \t \t \t \t \t \t \tINSERT \tINTO \tcontainer_stat \t(object_count, \tbytes_used)\\n \t \t \t \t \t \t \t \t \t \t \t \tVALUES \t(0, \t0);\\n \t \t \t \t\") \n\tconn.execute('\\n \t \t \t \t \t \t \t \tUPDATE \tcontainer_stat\\n \t \t \t \t \t \t \t \tSET \taccount \t= \t?, \tcontainer \t= \t?, \tcreated_at \t= \t?, \tid \t= \t?,\\n \t \t \t \t \t \t \t \t \t \t \t \tput_timestamp \t= \t?\\n \t \t \t \t', (self.account, self.container, Timestamp(time()).internal, str(uuid4()), put_timestamp))\n", 
" \tif (put_timestamp is None): \n\t \tput_timestamp = Timestamp(0).internal \n\tconn.executescript(\"\\n \t \t \t \t \t \t \t \tCREATE \tTABLE \tcontainer_stat \t(\\n \t \t \t \t \t \t \t \t \t \t \t \taccount \tTEXT,\\n \t \t \t \t \t \t \t \t \t \t \t \tcontainer \tTEXT,\\n \t \t \t \t \t \t \t \t \t \t \t \tcreated_at \tTEXT,\\n \t \t \t \t \t \t \t \t \t \t \t \tput_timestamp \tTEXT \tDEFAULT \t'0',\\n \t \t \t \t \t \t \t \t \t \t \t \tdelete_timestamp \tTEXT \tDEFAULT \t'0',\\n \t \t \t \t \t \t \t \t \t \t \t \tobject_count \tINTEGER,\\n \t \t \t \t \t \t \t \t \t \t \t \tbytes_used \tINTEGER,\\n \t \t \t \t \t \t \t \t \t \t \t \treported_put_timestamp \tTEXT \tDEFAULT \t'0',\\n \t \t \t \t \t \t \t \t \t \t \t \treported_delete_timestamp \tTEXT \tDEFAULT \t'0',\\n \t \t \t \t \t \t \t \t \t \t \t \treported_object_count \tINTEGER \tDEFAULT \t0,\\n \t \t \t \t \t \t \t \t \t \t \t \treported_bytes_used \tINTEGER \tDEFAULT \t0,\\n \t \t \t \t \t \t \t \t \t \t \t \thash \tTEXT \tdefault \t'00000000000000000000000000000000',\\n \t \t \t \t \t \t \t \t \t \t \t \tid \tTEXT,\\n \t \t \t \t \t \t \t \t \t \t \t \tstatus \tTEXT \tDEFAULT \t'',\\n \t \t \t \t \t \t \t \t \t \t \t \tstatus_changed_at \tTEXT \tDEFAULT \t'0',\\n \t \t \t \t \t \t \t \t \t \t \t \tmetadata \tTEXT \tDEFAULT \t'',\\n \t \t \t \t \t \t \t \t \t \t \t \tx_container_sync_point1 \tINTEGER \tDEFAULT \t-1,\\n \t \t \t \t \t \t \t \t \t \t \t \tx_container_sync_point2 \tINTEGER \tDEFAULT \t-1\\n \t \t \t \t \t \t \t \t);\\n\\n \t \t \t \t \t \t \t \tINSERT \tINTO \tcontainer_stat \t(object_count, \tbytes_used)\\n \t \t \t \t \t \t \t \t \t \t \t \tVALUES \t(0, \t0);\\n \t \t \t \t\") \n\tconn.execute('\\n \t \t \t \t \t \t \t \tUPDATE \tcontainer_stat\\n \t \t \t \t \t \t \t \tSET \taccount \t= \t?, \tcontainer \t= \t?, \tcreated_at \t= \t?, \tid \t= \t?,\\n \t \t \t \t \t \t \t \t \t \t \t \tput_timestamp \t= \t?\\n \t \t \t \t', (self.account, self.container, Timestamp(time()).internal, str(uuid4()), put_timestamp))\n", 
" \tconn.executescript(\"\\n \t \t \t \t \t \t \t \tCREATE \tTABLE \taccount_stat \t(\\n \t \t \t \t \t \t \t \t \t \t \t \taccount \tTEXT,\\n \t \t \t \t \t \t \t \t \t \t \t \tcreated_at \tTEXT,\\n \t \t \t \t \t \t \t \t \t \t \t \tput_timestamp \tTEXT \tDEFAULT \t'0',\\n \t \t \t \t \t \t \t \t \t \t \t \tdelete_timestamp \tTEXT \tDEFAULT \t'0',\\n \t \t \t \t \t \t \t \t \t \t \t \tcontainer_count \tINTEGER,\\n \t \t \t \t \t \t \t \t \t \t \t \tobject_count \tINTEGER \tDEFAULT \t0,\\n \t \t \t \t \t \t \t \t \t \t \t \tbytes_used \tINTEGER \tDEFAULT \t0,\\n \t \t \t \t \t \t \t \t \t \t \t \thash \tTEXT \tdefault \t'00000000000000000000000000000000',\\n \t \t \t \t \t \t \t \t \t \t \t \tid \tTEXT,\\n \t \t \t \t \t \t \t \t \t \t \t \tstatus \tTEXT \tDEFAULT \t'',\\n \t \t \t \t \t \t \t \t \t \t \t \tstatus_changed_at \tTEXT \tDEFAULT \t'0'\\n \t \t \t \t \t \t \t \t);\\n\\n \t \t \t \t \t \t \t \tINSERT \tINTO \taccount_stat \t(container_count) \tVALUES \t(0);\\n \t \t \t \t\") \n\tconn.execute('\\n \t \t \t \t \t \t \t \tUPDATE \taccount_stat \tSET \taccount \t= \t?, \tcreated_at \t= \t?, \tid \t= \t?,\\n \t \t \t \t \t \t \t \t \t \t \t \t \t \t \tput_timestamp \t= \t?\\n \t \t \t \t \t \t \t \t', (self.account, Timestamp(time()).internal, str(uuid4()), put_timestamp))\n", 
" \tconf = global_conf.copy() \n\tconf.update(local_conf) \n\treturn ContainerController(conf)\n", 
" \tconf = global_conf.copy() \n\tconf.update(local_conf) \n\taccount_ratelimit = float(conf.get('account_ratelimit', 0)) \n\tmax_sleep_time_seconds = float(conf.get('max_sleep_time_seconds', 60)) \n\t(container_ratelimits, cont_limit_info) = interpret_conf_limits(conf, 'container_ratelimit_', info=1) \n\t(container_listing_ratelimits, cont_list_limit_info) = interpret_conf_limits(conf, 'container_listing_ratelimit_', info=1) \n\tregister_swift_info('ratelimit', account_ratelimit=account_ratelimit, max_sleep_time_seconds=max_sleep_time_seconds, container_ratelimits=cont_limit_info, container_listing_ratelimits=cont_list_limit_info) \n\tdef limit_filter(app): \n\t \treturn RateLimitMiddleware(app, conf) \n\treturn limit_filter\n", 
" \tif hasattr(headers, 'items'): \n\t \theaders = headers.items() \n\tfor (name, value) in headers: \n\t \tif (name == 'etag'): \n\t \t \tresponse.headers[name] = value.replace('\"', '') \n\t \telif (name not in ('date', 'content-length', 'content-type', 'connection', 'x-put-timestamp', 'x-delete-after')): \n\t \t \tresponse.headers[name] = value\n", 
" \treturn Timestamp((resp.getheader('x-backend-timestamp') or resp.getheader('x-put-timestamp') or resp.getheader('x-timestamp') or 0))\n", 
" \tfunc.delay_denial = True \n\treturn func\n", 
" \t(headers, meta, sysmeta) = _prep_headers_to_info(headers, 'account') \n\taccount_info = {'status': status_int, 'container_count': headers.get('x-account-container-count'), 'total_object_count': headers.get('x-account-object-count'), 'bytes': headers.get('x-account-bytes-used'), 'meta': meta, 'sysmeta': sysmeta} \n\tif is_success(status_int): \n\t \taccount_info['account_really_exists'] = (not config_true_value(headers.get('x-backend-fake-account-listing'))) \n\treturn account_info\n", 
" \t(headers, meta, sysmeta) = _prep_headers_to_info(headers, 'container') \n\treturn {'status': status_int, 'read_acl': headers.get('x-container-read'), 'write_acl': headers.get('x-container-write'), 'sync_key': headers.get('x-container-sync-key'), 'object_count': headers.get('x-container-object-count'), 'bytes': headers.get('x-container-bytes-used'), 'versions': headers.get('x-versions-location'), 'storage_policy': headers.get('x-backend-storage-policy-index', '0'), 'cors': {'allow_origin': meta.get('access-control-allow-origin'), 'expose_headers': meta.get('access-control-expose-headers'), 'max_age': meta.get('access-control-max-age')}, 'meta': meta, 'sysmeta': sysmeta}\n", 
" \t@functools.wraps(func) \n\tdef wrapped(*a, **kw): \n\t \tcontroller = a[0] \n\t \treq = a[1] \n\t \treq_origin = req.headers.get('Origin', None) \n\t \tif req_origin: \n\t \t \tcontainer_info = controller.container_info(controller.account_name, controller.container_name, req) \n\t \t \tcors_info = container_info.get('cors', {}) \n\t \t \tresp = func(*a, **kw) \n\t \t \tif (controller.app.strict_cors_mode and (not controller.is_origin_allowed(cors_info, req_origin))): \n\t \t \t \treturn resp \n\t \t \tif ('Access-Control-Expose-Headers' not in resp.headers): \n\t \t \t \texpose_headers = set(['cache-control', 'content-language', 'content-type', 'expires', 'last-modified', 'pragma', 'etag', 'x-timestamp', 'x-trans-id', 'x-openstack-request-id']) \n\t \t \t \tfor header in resp.headers: \n\t \t \t \t \tif (header.startswith('X-Container-Meta') or header.startswith('X-Object-Meta')): \n\t \t \t \t \t \texpose_headers.add(header.lower()) \n\t \t \t \tif cors_info.get('expose_headers'): \n\t \t \t \t \texpose_headers = expose_headers.union([header_line.strip().lower() for header_line in cors_info['expose_headers'].split(' \t') if header_line.strip()]) \n\t \t \t \tresp.headers['Access-Control-Expose-Headers'] = ', \t'.join(expose_headers) \n\t \t \tif ('Access-Control-Allow-Origin' not in resp.headers): \n\t \t \t \tif (cors_info['allow_origin'] and (cors_info['allow_origin'].strip() == '*')): \n\t \t \t \t \tresp.headers['Access-Control-Allow-Origin'] = '*' \n\t \t \t \telse: \n\t \t \t \t \tresp.headers['Access-Control-Allow-Origin'] = req_origin \n\t \t \treturn resp \n\t \telse: \n\t \t \treturn func(*a, **kw) \n\treturn wrapped\n", 
" \t(version, account, container, unused) = split_path(env['PATH_INFO'], 3, 4, True) \n\tinfo = _get_info_from_caches(app, env, account, container) \n\tif (not info): \n\t \tenv.setdefault('swift.infocache', {}) \n\t \tis_autocreate_account = account.startswith(getattr(app, 'auto_create_account_prefix', '.')) \n\t \tif (not is_autocreate_account): \n\t \t \taccount_info = get_account_info(env, app, swift_source) \n\t \t \tif ((not account_info) or (not is_success(account_info['status']))): \n\t \t \t \treturn headers_to_container_info({}, 0) \n\t \treq = _prepare_pre_auth_info_request(env, ('/%s/%s/%s' % (version, account, container)), (swift_source or 'GET_CONTAINER_INFO')) \n\t \tresp = req.get_response(app) \n\t \tinfo = _get_info_from_infocache(env, account, container) \n\t \tif (info is None): \n\t \t \tinfo = set_info_cache(app, env, account, container, resp) \n\tif info: \n\t \tinfo = deepcopy(info) \n\telse: \n\t \tinfo = headers_to_container_info({}, 0) \n\tif (('object_count' not in info) and ('container_size' in info)): \n\t \tinfo['object_count'] = info.pop('container_size') \n\tfor field in ('storage_policy', 'bytes', 'object_count'): \n\t \tif (info.get(field) is None): \n\t \t \tinfo[field] = 0 \n\t \telse: \n\t \t \tinfo[field] = int(info[field]) \n\treturn info\n", 
" \t(version, account, _junk, _junk) = split_path(env['PATH_INFO'], 2, 4, True) \n\tinfo = _get_info_from_caches(app, env, account) \n\tif (not info): \n\t \tenv.setdefault('swift.infocache', {}) \n\t \treq = _prepare_pre_auth_info_request(env, ('/%s/%s' % (version, account)), (swift_source or 'GET_ACCOUNT_INFO')) \n\t \tresp = req.get_response(app) \n\t \tinfo = _get_info_from_infocache(env, account) \n\t \tif (info is None): \n\t \t \tinfo = set_info_cache(app, env, account, None, resp) \n\tif info: \n\t \tinfo = info.copy() \n\telse: \n\t \tinfo = headers_to_account_info({}, 0) \n\tfor field in ('container_count', 'bytes', 'total_object_count'): \n\t \tif (info.get(field) is None): \n\t \t \tinfo[field] = 0 \n\t \telse: \n\t \t \tinfo[field] = int(info[field]) \n\treturn info\n", 
" \tfor (k, v) in from_r.headers.items(): \n\t \tif condition(k): \n\t \t \tto_r.headers[k] = v\n", 
" \treturn (100 <= status <= 199)\n", 
" \treturn (200 <= status <= 299)\n", 
" \treturn (300 <= status <= 399)\n", 
" \treturn (400 <= status <= 499)\n", 
" \treturn (500 <= status <= 599)\n", 
" \ttarget_type = target_type.lower() \n\tprefix = ('x-%s-meta-' % target_type) \n\tmeta_count = 0 \n\tmeta_size = 0 \n\tfor (key, value) in req.headers.items(): \n\t \tif (isinstance(value, six.string_types) and (len(value) > MAX_HEADER_SIZE)): \n\t \t \treturn HTTPBadRequest(body=('Header \tvalue \ttoo \tlong: \t%s' % key[:MAX_META_NAME_LENGTH]), request=req, content_type='text/plain') \n\t \tif (not key.lower().startswith(prefix)): \n\t \t \tcontinue \n\t \tkey = key[len(prefix):] \n\t \tif (not key): \n\t \t \treturn HTTPBadRequest(body='Metadata \tname \tcannot \tbe \tempty', request=req, content_type='text/plain') \n\t \tbad_key = (not check_utf8(key)) \n\t \tbad_value = (value and (not check_utf8(value))) \n\t \tif ((target_type in ('account', 'container')) and (bad_key or bad_value)): \n\t \t \treturn HTTPBadRequest(body='Metadata \tmust \tbe \tvalid \tUTF-8', request=req, content_type='text/plain') \n\t \tmeta_count += 1 \n\t \tmeta_size += (len(key) + len(value)) \n\t \tif (len(key) > MAX_META_NAME_LENGTH): \n\t \t \treturn HTTPBadRequest(body=('Metadata \tname \ttoo \tlong: \t%s%s' % (prefix, key)), request=req, content_type='text/plain') \n\t \tif (len(value) > MAX_META_VALUE_LENGTH): \n\t \t \treturn HTTPBadRequest(body=('Metadata \tvalue \tlonger \tthan \t%d: \t%s%s' % (MAX_META_VALUE_LENGTH, prefix, key)), request=req, content_type='text/plain') \n\t \tif (meta_count > MAX_META_COUNT): \n\t \t \treturn HTTPBadRequest(body=('Too \tmany \tmetadata \titems; \tmax \t%d' % MAX_META_COUNT), request=req, content_type='text/plain') \n\t \tif (meta_size > MAX_META_OVERALL_SIZE): \n\t \t \treturn HTTPBadRequest(body=('Total \tmetadata \ttoo \tlarge; \tmax \t%d' % MAX_META_OVERALL_SIZE), request=req, content_type='text/plain') \n\treturn None\n", 
" \ttry: \n\t \tml = req.message_length() \n\texcept ValueError as e: \n\t \treturn HTTPBadRequest(request=req, content_type='text/plain', body=str(e)) \n\texcept AttributeError as e: \n\t \treturn HTTPNotImplemented(request=req, content_type='text/plain', body=str(e)) \n\tif ((ml is not None) and (ml > MAX_FILE_SIZE)): \n\t \treturn HTTPRequestEntityTooLarge(body='Your \trequest \tis \ttoo \tlarge.', request=req, content_type='text/plain') \n\tif ((req.content_length is None) and (req.headers.get('transfer-encoding') != 'chunked')): \n\t \treturn HTTPLengthRequired(body='Missing \tContent-Length \theader.', request=req, content_type='text/plain') \n\tif (len(object_name) > MAX_OBJECT_NAME_LENGTH): \n\t \treturn HTTPBadRequest(body=('Object \tname \tlength \tof \t%d \tlonger \tthan \t%d' % (len(object_name), MAX_OBJECT_NAME_LENGTH)), request=req, content_type='text/plain') \n\tif ('Content-Type' not in req.headers): \n\t \treturn HTTPBadRequest(request=req, content_type='text/plain', body='No \tcontent \ttype') \n\ttry: \n\t \treq = check_delete_headers(req) \n\texcept HTTPException as e: \n\t \treturn HTTPBadRequest(request=req, body=e.body, content_type='text/plain') \n\tif (not check_utf8(req.headers['Content-Type'])): \n\t \treturn HTTPBadRequest(request=req, body='Invalid \tContent-Type', content_type='text/plain') \n\treturn check_metadata(req, 'object')\n", 
" \tif (not (urllib.parse.quote_plus(drive) == drive)): \n\t \treturn False \n\tpath = os.path.join(root, drive) \n\treturn utils.ismount(path)\n", 
" \ttry: \n\t \tfloat(string) \n\t \treturn True \n\texcept ValueError: \n\t \treturn False\n", 
" \tif (not string): \n\t \treturn False \n\ttry: \n\t \tif isinstance(string, six.text_type): \n\t \t \tstring.encode('utf-8') \n\t \telse: \n\t \t \tdecoded = string.decode('UTF-8') \n\t \t \tif (decoded.encode('UTF-8') != string): \n\t \t \t \treturn False \n\t \t \tif any(((55296 <= ord(codepoint) <= 57343) for codepoint in decoded)): \n\t \t \t \treturn False \n\t \treturn ('\\x00' not in string) \n\texcept UnicodeError: \n\t \treturn False\n", 
" \tif six.PY3: \n\t \treturn \n\torig_parsetype = mimetools.Message.parsetype \n\tdef parsetype(self): \n\t \tif (not self.typeheader): \n\t \t \tself.type = None \n\t \t \tself.maintype = None \n\t \t \tself.subtype = None \n\t \t \tself.plisttext = '' \n\t \telse: \n\t \t \torig_parsetype(self) \n\tparsetype.patched = True \n\tif (not getattr(mimetools.Message.parsetype, 'patched', None)): \n\t \tmimetools.Message.parsetype = parsetype\n", 
" \tbind_addr = get_bind_addr(conf, default_port) \n\taddress_family = [addr[0] for addr in socket.getaddrinfo(bind_addr[0], bind_addr[1], socket.AF_UNSPEC, socket.SOCK_STREAM) if (addr[0] in (socket.AF_INET, socket.AF_INET6))][0] \n\tcert_file = conf.cert_file \n\tkey_file = conf.key_file \n\tuse_ssl = (cert_file or key_file) \n\tif (use_ssl and ((not cert_file) or (not key_file))): \n\t \traise RuntimeError(_('When \trunning \tserver \tin \tSSL \tmode, \tyou \tmust \tspecify \tboth \ta \tcert_file \tand \tkey_file \toption \tvalue \tin \tyour \tconfiguration \tfile')) \n\tsock = None \n\tretry_until = (time.time() + 30) \n\twhile ((not sock) and (time.time() < retry_until)): \n\t \ttry: \n\t \t \tsock = eventlet.listen(bind_addr, backlog=conf.backlog, family=address_family) \n\t \texcept socket.error as err: \n\t \t \tif (err.args[0] != errno.EADDRINUSE): \n\t \t \t \traise \n\t \t \teventlet.sleep(0.1) \n\tif (not sock): \n\t \traise RuntimeError((_('Could \tnot \tbind \tto \t%(bind_addr)safter \ttrying \tfor \t30 \tseconds') % {'bind_addr': bind_addr})) \n\treturn sock\n", 
" \tif (CONF.workers is None): \n\t \treturn processutils.get_worker_count() \n\treturn CONF.workers\n", 
" \t(conf, logger, log_name) = _initrp(conf_path, app_section, *args, **kwargs) \n\tapp = loadapp(conf_path, global_conf={'log_name': log_name}) \n\treturn (app, conf, logger, log_name)\n", 
" \tquery_string = None \n\tpath = (path or '') \n\tif (path and ('?' in path)): \n\t \t(path, query_string) = path.split('?', 1) \n\tnewenv = make_env(env, method, path=unquote(path), agent=agent, query_string=query_string, swift_source=swift_source) \n\tif (not headers): \n\t \theaders = {} \n\tif body: \n\t \treturn Request.blank(path, environ=newenv, body=body, headers=headers) \n\telse: \n\t \treturn Request.blank(path, environ=newenv, headers=headers)\n", 
" \tnewenv = {} \n\tfor name in ('HTTP_USER_AGENT', 'HTTP_HOST', 'PATH_INFO', 'QUERY_STRING', 'REMOTE_USER', 'REQUEST_METHOD', 'SCRIPT_NAME', 'SERVER_NAME', 'SERVER_PORT', 'HTTP_ORIGIN', 'HTTP_ACCESS_CONTROL_REQUEST_METHOD', 'SERVER_PROTOCOL', 'swift.cache', 'swift.source', 'swift.trans_id', 'swift.authorize_override', 'swift.authorize', 'HTTP_X_USER_ID', 'HTTP_X_PROJECT_ID', 'HTTP_REFERER', 'swift.orig_req_method', 'swift.log_info', 'swift.infocache'): \n\t \tif (name in env): \n\t \t \tnewenv[name] = env[name] \n\tif method: \n\t \tnewenv['REQUEST_METHOD'] = method \n\tif path: \n\t \tnewenv['PATH_INFO'] = path \n\t \tnewenv['SCRIPT_NAME'] = '' \n\tif (query_string is not None): \n\t \tnewenv['QUERY_STRING'] = query_string \n\tif agent: \n\t \tnewenv['HTTP_USER_AGENT'] = (agent % {'orig': env.get('HTTP_USER_AGENT', '')}).strip() \n\telif ((agent == '') and ('HTTP_USER_AGENT' in newenv)): \n\t \tdel newenv['HTTP_USER_AGENT'] \n\tif swift_source: \n\t \tnewenv['swift.source'] = swift_source \n\tnewenv['wsgi.input'] = BytesIO() \n\tif ('SCRIPT_NAME' not in newenv): \n\t \tnewenv['SCRIPT_NAME'] = '' \n\treturn newenv\n", 
" \tpool = Pool(size) \n\tdef send(r): \n\t \treturn r.send(stream=stream) \n\tfor request in pool.imap_unordered(send, requests): \n\t \tif (request.response is not None): \n\t \t \t(yield request.response) \n\t \telif exception_handler: \n\t \t \texception_handler(request, request.exception) \n\tpool.join()\n", 
" \treturn bench_func.__name__[(len(BENCH_METHOD_PREFIX) + 1):]\n", 
" \treturn bench_func.__name__[(len(BENCH_METHOD_PREFIX) + 1):]\n", 
" \tpath = ('/' + account) \n\treturn _get_direct_account_container(path, 'Account', node, part, marker=marker, limit=limit, prefix=prefix, delimiter=delimiter, end_marker=end_marker, reverse=reverse, conn_timeout=conn_timeout, response_timeout=response_timeout)\n", 
" \tpath = ('/%s/%s' % (account, container)) \n\tresp = _make_req(node, part, 'HEAD', path, gen_headers(), 'Container', conn_timeout, response_timeout) \n\tresp_headers = HeaderKeyDict() \n\tfor (header, value) in resp.getheaders(): \n\t \tresp_headers[header] = value \n\treturn resp_headers\n", 
" \tpath = ('/%s/%s' % (account, container)) \n\treturn _get_direct_account_container(path, 'Container', node, part, marker=marker, limit=limit, prefix=prefix, delimiter=delimiter, end_marker=end_marker, reverse=reverse, conn_timeout=conn_timeout, response_timeout=response_timeout)\n", 
" \tif (headers is None): \n\t \theaders = {} \n\theaders = gen_headers(headers) \n\tpath = ('/%s/%s/%s' % (account, container, obj)) \n\tresp = _make_req(node, part, 'HEAD', path, headers, 'Object', conn_timeout, response_timeout) \n\tresp_headers = HeaderKeyDict() \n\tfor (header, value) in resp.getheaders(): \n\t \tresp_headers[header] = value \n\treturn resp_headers\n", 
" \tif (headers is None): \n\t \theaders = {} \n\tpath = ('/%s/%s/%s' % (account, container, obj)) \n\twith Timeout(conn_timeout): \n\t \tconn = http_connect(node['ip'], node['port'], node['device'], part, 'GET', path, headers=gen_headers(headers)) \n\twith Timeout(response_timeout): \n\t \tresp = conn.getresponse() \n\tif (not is_success(resp.status)): \n\t \tresp.read() \n\t \traise DirectClientException('Object', 'GET', node, part, path, resp) \n\tif resp_chunk_size: \n\t \tdef _object_body(): \n\t \t \tbuf = resp.read(resp_chunk_size) \n\t \t \twhile buf: \n\t \t \t \t(yield buf) \n\t \t \t \tbuf = resp.read(resp_chunk_size) \n\t \tobject_body = _object_body() \n\telse: \n\t \tobject_body = resp.read() \n\tresp_headers = HeaderKeyDict() \n\tfor (header, value) in resp.getheaders(): \n\t \tresp_headers[header] = value \n\treturn (resp_headers, object_body)\n", 
" \tpath = ('/%s/%s/%s' % (account, container, name)) \n\tif (headers is None): \n\t \theaders = {} \n\tif etag: \n\t \theaders['ETag'] = etag.strip('\"') \n\tif (content_length is not None): \n\t \theaders['Content-Length'] = str(content_length) \n\telse: \n\t \tfor (n, v) in headers.items(): \n\t \t \tif (n.lower() == 'content-length'): \n\t \t \t \tcontent_length = int(v) \n\tif (content_type is not None): \n\t \theaders['Content-Type'] = content_type \n\telse: \n\t \theaders['Content-Type'] = 'application/octet-stream' \n\tif (not contents): \n\t \theaders['Content-Length'] = '0' \n\tif isinstance(contents, six.string_types): \n\t \tcontents = [contents] \n\tadd_ts = ('X-Timestamp' not in headers) \n\tif (content_length is None): \n\t \theaders['Transfer-Encoding'] = 'chunked' \n\twith Timeout(conn_timeout): \n\t \tconn = http_connect(node['ip'], node['port'], node['device'], part, 'PUT', path, headers=gen_headers(headers, add_ts)) \n\tcontents_f = FileLikeIter(contents) \n\tif (content_length is None): \n\t \tchunk = contents_f.read(chunk_size) \n\t \twhile chunk: \n\t \t \tconn.send(('%x\\r\\n%s\\r\\n' % (len(chunk), chunk))) \n\t \t \tchunk = contents_f.read(chunk_size) \n\t \tconn.send('0\\r\\n\\r\\n') \n\telse: \n\t \tleft = content_length \n\t \twhile (left > 0): \n\t \t \tsize = chunk_size \n\t \t \tif (size > left): \n\t \t \t \tsize = left \n\t \t \tchunk = contents_f.read(size) \n\t \t \tif (not chunk): \n\t \t \t \tbreak \n\t \t \tconn.send(chunk) \n\t \t \tleft -= len(chunk) \n\twith Timeout(response_timeout): \n\t \tresp = conn.getresponse() \n\t \tresp.read() \n\tif (not is_success(resp.status)): \n\t \traise DirectClientException('Object', 'PUT', node, part, path, resp) \n\treturn resp.getheader('etag').strip('\"')\n", 
" \tpath = ('/%s/%s/%s' % (account, container, name)) \n\t_make_req(node, part, 'POST', path, gen_headers(headers, True), 'Object', conn_timeout, response_timeout)\n", 
" \tif (headers is None): \n\t \theaders = {} \n\theaders = gen_headers(headers, add_ts=('x-timestamp' not in (k.lower() for k in headers))) \n\tpath = ('/%s/%s/%s' % (account, container, obj)) \n\t_make_req(node, part, 'DELETE', path, headers, 'Object', conn_timeout, response_timeout)\n", 
" \tretries = kwargs.pop('retries', 5) \n\terror_log = kwargs.pop('error_log', None) \n\tattempts = 0 \n\tbackoff = 1 \n\twhile (attempts <= retries): \n\t \tattempts += 1 \n\t \ttry: \n\t \t \treturn (attempts, func(*args, **kwargs)) \n\t \texcept (socket.error, HTTPException, Timeout) as err: \n\t \t \tif error_log: \n\t \t \t \terror_log(err) \n\t \t \tif (attempts > retries): \n\t \t \t \traise \n\t \texcept ClientException as err: \n\t \t \tif error_log: \n\t \t \t \terror_log(err) \n\t \t \tif ((attempts > retries) or (not is_server_error(err.http_status)) or (err.http_status == HTTP_INSUFFICIENT_STORAGE)): \n\t \t \t \traise \n\t \tsleep(backoff) \n\t \tbackoff *= 2 \n\tif (args and ('ip' in args[0])): \n\t \traise ClientException('Raise \ttoo \tmany \tretries', http_host=args[0]['ip'], http_port=args[0]['port'], http_device=args[0]['device']) \n\telse: \n\t \traise ClientException('Raise \ttoo \tmany \tretries')\n", 
" \tf.seek(0, os.SEEK_END) \n\tif (f.tell() == 0): \n\t \treturn \n\tlast_row = '' \n\twhile (f.tell() != 0): \n\t \ttry: \n\t \t \tf.seek((- blocksize), os.SEEK_CUR) \n\t \texcept IOError: \n\t \t \tblocksize = f.tell() \n\t \t \tf.seek((- blocksize), os.SEEK_CUR) \n\t \tblock = f.read(blocksize) \n\t \tf.seek((- blocksize), os.SEEK_CUR) \n\t \trows = block.split('\\n') \n\t \trows[(-1)] = (rows[(-1)] + last_row) \n\t \twhile rows: \n\t \t \tlast_row = rows.pop((-1)) \n\t \t \tif (rows and last_row): \n\t \t \t \t(yield last_row) \n\t(yield last_row)\n", 
" \treturn ((value is True) or (isinstance(value, six.string_types) and (value.lower() in TRUE_VALUES)))\n", 
" \ttry: \n\t \tlibc = ctypes.CDLL(ctypes.util.find_library('c'), use_errno=True) \n\t \tfunc = getattr(libc, func_name) \n\texcept AttributeError: \n\t \tif fail_if_missing: \n\t \t \traise \n\t \tif log_error: \n\t \t \tlogging.warning(_('Unable \tto \tlocate \t%s \tin \tlibc. \t \tLeaving \tas \ta \tno-op.'), func_name) \n\t \treturn noop_libc_function \n\tif errcheck: \n\t \tdef _errcheck(result, f, args): \n\t \t \tif (result == (-1)): \n\t \t \t \terrcode = ctypes.get_errno() \n\t \t \t \traise OSError(errcode, os.strerror(errcode)) \n\t \t \treturn result \n\t \tfunc.errcheck = _errcheck \n\treturn func\n", 
" \tvalue = req.params.get(name, default) \n\tif (value and (not isinstance(value, six.text_type))): \n\t \ttry: \n\t \t \tvalue.decode('utf8') \n\t \texcept UnicodeDecodeError: \n\t \t \traise HTTPBadRequest(request=req, content_type='text/plain', body=('\"%s\" \tparameter \tnot \tvalid \tUTF-8' % name)) \n\treturn value\n", 
" \tglobal _sys_fallocate \n\tif (_sys_fallocate is None): \n\t \t_sys_fallocate = FallocateWrapper() \n\tif (size < 0): \n\t \tsize = 0 \n\tret = _sys_fallocate(fd, 1, 0, ctypes.c_uint64(size)) \n\terr = ctypes.get_errno() \n\tif (ret and (err not in (0, errno.ENOSYS, errno.EOPNOTSUPP, errno.EINVAL))): \n\t \traise OSError(err, ('Unable \tto \tfallocate(%s)' % size))\n", 
" \tif hasattr(fcntl, 'F_FULLSYNC'): \n\t \ttry: \n\t \t \tfcntl.fcntl(fd, fcntl.F_FULLSYNC) \n\t \texcept IOError as e: \n\t \t \traise OSError(e.errno, ('Unable \tto \tF_FULLSYNC(%s)' % fd)) \n\telse: \n\t \tos.fsync(fd)\n", 
" \ttry: \n\t \tos.fdatasync(fd) \n\texcept AttributeError: \n\t \tfsync(fd)\n", 
" \tglobal _posix_fadvise \n\tif (_posix_fadvise is None): \n\t \t_posix_fadvise = load_libc_function('posix_fadvise64') \n\tret = _posix_fadvise(fd, ctypes.c_uint64(offset), ctypes.c_uint64(length), 4) \n\tif (ret != 0): \n\t \tlogging.warning('posix_fadvise64(%(fd)s, \t%(offset)s, \t%(length)s, \t4) \t-> \t%(ret)s', {'fd': fd, 'offset': offset, 'length': length, 'ret': ret})\n", 
" \treturn ('%010d' % min(max(0, float(timestamp)), 9999999999))\n", 
" \tif (not os.path.isdir(path)): \n\t \ttry: \n\t \t \tos.makedirs(path) \n\t \texcept OSError as err: \n\t \t \tif ((err.errno != errno.EEXIST) or (not os.path.isdir(path))): \n\t \t \t \traise\n", 
" \tdirpath = os.path.dirname(new) \n\ttry: \n\t \tcount = makedirs_count(dirpath) \n\t \tos.rename(old, new) \n\texcept OSError: \n\t \tcount = makedirs_count(dirpath) \n\t \tos.rename(old, new) \n\tif fsync: \n\t \tfor i in range(0, (count + 1)): \n\t \t \tfsync_dir(dirpath) \n\t \t \tdirpath = os.path.dirname(dirpath)\n", 
" \tif (not maxsegs): \n\t \tmaxsegs = minsegs \n\tif (minsegs > maxsegs): \n\t \traise ValueError(('minsegs \t> \tmaxsegs: \t%d \t> \t%d' % (minsegs, maxsegs))) \n\tif rest_with_last: \n\t \tsegs = path.split('/', maxsegs) \n\t \tminsegs += 1 \n\t \tmaxsegs += 1 \n\t \tcount = len(segs) \n\t \tif (segs[0] or (count < minsegs) or (count > maxsegs) or ('' in segs[1:minsegs])): \n\t \t \traise ValueError(('Invalid \tpath: \t%s' % quote(path))) \n\telse: \n\t \tminsegs += 1 \n\t \tmaxsegs += 1 \n\t \tsegs = path.split('/', maxsegs) \n\t \tcount = len(segs) \n\t \tif (segs[0] or (count < minsegs) or (count > (maxsegs + 1)) or ('' in segs[1:minsegs]) or ((count == (maxsegs + 1)) and segs[maxsegs])): \n\t \t \traise ValueError(('Invalid \tpath: \t%s' % quote(path))) \n\tsegs = segs[1:maxsegs] \n\tsegs.extend(([None] * ((maxsegs - 1) - len(segs)))) \n\treturn segs\n", 
" \tif ((not device) or ('/' in device) or (device in ['.', '..'])): \n\t \traise ValueError(('Invalid \tdevice: \t%s' % quote((device or '')))) \n\tif ((not partition) or ('/' in partition) or (partition in ['.', '..'])): \n\t \traise ValueError(('Invalid \tpartition: \t%s' % quote((partition or ''))))\n", 
" \tdef decorating_func(func): \n\t \tmethod = func.__name__ \n\t \t@functools.wraps(func) \n\t \tdef _timing_stats(ctrl, *args, **kwargs): \n\t \t \tstart_time = time.time() \n\t \t \tresp = func(ctrl, *args, **kwargs) \n\t \t \tif server_handled_successfully(resp.status_int): \n\t \t \t \tctrl.logger.timing_since((method + '.timing'), start_time, **dec_kwargs) \n\t \t \telse: \n\t \t \t \tctrl.logger.timing_since((method + '.errors.timing'), start_time, **dec_kwargs) \n\t \t \treturn resp \n\t \treturn _timing_stats \n\treturn decorating_func\n", 
" \tif (not conf): \n\t \tconf = {} \n\tif (name is None): \n\t \tname = conf.get('log_name', 'swift') \n\tif (not log_route): \n\t \tlog_route = name \n\tlogger = logging.getLogger(log_route) \n\tlogger.propagate = False \n\tformatter = SwiftLogFormatter(fmt=fmt, max_line_length=int(conf.get('log_max_line_length', 0))) \n\tif (not hasattr(get_logger, 'handler4logger')): \n\t \tget_logger.handler4logger = {} \n\tif (logger in get_logger.handler4logger): \n\t \tlogger.removeHandler(get_logger.handler4logger[logger]) \n\tfacility = getattr(SysLogHandler, conf.get('log_facility', 'LOG_LOCAL0'), SysLogHandler.LOG_LOCAL0) \n\tudp_host = conf.get('log_udp_host') \n\tif udp_host: \n\t \tudp_port = int(conf.get('log_udp_port', logging.handlers.SYSLOG_UDP_PORT)) \n\t \thandler = SysLogHandler(address=(udp_host, udp_port), facility=facility) \n\telse: \n\t \tlog_address = conf.get('log_address', '/dev/log') \n\t \ttry: \n\t \t \thandler = SysLogHandler(address=log_address, facility=facility) \n\t \texcept socket.error as e: \n\t \t \tif (e.errno not in [errno.ENOTSOCK, errno.ENOENT]): \n\t \t \t \traise \n\t \t \thandler = SysLogHandler(facility=facility) \n\thandler.setFormatter(formatter) \n\tlogger.addHandler(handler) \n\tget_logger.handler4logger[logger] = handler \n\tif (log_to_console or hasattr(get_logger, 'console_handler4logger')): \n\t \tif (not hasattr(get_logger, 'console_handler4logger')): \n\t \t \tget_logger.console_handler4logger = {} \n\t \tif (logger in get_logger.console_handler4logger): \n\t \t \tlogger.removeHandler(get_logger.console_handler4logger[logger]) \n\t \tconsole_handler = logging.StreamHandler(sys.__stderr__) \n\t \tconsole_handler.setFormatter(formatter) \n\t \tlogger.addHandler(console_handler) \n\t \tget_logger.console_handler4logger[logger] = console_handler \n\tlogger.setLevel(getattr(logging, conf.get('log_level', 'INFO').upper(), logging.INFO)) \n\tstatsd_host = conf.get('log_statsd_host') \n\tif statsd_host: \n\t \tstatsd_port = int(conf.get('log_statsd_port', 8125)) \n\t \tbase_prefix = conf.get('log_statsd_metric_prefix', '') \n\t \tdefault_sample_rate = float(conf.get('log_statsd_default_sample_rate', 1)) \n\t \tsample_rate_factor = float(conf.get('log_statsd_sample_rate_factor', 1)) \n\t \tstatsd_client = StatsdClient(statsd_host, statsd_port, base_prefix, name, default_sample_rate, sample_rate_factor, logger=logger) \n\t \tlogger.statsd_client = statsd_client \n\telse: \n\t \tlogger.statsd_client = None \n\tadapted_logger = LogAdapter(logger, name) \n\tother_handlers = conf.get('log_custom_handlers', None) \n\tif other_handlers: \n\t \tlog_custom_handlers = [s.strip() for s in other_handlers.split(',') if s.strip()] \n\t \tfor hook in log_custom_handlers: \n\t \t \ttry: \n\t \t \t \t(mod, fnc) = hook.rsplit('.', 1) \n\t \t \t \tlogger_hook = getattr(__import__(mod, fromlist=[fnc]), fnc) \n\t \t \t \tlogger_hook(conf, name, log_to_console, log_route, fmt, logger, adapted_logger) \n\t \t \texcept (AttributeError, ImportError): \n\t \t \t \tprint(('Error \tcalling \tcustom \thandler \t[%s]' % hook), file=sys.stderr) \n\t \t \texcept ValueError: \n\t \t \t \tprint(('Invalid \tcustom \thandler \tformat \t[%s]' % hook), file=sys.stderr) \n\treturn adapted_logger\n", 
" \ttry: \n\t \timport select \n\t \tif hasattr(select, 'poll'): \n\t \t \treturn 'poll' \n\t \treturn 'selects' \n\texcept ImportError: \n\t \treturn None\n", 
" \tif (os.geteuid() == 0): \n\t \tgroups = [g.gr_gid for g in grp.getgrall() if (user in g.gr_mem)] \n\t \tos.setgroups(groups) \n\tuser = pwd.getpwnam(user) \n\tos.setgid(user[3]) \n\tos.setuid(user[2]) \n\tos.environ['HOME'] = user[5] \n\tif call_setsid: \n\t \ttry: \n\t \t \tos.setsid() \n\t \texcept OSError: \n\t \t \tpass \n\tos.chdir('/') \n\tos.umask(18)\n", 
" \tsys.excepthook = (lambda *exc_info: logger.critical(_('UNCAUGHT \tEXCEPTION'), exc_info=exc_info)) \n\tstdio_files = [sys.stdin, sys.stdout, sys.stderr] \n\tconsole_fds = [h.stream.fileno() for (_junk, h) in getattr(get_logger, 'console_handler4logger', {}).items()] \n\tstdio_files = [f for f in stdio_files if (f.fileno() not in console_fds)] \n\twith open(os.devnull, 'r+b') as nullfile: \n\t \tfor f in stdio_files: \n\t \t \ttry: \n\t \t \t \tf.flush() \n\t \t \texcept IOError: \n\t \t \t \tpass \n\t \t \ttry: \n\t \t \t \tos.dup2(nullfile.fileno(), f.fileno()) \n\t \t \texcept OSError: \n\t \t \t \tpass \n\tif kwargs.pop('capture_stdout', True): \n\t \tsys.stdout = LoggerFileObject(logger) \n\tif kwargs.pop('capture_stderr', True): \n\t \tsys.stderr = LoggerFileObject(logger, 'STDERR')\n", 
" \tif (not parser): \n\t \tparser = OptionParser(usage='%prog \tCONFIG \t[options]') \n\tparser.add_option('-v', '--verbose', default=False, action='store_true', help='log \tto \tconsole') \n\tif once: \n\t \tparser.add_option('-o', '--once', default=False, action='store_true', help='only \trun \tone \tpass \tof \tdaemon') \n\t(options, args) = parser.parse_args(args=test_args) \n\tif (not args): \n\t \tparser.print_usage() \n\t \tprint(_('Error: \tmissing \tconfig \tpath \targument')) \n\t \tsys.exit(1) \n\tconfig = os.path.abspath(args.pop(0)) \n\tif (not os.path.exists(config)): \n\t \tparser.print_usage() \n\t \tprint((_('Error: \tunable \tto \tlocate \t%s') % config)) \n\t \tsys.exit(1) \n\textra_args = [] \n\tfor arg in args: \n\t \tif (arg in options.__dict__): \n\t \t \tsetattr(options, arg, True) \n\t \telse: \n\t \t \textra_args.append(arg) \n\toptions = vars(options) \n\tif extra_args: \n\t \toptions['extra_args'] = extra_args \n\treturn (config, options)\n", 
" \taddresses = [] \n\tfor interface in netifaces.interfaces(): \n\t \ttry: \n\t \t \tiface_data = netifaces.ifaddresses(interface) \n\t \t \tfor family in iface_data: \n\t \t \t \tif (family not in (netifaces.AF_INET, netifaces.AF_INET6)): \n\t \t \t \t \tcontinue \n\t \t \t \tfor address in iface_data[family]: \n\t \t \t \t \taddr = address['addr'] \n\t \t \t \t \tif (family == netifaces.AF_INET6): \n\t \t \t \t \t \taddr = addr.split('%')[0] \n\t \t \t \t \taddresses.append(addr) \n\t \texcept ValueError: \n\t \t \tpass \n\treturn addresses\n", 
" \treturn os.path.join(datadir, str(partition), name_hash[(-3):], name_hash)\n", 
" \tif (object and (not container)): \n\t \traise ValueError('container \tis \trequired \tif \tobject \tis \tprovided') \n\tpaths = [account] \n\tif container: \n\t \tpaths.append(container) \n\tif object: \n\t \tpaths.append(object) \n\tif raw_digest: \n\t \treturn md5((((HASH_PATH_PREFIX + '/') + '/'.join(paths)) + HASH_PATH_SUFFIX)).digest() \n\telse: \n\t \treturn md5((((HASH_PATH_PREFIX + '/') + '/'.join(paths)) + HASH_PATH_SUFFIX)).hexdigest()\n", 
" \tif (timeout_class is None): \n\t \ttimeout_class = swift.common.exceptions.LockTimeout \n\tmkdirs(directory) \n\tlockpath = ('%s/.lock' % directory) \n\tfd = os.open(lockpath, (os.O_WRONLY | os.O_CREAT)) \n\tsleep_time = 0.01 \n\tslower_sleep_time = max((timeout * 0.01), sleep_time) \n\tslowdown_at = (timeout * 0.01) \n\ttime_slept = 0 \n\ttry: \n\t \twith timeout_class(timeout, lockpath): \n\t \t \twhile True: \n\t \t \t \ttry: \n\t \t \t \t \tfcntl.flock(fd, (fcntl.LOCK_EX | fcntl.LOCK_NB)) \n\t \t \t \t \tbreak \n\t \t \t \texcept IOError as err: \n\t \t \t \t \tif (err.errno != errno.EAGAIN): \n\t \t \t \t \t \traise \n\t \t \t \tif (time_slept > slowdown_at): \n\t \t \t \t \tsleep_time = slower_sleep_time \n\t \t \t \tsleep(sleep_time) \n\t \t \t \ttime_slept += sleep_time \n\t \t(yield True) \n\tfinally: \n\t \tos.close(fd)\n", 
" \tflags = (os.O_CREAT | os.O_RDWR) \n\tif append: \n\t \tflags |= os.O_APPEND \n\t \tmode = 'a+' \n\telse: \n\t \tmode = 'r+' \n\twhile True: \n\t \tfd = os.open(filename, flags) \n\t \tfile_obj = os.fdopen(fd, mode) \n\t \ttry: \n\t \t \twith swift.common.exceptions.LockTimeout(timeout, filename): \n\t \t \t \twhile True: \n\t \t \t \t \ttry: \n\t \t \t \t \t \tfcntl.flock(fd, (fcntl.LOCK_EX | fcntl.LOCK_NB)) \n\t \t \t \t \t \tbreak \n\t \t \t \t \texcept IOError as err: \n\t \t \t \t \t \tif (err.errno != errno.EAGAIN): \n\t \t \t \t \t \t \traise \n\t \t \t \t \tsleep(0.01) \n\t \t \ttry: \n\t \t \t \tif (os.stat(filename).st_ino != os.fstat(fd).st_ino): \n\t \t \t \t \tcontinue \n\t \t \texcept OSError as err: \n\t \t \t \tif (err.errno == errno.ENOENT): \n\t \t \t \t \tcontinue \n\t \t \t \traise \n\t \t \t(yield file_obj) \n\t \t \tif unlink: \n\t \t \t \tos.unlink(filename) \n\t \t \tbreak \n\t \tfinally: \n\t \t \tfile_obj.close()\n", 
" \treturn lock_path(os.path.dirname(filename), timeout=timeout)\n", 
" \ttime_unit = 's' \n\tif (time_amount > 60): \n\t \ttime_amount /= 60 \n\t \ttime_unit = 'm' \n\t \tif (time_amount > 60): \n\t \t \ttime_amount /= 60 \n\t \t \ttime_unit = 'h' \n\treturn (time_amount, time_unit)\n", 
" \telapsed = (time.time() - start_time) \n\tcompletion = ((float(current_value) / final_value) or 1e-05) \n\treturn get_time_units((((1.0 / completion) * elapsed) - elapsed))\n", 
" \t_validate_device(device) \n\tif unit: \n\t \tif (unit not in VALID_UNITS): \n\t \t \traise CommandExecutionError('Invalid \tunit \tpassed \tto \tpartition.part_list') \n\t \tcmd = 'parted \t-m \t-s \t{0} \tunit \t{1} \tprint'.format(device, unit) \n\telse: \n\t \tcmd = 'parted \t-m \t-s \t{0} \tprint'.format(device) \n\tout = __salt__['cmd.run_stdout'](cmd).splitlines() \n\tret = {'info': {}, 'partitions': {}} \n\tmode = 'info' \n\tfor line in out: \n\t \tif (line in ('BYT;', 'CHS;', 'CYL;')): \n\t \t \tcontinue \n\t \tcols = line.replace(';', '').split(':') \n\t \tif (mode == 'info'): \n\t \t \tif (7 <= len(cols) <= 8): \n\t \t \t \tret['info'] = {'disk': cols[0], 'size': cols[1], 'interface': cols[2], 'logical \tsector': cols[3], 'physical \tsector': cols[4], 'partition \ttable': cols[5], 'model': cols[6]} \n\t \t \t \tif (len(cols) == 8): \n\t \t \t \t \tret['info']['disk \tflags'] = cols[7] \n\t \t \t \tmode = 'partitions' \n\t \t \telse: \n\t \t \t \traise CommandExecutionError('Problem \tencountered \twhile \tparsing \toutput \tfrom \tparted') \n\t \telif (len(cols) == 7): \n\t \t \tret['partitions'][cols[0]] = {'number': cols[0], 'start': cols[1], 'end': cols[2], 'size': cols[3], 'type': cols[4], 'file \tsystem': cols[5], 'flags': cols[6]} \n\t \telse: \n\t \t \traise CommandExecutionError('Problem \tencountered \twhile \tparsing \toutput \tfrom \tparted') \n\treturn ret\n", 
" \tfilepaths = map(functools.partial(os.path.join, path), listdir(path)) \n\treturn unlink_paths_older_than(filepaths, mtime)\n", 
" \titem = env.get(item_name, None) \n\tif ((item is None) and (not allow_none)): \n\t \tlogging.error('ERROR: \t%s \tcould \tnot \tbe \tfound \tin \tenv!', item_name) \n\treturn item\n", 
" \treturn item_from_env(env, 'swift.cache', allow_none)\n", 
" \tif (defaults is None): \n\t \tdefaults = {} \n\tif raw: \n\t \tc = RawConfigParser(defaults) \n\telse: \n\t \tc = ConfigParser(defaults) \n\tif hasattr(conf_path, 'readline'): \n\t \tc.readfp(conf_path) \n\telse: \n\t \tif os.path.isdir(conf_path): \n\t \t \tsuccess = read_conf_dir(c, conf_path) \n\t \telse: \n\t \t \tsuccess = c.read(conf_path) \n\t \tif (not success): \n\t \t \traise IOError((_('Unable \tto \tread \tconfig \tfrom \t%s') % conf_path)) \n\tif section_name: \n\t \tif c.has_section(section_name): \n\t \t \tconf = dict(c.items(section_name)) \n\t \telse: \n\t \t \traise ValueError((_('Unable \tto \tfind \t%(section)s \tconfig \tsection \tin \t%(conf)s') % {'section': section_name, 'conf': conf_path})) \n\t \tif ('log_name' not in conf): \n\t \t \tif (log_name is not None): \n\t \t \t \tconf['log_name'] = log_name \n\t \t \telse: \n\t \t \t \tconf['log_name'] = section_name \n\telse: \n\t \tconf = {} \n\t \tfor s in c.sections(): \n\t \t \tconf.update({s: dict(c.items(s))}) \n\t \tif ('log_name' not in conf): \n\t \t \tconf['log_name'] = log_name \n\tconf['__file__'] = conf_path \n\treturn conf\n", 
" \tif (tmp is None): \n\t \ttmp = os.path.dirname(dest) \n\tmkdirs(tmp) \n\t(fd, tmppath) = mkstemp(dir=tmp, suffix='.tmp') \n\twith os.fdopen(fd, 'wb') as fo: \n\t \tpickle.dump(obj, fo, pickle_protocol) \n\t \tfo.flush() \n\t \tos.fsync(fd) \n\t \trenamer(tmppath, dest)\n", 
" \texts = (exts or [ext]) \n\tfound_files = [] \n\tfor path in glob.glob(os.path.join(root, glob_match)): \n\t \tif os.path.isdir(path): \n\t \t \tfor (root, dirs, files) in os.walk(path): \n\t \t \t \tif (dir_ext and root.endswith(dir_ext)): \n\t \t \t \t \tfound_files.append(root) \n\t \t \t \t \tbreak \n\t \t \t \tfor file_ in files: \n\t \t \t \t \tif (any(exts) and (not any((file_.endswith(e) for e in exts)))): \n\t \t \t \t \t \tcontinue \n\t \t \t \t \tfound_files.append(os.path.join(root, file_)) \n\t \t \t \tfound_dir = False \n\t \t \t \tfor dir_ in dirs: \n\t \t \t \t \tif (dir_ext and dir_.endswith(dir_ext)): \n\t \t \t \t \t \tfound_dir = True \n\t \t \t \t \t \tfound_files.append(os.path.join(root, dir_)) \n\t \t \t \tif found_dir: \n\t \t \t \t \tbreak \n\t \telse: \n\t \t \tif (ext and (not path.endswith(ext))): \n\t \t \t \tcontinue \n\t \t \tfound_files.append(path) \n\treturn sorted(found_files)\n", 
" \t(dirname, name) = os.path.split(path) \n\tif (not os.path.exists(dirname)): \n\t \ttry: \n\t \t \tos.makedirs(dirname) \n\t \texcept OSError as err: \n\t \t \tif (err.errno == errno.EACCES): \n\t \t \t \tsys.exit(('Unable \tto \tcreate \t%s. \t \tRunning \tas \tnon-root?' % dirname)) \n\twith open(path, 'w') as f: \n\t \tf.write(('%s' % contents))\n", 
" \ttry: \n\t \tos.unlink(path) \n\texcept OSError: \n\t \tpass\n", 
" \tdevice_dir = listdir(devices) \n\tshuffle(device_dir) \n\tfor device in device_dir: \n\t \tif (mount_check and (not ismount(os.path.join(devices, device)))): \n\t \t \tif logger: \n\t \t \t \tlogger.warning(_('Skipping \t%s \tas \tit \tis \tnot \tmounted'), device) \n\t \t \tcontinue \n\t \tdatadir_path = os.path.join(devices, device, datadir) \n\t \ttry: \n\t \t \tpartitions = listdir(datadir_path) \n\t \texcept OSError as e: \n\t \t \tif logger: \n\t \t \t \tlogger.warning(_('Skipping \t%(datadir)s \tbecause \t%(err)s'), {'datadir': datadir_path, 'err': e}) \n\t \t \tcontinue \n\t \tfor partition in partitions: \n\t \t \tpart_path = os.path.join(datadir_path, partition) \n\t \t \ttry: \n\t \t \t \tsuffixes = listdir(part_path) \n\t \t \texcept OSError as e: \n\t \t \t \tif (e.errno != errno.ENOTDIR): \n\t \t \t \t \traise \n\t \t \t \tcontinue \n\t \t \tfor asuffix in suffixes: \n\t \t \t \tsuff_path = os.path.join(part_path, asuffix) \n\t \t \t \ttry: \n\t \t \t \t \thashes = listdir(suff_path) \n\t \t \t \texcept OSError as e: \n\t \t \t \t \tif (e.errno != errno.ENOTDIR): \n\t \t \t \t \t \traise \n\t \t \t \t \tcontinue \n\t \t \t \tfor hsh in hashes: \n\t \t \t \t \thash_path = os.path.join(suff_path, hsh) \n\t \t \t \t \ttry: \n\t \t \t \t \t \tfiles = sorted(listdir(hash_path), reverse=True) \n\t \t \t \t \texcept OSError as e: \n\t \t \t \t \t \tif (e.errno != errno.ENOTDIR): \n\t \t \t \t \t \t \traise \n\t \t \t \t \t \tcontinue \n\t \t \t \t \tfor fname in files: \n\t \t \t \t \t \tif (suffix and (not fname.endswith(suffix))): \n\t \t \t \t \t \t \tcontinue \n\t \t \t \t \t \tpath = os.path.join(hash_path, fname) \n\t \t \t \t \t \t(yield (path, device, partition))\n", 
" \tif ((max_rate <= 0) or (incr_by <= 0)): \n\t \treturn running_time \n\tclock_accuracy = 1000.0 \n\tnow = (time.time() * clock_accuracy) \n\ttime_per_request = (clock_accuracy * (float(incr_by) / max_rate)) \n\tif ((now - running_time) > (rate_buffer * clock_accuracy)): \n\t \trunning_time = now \n\telif ((running_time - now) > time_per_request): \n\t \teventlet.sleep(((running_time - now) / clock_accuracy)) \n\treturn (running_time + time_per_request)\n", 
" \treturn ModifiedParseResult(*stdlib_urlparse(url))\n", 
" \tvalue = float(value) \n\tindex = (-1) \n\tsuffixes = 'KMGTPEZY' \n\twhile ((value >= 1024) and ((index + 1) < len(suffixes))): \n\t \tindex += 1 \n\t \tvalue = round((value / 1024)) \n\tif (index == (-1)): \n\t \treturn ('%d' % value) \n\treturn ('%d%si' % (round(value), suffixes[index]))\n", 
" \ttry: \n\t \twith lock_file(cache_file, lock_timeout, unlink=False) as cf: \n\t \t \tcache_entry = {} \n\t \t \ttry: \n\t \t \t \texisting_entry = cf.readline() \n\t \t \t \tif existing_entry: \n\t \t \t \t \tcache_entry = json.loads(existing_entry) \n\t \t \texcept ValueError: \n\t \t \t \tpass \n\t \t \tfor (cache_key, cache_value) in cache_dict.items(): \n\t \t \t \tput_recon_cache_entry(cache_entry, cache_key, cache_value) \n\t \t \ttf = None \n\t \t \ttry: \n\t \t \t \twith NamedTemporaryFile(dir=os.path.dirname(cache_file), delete=False) as tf: \n\t \t \t \t \ttf.write((json.dumps(cache_entry) + '\\n')) \n\t \t \t \tif set_owner: \n\t \t \t \t \tos.chown(tf.name, pwd.getpwnam(set_owner).pw_uid, (-1)) \n\t \t \t \trenamer(tf.name, cache_file, fsync=False) \n\t \t \tfinally: \n\t \t \t \tif (tf is not None): \n\t \t \t \t \ttry: \n\t \t \t \t \t \tos.unlink(tf.name) \n\t \t \t \t \texcept OSError as err: \n\t \t \t \t \t \tif (err.errno != errno.ENOENT): \n\t \t \t \t \t \t \traise \n\texcept (Exception, Timeout): \n\t \tlogger.exception(_('Exception \tdumping \trecon \tcache'))\n", 
" \tif (len(s1) != len(s2)): \n\t \treturn False \n\tresult = 0 \n\tfor (a, b) in zip(s1, s2): \n\t \tresult |= (ord(a) ^ ord(b)) \n\treturn (result == 0)\n", 
" \tfunc.publicly_accessible = True \n\treturn func\n", 
" \treturn (('[%s]' % ip) if is_valid_ipv6(ip) else ip)\n", 
" \tif isinstance(str_or_unicode, six.text_type): \n\t \t(str_or_unicode, _len) = utf8_encoder(str_or_unicode, 'replace') \n\t(valid_utf8_str, _len) = utf8_decoder(str_or_unicode, 'replace') \n\treturn valid_utf8_str.encode('utf-8')\n", 
" \tif comma_separated_str: \n\t \treturn [v.strip() for v in comma_separated_str.split(',') if v.strip()] \n\treturn []\n", 
" \tif csv_string: \n\t \treturn ','.join((csv_string, item)) \n\telse: \n\t \treturn item\n", 
" \tif isinstance(iterable, (list, tuple)): \n\t \treturn iterable \n\telse: \n\t \titerator = iter(iterable) \n\t \ttry: \n\t \t \tchunk = '' \n\t \t \twhile (not chunk): \n\t \t \t \tchunk = next(iterator) \n\t \t \treturn CloseableChain([chunk], iterator) \n\t \texcept StopIteration: \n\t \t \treturn []\n", 
" \ttry: \n\t \tanswer = dns.resolver.query(domain, 'CNAME').rrset \n\t \tttl = answer.ttl \n\t \tresult = answer.items[0].to_text() \n\t \tresult = result.rstrip('.') \n\t \treturn (ttl, result) \n\texcept (DNSException, NXDOMAIN, NoAnswer): \n\t \treturn (0, None)\n", 
" \tconf = global_conf.copy() \n\tconf.update(local_conf) \n\tdef auth_filter(app): \n\t \treturn KeystonePasswordAuthProtocol(app, conf) \n\treturn auth_filter\n", 
" \tconf = global_conf.copy() \n\tconf.update(local_conf) \n\tdef auth_filter(app): \n\t \treturn KeystonePasswordAuthProtocol(app, conf) \n\treturn auth_filter\n", 
" \tconf = global_conf.copy() \n\tconf.update(local_conf) \n\tdef auth_filter(app): \n\t \treturn KeystonePasswordAuthProtocol(app, conf) \n\treturn auth_filter\n", 
" \tconf = global_conf.copy() \n\tconf.update(local_conf) \n\tregister_swift_info('formpost') \n\treturn (lambda app: FormPost(app, conf))\n", 
" \tconf = global_conf.copy() \n\tconf.update(local_conf) \n\taccount_ratelimit = float(conf.get('account_ratelimit', 0)) \n\tmax_sleep_time_seconds = float(conf.get('max_sleep_time_seconds', 60)) \n\t(container_ratelimits, cont_limit_info) = interpret_conf_limits(conf, 'container_ratelimit_', info=1) \n\t(container_listing_ratelimits, cont_list_limit_info) = interpret_conf_limits(conf, 'container_listing_ratelimit_', info=1) \n\tregister_swift_info('ratelimit', account_ratelimit=account_ratelimit, max_sleep_time_seconds=max_sleep_time_seconds, container_ratelimits=cont_limit_info, container_listing_ratelimits=cont_list_limit_info) \n\tdef limit_filter(app): \n\t \treturn RateLimitMiddleware(app, conf) \n\treturn limit_filter\n", 
" \treturn _quote(get_valid_utf8_str(value), safe)\n", 
" \tconf = global_conf.copy() \n\tconf.update(local_conf) \n\tregister_swift_info('staticweb') \n\tdef staticweb_filter(app): \n\t \treturn StaticWeb(app, conf) \n\treturn staticweb_filter\n", 
" \ttry: \n\t \tparsed_data = json.loads(req_body) \n\texcept ValueError: \n\t \traise HTTPBadRequest('Manifest \tmust \tbe \tvalid \tJSON.\\n') \n\tif (not isinstance(parsed_data, list)): \n\t \traise HTTPBadRequest('Manifest \tmust \tbe \ta \tlist.\\n') \n\t(vrs, account, _junk) = split_path(req_path, 3, 3, True) \n\terrors = [] \n\tfor (seg_index, seg_dict) in enumerate(parsed_data): \n\t \tif (not isinstance(seg_dict, dict)): \n\t \t \terrors.append(('Index \t%d: \tnot \ta \tJSON \tobject' % seg_index)) \n\t \t \tcontinue \n\t \tmissing_keys = [k for k in REQUIRED_SLO_KEYS if (k not in seg_dict)] \n\t \tif missing_keys: \n\t \t \terrors.append(('Index \t%d: \tmissing \tkeys \t%s' % (seg_index, ', \t'.join((('\"%s\"' % (mk,)) for mk in sorted(missing_keys)))))) \n\t \t \tcontinue \n\t \textraneous_keys = [k for k in seg_dict if (k not in ALLOWED_SLO_KEYS)] \n\t \tif extraneous_keys: \n\t \t \terrors.append(('Index \t%d: \textraneous \tkeys \t%s' % (seg_index, ', \t'.join((('\"%s\"' % (ek,)) for ek in sorted(extraneous_keys)))))) \n\t \t \tcontinue \n\t \tif (not isinstance(seg_dict['path'], six.string_types)): \n\t \t \terrors.append(('Index \t%d: \t\"path\" \tmust \tbe \ta \tstring' % seg_index)) \n\t \t \tcontinue \n\t \tif (not ((seg_dict.get('etag') is None) or isinstance(seg_dict['etag'], six.string_types))): \n\t \t \terrors.append(('Index \t%d: \t\"etag\" \tmust \tbe \ta \tstring \tor \tnull \t(if \tprovided)' % seg_index)) \n\t \t \tcontinue \n\t \tif ('/' not in seg_dict['path'].strip('/')): \n\t \t \terrors.append(('Index \t%d: \tpath \tdoes \tnot \trefer \tto \tan \tobject. \tPath \tmust \tbe \tof \tthe \tform \t/container/object.' % seg_index)) \n\t \t \tcontinue \n\t \tseg_size = seg_dict.get('size_bytes') \n\t \tif (seg_size is not None): \n\t \t \ttry: \n\t \t \t \tseg_size = int(seg_size) \n\t \t \t \tseg_dict['size_bytes'] = seg_size \n\t \t \texcept (TypeError, ValueError): \n\t \t \t \terrors.append(('Index \t%d: \tinvalid \tsize_bytes' % seg_index)) \n\t \t \t \tcontinue \n\t \t \tif (seg_size < 1): \n\t \t \t \terrors.append(('Index \t%d: \ttoo \tsmall; \teach \tsegment \tmust \tbe \tat \tleast \t1 \tbyte.' % (seg_index,))) \n\t \t \t \tcontinue \n\t \tobj_path = '/'.join(['', vrs, account, seg_dict['path'].lstrip('/')]) \n\t \tif (req_path == quote(obj_path)): \n\t \t \terrors.append(('Index \t%d: \tmanifest \tmust \tnot \tinclude \titself \tas \ta \tsegment' % seg_index)) \n\t \t \tcontinue \n\t \tif seg_dict.get('range'): \n\t \t \ttry: \n\t \t \t \tseg_dict['range'] = Range(('bytes=%s' % seg_dict['range'])) \n\t \t \texcept ValueError: \n\t \t \t \terrors.append(('Index \t%d: \tinvalid \trange' % seg_index)) \n\t \t \t \tcontinue \n\t \t \tif (len(seg_dict['range'].ranges) > 1): \n\t \t \t \terrors.append(('Index \t%d: \tmultiple \tranges \t(only \tone \tallowed)' % seg_index)) \n\t \t \t \tcontinue \n\t \t \tif ((seg_size is not None) and (len(seg_dict['range'].ranges_for_length(seg_size)) != 1)): \n\t \t \t \terrors.append(('Index \t%d: \tunsatisfiable \trange' % seg_index)) \n\t \t \t \tcontinue \n\tif errors: \n\t \terror_message = ''.join(((e + '\\n') for e in errors)) \n\t \traise HTTPBadRequest(error_message, headers={'Content-Type': 'text/plain'}) \n\treturn parsed_data\n", 
" \tattributes = {} \n\tattrs = '' \n\tif (';' in header): \n\t \t(header, attrs) = [x.strip() for x in header.split(';', 1)] \n\tm = True \n\twhile m: \n\t \tm = ATTRIBUTES_RE.match(attrs) \n\t \tif m: \n\t \t \tattrs = attrs[len(m.group(0)):] \n\t \t \tattributes[m.group(1)] = m.group(2).strip('\"') \n\treturn (header, attributes)\n", 
" \tboundary = ('--' + boundary) \n\tblen = (len(boundary) + 2) \n\ttry: \n\t \tgot = wsgi_input.readline(blen) \n\t \twhile (got == '\\r\\n'): \n\t \t \tgot = wsgi_input.readline(blen) \n\texcept (IOError, ValueError) as e: \n\t \traise swift.common.exceptions.ChunkReadError(str(e)) \n\tif (got.strip() != boundary): \n\t \traise swift.common.exceptions.MimeInvalid('invalid \tstarting \tboundary: \twanted \t%r, \tgot \t%r', (boundary, got)) \n\tboundary = ('\\r\\n' + boundary) \n\tinput_buffer = '' \n\tdone = False \n\twhile (not done): \n\t \tit = _MultipartMimeFileLikeObject(wsgi_input, boundary, input_buffer, read_chunk_size) \n\t \t(yield it) \n\t \tdone = it.no_more_files \n\t \tinput_buffer = it.input_buffer\n", 
" \tconf = global_conf.copy() \n\tconf.update(local_conf) \n\tregister_swift_info('formpost') \n\treturn (lambda app: FormPost(app, conf))\n", 
" \tname = name.lower() \n\tvalues = [] \n\tfor raw_value in value.split(','): \n\t \traw_value = raw_value.strip() \n\t \tif (not raw_value): \n\t \t \tcontinue \n\t \tif (':' not in raw_value): \n\t \t \tvalues.append(raw_value) \n\t \t \tcontinue \n\t \t(first, second) = (v.strip() for v in raw_value.split(':', 1)) \n\t \tif ((not first) or (not first.startswith('.'))): \n\t \t \tvalues.append(raw_value) \n\t \telif (first in ('.r', '.ref', '.referer', '.referrer')): \n\t \t \tif ('write' in name): \n\t \t \t \traise ValueError(('Referrers \tnot \tallowed \tin \twrite \tACL: \t%s' % repr(raw_value))) \n\t \t \tnegate = False \n\t \t \tif (second and second.startswith('-')): \n\t \t \t \tnegate = True \n\t \t \t \tsecond = second[1:].strip() \n\t \t \tif (second and (second != '*') and second.startswith('*')): \n\t \t \t \tsecond = second[1:].strip() \n\t \t \tif ((not second) or (second == '.')): \n\t \t \t \traise ValueError(('No \thost/domain \tvalue \tafter \treferrer \tdesignation \tin \tACL: \t%s' % repr(raw_value))) \n\t \t \tvalues.append(('.r:%s%s' % (('-' if negate else ''), second))) \n\t \telse: \n\t \t \traise ValueError(('Unknown \tdesignator \t%s \tin \tACL: \t%s' % (repr(first), repr(raw_value)))) \n\treturn ','.join(values)\n", 
" \treferrers = [] \n\tgroups = [] \n\tif acl_string: \n\t \tfor value in acl_string.split(','): \n\t \t \tif value.startswith('.r:'): \n\t \t \t \treferrers.append(value[len('.r:'):]) \n\t \t \telse: \n\t \t \t \tgroups.append(value) \n\treturn (referrers, groups)\n", 
" \tallow = False \n\tif referrer_acl: \n\t \trhost = (urlparse((referrer or '')).hostname or 'unknown') \n\t \tfor mhost in referrer_acl: \n\t \t \tif mhost.startswith('-'): \n\t \t \t \tmhost = mhost[1:] \n\t \t \t \tif ((mhost == rhost) or (mhost.startswith('.') and rhost.endswith(mhost))): \n\t \t \t \t \tallow = False \n\t \t \telif ((mhost == '*') or (mhost == rhost) or (mhost.startswith('.') and rhost.endswith(mhost))): \n\t \t \t \tallow = True \n\treturn allow\n", 
" \tif (data_format == 'application/json'): \n\t \tdata_dict['Errors'] = error_list \n\t \treturn json.dumps(data_dict) \n\tif (data_format and data_format.endswith('/xml')): \n\t \toutput = '<delete>\\n' \n\t \tfor key in sorted(data_dict): \n\t \t \txml_key = key.replace(' \t', '_').lower() \n\t \t \toutput += ('<%s>%s</%s>\\n' % (xml_key, data_dict[key], xml_key)) \n\t \toutput += '<errors>\\n' \n\t \toutput += '\\n'.join([('<object><name>%s</name><status>%s</status></object>' % (saxutils.escape(name), status)) for (name, status) in error_list]) \n\t \toutput += '</errors>\\n</delete>\\n' \n\t \treturn output \n\toutput = '' \n\tfor key in sorted(data_dict): \n\t \toutput += ('%s: \t%s\\n' % (key, data_dict[key])) \n\toutput += 'Errors:\\n' \n\toutput += '\\n'.join([('%s, \t%s' % (name, status)) for (name, status) in error_list]) \n\treturn output\n", 
" \ttry: \n\t \tresource.setrlimit(resource.RLIMIT_NOFILE, (MAX_DESCRIPTORS, MAX_DESCRIPTORS)) \n\texcept ValueError: \n\t \tprint(_('WARNING: \tUnable \tto \tmodify \tfile \tdescriptor \tlimit. \t \tRunning \tas \tnon-root?')) \n\ttry: \n\t \tresource.setrlimit(resource.RLIMIT_DATA, (MAX_MEMORY, MAX_MEMORY)) \n\texcept ValueError: \n\t \tprint(_('WARNING: \tUnable \tto \tmodify \tmemory \tlimit. \t \tRunning \tas \tnon-root?')) \n\ttry: \n\t \tresource.setrlimit(resource.RLIMIT_NPROC, (MAX_PROCS, MAX_PROCS)) \n\texcept ValueError: \n\t \tprint(_('WARNING: \tUnable \tto \tmodify \tmax \tprocess \tlimit. \t \tRunning \tas \tnon-root?')) \n\tos.environ.setdefault('PYTHON_EGG_CACHE', '/tmp')\n", 
" \tfunc.publicly_accessible = True \n\t@functools.wraps(func) \n\tdef wrapped(*a, **kw): \n\t \trv = func(*a, **kw) \n\t \treturn (1 if rv else 0) \n\treturn wrapped\n", 
" \tstatus = {} \n\tstart = time.time() \n\tend = (start + interval) \n\tserver_pids = dict(server_pids) \n\twhile True: \n\t \tfor (server, pids) in server_pids.items(): \n\t \t \tfor pid in pids: \n\t \t \t \ttry: \n\t \t \t \t \tos.waitpid(pid, os.WNOHANG) \n\t \t \t \texcept OSError as e: \n\t \t \t \t \tif (e.errno not in (errno.ECHILD, errno.ESRCH)): \n\t \t \t \t \t \traise \n\t \t \tstatus[server] = server.get_running_pids(**kwargs) \n\t \t \tfor pid in pids: \n\t \t \t \tif (pid not in status[server]): \n\t \t \t \t \t(yield (server, pid)) \n\t \t \tserver_pids[server] = status[server] \n\t \tif (not [p for (server, pids) in status.items() for p in pids]): \n\t \t \tbreak \n\t \tif (time.time() > end): \n\t \t \tbreak \n\t \telse: \n\t \t \ttime.sleep(0.1)\n", 
" \tdef getter(self): \n\t \tvalue = self.headers.get(header, None) \n\t \tif (value is not None): \n\t \t \ttry: \n\t \t \t \tparts = parsedate(self.headers[header])[:7] \n\t \t \t \treturn datetime(*(parts + (UTC,))) \n\t \t \texcept Exception: \n\t \t \t \treturn None \n\tdef setter(self, value): \n\t \tif isinstance(value, ((float,) + six.integer_types)): \n\t \t \tself.headers[header] = time.strftime('%a, \t%d \t%b \t%Y \t%H:%M:%S \tGMT', time.gmtime(value)) \n\t \telif isinstance(value, datetime): \n\t \t \tself.headers[header] = value.strftime('%a, \t%d \t%b \t%Y \t%H:%M:%S \tGMT') \n\t \telse: \n\t \t \tself.headers[header] = value \n\treturn property(getter, setter, doc=('Retrieve \tand \tset \tthe \t%s \theader \tas \ta \tdatetime, \tset \tit \twith \ta \tdatetime, \tint, \tor \tstr' % header))\n", 
" \tdef getter(self): \n\t \tval = self.headers.get(header, None) \n\t \tif (val is not None): \n\t \t \tval = int(val) \n\t \treturn val \n\tdef setter(self, value): \n\t \tself.headers[header] = value \n\treturn property(getter, setter, doc=('Retrieve \tand \tset \tthe \t%s \theader \tas \tan \tint' % header))\n", 
" \tdef getter(self): \n\t \tval = self.headers.get(header, None) \n\t \tif (val is not None): \n\t \t \tval = int(val) \n\t \treturn val \n\tdef setter(self, value): \n\t \tself.headers[header] = value \n\treturn property(getter, setter, doc=('Retrieve \tand \tset \tthe \t%s \theader \tas \tan \tint' % header))\n", 
" \tdef getter(self): \n\t \treturn ('%s \t%s' % (self.status_int, self.title)) \n\tdef setter(self, value): \n\t \tif isinstance(value, six.integer_types): \n\t \t \tself.status_int = value \n\t \t \tself.explanation = self.title = RESPONSE_REASONS[value][0] \n\t \telse: \n\t \t \tif isinstance(value, six.text_type): \n\t \t \t \tvalue = value.encode('utf-8') \n\t \t \tself.status_int = int(value.split(' \t', 1)[0]) \n\t \t \tself.explanation = self.title = value.split(' \t', 1)[1] \n\treturn property(getter, setter, doc=\"Retrieve \tand \tset \tthe \tResponse \tstatus, \te.g. \t'200 \tOK'\")\n", 
" \tdef getter(self): \n\t \tif (not self._body): \n\t \t \tif (not self._app_iter): \n\t \t \t \treturn '' \n\t \t \tself._body = ''.join(self._app_iter) \n\t \t \tself._app_iter = None \n\t \treturn self._body \n\tdef setter(self, value): \n\t \tif isinstance(value, six.text_type): \n\t \t \tvalue = value.encode('utf-8') \n\t \tif isinstance(value, str): \n\t \t \tself.content_length = len(value) \n\t \t \tself._app_iter = None \n\t \tself._body = value \n\treturn property(getter, setter, doc='Retrieve \tand \tset \tthe \tResponse \tbody \tstr')\n", 
" \tdef getter(self): \n\t \tetag = self.headers.get('etag', None) \n\t \tif etag: \n\t \t \tetag = etag.replace('\"', '') \n\t \treturn etag \n\tdef setter(self, value): \n\t \tif (value is None): \n\t \t \tself.headers['etag'] = None \n\t \telse: \n\t \t \tself.headers['etag'] = ('\"%s\"' % value) \n\treturn property(getter, setter, doc='Retrieve \tand \tset \tthe \tresponse \tEtag \theader')\n", 
" \tdef getter(self): \n\t \tif ('content-type' in self.headers): \n\t \t \treturn self.headers.get('content-type').split(';')[0] \n\tdef setter(self, value): \n\t \tself.headers['content-type'] = value \n\treturn property(getter, setter, doc='Retrieve \tand \tset \tthe \tresponse \tContent-Type \theader')\n", 
" \tdef getter(self): \n\t \tif ('; \tcharset=' in self.headers['content-type']): \n\t \t \treturn self.headers['content-type'].split('; \tcharset=')[1] \n\tdef setter(self, value): \n\t \tif ('content-type' in self.headers): \n\t \t \tself.headers['content-type'] = self.headers['content-type'].split(';')[0] \n\t \t \tif value: \n\t \t \t \tself.headers['content-type'] += ('; \tcharset=' + value) \n\treturn property(getter, setter, doc='Retrieve \tand \tset \tthe \tresponse \tcharset')\n", 
" \tdef getter(self): \n\t \treturn self._app_iter \n\tdef setter(self, value): \n\t \tif isinstance(value, (list, tuple)): \n\t \t \tself.content_length = sum(map(len, value)) \n\t \telif (value is not None): \n\t \t \tself.content_length = None \n\t \t \tself._body = None \n\t \tself._app_iter = value \n\treturn property(getter, setter, doc='Retrieve \tand \tset \tthe \tresponse \tapp_iter')\n", 
" \tdef getter(self): \n\t \ttry: \n\t \t \tif ((header in self.headers) or even_if_nonexistent): \n\t \t \t \treturn cls(self.headers.get(header)) \n\t \texcept ValueError: \n\t \t \treturn None \n\tdef setter(self, value): \n\t \tself.headers[header] = value \n\treturn property(getter, setter, doc=('Retrieve \tand \tset \tthe \t%s \tproperty \tin \tthe \tWSGI \tenviron, \tas \ta \t%s \tobject' % (header, cls.__name__)))\n", 
" \tdef getter(self): \n\t \treturn self.environ.get(environ_field, None) \n\tdef setter(self, value): \n\t \tif isinstance(value, six.text_type): \n\t \t \tself.environ[environ_field] = value.encode('utf-8') \n\t \telse: \n\t \t \tself.environ[environ_field] = value \n\treturn property(getter, setter, doc=('Get \tand \tset \tthe \t%s \tproperty \tin \tthe \tWSGI \tenvironment' % environ_field))\n", 
" \tdef getter(self): \n\t \tbody = self.environ['wsgi.input'].read() \n\t \tself.environ['wsgi.input'] = WsgiBytesIO(body) \n\t \treturn body \n\tdef setter(self, value): \n\t \tself.environ['wsgi.input'] = WsgiBytesIO(value) \n\t \tself.environ['CONTENT_LENGTH'] = str(len(value)) \n\treturn property(getter, setter, doc='Get \tand \tset \tthe \trequest \tbody \tstr')\n", 
" \tdef getter(self): \n\t \tif ('HTTP_HOST' in self.environ): \n\t \t \thost = self.environ['HTTP_HOST'] \n\t \telse: \n\t \t \thost = ('%s:%s' % (self.environ['SERVER_NAME'], self.environ['SERVER_PORT'])) \n\t \tscheme = self.environ.get('wsgi.url_scheme', 'http') \n\t \tif ((scheme == 'http') and host.endswith(':80')): \n\t \t \t(host, port) = host.rsplit(':', 1) \n\t \telif ((scheme == 'https') and host.endswith(':443')): \n\t \t \t(host, port) = host.rsplit(':', 1) \n\t \treturn ('%s://%s' % (scheme, host)) \n\treturn property(getter, doc='Get \turl \tfor \trequest/response \tup \tto \tpath')\n", 
" \targspec = inspect.getargspec(func) \n\tif (argspec.args and (argspec.args[0] == 'self')): \n\t \t@functools.wraps(func) \n\t \tdef _wsgify_self(self, env, start_response): \n\t \t \ttry: \n\t \t \t \treturn func(self, Request(env))(env, start_response) \n\t \t \texcept HTTPException as err_resp: \n\t \t \t \treturn err_resp(env, start_response) \n\t \treturn _wsgify_self \n\telse: \n\t \t@functools.wraps(func) \n\t \tdef _wsgify_bare(env, start_response): \n\t \t \ttry: \n\t \t \t \treturn func(Request(env))(env, start_response) \n\t \t \texcept HTTPException as err_resp: \n\t \t \t \treturn err_resp(env, start_response) \n\t \treturn _wsgify_bare\n", 
" \tt1 = dev['region'] \n\tt2 = dev['zone'] \n\tt3 = dev['ip'] \n\tt4 = dev['id'] \n\treturn ((t1,), (t1, t2), (t1, t2, t3), (t1, t2, t3, t4))\n", 
" \ttier2children = defaultdict(set) \n\tfor dev in devices: \n\t \tfor tier in tiers_for_dev(dev): \n\t \t \tif (len(tier) > 1): \n\t \t \t \ttier2children[tier[0:(-1)]].add(tier) \n\t \t \telse: \n\t \t \t \ttier2children[()].add(tier) \n\treturn tier2children\n", 
" \tif (timeout > (((30 * 24) * 60) * 60)): \n\t \ttimeout += time.time() \n\treturn timeout\n", 
" \tobject_dir = os.path.dirname(object_file) \n\tquarantine_dir = os.path.abspath(os.path.join(object_dir, '..', '..', '..', '..', 'quarantined', (server_type + 's'), os.path.basename(object_dir))) \n\ttry: \n\t \trenamer(object_dir, quarantine_dir, fsync=False) \n\texcept OSError as e: \n\t \tif (e.errno not in (errno.EEXIST, errno.ENOTEMPTY)): \n\t \t \traise \n\t \tquarantine_dir = ('%s-%s' % (quarantine_dir, uuid.uuid4().hex)) \n\t \trenamer(object_dir, quarantine_dir, fsync=False)\n", 
" \tdef walk_datadir(datadir, node_id): \n\t \tpartitions = os.listdir(datadir) \n\t \trandom.shuffle(partitions) \n\t \tfor partition in partitions: \n\t \t \tpart_dir = os.path.join(datadir, partition) \n\t \t \tif (not os.path.isdir(part_dir)): \n\t \t \t \tcontinue \n\t \t \tsuffixes = os.listdir(part_dir) \n\t \t \tif (not suffixes): \n\t \t \t \tos.rmdir(part_dir) \n\t \t \t \tcontinue \n\t \t \tfor suffix in suffixes: \n\t \t \t \tsuff_dir = os.path.join(part_dir, suffix) \n\t \t \t \tif (not os.path.isdir(suff_dir)): \n\t \t \t \t \tcontinue \n\t \t \t \thashes = os.listdir(suff_dir) \n\t \t \t \tif (not hashes): \n\t \t \t \t \tos.rmdir(suff_dir) \n\t \t \t \t \tcontinue \n\t \t \t \tfor hsh in hashes: \n\t \t \t \t \thash_dir = os.path.join(suff_dir, hsh) \n\t \t \t \t \tif (not os.path.isdir(hash_dir)): \n\t \t \t \t \t \tcontinue \n\t \t \t \t \tobject_file = os.path.join(hash_dir, (hsh + '.db')) \n\t \t \t \t \tif os.path.exists(object_file): \n\t \t \t \t \t \t(yield (partition, object_file, node_id)) \n\t \t \t \t \telse: \n\t \t \t \t \t \ttry: \n\t \t \t \t \t \t \tos.rmdir(hash_dir) \n\t \t \t \t \t \texcept OSError as e: \n\t \t \t \t \t \t \tif (e.errno is not errno.ENOTEMPTY): \n\t \t \t \t \t \t \t \traise \n\tits = [walk_datadir(datadir, node_id) for (datadir, node_id) in datadirs] \n\twhile its: \n\t \tfor it in its: \n\t \t \ttry: \n\t \t \t \t(yield next(it)) \n\t \t \texcept StopIteration: \n\t \t \t \tits.remove(it)\n", 
" \tif (not port): \n\t \tport = (443 if ssl else 80) \n\tif ssl: \n\t \tconn = HTTPSConnection(('%s:%s' % (ipaddr, port))) \n\telse: \n\t \tconn = BufferedHTTPConnection(('%s:%s' % (ipaddr, port))) \n\tif query_string: \n\t \tpath += ('?' + query_string) \n\tconn.path = path \n\tconn.putrequest(method, path, skip_host=(headers and ('Host' in headers))) \n\tif headers: \n\t \tfor (header, value) in headers.items(): \n\t \t \tconn.putheader(header, str(value)) \n\tconn.endheaders() \n\treturn conn\n", 
" \tif (not port): \n\t \tport = (443 if ssl else 80) \n\tif ssl: \n\t \tconn = HTTPSConnection(('%s:%s' % (ipaddr, port))) \n\telse: \n\t \tconn = BufferedHTTPConnection(('%s:%s' % (ipaddr, port))) \n\tif query_string: \n\t \tpath += ('?' + query_string) \n\tconn.path = path \n\tconn.putrequest(method, path, skip_host=(headers and ('Host' in headers))) \n\tif headers: \n\t \tfor (header, value) in headers.items(): \n\t \t \tconn.putheader(header, str(value)) \n\tconn.endheaders() \n\treturn conn\n", 
" \tif (section_name is ''): \n\t \tsection_name = sub('([a-z])([A-Z])', '\\\\1-\\\\2', klass.__name__).lower() \n\ttry: \n\t \tconf = utils.readconf(conf_file, section_name, log_name=kwargs.get('log_name')) \n\texcept (ValueError, IOError) as e: \n\t \tsys.exit(e) \n\tonce = (once or (not utils.config_true_value(conf.get('daemonize', 'true')))) \n\tif ('logger' in kwargs): \n\t \tlogger = kwargs.pop('logger') \n\telse: \n\t \tlogger = utils.get_logger(conf, conf.get('log_name', section_name), log_to_console=kwargs.pop('verbose', False), log_route=section_name) \n\tutils.modify_priority(conf, logger) \n\tif utils.config_true_value(conf.get('disable_fallocate', 'no')): \n\t \tutils.disable_fallocate() \n\t(utils.FALLOCATE_RESERVE, utils.FALLOCATE_IS_PERCENT) = utils.config_fallocate_value(conf.get('fallocate_reserve', '1%')) \n\teventlet_debug = utils.config_true_value(conf.get('eventlet_debug', 'no')) \n\teventlet.debug.hub_exceptions(eventlet_debug) \n\tos.environ['TZ'] = time.strftime('%z', time.gmtime()) \n\ttry: \n\t \tklass(conf).run(once=once, **kwargs) \n\texcept KeyboardInterrupt: \n\t \tlogger.info('User \tquit') \n\tlogger.info('Exited')\n", 
" \treturn dict(((col[0], row[idx]) for (idx, col) in enumerate(crs.description)))\n", 
" \tif (name is None): \n\t \traise Exception('name \tis \tNone!') \n\tnew = hashlib.md5(('%s-%s' % (name, timestamp)).encode('utf8')).hexdigest() \n\treturn ('%032x' % (int(old, 16) ^ int(new, 16)))\n", 
" \ttry: \n\t \tconnect_time = time.time() \n\t \tconn = sqlite3.connect(path, check_same_thread=False, factory=GreenDBConnection, timeout=timeout) \n\t \tif ((path != ':memory:') and (not okay_to_create)): \n\t \t \tstat = os.stat(path) \n\t \t \tif ((stat.st_size == 0) and (stat.st_ctime >= connect_time)): \n\t \t \t \tos.unlink(path) \n\t \t \t \traise DatabaseConnectionError(path, 'DB \tfile \tcreated \tby \tconnect?') \n\t \tconn.row_factory = sqlite3.Row \n\t \tconn.text_factory = str \n\t \twith closing(conn.cursor()) as cur: \n\t \t \tcur.execute('PRAGMA \tsynchronous \t= \tNORMAL') \n\t \t \tcur.execute('PRAGMA \tcount_changes \t= \tOFF') \n\t \t \tcur.execute('PRAGMA \ttemp_store \t= \tMEMORY') \n\t \t \tcur.execute('PRAGMA \tjournal_mode \t= \tDELETE') \n\t \tconn.create_function('chexor', 3, chexor) \n\texcept sqlite3.DatabaseError: \n\t \timport traceback \n\t \traise DatabaseConnectionError(path, traceback.format_exc(), timeout=timeout) \n\treturn conn\n", 
" \tmetadata = '' \n\tkey = 0 \n\ttry: \n\t \twhile True: \n\t \t \tmetadata += xattr.getxattr(fd, ('%s%s' % (METADATA_KEY, (key or '')))) \n\t \t \tkey += 1 \n\texcept (IOError, OSError) as e: \n\t \tfor err in ('ENOTSUP', 'EOPNOTSUPP'): \n\t \t \tif (hasattr(errno, err) and (e.errno == getattr(errno, err))): \n\t \t \t \tmsg = ('Filesystem \tat \t%s \tdoes \tnot \tsupport \txattr' % _get_filename(fd)) \n\t \t \t \tlogging.exception(msg) \n\t \t \t \traise DiskFileXattrNotSupported(e) \n\t \tif (e.errno == errno.ENOENT): \n\t \t \traise DiskFileNotExist() \n\treturn pickle.loads(metadata)\n", 
" \tmetastr = pickle.dumps(metadata, PICKLE_PROTOCOL) \n\tkey = 0 \n\twhile metastr: \n\t \ttry: \n\t \t \txattr.setxattr(fd, ('%s%s' % (METADATA_KEY, (key or ''))), metastr[:xattr_size]) \n\t \t \tmetastr = metastr[xattr_size:] \n\t \t \tkey += 1 \n\t \texcept IOError as e: \n\t \t \tfor err in ('ENOTSUP', 'EOPNOTSUPP'): \n\t \t \t \tif (hasattr(errno, err) and (e.errno == getattr(errno, err))): \n\t \t \t \t \tmsg = ('Filesystem \tat \t%s \tdoes \tnot \tsupport \txattr' % _get_filename(fd)) \n\t \t \t \t \tlogging.exception(msg) \n\t \t \t \t \traise DiskFileXattrNotSupported(e) \n\t \t \tif (e.errno in (errno.ENOSPC, errno.EDQUOT)): \n\t \t \t \tmsg = ('No \tspace \tleft \ton \tdevice \tfor \t%s' % _get_filename(fd)) \n\t \t \t \tlogging.exception(msg) \n\t \t \t \traise DiskFileNoSpace() \n\t \t \traise\n", 
" \tconf = global_conf.copy() \n\tconf.update(local_conf) \n\treturn ObjectController(conf)\n", 
" \tpolicy = extract_policy(corrupted_file_path) \n\tif (policy is None): \n\t \tpolicy = POLICIES.legacy \n\tfrom_dir = dirname(corrupted_file_path) \n\tto_dir = join(device_path, 'quarantined', get_data_dir(policy), basename(from_dir)) \n\tinvalidate_hash(dirname(from_dir)) \n\ttry: \n\t \trenamer(from_dir, to_dir, fsync=False) \n\texcept OSError as e: \n\t \tif (e.errno not in (errno.EEXIST, errno.ENOTEMPTY)): \n\t \t \traise \n\t \tto_dir = ('%s-%s' % (to_dir, uuid.uuid4().hex)) \n\t \trenamer(from_dir, to_dir, fsync=False) \n\treturn to_dir\n", 
" \twith open(file_path, 'rb') as f: \n\t \tfile_content = f.read() \n\treturn hash_from_code(file_content)\n", 
" \tsuffix = basename(suffix_dir) \n\tpartition_dir = dirname(suffix_dir) \n\tinvalidations_file = join(partition_dir, HASH_INVALIDATIONS_FILE) \n\twith lock_path(partition_dir): \n\t \twith open(invalidations_file, 'ab') as inv_fh: \n\t \t \tinv_fh.write((suffix + '\\n'))\n", 
" \tfilename = os.path.basename(path) \n\tsuffixes = map((lambda info: ((- len(info[0])), info[0], info[1], info[2])), imp.get_suffixes()) \n\tsuffixes.sort() \n\tfor (neglen, suffix, mode, mtype) in suffixes: \n\t \tif (filename[neglen:] == suffix): \n\t \t \treturn ModuleInfo(filename[:neglen], suffix, mode, mtype)\n", 
" \tdef inner(): \n\t \ttry: \n\t \t \treturn func(*args, **kwargs) \n\t \texcept BaseException as err: \n\t \t \treturn err \n\tresp = tpool.execute(inner) \n\tif isinstance(resp, BaseException): \n\t \traise resp \n\treturn resp\n", 
" \tconf = global_conf.copy() \n\tconf.update(local_conf) \n\treturn AccountController(conf)\n", 
" \tif logical_line.startswith('LOG.debug(_('): \n\t \t(yield (0, \"N319 \tDon't \ttranslate \tdebug \tlevel \tlogs\"))\n", 
" \tif isinstance(ind, list): \n\t \tresult = nth_list(ind, coll) \n\telif isinstance(ind, slice): \n\t \tresult = islice(coll, ind.start, ind.stop, ind.step) \n\telif isinstance(coll, Iterator): \n\t \tresult = nth(ind, coll) \n\telse: \n\t \tresult = coll[ind] \n\tif ((not lazy) and isinstance(result, Iterator)): \n\t \tresult = tuple(result) \n\treturn result\n", 
" \tif isinstance(ind, list): \n\t \tresult = nth_list(ind, coll) \n\telif isinstance(ind, slice): \n\t \tresult = islice(coll, ind.start, ind.stop, ind.step) \n\telif isinstance(coll, Iterator): \n\t \tresult = nth(ind, coll) \n\telse: \n\t \tresult = coll[ind] \n\tif ((not lazy) and isinstance(result, Iterator)): \n\t \tresult = tuple(result) \n\treturn result\n", 
" \tif isinstance(ind, list): \n\t \tresult = nth_list(ind, coll) \n\telif isinstance(ind, slice): \n\t \tresult = islice(coll, ind.start, ind.stop, ind.step) \n\telif isinstance(coll, Iterator): \n\t \tresult = nth(ind, coll) \n\telse: \n\t \tresult = coll[ind] \n\tif ((not lazy) and isinstance(result, Iterator)): \n\t \tresult = tuple(result) \n\treturn result\n", 
" \tdefault_deserializers = {'application/json': wsgi.JSONDeserializer()} \n\tdefault_serializers = {'application/json': wsgi.JSONDictSerializer()} \n\tformat_types = {'json': 'application/json'} \n\taction_status = (action_status or dict(create=201, delete=204)) \n\tdefault_deserializers.update((deserializers or {})) \n\tdefault_serializers.update((serializers or {})) \n\tdeserializers = default_deserializers \n\tserializers = default_serializers \n\tfaults = (faults or {}) \n\t@webob.dec.wsgify(RequestClass=Request) \n\tdef resource(request): \n\t \troute_args = request.environ.get('wsgiorg.routing_args') \n\t \tif route_args: \n\t \t \targs = route_args[1].copy() \n\t \telse: \n\t \t \targs = {} \n\t \targs.pop('controller', None) \n\t \tfmt = args.pop('format', None) \n\t \taction = args.pop('action', None) \n\t \tcontent_type = format_types.get(fmt, request.best_match_content_type()) \n\t \tlanguage = request.best_match_language() \n\t \tdeserializer = deserializers.get(content_type) \n\t \tserializer = serializers.get(content_type) \n\t \ttry: \n\t \t \tif request.body: \n\t \t \t \targs['body'] = deserializer.deserialize(request.body)['body'] \n\t \t \tmethod = getattr(controller, action) \n\t \t \tresult = method(request=request, **args) \n\t \texcept Exception as e: \n\t \t \tmapped_exc = api_common.convert_exception_to_http_exc(e, faults, language) \n\t \t \tif (hasattr(mapped_exc, 'code') and (400 <= mapped_exc.code < 500)): \n\t \t \t \tLOG.info(_LI('%(action)s \tfailed \t(client \terror): \t%(exc)s'), {'action': action, 'exc': mapped_exc}) \n\t \t \telse: \n\t \t \t \tLOG.exception(_LE('%(action)s \tfailed: \t%(details)s'), {'action': action, 'details': utils.extract_exc_details(e)}) \n\t \t \traise mapped_exc \n\t \tstatus = action_status.get(action, 200) \n\t \tbody = serializer.serialize(result) \n\t \tif (status == 204): \n\t \t \tcontent_type = '' \n\t \t \tbody = None \n\t \treturn webob.Response(request=request, status=status, content_type=content_type, body=body) \n\tsetattr(resource, 'controller', controller) \n\treturn resource\n", 
" \tif (key in item): \n\t \treturn item[key] \n\tnested_item = item \n\tfor subkey in key.split('.'): \n\t \tif (not subkey): \n\t \t \traise ValueError(('empty \tsubkey \tin \t%r' % key)) \n\t \ttry: \n\t \t \tnested_item = nested_item[subkey] \n\t \texcept KeyError as e: \n\t \t \traise KeyError(('%r \t- \tlooking \tup \tkey \t%r \tin \t%r' % (e, key, nested_item))) \n\telse: \n\t \treturn nested_item\n", 
" \toctets = ip_address.split('.') \n\tif (len(octets) != 4): \n\t \treturn False \n\tfor (i, octet) in enumerate(octets): \n\t \ttry: \n\t \t \toctets[i] = int(octet) \n\t \texcept ValueError: \n\t \t \treturn False \n\t(first_octet, second_octet, third_octet, fourth_octet) = octets \n\tif (first_octet < 1): \n\t \treturn False \n\telif (first_octet > 223): \n\t \treturn False \n\telif (first_octet == 127): \n\t \treturn False \n\tif ((first_octet == 169) and (second_octet == 254)): \n\t \treturn False \n\tfor octet in (second_octet, third_octet, fourth_octet): \n\t \tif ((octet < 0) or (octet > 255)): \n\t \t \treturn False \n\treturn True\n", 
" \tif (not isinstance(data, dict)): \n\t \traise errors.AnsibleFilterError(('|failed \texpects \tfirst \tparam \tis \ta \tdict \t[oo_combine_dict]. \tGot \t%s. \tType: \t%s' % (str(data), str(type(data))))) \n\treturn out_joiner.join([in_joiner.join([k, str(v)]) for (k, v) in data.items()])\n", 
" \tif (not isinstance(data, list)): \n\t \traise errors.AnsibleFilterError('|failed \texpects \tfirst \tparam \tis \ta \tlist') \n\trval = [] \n\tfor item in data: \n\t \trval.append(('%s%s%s' % (item['key'], joiner, item['value']))) \n\treturn rval\n", 
" \tskips = (skips or []) \n\tres = {} \n\tfor (key, values) in data.items(): \n\t \tif ((key in skips) or hasattr(model_base.BASEV2, key)): \n\t \t \tcontinue \n\t \tvalues = [v for v in values if v] \n\t \tkey_attr_info = attr_info.get(key, {}) \n\t \tif ('convert_list_to' in key_attr_info): \n\t \t \tvalues = key_attr_info['convert_list_to'](values) \n\t \telif ('convert_to' in key_attr_info): \n\t \t \tconvert_to = key_attr_info['convert_to'] \n\t \t \tvalues = [convert_to(v) for v in values] \n\t \tif values: \n\t \t \tres[key] = values \n\treturn res\n", 
" \tmax_limit = _get_pagination_max_limit() \n\tlimit = _get_limit_param(request) \n\tif (max_limit > 0): \n\t \tlimit = (min(max_limit, limit) or max_limit) \n\tif (not limit): \n\t \treturn (None, None) \n\tmarker = request.GET.get('marker', None) \n\treturn (limit, marker)\n", 
" \tlimit = request.GET.get('limit', 0) \n\ttry: \n\t \tlimit = int(limit) \n\t \tif (limit >= 0): \n\t \t \treturn limit \n\texcept ValueError: \n\t \tpass \n\tmsg = (_(\"Limit \tmust \tbe \tan \tinteger \t0 \tor \tgreater \tand \tnot \t'%s'\") % limit) \n\traise exceptions.BadRequest(resource='limit', msg=msg)\n", 
" \treturn [v for v in request.GET.getall(arg) if v]\n", 
" \tsort_keys = list_args(request, 'sort_key') \n\tsort_dirs = list_args(request, 'sort_dir') \n\tif (len(sort_keys) != len(sort_dirs)): \n\t \tmsg = _('The \tnumber \tof \tsort_keys \tand \tsort_dirs \tmust \tbe \tsame') \n\t \traise exc.HTTPBadRequest(explanation=msg) \n\tvalid_dirs = [constants.SORT_DIRECTION_ASC, constants.SORT_DIRECTION_DESC] \n\tabsent_keys = [x for x in sort_keys if (x not in attr_info)] \n\tif absent_keys: \n\t \tmsg = (_('%s \tis \tinvalid \tattribute \tfor \tsort_keys') % absent_keys) \n\t \traise exc.HTTPBadRequest(explanation=msg) \n\tinvalid_dirs = [x for x in sort_dirs if (x not in valid_dirs)] \n\tif invalid_dirs: \n\t \tmsg = (_(\"%(invalid_dirs)s \tis \tinvalid \tvalue \tfor \tsort_dirs, \tvalid \tvalue \tis \t'%(asc)s' \tand \t'%(desc)s'\") % {'invalid_dirs': invalid_dirs, 'asc': constants.SORT_DIRECTION_ASC, 'desc': constants.SORT_DIRECTION_DESC}) \n\t \traise exc.HTTPBadRequest(explanation=msg) \n\treturn list(zip(sort_keys, [(x == constants.SORT_DIRECTION_ASC) for x in sort_dirs]))\n", 
" \tdef _factory(app): \n\t \text_mgr = PluginAwareExtensionManager.get_instance() \n\t \treturn ExtensionMiddleware(app, ext_mgr=ext_mgr) \n\treturn _factory\n", 
" \tsys.stdout.flush() \n\tsys.stderr.flush() \n\tclose_fds = (sys.platform != 'win32') \n\tshell = isinstance(cmd, str) \n\texecutable = None \n\tif (shell and (os.name == 'posix') and ('SHELL' in os.environ)): \n\t \texecutable = os.environ['SHELL'] \n\tp = subprocess.Popen(cmd, shell=shell, executable=executable, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=stderr, close_fds=close_fds) \n\ttry: \n\t \tout = callback(p) \n\texcept KeyboardInterrupt: \n\t \tprint '^C' \n\t \tsys.stdout.flush() \n\t \tsys.stderr.flush() \n\t \tout = None \n\tfinally: \n\t \tif (p.returncode is None): \n\t \t \ttry: \n\t \t \t \tp.terminate() \n\t \t \t \tp.poll() \n\t \t \texcept OSError: \n\t \t \t \tpass \n\t \tif (p.returncode is None): \n\t \t \ttry: \n\t \t \t \tp.kill() \n\t \t \texcept OSError: \n\t \t \t \tpass \n\treturn out\n", 
" \ttry: \n\t \tuuid.UUID(value) \n\t \treturn value \n\texcept (ValueError, AttributeError): \n\t \treturn int(value)\n", 
" \t(orig_exc_type, orig_exc_value, orig_exc_traceback) = sys.exc_info() \n\tif isinstance(new_exc, six.string_types): \n\t \tnew_exc = orig_exc_type(new_exc) \n\tif hasattr(new_exc, 'args'): \n\t \tif (len(new_exc.args) > 0): \n\t \t \tnew_message = ', \t'.join((str(arg) for arg in new_exc.args)) \n\t \telse: \n\t \t \tnew_message = '' \n\t \tnew_message += ('\\n\\nOriginal \texception:\\n DCTB ' + orig_exc_type.__name__) \n\t \tif (hasattr(orig_exc_value, 'args') and (len(orig_exc_value.args) > 0)): \n\t \t \tif getattr(orig_exc_value, 'reraised', False): \n\t \t \t \tnew_message += (': \t' + str(orig_exc_value.args[0])) \n\t \t \telse: \n\t \t \t \tnew_message += (': \t' + ', \t'.join((str(arg) for arg in orig_exc_value.args))) \n\t \tnew_exc.args = ((new_message,) + new_exc.args[1:]) \n\tnew_exc.__cause__ = orig_exc_value \n\tnew_exc.reraised = True \n\tsix.reraise(type(new_exc), new_exc, orig_exc_traceback)\n", 
" \tret = _ConvertToList(arg) \n\tfor element in ret: \n\t \tif (not isinstance(element, element_type)): \n\t \t \traise TypeError(('%s \tshould \tbe \tsingle \telement \tor \tlist \tof \ttype \t%s' % (arg_name, element_type))) \n\treturn ret\n", 
" \tlogger = logging.getLogger() \n\tloglevel = get_loglevel((loglevel or u'ERROR')) \n\tlogfile = (logfile if logfile else sys.__stderr__) \n\tif (not logger.handlers): \n\t \tif hasattr(logfile, u'write'): \n\t \t \thandler = logging.StreamHandler(logfile) \n\t \telse: \n\t \t \thandler = WatchedFileHandler(logfile) \n\t \tlogger.addHandler(handler) \n\t \tlogger.setLevel(loglevel) \n\treturn logger\n", 
" \treturn datetime.datetime.strptime(d['isostr'], _DATETIME_FORMAT)\n", 
" \treturn _aliases.get(alias.lower(), alias)\n", 
" \twith open(CHANGELOG) as f: \n\t \tcur_index = 0 \n\t \tfor line in f: \n\t \t \tmatch = re.search('^\\\\d+\\\\.\\\\d+\\\\.\\\\d+', line) \n\t \t \tif match: \n\t \t \t \tif (cur_index == index): \n\t \t \t \t \treturn match.group(0) \n\t \t \t \telse: \n\t \t \t \t \tcur_index += 1\n", 
" \tgitstr = _git_str() \n\tif (gitstr is None): \n\t \tgitstr = '' \n\tpath = os.path.join(BASEDIR, 'qutebrowser', 'git-commit-id') \n\twith _open(path, 'w', encoding='ascii') as f: \n\t \tf.write(gitstr)\n", 
" \tif (len(sys.argv) < 2): \n\t \treturn True \n\tinfo_commands = ['--help-commands', '--name', '--version', '-V', '--fullname', '--author', '--author-email', '--maintainer', '--maintainer-email', '--contact', '--contact-email', '--url', '--license', '--description', '--long-description', '--platforms', '--classifiers', '--keywords', '--provides', '--requires', '--obsoletes'] \n\tinfo_commands.extend(['egg_info', 'install_egg_info', 'rotate']) \n\tfor command in info_commands: \n\t \tif (command in sys.argv[1:]): \n\t \t \treturn False \n\treturn True\n", 
" \trefs = list_refs(refnames=[refname], repo_dir=repo_dir, limit_to_heads=True) \n\tl = tuple(islice(refs, 2)) \n\tif l: \n\t \tassert (len(l) == 1) \n\t \treturn l[0][1] \n\telse: \n\t \treturn None\n", 
" \tif isinstance(fileIshObject, TextIOBase): \n\t \treturn unicode \n\tif isinstance(fileIshObject, IOBase): \n\t \treturn bytes \n\tencoding = getattr(fileIshObject, 'encoding', None) \n\timport codecs \n\tif isinstance(fileIshObject, (codecs.StreamReader, codecs.StreamWriter)): \n\t \tif encoding: \n\t \t \treturn unicode \n\t \telse: \n\t \t \treturn bytes \n\tif (not _PY3): \n\t \tif isinstance(fileIshObject, file): \n\t \t \tif (encoding is not None): \n\t \t \t \treturn basestring \n\t \t \telse: \n\t \t \t \treturn bytes \n\t \tfrom cStringIO import InputType, OutputType \n\t \tfrom StringIO import StringIO \n\t \tif isinstance(fileIshObject, (StringIO, InputType, OutputType)): \n\t \t \treturn bytes \n\treturn default\n", 
" \tcmd = (_pkg(jail, chroot, root) + ['--version']) \n\treturn __salt__['cmd.run'](cmd).strip()\n", 
" \tcmd = (_pkg(jail, chroot, root) + ['--version']) \n\treturn __salt__['cmd.run'](cmd).strip()\n", 
" \thost = node \n\tport = 27017 \n\tidx = node.rfind(':') \n\tif (idx != (-1)): \n\t \t(host, port) = (node[:idx], int(node[(idx + 1):])) \n\tif host.startswith('['): \n\t \thost = host[1:(-1)] \n\treturn (host, port)\n", 
" \tconn = _get_conn(region=region, key=key, keyid=keyid, profile=profile) \n\tpolicy_arn = _get_policy_arn(policy_name, region, key, keyid, profile) \n\ttry: \n\t \tconn.set_default_policy_version(policy_arn, version_id) \n\t \tlog.info('Set \t{0} \tpolicy \tto \tversion \t{1}.'.format(policy_name, version_id)) \n\texcept boto.exception.BotoServerError as e: \n\t \taws = __utils__['boto.get_error'](e) \n\t \tlog.debug(aws) \n\t \tmsg = 'Failed \tto \tset \t{0} \tpolicy \tto \tversion \t{1}: \t{2}' \n\t \tlog.error(msg.format(policy_name, version_id, aws.get('message'))) \n\t \treturn False \n\treturn True\n", 
" \tvalidate = attr['validate'] \n\tkey = [k for k in validate.keys() if k.startswith('type:dict')] \n\tif (not key): \n\t \tLOG.warning(_LW('Unable \tto \tfind \tdata \ttype \tdescriptor \tfor \tattribute \t%s'), attr_name) \n\t \treturn \n\tdata = validate[key[0]] \n\tif (not isinstance(data, dict)): \n\t \tLOG.debug('Attribute \ttype \tdescriptor \tis \tnot \ta \tdict. \tUnable \tto \tgenerate \tany \tsub-attr \tpolicy \trule \tfor \t%s.', attr_name) \n\t \treturn \n\tsub_attr_rules = [policy.RuleCheck('rule', ('%s:%s:%s' % (action, attr_name, sub_attr_name))) for sub_attr_name in data if (sub_attr_name in target[attr_name])] \n\treturn policy.AndCheck(sub_attr_rules)\n", 
" \trule_method = ('telemetry:' + policy_name) \n\theaders = request.headers \n\tpolicy_dict = dict() \n\tpolicy_dict['roles'] = headers.get('X-Roles', '').split(',') \n\tpolicy_dict['user_id'] = headers.get('X-User-Id') \n\tpolicy_dict['project_id'] = headers.get('X-Project-Id') \n\tif ((_has_rule('default') or _has_rule(rule_method)) and (not pecan.request.enforcer.enforce(rule_method, {}, policy_dict))): \n\t \tpecan.core.abort(status_code=403, detail='RBAC \tAuthorization \tFailed')\n", 
" \ttry: \n\t \ttree_gen = parse(file, format, **kwargs) \n\t \ttree = next(tree_gen) \n\texcept StopIteration: \n\t \traise ValueError('There \tare \tno \ttrees \tin \tthis \tfile.') \n\ttry: \n\t \tnext(tree_gen) \n\texcept StopIteration: \n\t \treturn tree \n\telse: \n\t \traise ValueError('There \tare \tmultiple \ttrees \tin \tthis \tfile; \tuse \tparse() \tinstead.')\n", 
" \tfrom django.contrib.syndication.feeds import Feed as LegacyFeed \n\timport warnings \n\twarnings.warn('The \tsyndication \tfeed() \tview \tis \tdeprecated. \tPlease \tuse \tthe \tnew \tclass \tbased \tview \tAPI.', category=PendingDeprecationWarning) \n\tif (not feed_dict): \n\t \traise Http404('No \tfeeds \tare \tregistered.') \n\ttry: \n\t \t(slug, param) = url.split('/', 1) \n\texcept ValueError: \n\t \t(slug, param) = (url, '') \n\ttry: \n\t \tf = feed_dict[slug] \n\texcept KeyError: \n\t \traise Http404((\"Slug \t%r \tisn't \tregistered.\" % slug)) \n\tif (not issubclass(f, LegacyFeed)): \n\t \tinstance = f() \n\t \tinstance.feed_url = (getattr(f, 'feed_url', None) or request.path) \n\t \tinstance.title_template = (f.title_template or ('feeds/%s_title.html' % slug)) \n\t \tinstance.description_template = (f.description_template or ('feeds/%s_description.html' % slug)) \n\t \treturn instance(request) \n\ttry: \n\t \tfeedgen = f(slug, request).get_feed(param) \n\texcept FeedDoesNotExist: \n\t \traise Http404(('Invalid \tfeed \tparameters. \tSlug \t%r \tis \tvalid, \tbut \tother \tparameters, \tor \tlack \tthereof, \tare \tnot.' % slug)) \n\tresponse = HttpResponse(mimetype=feedgen.mime_type) \n\tfeedgen.write(response, 'utf-8') \n\treturn response\n", 
" \tret = {} \n\tcmd = __execute_kadmin('list_policies') \n\tif ((cmd['retcode'] != 0) or cmd['stderr']): \n\t \tret['comment'] = cmd['stderr'].splitlines()[(-1)] \n\t \tret['result'] = False \n\t \treturn ret \n\tret = {'policies': []} \n\tfor i in cmd['stdout'].splitlines()[1:]: \n\t \tret['policies'].append(i) \n\treturn ret\n", 
" \tdef call_and_assert(arg, context=None): \n\t \tif (context is None): \n\t \t \tcontext = {} \n\t \tresult = function(arg, context=context) \n\t \tassert (result == arg), 'Should \treturn \tthe \targument \tthat \twas \tpassed \tto \tit, \tunchanged \t({arg})'.format(arg=repr(arg)) \n\t \treturn result \n\treturn call_and_assert\n", 
" \tif (not app_messages): \n\t \tapp_messages = get_messages_for_app(app) \n\tif (not app_messages): \n\t \treturn \n\ttpath = frappe.get_pymodule_path(app, u'translations') \n\tfrappe.create_folder(tpath) \n\twrite_csv_file(os.path.join(tpath, (lang + u'.csv')), app_messages, (full_dict or get_full_dict(lang)))\n", 
" \ttb = treebuilders.getTreeBuilder(treebuilder) \n\tp = HTMLParser(tb, namespaceHTMLElements=namespaceHTMLElements) \n\treturn p.parse(doc, encoding=encoding)\n", 
" \tfrom config import get_os, get_checks_places, get_valid_check_class \n\tosname = get_os() \n\tchecks_places = get_checks_places(osname, agentConfig) \n\tfor check_path_builder in checks_places: \n\t \tcheck_path = check_path_builder(check_name) \n\t \tif (not os.path.exists(check_path)): \n\t \t \tcontinue \n\t \t(check_is_valid, check_class, load_failure) = get_valid_check_class(check_name, check_path) \n\t \tif check_is_valid: \n\t \t \treturn check_class \n\tlog.warning(('Failed \tto \tload \tthe \tcheck \tclass \tfor \t%s.' % check_name)) \n\treturn None\n", 
" \tmsg = 'N340: \tUse \tnova.utils.%(spawn)s() \trather \tthan \tgreenthread.%(spawn)s() \tand \teventlet.%(spawn)s()' \n\tif (('nova/utils.py' in filename) or ('nova/tests/' in filename)): \n\t \treturn \n\tmatch = re.match(spawn_re, logical_line) \n\tif match: \n\t \t(yield (0, (msg % {'spawn': match.group('spawn_part')})))\n", 
" \tdef wrapper(func): \n\t \t@functools.wraps(func) \n\t \tdef inner(*args, **kwds): \n\t \t \tlock.acquire() \n\t \t \ttry: \n\t \t \t \treturn func(*args, **kwds) \n\t \t \tfinally: \n\t \t \t \tlock.release() \n\t \treturn inner \n\treturn wrapper\n", 
" \treturn auth_is_loggedin_user()\n", 
" \tif ((r.representation == 'html') and (r.name == 'shelter') and r.id and (not r.component)): \n\t \tT = current.T \n\t \tmsg = current.msg \n\t \ts3db = current.s3db \n\t \trecord = r.record \n\t \tctable = s3db.pr_contact \n\t \tstable = s3db.cr_shelter \n\t \tmessage = '' \n\t \ttext = '' \n\t \ts_id = record.id \n\t \ts_name = record.name \n\t \ts_phone = record.phone \n\t \ts_email = record.email \n\t \ts_status = record.status \n\t \tif (s_phone in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_email in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_status in ('', None)): \n\t \t \ts_status = T('Not \tDefined') \n\t \telif (s_status == 1): \n\t \t \ts_status = 'Open' \n\t \telif (s_status == 2): \n\t \t \ts_status = 'Close' \n\t \telse: \n\t \t \ts_status = 'Unassigned \tShelter \tStatus' \n\t \ttext += '************************************************' \n\t \ttext += ('\\n%s \t' % T('Automatic \tMessage')) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Shelter \tID'), s_id)) \n\t \ttext += (' \t%s: \t%s' % (T('Shelter \tname'), s_name)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Email'), s_email)) \n\t \ttext += (' \t%s: \t%s' % (T('Phone'), s_phone)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Working \tStatus'), s_status)) \n\t \ttext += '\\n************************************************\\n' \n\t \turl = URL(c='cr', f='shelter', args=r.id) \n\t \topts = dict(type='SMS', subject=T('Deployment \tRequest'), message=(message + text), url=url) \n\t \toutput = msg.compose(**opts) \n\t \tif attr.get('rheader'): \n\t \t \trheader = attr['rheader'](r) \n\t \t \tif rheader: \n\t \t \t \toutput['rheader'] = rheader \n\t \toutput['title'] = T('Send \tNotification') \n\t \tcurrent.response.view = 'msg/compose.html' \n\t \treturn output \n\telse: \n\t \traise HTTP(501, current.messages.BADMETHOD)\n", 
" \tcan_users_receive_email = email_manager.can_users_receive_thread_email(recipient_list, exploration_id, has_suggestion) \n\tfor (index, recipient_id) in enumerate(recipient_list): \n\t \tif can_users_receive_email[index]: \n\t \t \ttransaction_services.run_in_transaction(_enqueue_feedback_thread_status_change_email_task, recipient_id, feedback_message_reference, old_status, new_status)\n", 
" \tglobal _notifier \n\tif (_notifier is None): \n\t \thost = (CONF.default_publisher_id or socket.gethostname()) \n\t \ttry: \n\t \t \ttransport = oslo_messaging.get_notification_transport(CONF) \n\t \t \t_notifier = oslo_messaging.Notifier(transport, ('identity.%s' % host)) \n\t \texcept Exception: \n\t \t \tLOG.exception(_LE('Failed \tto \tconstruct \tnotifier')) \n\t \t \t_notifier = False \n\treturn _notifier\n", 
" \tcan_users_receive_email = email_manager.can_users_receive_thread_email(recipient_list, exploration_id, has_suggestion) \n\tfor (index, recipient_id) in enumerate(recipient_list): \n\t \tif can_users_receive_email[index]: \n\t \t \ttransaction_services.run_in_transaction(_enqueue_feedback_thread_status_change_email_task, recipient_id, feedback_message_reference, old_status, new_status)\n", 
" \tdef wrapped_func(*args, **kwarg): \n\t \tCALLED_FUNCTION.append(name) \n\t \treturn function(*args, **kwarg) \n\treturn wrapped_func\n", 
" \tlevel = notification.get_default_level() \n\tif (stack.status == stack.IN_PROGRESS): \n\t \tsuffix = 'start' \n\telif (stack.status == stack.COMPLETE): \n\t \tsuffix = 'end' \n\telse: \n\t \tsuffix = 'error' \n\t \tlevel = notification.ERROR \n\tevent_type = ('%s.%s.%s' % ('stack', stack.action.lower(), suffix)) \n\tnotification.notify(stack.context, event_type, level, engine_api.format_notification_body(stack))\n", 
" \ttry: \n\t \tdriver = _DRIVERS[driver_name] \n\texcept KeyError: \n\t \traise DriverNotFoundError(('No \tdriver \tfor \t%s' % driver_name)) \n\treturn driver(*args, **kwargs)\n", 
" \tlevel = notification.get_default_level() \n\tif (stack.status == stack.IN_PROGRESS): \n\t \tsuffix = 'start' \n\telif (stack.status == stack.COMPLETE): \n\t \tsuffix = 'end' \n\telse: \n\t \tsuffix = 'error' \n\t \tlevel = notification.ERROR \n\tevent_type = ('%s.%s.%s' % ('stack', stack.action.lower(), suffix)) \n\tnotification.notify(stack.context, event_type, level, engine_api.format_notification_body(stack))\n", 
" \tseed = (pseed or config.unittests.rseed) \n\tif (seed == 'random'): \n\t \tseed = None \n\ttry: \n\t \tif seed: \n\t \t \tseed = int(seed) \n\t \telse: \n\t \t \tseed = None \n\texcept ValueError: \n\t \tprint('Error: \tconfig.unittests.rseed \tcontains \tinvalid \tseed, \tusing \tNone \tinstead', file=sys.stderr) \n\t \tseed = None \n\treturn seed\n", 
" \tif ((r.representation == 'html') and (r.name == 'shelter') and r.id and (not r.component)): \n\t \tT = current.T \n\t \tmsg = current.msg \n\t \ts3db = current.s3db \n\t \trecord = r.record \n\t \tctable = s3db.pr_contact \n\t \tstable = s3db.cr_shelter \n\t \tmessage = '' \n\t \ttext = '' \n\t \ts_id = record.id \n\t \ts_name = record.name \n\t \ts_phone = record.phone \n\t \ts_email = record.email \n\t \ts_status = record.status \n\t \tif (s_phone in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_email in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_status in ('', None)): \n\t \t \ts_status = T('Not \tDefined') \n\t \telif (s_status == 1): \n\t \t \ts_status = 'Open' \n\t \telif (s_status == 2): \n\t \t \ts_status = 'Close' \n\t \telse: \n\t \t \ts_status = 'Unassigned \tShelter \tStatus' \n\t \ttext += '************************************************' \n\t \ttext += ('\\n%s \t' % T('Automatic \tMessage')) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Shelter \tID'), s_id)) \n\t \ttext += (' \t%s: \t%s' % (T('Shelter \tname'), s_name)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Email'), s_email)) \n\t \ttext += (' \t%s: \t%s' % (T('Phone'), s_phone)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Working \tStatus'), s_status)) \n\t \ttext += '\\n************************************************\\n' \n\t \turl = URL(c='cr', f='shelter', args=r.id) \n\t \topts = dict(type='SMS', subject=T('Deployment \tRequest'), message=(message + text), url=url) \n\t \toutput = msg.compose(**opts) \n\t \tif attr.get('rheader'): \n\t \t \trheader = attr['rheader'](r) \n\t \t \tif rheader: \n\t \t \t \toutput['rheader'] = rheader \n\t \toutput['title'] = T('Send \tNotification') \n\t \tcurrent.response.view = 'msg/compose.html' \n\t \treturn output \n\telse: \n\t \traise HTTP(501, current.messages.BADMETHOD)\n", 
" \ttrunc = 20 \n\targspec = inspect.getargspec(fobj) \n\targ_list = [] \n\tif argspec.args: \n\t \tfor arg in argspec.args: \n\t \t \targ_list.append(str(arg)) \n\targ_list.reverse() \n\tif argspec.defaults: \n\t \tfor i in range(len(argspec.defaults)): \n\t \t \targ_list[i] = ((str(arg_list[i]) + '=') + str(argspec.defaults[(- i)])) \n\targ_list.reverse() \n\tif argspec.varargs: \n\t \targ_list.append(argspec.varargs) \n\tif argspec.keywords: \n\t \targ_list.append(argspec.keywords) \n\targ_list = [x[:trunc] for x in arg_list] \n\tstr_param = ('%s(%s)' % (name, ', \t'.join(arg_list))) \n\treturn str_param\n", 
" \treturn salt.utils.etcd_util.get_conn(profile)\n", 
" \treturn RetryWithBackoff(callable_func, retry_notify_func, delay, 1, delay, max_tries)\n", 
" \tmsg_tested_versions = ['xp', 'vista', '2008', '2003'] \n\tmsg_args = ['/c', '%SystemRoot%\\\\System32\\\\msg.exe', '*', '/TIME:0'] \n\thost_version = platform.platform().lower() \n\tif (not msg): \n\t \treturn ('Command \tnot \tran.', 'Empty \tmessage.', (-1)) \n\telse: \n\t \tmsg_args.extend([msg]) \n\tfor version in msg_tested_versions: \n\t \tif (host_version.find(version) != (-1)): \n\t \t \tres = client_utils_common.Execute('cmd', msg_args, time_limit=(-1), bypass_whitelist=True) \n\t \t \treturn res \n\treturn ('', 'Command \tnot \tavailable \tfor \tthis \tversion.', (-1))\n", 
" \tmsg_tested_versions = ['xp', 'vista', '2008', '2003'] \n\tmsg_args = ['/c', '%SystemRoot%\\\\System32\\\\msg.exe', '*', '/TIME:0'] \n\thost_version = platform.platform().lower() \n\tif (not msg): \n\t \treturn ('Command \tnot \tran.', 'Empty \tmessage.', (-1)) \n\telse: \n\t \tmsg_args.extend([msg]) \n\tfor version in msg_tested_versions: \n\t \tif (host_version.find(version) != (-1)): \n\t \t \tres = client_utils_common.Execute('cmd', msg_args, time_limit=(-1), bypass_whitelist=True) \n\t \t \treturn res \n\treturn ('', 'Command \tnot \tavailable \tfor \tthis \tversion.', (-1))\n", 
" \tex = copy(event_exchange) \n\tif (conn.transport.driver_type == u'redis'): \n\t \tex.type = u'fanout' \n\treturn ex\n", 
" \tmsg_tested_versions = ['xp', 'vista', '2008', '2003'] \n\tmsg_args = ['/c', '%SystemRoot%\\\\System32\\\\msg.exe', '*', '/TIME:0'] \n\thost_version = platform.platform().lower() \n\tif (not msg): \n\t \treturn ('Command \tnot \tran.', 'Empty \tmessage.', (-1)) \n\telse: \n\t \tmsg_args.extend([msg]) \n\tfor version in msg_tested_versions: \n\t \tif (host_version.find(version) != (-1)): \n\t \t \tres = client_utils_common.Execute('cmd', msg_args, time_limit=(-1), bypass_whitelist=True) \n\t \t \treturn res \n\treturn ('', 'Command \tnot \tavailable \tfor \tthis \tversion.', (-1))\n", 
" \tex = copy(event_exchange) \n\tif (conn.transport.driver_type == u'redis'): \n\t \tex.type = u'fanout' \n\treturn ex\n", 
" \tif ((r.representation == 'html') and (r.name == 'shelter') and r.id and (not r.component)): \n\t \tT = current.T \n\t \tmsg = current.msg \n\t \ts3db = current.s3db \n\t \trecord = r.record \n\t \tctable = s3db.pr_contact \n\t \tstable = s3db.cr_shelter \n\t \tmessage = '' \n\t \ttext = '' \n\t \ts_id = record.id \n\t \ts_name = record.name \n\t \ts_phone = record.phone \n\t \ts_email = record.email \n\t \ts_status = record.status \n\t \tif (s_phone in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_email in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_status in ('', None)): \n\t \t \ts_status = T('Not \tDefined') \n\t \telif (s_status == 1): \n\t \t \ts_status = 'Open' \n\t \telif (s_status == 2): \n\t \t \ts_status = 'Close' \n\t \telse: \n\t \t \ts_status = 'Unassigned \tShelter \tStatus' \n\t \ttext += '************************************************' \n\t \ttext += ('\\n%s \t' % T('Automatic \tMessage')) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Shelter \tID'), s_id)) \n\t \ttext += (' \t%s: \t%s' % (T('Shelter \tname'), s_name)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Email'), s_email)) \n\t \ttext += (' \t%s: \t%s' % (T('Phone'), s_phone)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Working \tStatus'), s_status)) \n\t \ttext += '\\n************************************************\\n' \n\t \turl = URL(c='cr', f='shelter', args=r.id) \n\t \topts = dict(type='SMS', subject=T('Deployment \tRequest'), message=(message + text), url=url) \n\t \toutput = msg.compose(**opts) \n\t \tif attr.get('rheader'): \n\t \t \trheader = attr['rheader'](r) \n\t \t \tif rheader: \n\t \t \t \toutput['rheader'] = rheader \n\t \toutput['title'] = T('Send \tNotification') \n\t \tcurrent.response.view = 'msg/compose.html' \n\t \treturn output \n\telse: \n\t \traise HTTP(501, current.messages.BADMETHOD)\n", 
" \treturn salt.utils.etcd_util.get_conn(profile)\n", 
" \t@functools.wraps(f) \n\tdef Wrapper(self, request): \n\t \t'Wrap \tthe \tfunction \tcan \tcatch \texceptions, \tconverting \tthem \tto \tstatus.' \n\t \tfailed = True \n\t \tresponse = rdf_data_store.DataStoreResponse() \n\t \tresponse.status = rdf_data_store.DataStoreResponse.Status.OK \n\t \ttry: \n\t \t \tf(self, request, response) \n\t \t \tfailed = False \n\t \texcept access_control.UnauthorizedAccess as e: \n\t \t \tresponse.Clear() \n\t \t \tresponse.request = request \n\t \t \tresponse.status = rdf_data_store.DataStoreResponse.Status.AUTHORIZATION_DENIED \n\t \t \tif e.subject: \n\t \t \t \tresponse.failed_subject = utils.SmartUnicode(e.subject) \n\t \t \tresponse.status_desc = utils.SmartUnicode(e) \n\t \texcept data_store.Error as e: \n\t \t \tresponse.Clear() \n\t \t \tresponse.request = request \n\t \t \tresponse.status = rdf_data_store.DataStoreResponse.Status.DATA_STORE_ERROR \n\t \t \tresponse.status_desc = utils.SmartUnicode(e) \n\t \texcept access_control.ExpiryError as e: \n\t \t \tresponse.Clear() \n\t \t \tresponse.request = request \n\t \t \tresponse.status = rdf_data_store.DataStoreResponse.Status.TIMEOUT_ERROR \n\t \t \tresponse.status_desc = utils.SmartUnicode(e) \n\t \tif failed: \n\t \t \tlogging.info('Failed: \t%s', utils.SmartStr(response)[:1000]) \n\t \tserialized_response = response.SerializeToString() \n\t \treturn serialized_response \n\treturn Wrapper\n", 
" \treturn RetryWithBackoff(callable_func, retry_notify_func, delay, 1, delay, max_tries)\n", 
" \tmsg_tested_versions = ['xp', 'vista', '2008', '2003'] \n\tmsg_args = ['/c', '%SystemRoot%\\\\System32\\\\msg.exe', '*', '/TIME:0'] \n\thost_version = platform.platform().lower() \n\tif (not msg): \n\t \treturn ('Command \tnot \tran.', 'Empty \tmessage.', (-1)) \n\telse: \n\t \tmsg_args.extend([msg]) \n\tfor version in msg_tested_versions: \n\t \tif (host_version.find(version) != (-1)): \n\t \t \tres = client_utils_common.Execute('cmd', msg_args, time_limit=(-1), bypass_whitelist=True) \n\t \t \treturn res \n\treturn ('', 'Command \tnot \tavailable \tfor \tthis \tversion.', (-1))\n", 
" \tcache_key = _cache_get_key() \n\ttry: \n\t \treturn __context__[cache_key] \n\texcept KeyError: \n\t \tpass \n\tconn = _get_conn(region=region, key=key, keyid=keyid, profile=profile) \n\t__context__[cache_key] = {} \n\ttopics = conn.get_all_topics() \n\tfor t in topics['ListTopicsResponse']['ListTopicsResult']['Topics']: \n\t \tshort_name = t['TopicArn'].split(':')[(-1)] \n\t \t__context__[cache_key][short_name] = t['TopicArn'] \n\treturn __context__[cache_key]\n", 
" \treturn salt.utils.etcd_util.get_conn(profile)\n", 
" \treturn RetryWithBackoff(callable_func, retry_notify_func, delay, 1, delay, max_tries)\n", 
" \tmsg_tested_versions = ['xp', 'vista', '2008', '2003'] \n\tmsg_args = ['/c', '%SystemRoot%\\\\System32\\\\msg.exe', '*', '/TIME:0'] \n\thost_version = platform.platform().lower() \n\tif (not msg): \n\t \treturn ('Command \tnot \tran.', 'Empty \tmessage.', (-1)) \n\telse: \n\t \tmsg_args.extend([msg]) \n\tfor version in msg_tested_versions: \n\t \tif (host_version.find(version) != (-1)): \n\t \t \tres = client_utils_common.Execute('cmd', msg_args, time_limit=(-1), bypass_whitelist=True) \n\t \t \treturn res \n\treturn ('', 'Command \tnot \tavailable \tfor \tthis \tversion.', (-1))\n", 
" \tmsg_tested_versions = ['xp', 'vista', '2008', '2003'] \n\tmsg_args = ['/c', '%SystemRoot%\\\\System32\\\\msg.exe', '*', '/TIME:0'] \n\thost_version = platform.platform().lower() \n\tif (not msg): \n\t \treturn ('Command \tnot \tran.', 'Empty \tmessage.', (-1)) \n\telse: \n\t \tmsg_args.extend([msg]) \n\tfor version in msg_tested_versions: \n\t \tif (host_version.find(version) != (-1)): \n\t \t \tres = client_utils_common.Execute('cmd', msg_args, time_limit=(-1), bypass_whitelist=True) \n\t \t \treturn res \n\treturn ('', 'Command \tnot \tavailable \tfor \tthis \tversion.', (-1))\n", 
" \tex = copy(event_exchange) \n\tif (conn.transport.driver_type == u'redis'): \n\t \tex.type = u'fanout' \n\treturn ex\n", 
" \tmsg_tested_versions = ['xp', 'vista', '2008', '2003'] \n\tmsg_args = ['/c', '%SystemRoot%\\\\System32\\\\msg.exe', '*', '/TIME:0'] \n\thost_version = platform.platform().lower() \n\tif (not msg): \n\t \treturn ('Command \tnot \tran.', 'Empty \tmessage.', (-1)) \n\telse: \n\t \tmsg_args.extend([msg]) \n\tfor version in msg_tested_versions: \n\t \tif (host_version.find(version) != (-1)): \n\t \t \tres = client_utils_common.Execute('cmd', msg_args, time_limit=(-1), bypass_whitelist=True) \n\t \t \treturn res \n\treturn ('', 'Command \tnot \tavailable \tfor \tthis \tversion.', (-1))\n", 
" \tex = copy(event_exchange) \n\tif (conn.transport.driver_type == u'redis'): \n\t \tex.type = u'fanout' \n\treturn ex\n", 
" \tif ((r.representation == 'html') and (r.name == 'shelter') and r.id and (not r.component)): \n\t \tT = current.T \n\t \tmsg = current.msg \n\t \ts3db = current.s3db \n\t \trecord = r.record \n\t \tctable = s3db.pr_contact \n\t \tstable = s3db.cr_shelter \n\t \tmessage = '' \n\t \ttext = '' \n\t \ts_id = record.id \n\t \ts_name = record.name \n\t \ts_phone = record.phone \n\t \ts_email = record.email \n\t \ts_status = record.status \n\t \tif (s_phone in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_email in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_status in ('', None)): \n\t \t \ts_status = T('Not \tDefined') \n\t \telif (s_status == 1): \n\t \t \ts_status = 'Open' \n\t \telif (s_status == 2): \n\t \t \ts_status = 'Close' \n\t \telse: \n\t \t \ts_status = 'Unassigned \tShelter \tStatus' \n\t \ttext += '************************************************' \n\t \ttext += ('\\n%s \t' % T('Automatic \tMessage')) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Shelter \tID'), s_id)) \n\t \ttext += (' \t%s: \t%s' % (T('Shelter \tname'), s_name)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Email'), s_email)) \n\t \ttext += (' \t%s: \t%s' % (T('Phone'), s_phone)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Working \tStatus'), s_status)) \n\t \ttext += '\\n************************************************\\n' \n\t \turl = URL(c='cr', f='shelter', args=r.id) \n\t \topts = dict(type='SMS', subject=T('Deployment \tRequest'), message=(message + text), url=url) \n\t \toutput = msg.compose(**opts) \n\t \tif attr.get('rheader'): \n\t \t \trheader = attr['rheader'](r) \n\t \t \tif rheader: \n\t \t \t \toutput['rheader'] = rheader \n\t \toutput['title'] = T('Send \tNotification') \n\t \tcurrent.response.view = 'msg/compose.html' \n\t \treturn output \n\telse: \n\t \traise HTTP(501, current.messages.BADMETHOD)\n", 
" \tif (not settings.FEATURES.get('ENABLE_FEEDBACK_SUBMISSION', False)): \n\t \traise Http404() \n\tif (request.method != 'POST'): \n\t \treturn HttpResponseNotAllowed(['POST']) \n\tdef build_error_response(status_code, field, err_msg): \n\t \treturn HttpResponse(json.dumps({'field': field, 'error': err_msg}), status=status_code) \n\trequired_fields = ['subject', 'details'] \n\tif (not request.user.is_authenticated()): \n\t \trequired_fields += ['name', 'email'] \n\trequired_field_errs = {'subject': 'Please \tprovide \ta \tsubject.', 'details': 'Please \tprovide \tdetails.', 'name': 'Please \tprovide \tyour \tname.', 'email': 'Please \tprovide \ta \tvalid \te-mail.'} \n\tfor field in required_fields: \n\t \tif ((field not in request.POST) or (not request.POST[field])): \n\t \t \treturn build_error_response(400, field, required_field_errs[field]) \n\tif (not request.user.is_authenticated()): \n\t \ttry: \n\t \t \tvalidate_email(request.POST['email']) \n\t \texcept ValidationError: \n\t \t \treturn build_error_response(400, 'email', required_field_errs['email']) \n\tsuccess = False \n\tcontext = get_feedback_form_context(request) \n\tsupport_backend = configuration_helpers.get_value('CONTACT_FORM_SUBMISSION_BACKEND', SUPPORT_BACKEND_ZENDESK) \n\tif (support_backend == SUPPORT_BACKEND_EMAIL): \n\t \ttry: \n\t \t \tsend_mail(subject=render_to_string('emails/contact_us_feedback_email_subject.txt', context), message=render_to_string('emails/contact_us_feedback_email_body.txt', context), from_email=context['support_email'], recipient_list=[context['support_email']], fail_silently=False) \n\t \t \tsuccess = True \n\t \texcept SMTPException: \n\t \t \tlog.exception('Error \tsending \tfeedback \tto \tcontact_us \temail \taddress.') \n\t \t \tsuccess = False \n\telse: \n\t \tif ((not settings.ZENDESK_URL) or (not settings.ZENDESK_USER) or (not settings.ZENDESK_API_KEY)): \n\t \t \traise Exception('Zendesk \tenabled \tbut \tnot \tconfigured') \n\t \tcustom_fields = None \n\t \tif settings.ZENDESK_CUSTOM_FIELDS: \n\t \t \tcustom_field_context = _get_zendesk_custom_field_context(request) \n\t \t \tcustom_fields = _format_zendesk_custom_fields(custom_field_context) \n\t \tsuccess = _record_feedback_in_zendesk(context['realname'], context['email'], context['subject'], context['details'], context['tags'], context['additional_info'], support_email=context['support_email'], custom_fields=custom_fields) \n\t_record_feedback_in_datadog(context['tags']) \n\treturn HttpResponse(status=(200 if success else 500))\n", 
" \tfor v in args: \n\t \tsys.stderr.write(str(v)) \n\tsys.stderr.write('\\n')\n", 
" \tif isinstance(arg, (list, tuple)): \n\t \treturn list(arg) \n\telse: \n\t \treturn [arg]\n", 
" \treturn survey_link.format(UNIQUE_ID=unique_id_for_user(user))\n", 
" \treturn salt.utils.etcd_util.get_conn(profile)\n", 
" \treturn RetryWithBackoff(callable_func, retry_notify_func, delay, 1, delay, max_tries)\n", 
" \tmsg_tested_versions = ['xp', 'vista', '2008', '2003'] \n\tmsg_args = ['/c', '%SystemRoot%\\\\System32\\\\msg.exe', '*', '/TIME:0'] \n\thost_version = platform.platform().lower() \n\tif (not msg): \n\t \treturn ('Command \tnot \tran.', 'Empty \tmessage.', (-1)) \n\telse: \n\t \tmsg_args.extend([msg]) \n\tfor version in msg_tested_versions: \n\t \tif (host_version.find(version) != (-1)): \n\t \t \tres = client_utils_common.Execute('cmd', msg_args, time_limit=(-1), bypass_whitelist=True) \n\t \t \treturn res \n\treturn ('', 'Command \tnot \tavailable \tfor \tthis \tversion.', (-1))\n", 
" \tmsg_tested_versions = ['xp', 'vista', '2008', '2003'] \n\tmsg_args = ['/c', '%SystemRoot%\\\\System32\\\\msg.exe', '*', '/TIME:0'] \n\thost_version = platform.platform().lower() \n\tif (not msg): \n\t \treturn ('Command \tnot \tran.', 'Empty \tmessage.', (-1)) \n\telse: \n\t \tmsg_args.extend([msg]) \n\tfor version in msg_tested_versions: \n\t \tif (host_version.find(version) != (-1)): \n\t \t \tres = client_utils_common.Execute('cmd', msg_args, time_limit=(-1), bypass_whitelist=True) \n\t \t \treturn res \n\treturn ('', 'Command \tnot \tavailable \tfor \tthis \tversion.', (-1))\n", 
" \tex = copy(event_exchange) \n\tif (conn.transport.driver_type == u'redis'): \n\t \tex.type = u'fanout' \n\treturn ex\n", 
" \tmsg_tested_versions = ['xp', 'vista', '2008', '2003'] \n\tmsg_args = ['/c', '%SystemRoot%\\\\System32\\\\msg.exe', '*', '/TIME:0'] \n\thost_version = platform.platform().lower() \n\tif (not msg): \n\t \treturn ('Command \tnot \tran.', 'Empty \tmessage.', (-1)) \n\telse: \n\t \tmsg_args.extend([msg]) \n\tfor version in msg_tested_versions: \n\t \tif (host_version.find(version) != (-1)): \n\t \t \tres = client_utils_common.Execute('cmd', msg_args, time_limit=(-1), bypass_whitelist=True) \n\t \t \treturn res \n\treturn ('', 'Command \tnot \tavailable \tfor \tthis \tversion.', (-1))\n", 
" \tex = copy(event_exchange) \n\tif (conn.transport.driver_type == u'redis'): \n\t \tex.type = u'fanout' \n\treturn ex\n", 
" \tif ((r.representation == 'html') and (r.name == 'shelter') and r.id and (not r.component)): \n\t \tT = current.T \n\t \tmsg = current.msg \n\t \ts3db = current.s3db \n\t \trecord = r.record \n\t \tctable = s3db.pr_contact \n\t \tstable = s3db.cr_shelter \n\t \tmessage = '' \n\t \ttext = '' \n\t \ts_id = record.id \n\t \ts_name = record.name \n\t \ts_phone = record.phone \n\t \ts_email = record.email \n\t \ts_status = record.status \n\t \tif (s_phone in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_email in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_status in ('', None)): \n\t \t \ts_status = T('Not \tDefined') \n\t \telif (s_status == 1): \n\t \t \ts_status = 'Open' \n\t \telif (s_status == 2): \n\t \t \ts_status = 'Close' \n\t \telse: \n\t \t \ts_status = 'Unassigned \tShelter \tStatus' \n\t \ttext += '************************************************' \n\t \ttext += ('\\n%s \t' % T('Automatic \tMessage')) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Shelter \tID'), s_id)) \n\t \ttext += (' \t%s: \t%s' % (T('Shelter \tname'), s_name)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Email'), s_email)) \n\t \ttext += (' \t%s: \t%s' % (T('Phone'), s_phone)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Working \tStatus'), s_status)) \n\t \ttext += '\\n************************************************\\n' \n\t \turl = URL(c='cr', f='shelter', args=r.id) \n\t \topts = dict(type='SMS', subject=T('Deployment \tRequest'), message=(message + text), url=url) \n\t \toutput = msg.compose(**opts) \n\t \tif attr.get('rheader'): \n\t \t \trheader = attr['rheader'](r) \n\t \t \tif rheader: \n\t \t \t \toutput['rheader'] = rheader \n\t \toutput['title'] = T('Send \tNotification') \n\t \tcurrent.response.view = 'msg/compose.html' \n\t \treturn output \n\telse: \n\t \traise HTTP(501, current.messages.BADMETHOD)\n", 
" \treturn apiproxy_stub_map.UserRPC('images', deadline, callback)\n", 
" \tprint('got \tperspective1 \tref:', perspective) \n\tprint('asking \tit \tto \tfoo(13)') \n\treturn perspective.callRemote('foo', 13)\n", 
" \tif (name in _events): \n\t \tfor event in get_events(name): \n\t \t \tresult = event(*args, **kwargs) \n\t \t \tif (result is not None): \n\t \t \t \targs = ((result,) + args[1:]) \n\treturn (args and args[0])\n", 
" \targs = (['--version'] + _base_args(request.config)) \n\tproc = quteprocess.QuteProc(request) \n\tproc.proc.setProcessChannelMode(QProcess.SeparateChannels) \n\ttry: \n\t \tproc.start(args) \n\t \tproc.wait_for_quit() \n\texcept testprocess.ProcessExited: \n\t \tassert (proc.proc.exitStatus() == QProcess.NormalExit) \n\telse: \n\t \tpytest.fail('Process \tdid \tnot \texit!') \n\toutput = bytes(proc.proc.readAllStandardOutput()).decode('utf-8') \n\tassert (re.search('^qutebrowser\\\\s+v\\\\d+(\\\\.\\\\d+)', output) is not None)\n", 
" \tif (not callable(method)): \n\t \treturn None \n\ttry: \n\t \tmethod_info = method.remote \n\texcept AttributeError: \n\t \treturn None \n\tif (not isinstance(method_info, _RemoteMethodInfo)): \n\t \treturn None \n\treturn method_info\n", 
" \tif ((r.representation == 'html') and (r.name == 'shelter') and r.id and (not r.component)): \n\t \tT = current.T \n\t \tmsg = current.msg \n\t \ts3db = current.s3db \n\t \trecord = r.record \n\t \tctable = s3db.pr_contact \n\t \tstable = s3db.cr_shelter \n\t \tmessage = '' \n\t \ttext = '' \n\t \ts_id = record.id \n\t \ts_name = record.name \n\t \ts_phone = record.phone \n\t \ts_email = record.email \n\t \ts_status = record.status \n\t \tif (s_phone in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_email in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_status in ('', None)): \n\t \t \ts_status = T('Not \tDefined') \n\t \telif (s_status == 1): \n\t \t \ts_status = 'Open' \n\t \telif (s_status == 2): \n\t \t \ts_status = 'Close' \n\t \telse: \n\t \t \ts_status = 'Unassigned \tShelter \tStatus' \n\t \ttext += '************************************************' \n\t \ttext += ('\\n%s \t' % T('Automatic \tMessage')) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Shelter \tID'), s_id)) \n\t \ttext += (' \t%s: \t%s' % (T('Shelter \tname'), s_name)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Email'), s_email)) \n\t \ttext += (' \t%s: \t%s' % (T('Phone'), s_phone)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Working \tStatus'), s_status)) \n\t \ttext += '\\n************************************************\\n' \n\t \turl = URL(c='cr', f='shelter', args=r.id) \n\t \topts = dict(type='SMS', subject=T('Deployment \tRequest'), message=(message + text), url=url) \n\t \toutput = msg.compose(**opts) \n\t \tif attr.get('rheader'): \n\t \t \trheader = attr['rheader'](r) \n\t \t \tif rheader: \n\t \t \t \toutput['rheader'] = rheader \n\t \toutput['title'] = T('Send \tNotification') \n\t \tcurrent.response.view = 'msg/compose.html' \n\t \treturn output \n\telse: \n\t \traise HTTP(501, current.messages.BADMETHOD)\n", 
" \tif strip_tags: \n\t \ttags_start = name.find('[') \n\t \ttags_end = name.find(']') \n\t \tif ((tags_start > 0) and (tags_end > tags_start)): \n\t \t \tnewname = name[:tags_start] \n\t \t \tnewname += name[(tags_end + 1):] \n\t \t \tname = newname \n\tif strip_scenarios: \n\t \ttags_start = name.find('(') \n\t \ttags_end = name.find(')') \n\t \tif ((tags_start > 0) and (tags_end > tags_start)): \n\t \t \tnewname = name[:tags_start] \n\t \t \tnewname += name[(tags_end + 1):] \n\t \t \tname = newname \n\treturn name\n", 
" \tif (name in _events): \n\t \tfor event in get_events(name): \n\t \t \tresult = event(*args, **kwargs) \n\t \t \tif (result is not None): \n\t \t \t \targs = ((result,) + args[1:]) \n\treturn (args and args[0])\n", 
" \targs = (['--version'] + _base_args(request.config)) \n\tproc = quteprocess.QuteProc(request) \n\tproc.proc.setProcessChannelMode(QProcess.SeparateChannels) \n\ttry: \n\t \tproc.start(args) \n\t \tproc.wait_for_quit() \n\texcept testprocess.ProcessExited: \n\t \tassert (proc.proc.exitStatus() == QProcess.NormalExit) \n\telse: \n\t \tpytest.fail('Process \tdid \tnot \texit!') \n\toutput = bytes(proc.proc.readAllStandardOutput()).decode('utf-8') \n\tassert (re.search('^qutebrowser\\\\s+v\\\\d+(\\\\.\\\\d+)', output) is not None)\n", 
" \treturn IMPL.service_get_all_by_topic(context, topic)\n", 
" \tif (not os.path.isfile(filename)): \n\t \treturn {} \n\ttry: \n\t \twith open(filename, 'r') as fdesc: \n\t \t \tinp = fdesc.read() \n\t \tif (not inp): \n\t \t \treturn {} \n\t \treturn json.loads(inp) \n\texcept (IOError, ValueError) as error: \n\t \t_LOGGER.error('Reading \tconfig \tfile \t%s \tfailed: \t%s', filename, error) \n\t \treturn None\n", 
" \tif (output is None): \n\t \treturn '' \n\telse: \n\t \treturn output.rstrip('\\r\\n')\n", 
" \ttb = traceback.format_exception(*failure_info) \n\tfailure = failure_info[1] \n\tif log_failure: \n\t \tLOG.error(_LE('Returning \texception \t%s \tto \tcaller'), six.text_type(failure)) \n\t \tLOG.error(tb) \n\tkwargs = {} \n\tif hasattr(failure, 'kwargs'): \n\t \tkwargs = failure.kwargs \n\tcls_name = str(failure.__class__.__name__) \n\tmod_name = str(failure.__class__.__module__) \n\tif (cls_name.endswith(_REMOTE_POSTFIX) and mod_name.endswith(_REMOTE_POSTFIX)): \n\t \tcls_name = cls_name[:(- len(_REMOTE_POSTFIX))] \n\t \tmod_name = mod_name[:(- len(_REMOTE_POSTFIX))] \n\tdata = {'class': cls_name, 'module': mod_name, 'message': six.text_type(failure), 'tb': tb, 'args': failure.args, 'kwargs': kwargs} \n\tjson_data = jsonutils.dumps(data) \n\treturn json_data\n", 
" \twith warnings.catch_warnings(record=True) as w: \n\t \tif (clear is not None): \n\t \t \tif (not _is_list_like(clear)): \n\t \t \t \tclear = [clear] \n\t \t \tfor m in clear: \n\t \t \t \tgetattr(m, u'__warningregistry__', {}).clear() \n\t \tsaw_warning = False \n\t \twarnings.simplefilter(filter_level) \n\t \t(yield w) \n\t \textra_warnings = [] \n\t \tfor actual_warning in w: \n\t \t \tif (expected_warning and issubclass(actual_warning.category, expected_warning)): \n\t \t \t \tsaw_warning = True \n\t \t \telse: \n\t \t \t \textra_warnings.append(actual_warning.category.__name__) \n\t \tif expected_warning: \n\t \t \tassert saw_warning, (u'Did \tnot \tsee \texpected \twarning \tof \tclass \t%r.' % expected_warning.__name__) \n\t \tassert (not extra_warnings), (u'Caused \tunexpected \twarning(s): \t%r.' % extra_warnings)\n", 
" \ttype1 = type(var1) \n\ttype2 = type(var2) \n\tif (type1 is type2): \n\t \treturn True \n\tif ((type1 is np.ndarray) and (var1.shape == ())): \n\t \treturn (type(var1.item()) is type2) \n\tif ((type2 is np.ndarray) and (var2.shape == ())): \n\t \treturn (type(var2.item()) is type1) \n\treturn False\n", 
" \thostname = urlparse(url).hostname \n\tif (not (('fc2.com' in hostname) or ('xiaojiadianvideo.asia' in hostname))): \n\t \treturn False \n\tupid = match1(url, '.+/content/(\\\\w+)') \n\tfc2video_download_by_upid(upid, output_dir, merge, info_only)\n", 
" \treturn json.loads(data)\n", 
" \tentity_moref = kwargs.get('entity_moref') \n\tentity_type = kwargs.get('entity_type') \n\talarm_moref = kwargs.get('alarm_moref') \n\tif ((not entity_moref) or (not entity_type) or (not alarm_moref)): \n\t \traise ValueError('entity_moref, \tentity_type, \tand \talarm_moref \tmust \tbe \tset') \n\tattribs = {'xmlns:xsd': 'http://www.w3.org/2001/XMLSchema', 'xmlns:xsi': 'http://www.w3.org/2001/XMLSchema-instance', 'xmlns:soap': 'http://schemas.xmlsoap.org/soap/envelope/'} \n\troot = Element('soap:Envelope', attribs) \n\tbody = SubElement(root, 'soap:Body') \n\talarm_status = SubElement(body, 'SetAlarmStatus', {'xmlns': 'urn:vim25'}) \n\tthis = SubElement(alarm_status, '_this', {'xsi:type': 'ManagedObjectReference', 'type': 'AlarmManager'}) \n\tthis.text = 'AlarmManager' \n\talarm = SubElement(alarm_status, 'alarm', {'type': 'Alarm'}) \n\talarm.text = alarm_moref \n\tentity = SubElement(alarm_status, 'entity', {'xsi:type': 'ManagedObjectReference', 'type': entity_type}) \n\tentity.text = entity_moref \n\tstatus = SubElement(alarm_status, 'status') \n\tstatus.text = 'green' \n\treturn '<?xml \tversion=\"1.0\" \tencoding=\"UTF-8\"?>{0}'.format(tostring(root))\n", 
" \tmsg_tested_versions = ['xp', 'vista', '2008', '2003'] \n\tmsg_args = ['/c', '%SystemRoot%\\\\System32\\\\msg.exe', '*', '/TIME:0'] \n\thost_version = platform.platform().lower() \n\tif (not msg): \n\t \treturn ('Command \tnot \tran.', 'Empty \tmessage.', (-1)) \n\telse: \n\t \tmsg_args.extend([msg]) \n\tfor version in msg_tested_versions: \n\t \tif (host_version.find(version) != (-1)): \n\t \t \tres = client_utils_common.Execute('cmd', msg_args, time_limit=(-1), bypass_whitelist=True) \n\t \t \treturn res \n\treturn ('', 'Command \tnot \tavailable \tfor \tthis \tversion.', (-1))\n", 
" \treturn (a * b)\n", 
" \trespbody = response.body \n\tcustom_properties = {} \n\tbroker_properties = None \n\tmessage_type = None \n\tmessage_location = None \n\tfor (name, value) in response.headers: \n\t \tif (name.lower() == 'brokerproperties'): \n\t \t \tbroker_properties = json.loads(value) \n\t \telif (name.lower() == 'content-type'): \n\t \t \tmessage_type = value \n\t \telif (name.lower() == 'location'): \n\t \t \tmessage_location = value \n\t \telif (name.lower() not in ['transfer-encoding', 'server', 'date', 'strict-transport-security']): \n\t \t \tif ('\"' in value): \n\t \t \t \tvalue = value[1:(-1)].replace('\\\\\"', '\"') \n\t \t \t \ttry: \n\t \t \t \t \tcustom_properties[name] = datetime.strptime(value, '%a, \t%d \t%b \t%Y \t%H:%M:%S \tGMT') \n\t \t \t \texcept ValueError: \n\t \t \t \t \tcustom_properties[name] = value \n\t \t \telif (value.lower() == 'true'): \n\t \t \t \tcustom_properties[name] = True \n\t \t \telif (value.lower() == 'false'): \n\t \t \t \tcustom_properties[name] = False \n\t \t \telse: \n\t \t \t \ttry: \n\t \t \t \t \tfloat_value = float(value) \n\t \t \t \t \tif (str(int(float_value)) == value): \n\t \t \t \t \t \tcustom_properties[name] = int(value) \n\t \t \t \t \telse: \n\t \t \t \t \t \tcustom_properties[name] = float_value \n\t \t \t \texcept ValueError: \n\t \t \t \t \tpass \n\tif (message_type is None): \n\t \tmessage = Message(respbody, service_instance, message_location, custom_properties, 'application/atom+xml;type=entry;charset=utf-8', broker_properties) \n\telse: \n\t \tmessage = Message(respbody, service_instance, message_location, custom_properties, message_type, broker_properties) \n\treturn message\n", 
" \tproducer.publish(msg, exchange=exchange, retry=retry, retry_policy=retry_policy, **dict({'routing_key': req.properties['reply_to'], 'correlation_id': req.properties.get('correlation_id'), 'serializer': serializers.type_to_name[req.content_type], 'content_encoding': req.content_encoding}, **props))\n", 
" \tproducer.publish(msg, exchange=exchange, retry=retry, retry_policy=retry_policy, **dict({'routing_key': req.properties['reply_to'], 'correlation_id': req.properties.get('correlation_id'), 'serializer': serializers.type_to_name[req.content_type], 'content_encoding': req.content_encoding}, **props))\n", 
" \tif ((r.representation == 'html') and (r.name == 'shelter') and r.id and (not r.component)): \n\t \tT = current.T \n\t \tmsg = current.msg \n\t \ts3db = current.s3db \n\t \trecord = r.record \n\t \tctable = s3db.pr_contact \n\t \tstable = s3db.cr_shelter \n\t \tmessage = '' \n\t \ttext = '' \n\t \ts_id = record.id \n\t \ts_name = record.name \n\t \ts_phone = record.phone \n\t \ts_email = record.email \n\t \ts_status = record.status \n\t \tif (s_phone in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_email in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_status in ('', None)): \n\t \t \ts_status = T('Not \tDefined') \n\t \telif (s_status == 1): \n\t \t \ts_status = 'Open' \n\t \telif (s_status == 2): \n\t \t \ts_status = 'Close' \n\t \telse: \n\t \t \ts_status = 'Unassigned \tShelter \tStatus' \n\t \ttext += '************************************************' \n\t \ttext += ('\\n%s \t' % T('Automatic \tMessage')) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Shelter \tID'), s_id)) \n\t \ttext += (' \t%s: \t%s' % (T('Shelter \tname'), s_name)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Email'), s_email)) \n\t \ttext += (' \t%s: \t%s' % (T('Phone'), s_phone)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Working \tStatus'), s_status)) \n\t \ttext += '\\n************************************************\\n' \n\t \turl = URL(c='cr', f='shelter', args=r.id) \n\t \topts = dict(type='SMS', subject=T('Deployment \tRequest'), message=(message + text), url=url) \n\t \toutput = msg.compose(**opts) \n\t \tif attr.get('rheader'): \n\t \t \trheader = attr['rheader'](r) \n\t \t \tif rheader: \n\t \t \t \toutput['rheader'] = rheader \n\t \toutput['title'] = T('Send \tNotification') \n\t \tcurrent.response.view = 'msg/compose.html' \n\t \treturn output \n\telse: \n\t \traise HTTP(501, current.messages.BADMETHOD)\n", 
" \tenter_return = None \n\ttry: \n\t \tif isinstance(enter_func, functools.partial): \n\t \t \tenter_func_name = enter_func.func.__name__ \n\t \telse: \n\t \t \tenter_func_name = enter_func.__name__ \n\t \tLOG.debug('Entering \tcontext. \tFunction: \t%(func_name)s, \tuse_enter_return: \t%(use)s.', {'func_name': enter_func_name, 'use': use_enter_return}) \n\t \tenter_return = enter_func() \n\t \t(yield enter_return) \n\tfinally: \n\t \tif isinstance(exit_func, functools.partial): \n\t \t \texit_func_name = exit_func.func.__name__ \n\t \telse: \n\t \t \texit_func_name = exit_func.__name__ \n\t \tLOG.debug('Exiting \tcontext. \tFunction: \t%(func_name)s, \tuse_enter_return: \t%(use)s.', {'func_name': exit_func_name, 'use': use_enter_return}) \n\t \tif (enter_return is not None): \n\t \t \tif use_enter_return: \n\t \t \t \tignore_exception(exit_func, enter_return) \n\t \t \telse: \n\t \t \t \tignore_exception(exit_func)\n", 
" \tdef decorator(fx): \n\t \tfx._event_id = id \n\t \tfx._event_name = event \n\t \treturn fx \n\treturn decorator\n", 
" \tglobal _path_created \n\tif (not isinstance(name, StringTypes)): \n\t \traise DistutilsInternalError, (\"mkpath: \t'name' \tmust \tbe \ta \tstring \t(got \t%r)\" % (name,)) \n\tname = os.path.normpath(name) \n\tcreated_dirs = [] \n\tif (os.path.isdir(name) or (name == '')): \n\t \treturn created_dirs \n\tif _path_created.get(os.path.abspath(name)): \n\t \treturn created_dirs \n\t(head, tail) = os.path.split(name) \n\ttails = [tail] \n\twhile (head and tail and (not os.path.isdir(head))): \n\t \t(head, tail) = os.path.split(head) \n\t \ttails.insert(0, tail) \n\tfor d in tails: \n\t \thead = os.path.join(head, d) \n\t \tabs_head = os.path.abspath(head) \n\t \tif _path_created.get(abs_head): \n\t \t \tcontinue \n\t \tif (verbose >= 1): \n\t \t \tlog.info('creating \t%s', head) \n\t \tif (not dry_run): \n\t \t \ttry: \n\t \t \t \tos.mkdir(head) \n\t \t \t \tcreated_dirs.append(head) \n\t \t \texcept OSError as exc: \n\t \t \t \traise DistutilsFileError, (\"could \tnot \tcreate \t'%s': \t%s\" % (head, exc[(-1)])) \n\t \t_path_created[abs_head] = 1 \n\treturn created_dirs\n", 
" \ttry: \n\t \t(module, attr) = name.rsplit('.', 1) \n\texcept ValueError as error: \n\t \traise ImproperlyConfigured(('Error \timporting \tclass \t%s \tin \t%s: \t\"%s\"' % (name, setting, error))) \n\ttry: \n\t \tmod = import_module(module) \n\texcept ImportError as error: \n\t \traise ImproperlyConfigured(('Error \timporting \tmodule \t%s \tin \t%s: \t\"%s\"' % (module, setting, error))) \n\ttry: \n\t \tcls = getattr(mod, attr) \n\texcept AttributeError: \n\t \traise ImproperlyConfigured(('Module \t\"%s\" \tdoes \tnot \tdefine \ta \t\"%s\" \tclass \tin \t%s' % (module, attr, setting))) \n\treturn cls\n", 
" \tos.environ['LANG'] = 'C' \n\tcli = _DRIVERS.get('HORCM') \n\treturn importutils.import_object('.'.join([_DRIVER_DIR, cli[driver_info['proto']]]), conf, driver_info, db)\n", 
" \tos.environ['LANG'] = 'C' \n\tcli = _DRIVERS.get('HORCM') \n\treturn importutils.import_object('.'.join([_DRIVER_DIR, cli[driver_info['proto']]]), conf, driver_info, db)\n", 
" \tif name.startswith('.'): \n\t \tif (not package): \n\t \t \traise TypeError(\"relative \timports \trequire \tthe \t'package' \targument\") \n\t \tlevel = 0 \n\t \tfor character in name: \n\t \t \tif (character != '.'): \n\t \t \t \tbreak \n\t \t \tlevel += 1 \n\t \tname = _resolve_name(name[level:], package, level) \n\t__import__(name) \n\treturn sys.modules[name]\n", 
" \ttry: \n\t \treturn namedModule(name) \n\texcept ImportError: \n\t \treturn default\n", 
" \tif (not at): \n\t \tat = utcnow() \n\tst = at.strftime((_ISO8601_TIME_FORMAT if (not subsecond) else _ISO8601_TIME_FORMAT_SUBSECOND)) \n\ttz = (at.tzinfo.tzname(None) if at.tzinfo else 'UTC') \n\tst += ('Z' if (tz == 'UTC') else tz) \n\treturn st\n", 
" \ttry: \n\t \treturn iso8601.parse_date(timestr) \n\texcept iso8601.ParseError as e: \n\t \traise ValueError(encodeutils.exception_to_unicode(e)) \n\texcept TypeError as e: \n\t \traise ValueError(encodeutils.exception_to_unicode(e))\n", 
" \tnow = datetime.utcnow().replace(tzinfo=iso8601.iso8601.UTC) \n\tif iso: \n\t \treturn datetime_tuple_to_iso(now) \n\telse: \n\t \treturn now\n", 
" \treturn formatdate(timegm(dt.utctimetuple()))\n", 
" \toffset = timestamp.utcoffset() \n\tif (offset is None): \n\t \treturn timestamp \n\treturn (timestamp.replace(tzinfo=None) - offset)\n", 
" \tif os.path.isfile(db_file_name): \n\t \tfrom datetime import timedelta \n\t \tfile_datetime = datetime.fromtimestamp(os.stat(db_file_name).st_ctime) \n\t \tif ((datetime.today() - file_datetime) >= timedelta(hours=older_than)): \n\t \t \tif verbose: \n\t \t \t \tprint u'File \tis \told' \n\t \t \treturn True \n\t \telse: \n\t \t \tif verbose: \n\t \t \t \tprint u'File \tis \trecent' \n\t \t \treturn False \n\telse: \n\t \tif verbose: \n\t \t \tprint u'File \tdoes \tnot \texist' \n\t \treturn True\n", 
" \tif (not os.path.exists(source)): \n\t \traise DistutilsFileError((\"file \t'%s' \tdoes \tnot \texist\" % os.path.abspath(source))) \n\tif (not os.path.exists(target)): \n\t \treturn True \n\treturn (os.stat(source)[ST_MTIME] > os.stat(target)[ST_MTIME])\n", 
" \tif utcnow.override_time: \n\t \ttry: \n\t \t \treturn utcnow.override_time.pop(0) \n\t \texcept AttributeError: \n\t \t \treturn utcnow.override_time \n\tif with_timezone: \n\t \treturn datetime.datetime.now(tz=iso8601.iso8601.UTC) \n\treturn datetime.datetime.utcnow()\n", 
" \tif utcnow.override_time: \n\t \ttry: \n\t \t \treturn utcnow.override_time.pop(0) \n\t \texcept AttributeError: \n\t \t \treturn utcnow.override_time \n\tif with_timezone: \n\t \treturn datetime.datetime.now(tz=iso8601.iso8601.UTC) \n\treturn datetime.datetime.utcnow()\n", 
" \treturn isotime(datetime.datetime.utcfromtimestamp(timestamp), microsecond)\n", 
" \tif utcnow.override_time: \n\t \ttry: \n\t \t \treturn utcnow.override_time.pop(0) \n\t \texcept AttributeError: \n\t \t \treturn utcnow.override_time \n\tif with_timezone: \n\t \treturn datetime.datetime.now(tz=iso8601.iso8601.UTC) \n\treturn datetime.datetime.utcnow()\n", 
" \tmatch = standard_duration_re.match(value) \n\tif (not match): \n\t \tmatch = iso8601_duration_re.match(value) \n\tif match: \n\t \tkw = match.groupdict() \n\t \tsign = ((-1) if (kw.pop('sign', '+') == '-') else 1) \n\t \tif kw.get('microseconds'): \n\t \t \tkw['microseconds'] = kw['microseconds'].ljust(6, '0') \n\t \tif (kw.get('seconds') and kw.get('microseconds') and kw['seconds'].startswith('-')): \n\t \t \tkw['microseconds'] = ('-' + kw['microseconds']) \n\t \tkw = {k: float(v) for (k, v) in kw.items() if (v is not None)} \n\t \treturn (sign * datetime.timedelta(**kw))\n", 
" \treturn call_talib_with_ohlc(barDs, count, talib.CDLADVANCEBLOCK)\n", 
" \ttry: \n\t \tparent = next(klass.local_attr_ancestors(name)) \n\texcept (StopIteration, KeyError): \n\t \treturn None \n\ttry: \n\t \tmeth_node = parent[name] \n\texcept KeyError: \n\t \treturn None \n\tif isinstance(meth_node, astroid.Function): \n\t \treturn meth_node \n\treturn None\n", 
" \tif value.tzinfo: \n\t \tvalue = value.astimezone(UTC) \n\treturn (long((calendar.timegm(value.timetuple()) * 1000000L)) + value.microsecond)\n", 
" \t(p, u) = getparser(use_datetime=use_datetime, use_builtin_types=use_builtin_types) \n\tp.feed(data) \n\tp.close() \n\treturn (u.close(), u.getmethodname())\n", 
" \tlater = (mktime(date1.timetuple()) + (date1.microsecond / 1000000.0)) \n\tearlier = (mktime(date2.timetuple()) + (date2.microsecond / 1000000.0)) \n\treturn (later - earlier)\n", 
" \tgenerate_completions(event)\n", 
" \tmappings = service_mapping(services, registry_path=registry_path) \n\treturn webapp2.WSGIApplication(routes=mappings, debug=debug, config=config)\n", 
" \tconnection = n_rpc.create_connection() \n\tfor details in topic_details: \n\t \t(topic, operation, node_name) = itertools.islice(itertools.chain(details, [None]), 3) \n\t \ttopic_name = topics.get_topic_name(prefix, topic, operation) \n\t \tconnection.create_consumer(topic_name, endpoints, fanout=True) \n\t \tif node_name: \n\t \t \tnode_topic_name = ('%s.%s' % (topic_name, node_name)) \n\t \t \tconnection.create_consumer(node_topic_name, endpoints, fanout=False) \n\tif start_listening: \n\t \tconnection.consume_in_threads() \n\treturn connection\n", 
" \treturn mark_safe(json.dumps(value, cls=SafeJSONEncoder))\n", 
" \tconf = cfg.CONF \n\tcmd.register_cmd_opts(cmd.ovs_opts, conf) \n\tl3_config.register_l3_agent_config_opts(l3_config.OPTS, conf) \n\tconf.register_opts(interface.OPTS) \n\tagent_config.register_interface_driver_opts_helper(conf) \n\treturn conf\n", 
" \tports = [] \n\tfor bridge in bridges: \n\t \tovs = ovs_lib.OVSBridge(bridge) \n\t \tports += [port.port_name for port in ovs.get_vif_ports()] \n\treturn ports\n", 
" \tfor port in ports: \n\t \tdevice = ip_lib.IPDevice(port) \n\t \tif device.exists(): \n\t \t \tdevice.link.delete() \n\t \t \tLOG.info(_LI('Deleting \tport: \t%s'), port)\n", 
" \tconf = setup_conf() \n\tconf() \n\tconfig.setup_logging() \n\tconfiguration_bridges = set([conf.ovs_integration_bridge, conf.external_network_bridge]) \n\tovs = ovs_lib.BaseOVS() \n\tovs_bridges = set(ovs.get_bridges()) \n\tavailable_configuration_bridges = (configuration_bridges & ovs_bridges) \n\tif conf.ovs_all_ports: \n\t \tbridges = ovs_bridges \n\telse: \n\t \tbridges = available_configuration_bridges \n\tports = collect_neutron_ports(available_configuration_bridges) \n\tfor bridge in bridges: \n\t \tLOG.info(_LI('Cleaning \tbridge: \t%s'), bridge) \n\t \tovs = ovs_lib.OVSBridge(bridge) \n\t \tovs.delete_ports(all_ports=conf.ovs_all_ports) \n\tdelete_neutron_ports(ports) \n\tLOG.info(_LI('OVS \tcleanup \tcompleted \tsuccessfully'))\n", 
" \tconf = cfg.CONF \n\tcmd.register_cmd_opts(cmd.ovs_opts, conf) \n\tl3_config.register_l3_agent_config_opts(l3_config.OPTS, conf) \n\tconf.register_opts(interface.OPTS) \n\tagent_config.register_interface_driver_opts_helper(conf) \n\treturn conf\n", 
" \tnetwork_id = namespace.replace(dhcp.NS_PREFIX, '') \n\tdhcp_driver = importutils.import_object(conf.dhcp_driver, conf=conf, process_monitor=_get_dhcp_process_monitor(conf), network=dhcp.NetModel({'id': network_id}), plugin=FakeDhcpPlugin()) \n\tif dhcp_driver.active: \n\t \tdhcp_driver.disable()\n", 
" \tif conf.agent_type: \n\t \tprefixes = NS_PREFIXES.get(conf.agent_type) \n\telse: \n\t \tprefixes = itertools.chain(*NS_PREFIXES.values()) \n\tns_mangling_pattern = ('(%s%s)' % ('|'.join(prefixes), constants.UUID_PATTERN)) \n\tif (not re.match(ns_mangling_pattern, namespace)): \n\t \treturn False \n\tip = ip_lib.IPWrapper(namespace=namespace) \n\treturn (force or ip.namespace_is_empty())\n", 
" \ttry: \n\t \tip = ip_lib.IPWrapper(namespace=namespace) \n\t \tif force: \n\t \t \tkill_dhcp(conf, namespace) \n\t \t \tif ip.netns.exists(namespace): \n\t \t \t \ttry: \n\t \t \t \t \tkill_listen_processes(namespace) \n\t \t \t \texcept PidsInNamespaceException: \n\t \t \t \t \tLOG.error(_LE('Not \tall \tprocesses \twere \tkilled \tin \t%s'), namespace) \n\t \t \t \tfor device in ip.get_devices(exclude_loopback=True): \n\t \t \t \t \tunplug_device(conf, device) \n\t \tip.garbage_collect_namespace() \n\texcept Exception: \n\t \tLOG.exception(_LE('Error \tunable \tto \tdestroy \tnamespace: \t%s'), namespace)\n", 
" \tconf = setup_conf() \n\tconf() \n\tconfig.setup_logging() \n\tcleanup_network_namespaces(conf)\n", 
" \treturn ((module in sys.modules) and isinstance(obj, getattr(import_module(module), class_name)))\n", 
" \tvalidate_config_version(config_details.config_files) \n\tprocessed_files = [process_config_file(config_file, config_details.environment) for config_file in config_details.config_files] \n\tconfig_details = config_details._replace(config_files=processed_files) \n\tmain_file = config_details.config_files[0] \n\tvolumes = load_mapping(config_details.config_files, u'get_volumes', u'Volume') \n\tnetworks = load_mapping(config_details.config_files, u'get_networks', u'Network') \n\tservice_dicts = load_services(config_details, main_file) \n\tif (main_file.version != V1): \n\t \tfor service_dict in service_dicts: \n\t \t \tmatch_named_volumes(service_dict, volumes) \n\tservices_using_deploy = [s for s in service_dicts if s.get(u'deploy')] \n\tif services_using_deploy: \n\t \tlog.warn(u\"Some \tservices \t({}) \tuse \tthe \t'deploy' \tkey, \twhich \twill \tbe \tignored. \tCompose \tdoes \tnot \tsupport \tdeploy \tconfiguration \t- \tuse \t`docker \tstack \tdeploy` \tto \tdeploy \tto \ta \tswarm.\".format(u', \t'.join(sorted((s[u'name'] for s in services_using_deploy))))) \n\treturn Config(main_file.version, service_dicts, volumes, networks)\n", 
" \ttry: \n\t \tvalue = args_list.pop(0) \n\texcept IndexError: \n\t \traise BadCommandUsage(msg) \n\tif ((expected_size_after is not None) and (len(args_list) > expected_size_after)): \n\t \traise BadCommandUsage('too \tmany \targuments') \n\treturn value\n", 
" \ttry: \n\t \tresource = describe_api_resource(restApiId, resourcePath, region=region, key=key, keyid=keyid, profile=profile).get('resource') \n\t \tif resource: \n\t \t \tconn = _get_conn(region=region, key=key, keyid=keyid, profile=profile) \n\t \t \tmethod = conn.get_method(restApiId=restApiId, resourceId=resource['id'], httpMethod=httpMethod) \n\t \t \treturn {'method': _convert_datetime_str(method)} \n\t \treturn {'error': 'get \tAPI \tmethod \tfailed: \tno \tsuch \tresource'} \n\texcept ClientError as e: \n\t \treturn {'error': salt.utils.boto3.get_error(e)}\n", 
" \tif (attrName is None): \n\t \tattrName = argName \n\tactual = {} \n\texpected = {'defaultVal': defaultVal, 'altVal': altVal} \n\to = cls() \n\tactual['defaultVal'] = getattr(o, attrName) \n\to = cls(**{argName: altVal}) \n\tactual['altVal'] = getattr(o, attrName) \n\ttestCase.assertEqual(expected, actual)\n", 
" \tif (target is None): \n\t \ttarget = {} \n\tmatch_rule = _build_match_rule(action, target, pluralized) \n\tcredentials = context.to_policy_values() \n\treturn (match_rule, target, credentials)\n", 
" \tmatch_rule = policy.RuleCheck('rule', action) \n\t(resource, enforce_attr_based_check) = get_resource_and_action(action, pluralized) \n\tif enforce_attr_based_check: \n\t \tres_map = attributes.RESOURCE_ATTRIBUTE_MAP \n\t \tif (resource in res_map): \n\t \t \tfor attribute_name in res_map[resource]: \n\t \t \t \tif _is_attribute_explicitly_set(attribute_name, res_map[resource], target, action): \n\t \t \t \t \tattribute = res_map[resource][attribute_name] \n\t \t \t \t \tif ('enforce_policy' in attribute): \n\t \t \t \t \t \tattr_rule = policy.RuleCheck('rule', ('%s:%s' % (action, attribute_name))) \n\t \t \t \t \t \tif _should_validate_sub_attributes(attribute, target[attribute_name]): \n\t \t \t \t \t \t \tattr_rule = policy.AndCheck([attr_rule, _build_subattr_match_rule(attribute_name, attribute, action, target)]) \n\t \t \t \t \t \tmatch_rule = policy.AndCheck([match_rule, attr_rule]) \n\treturn match_rule\n", 
" \tif context.is_admin: \n\t \treturn True \n\t(rule, target, credentials) = _prepare_check(context, action, target, pluralized) \n\ttry: \n\t \tresult = _ENFORCER.enforce(rule, target, credentials, action=action, do_raise=True) \n\texcept policy.PolicyNotAuthorized: \n\t \twith excutils.save_and_reraise_exception(): \n\t \t \tlog_rule_list(rule) \n\t \t \tLOG.debug(\"Failed \tpolicy \tcheck \tfor \t'%s'\", action) \n\treturn result\n", 
" \tif context.is_admin: \n\t \treturn True \n\t(rule, target, credentials) = _prepare_check(context, action, target, pluralized) \n\ttry: \n\t \tresult = _ENFORCER.enforce(rule, target, credentials, action=action, do_raise=True) \n\texcept policy.PolicyNotAuthorized: \n\t \twith excutils.save_and_reraise_exception(): \n\t \t \tlog_rule_list(rule) \n\t \t \tLOG.debug(\"Failed \tpolicy \tcheck \tfor \t'%s'\", action) \n\treturn result\n", 
" \tglobal _FILE_CACHE \n\tif force_reload: \n\t \tdelete_cached_file(filename) \n\treloaded = False \n\tmtime = os.path.getmtime(filename) \n\tcache_info = _FILE_CACHE.setdefault(filename, {}) \n\tif ((not cache_info) or (mtime > cache_info.get('mtime', 0))): \n\t \tLOG.debug('Reloading \tcached \tfile \t%s', filename) \n\t \twith open(filename) as fap: \n\t \t \tcache_info['data'] = fap.read() \n\t \tcache_info['mtime'] = mtime \n\t \treloaded = True \n\treturn (reloaded, cache_info['data'])\n", 
" \treturn salt.utils.which_bin(cmds)\n", 
" \tlog.warning('parse_key_value_list() \tis \tdeprecated \tand \twill \tbe \tremoved \tin \tv0.6.0') \n\tret = {} \n\tfor value in kv_string_list: \n\t \ttry: \n\t \t \t(k, v) = value.split('=', 1) \n\t \t \tret[k] = v \n\t \texcept ValueError: \n\t \t \terror_func((error_fmt % (value,))) \n\treturn ret\n", 
" \ta ^= b \n\treturn a\n", 
" \tif host: \n\t \treturn ('%s-%s-%s.%s' % (prefix, table, operation, host)) \n\treturn ('%s-%s-%s' % (prefix, table, operation))\n", 
" \tproduct_name = 'neutron' \n\tlogging.set_defaults(default_log_levels=(logging.get_default_log_levels() + EXTRA_LOG_LEVEL_DEFAULTS)) \n\tlogging.setup(cfg.CONF, product_name) \n\tLOG.info(_LI('Logging \tenabled!')) \n\tLOG.info(_LI('%(prog)s \tversion \t%(version)s'), {'prog': sys.argv[0], 'version': version.version_info.release_string()}) \n\tLOG.debug('command \tline: \t%s', ' \t'.join(sys.argv))\n", 
" \tloader = wsgi.Loader(cfg.CONF) \n\tapp = loader.load_app(app_name) \n\treturn app\n", 
" \tconfig = ConfigParser.ConfigParser() \n\tconfig.read(config_file) \n\tdict1 = {} \n\tif (section not in config.sections()): \n\t \treturn dict1 \n\toptions = config.options(section) \n\tfor option in options: \n\t \ttry: \n\t \t \tdict1[option] = config.get(section, option) \n\t \texcept: \n\t \t \tdict1[option] = None \n\treturn dict1\n", 
" \treverse_value_map = {'admin_state': {'down': 'shutdown', 'up': 'no \tshutdown'}} \n\tif vlan.get('admin_state'): \n\t \tvlan = apply_value_map(reverse_value_map, vlan) \n\tVLAN_ARGS = {'name': 'name \t{0}', 'vlan_state': 'state \t{0}', 'admin_state': '{0}', 'mode': 'mode \t{0}', 'mapped_vni': 'vn-segment \t{0}'} \n\tcommands = [] \n\tfor (param, value) in vlan.iteritems(): \n\t \tif ((param == 'mapped_vni') and (value == 'default')): \n\t \t \tcommand = 'no \tvn-segment' \n\t \telse: \n\t \t \tcommand = VLAN_ARGS.get(param).format(vlan.get(param)) \n\t \tif command: \n\t \t \tcommands.append(command) \n\tcommands.insert(0, ('vlan \t' + vid)) \n\tcommands.append('exit') \n\treturn commands\n", 
" \tnet = Mininet(host=RemoteHost, link=link) \n\th1 = net.addHost('h1') \n\th2 = net.addHost('h2', server=remote) \n\tnet.addLink(h1, h2) \n\tnet.start() \n\tnet.pingAll() \n\tnet.stop()\n", 
" \tif (len(port_ids) > MAX_PORTS_PER_QUERY): \n\t \tLOG.debug('Number \tof \tports \t%(pcount)s \texceeds \tthe \tmaximum \tper \tquery \t%(maxp)s. \tPartitioning \tqueries.', {'pcount': len(port_ids), 'maxp': MAX_PORTS_PER_QUERY}) \n\t \treturn (get_ports_and_sgs(context, port_ids[:MAX_PORTS_PER_QUERY]) + get_ports_and_sgs(context, port_ids[MAX_PORTS_PER_QUERY:])) \n\tLOG.debug('get_ports_and_sgs() \tcalled \tfor \tport_ids \t%s', port_ids) \n\tif (not port_ids): \n\t \treturn [] \n\tports_to_sg_ids = get_sg_ids_grouped_by_port(context, port_ids) \n\treturn [make_port_dict_with_security_groups(port, sec_groups) for (port, sec_groups) in six.iteritems(ports_to_sg_ids)]\n", 
" \treturn uuid.uuid4().hex\n", 
" \tlogger.info('Filtering \tthe \tdata \tto \tremove \tDC \toffset \tto \thelp \tdistinguish \tblinks \tfrom \tsaccades') \n\tfmax = np.minimum(45, ((sampling_rate / 2.0) - 0.75)) \n\tfilteog = np.array([filter_data(x, sampling_rate, 2, fmax, None, filter_length, 0.5, 0.5, phase='zero-double', fir_window='hann') for x in eog]) \n\ttemp = np.sqrt(np.sum((filteog ** 2), axis=1)) \n\tindexmax = np.argmax(temp) \n\tfilteog = filter_data(eog[indexmax], sampling_rate, l_freq, h_freq, None, filter_length, 0.5, 0.5, phase='zero-double', fir_window='hann') \n\tlogger.info('Now \tdetecting \tblinks \tand \tgenerating \tcorresponding \tevents') \n\ttemp = (filteog - np.mean(filteog)) \n\tn_samples_start = int((sampling_rate * tstart)) \n\tif (np.abs(np.max(temp)) > np.abs(np.min(temp))): \n\t \t(eog_events, _) = peak_finder(filteog[n_samples_start:], extrema=1) \n\telse: \n\t \t(eog_events, _) = peak_finder(filteog[n_samples_start:], extrema=(-1)) \n\teog_events += n_samples_start \n\tn_events = len(eog_events) \n\tlogger.info(('Number \tof \tEOG \tevents \tdetected \t: \t%d' % n_events)) \n\teog_events = np.array([(eog_events + first_samp), np.zeros(n_events, int), (event_id * np.ones(n_events, int))]).T \n\treturn eog_events\n", 
" \tlogger.info('Filtering \tthe \tdata \tto \tremove \tDC \toffset \tto \thelp \tdistinguish \tblinks \tfrom \tsaccades') \n\tfmax = np.minimum(45, ((sampling_rate / 2.0) - 0.75)) \n\tfilteog = np.array([filter_data(x, sampling_rate, 2, fmax, None, filter_length, 0.5, 0.5, phase='zero-double', fir_window='hann') for x in eog]) \n\ttemp = np.sqrt(np.sum((filteog ** 2), axis=1)) \n\tindexmax = np.argmax(temp) \n\tfilteog = filter_data(eog[indexmax], sampling_rate, l_freq, h_freq, None, filter_length, 0.5, 0.5, phase='zero-double', fir_window='hann') \n\tlogger.info('Now \tdetecting \tblinks \tand \tgenerating \tcorresponding \tevents') \n\ttemp = (filteog - np.mean(filteog)) \n\tn_samples_start = int((sampling_rate * tstart)) \n\tif (np.abs(np.max(temp)) > np.abs(np.min(temp))): \n\t \t(eog_events, _) = peak_finder(filteog[n_samples_start:], extrema=1) \n\telse: \n\t \t(eog_events, _) = peak_finder(filteog[n_samples_start:], extrema=(-1)) \n\teog_events += n_samples_start \n\tn_events = len(eog_events) \n\tlogger.info(('Number \tof \tEOG \tevents \tdetected \t: \t%d' % n_events)) \n\teog_events = np.array([(eog_events + first_samp), np.zeros(n_events, int), (event_id * np.ones(n_events, int))]).T \n\treturn eog_events\n", 
" \taffinity = {} \n\tvolume_bindings = dict((build_volume_binding(volume) for volume in volumes if volume.external)) \n\tif previous_container: \n\t \told_volumes = get_container_data_volumes(previous_container, volumes) \n\t \twarn_on_masked_volume(volumes, old_volumes, previous_container.service) \n\t \tvolume_bindings.update((build_volume_binding(volume) for volume in old_volumes)) \n\t \tif old_volumes: \n\t \t \taffinity = {u'affinity:container': (u'=' + previous_container.id)} \n\treturn (list(volume_bindings.values()), affinity)\n", 
" \tif (bind is None): \n\t \treturn \n\tmethod = bind.get('method', 'simple') \n\tif (method is None): \n\t \treturn \n\telif (method == 'simple'): \n\t \tl.simple_bind_s(bind.get('dn', ''), bind.get('password', '')) \n\telif (method == 'sasl'): \n\t \tsasl_class = getattr(ldap.sasl, bind.get('mechanism', 'EXTERNAL').lower()) \n\t \tcreds = bind.get('credentials', None) \n\t \tif (creds is None): \n\t \t \tcreds = {} \n\t \tauth = sasl_class(*creds.get('args', []), **creds.get('kwargs', {})) \n\t \tl.sasl_interactive_bind_s(bind.get('dn', ''), auth) \n\telse: \n\t \traise ValueError((('unsupported \tbind \tmethod \t\"' + method) + '\"; \tsupported \tbind \tmethods: \tsimple \tsasl'))\n", 
" \treverse_value_map = {'admin_state': {'down': 'shutdown', 'up': 'no \tshutdown'}} \n\tif vlan.get('admin_state'): \n\t \tvlan = apply_value_map(reverse_value_map, vlan) \n\tVLAN_ARGS = {'name': 'name \t{0}', 'vlan_state': 'state \t{0}', 'admin_state': '{0}', 'mode': 'mode \t{0}', 'mapped_vni': 'vn-segment \t{0}'} \n\tcommands = [] \n\tfor (param, value) in vlan.iteritems(): \n\t \tif ((param == 'mapped_vni') and (value == 'default')): \n\t \t \tcommand = 'no \tvn-segment' \n\t \telse: \n\t \t \tcommand = VLAN_ARGS.get(param).format(vlan.get(param)) \n\t \tif command: \n\t \t \tcommands.append(command) \n\tcommands.insert(0, ('vlan \t' + vid)) \n\tcommands.append('exit') \n\treturn commands\n", 
" \tif (bind is None): \n\t \treturn \n\tmethod = bind.get('method', 'simple') \n\tif (method is None): \n\t \treturn \n\telif (method == 'simple'): \n\t \tl.simple_bind_s(bind.get('dn', ''), bind.get('password', '')) \n\telif (method == 'sasl'): \n\t \tsasl_class = getattr(ldap.sasl, bind.get('mechanism', 'EXTERNAL').lower()) \n\t \tcreds = bind.get('credentials', None) \n\t \tif (creds is None): \n\t \t \tcreds = {} \n\t \tauth = sasl_class(*creds.get('args', []), **creds.get('kwargs', {})) \n\t \tl.sasl_interactive_bind_s(bind.get('dn', ''), auth) \n\telse: \n\t \traise ValueError((('unsupported \tbind \tmethod \t\"' + method) + '\"; \tsupported \tbind \tmethods: \tsimple \tsasl'))\n", 
" \tpscmd = list() \n\tname = _get_binding_info(hostheader, ipaddress, port) \n\tcurrent_bindings = list_bindings(site) \n\tif (name not in current_bindings): \n\t \t_LOG.debug('Binding \talready \tabsent: \t%s', name) \n\t \treturn True \n\tpscmd.append(\"Remove-WebBinding \t-HostHeader \t'{0}' \t\".format(hostheader)) \n\tpscmd.append(\" \t-IpAddress \t'{0}' \t-Port \t'{1}'\".format(ipaddress, port)) \n\tcmd_ret = _srvmgr(str().join(pscmd)) \n\tif (cmd_ret['retcode'] == 0): \n\t \tnew_bindings = list_bindings(site) \n\t \tif (name not in new_bindings): \n\t \t \t_LOG.debug('Binding \tremoved \tsuccessfully: \t%s', name) \n\t \t \treturn True \n\t_LOG.error('Unable \tto \tremove \tbinding: \t%s', name) \n\treturn False\n", 
" \tif (bind is None): \n\t \treturn \n\tmethod = bind.get('method', 'simple') \n\tif (method is None): \n\t \treturn \n\telif (method == 'simple'): \n\t \tl.simple_bind_s(bind.get('dn', ''), bind.get('password', '')) \n\telif (method == 'sasl'): \n\t \tsasl_class = getattr(ldap.sasl, bind.get('mechanism', 'EXTERNAL').lower()) \n\t \tcreds = bind.get('credentials', None) \n\t \tif (creds is None): \n\t \t \tcreds = {} \n\t \tauth = sasl_class(*creds.get('args', []), **creds.get('kwargs', {})) \n\t \tl.sasl_interactive_bind_s(bind.get('dn', ''), auth) \n\telse: \n\t \traise ValueError((('unsupported \tbind \tmethod \t\"' + method) + '\"; \tsupported \tbind \tmethods: \tsimple \tsasl'))\n", 
" \tif (bind is None): \n\t \treturn \n\tmethod = bind.get('method', 'simple') \n\tif (method is None): \n\t \treturn \n\telif (method == 'simple'): \n\t \tl.simple_bind_s(bind.get('dn', ''), bind.get('password', '')) \n\telif (method == 'sasl'): \n\t \tsasl_class = getattr(ldap.sasl, bind.get('mechanism', 'EXTERNAL').lower()) \n\t \tcreds = bind.get('credentials', None) \n\t \tif (creds is None): \n\t \t \tcreds = {} \n\t \tauth = sasl_class(*creds.get('args', []), **creds.get('kwargs', {})) \n\t \tl.sasl_interactive_bind_s(bind.get('dn', ''), auth) \n\telse: \n\t \traise ValueError((('unsupported \tbind \tmethod \t\"' + method) + '\"; \tsupported \tbind \tmethods: \tsimple \tsasl'))\n", 
" \tif (bind is None): \n\t \treturn \n\tmethod = bind.get('method', 'simple') \n\tif (method is None): \n\t \treturn \n\telif (method == 'simple'): \n\t \tl.simple_bind_s(bind.get('dn', ''), bind.get('password', '')) \n\telif (method == 'sasl'): \n\t \tsasl_class = getattr(ldap.sasl, bind.get('mechanism', 'EXTERNAL').lower()) \n\t \tcreds = bind.get('credentials', None) \n\t \tif (creds is None): \n\t \t \tcreds = {} \n\t \tauth = sasl_class(*creds.get('args', []), **creds.get('kwargs', {})) \n\t \tl.sasl_interactive_bind_s(bind.get('dn', ''), auth) \n\telse: \n\t \traise ValueError((('unsupported \tbind \tmethod \t\"' + method) + '\"; \tsupported \tbind \tmethods: \tsimple \tsasl'))\n", 
" \tfrom inbox.models.base import MailSyncBase \n\tfrom sqlalchemy import event, DDL \n\tincrement = ((key << 48) + 1) \n\tfor table in MailSyncBase.metadata.tables.values(): \n\t \tevent.listen(table, 'after_create', DDL('ALTER \tTABLE \t{tablename} \tAUTO_INCREMENT={increment}'.format(tablename=table, increment=increment))) \n\twith disabled_dubiously_many_queries_warning(): \n\t \tMailSyncBase.metadata.create_all(engine)\n", 
" \tfor archivableObject in xmlObject.archivableObjects: \n\t \tpaths += archivableObject.getPaths() \n\treturn paths\n", 
" \tparent = node.parent \n\twhile (parent is not None): \n\t \tif isinstance(parent, astroid.Decorators): \n\t \t \treturn True \n\t \tif (parent.is_statement or isinstance(parent, astroid.Lambda) or isinstance(parent, (scoped_nodes.ComprehensionScope, scoped_nodes.ListComp))): \n\t \t \tbreak \n\t \tparent = parent.parent \n\treturn False\n", 
" \tif ((request is not None) and (user is not None) and (request.get(u'type', u'') == u'reset')): \n\t \tuser.set_unusable_password() \n\t \tuser.save()\n", 
" \ttry: \n\t \tstatic_db = StaticAnalyzerAndroid(TITLE='Static \tAnalysis', APP_NAME=app_dic['app_name'], SIZE=app_dic['size'], MD5=app_dic['md5'], SHA1=app_dic['sha1'], SHA256=app_dic['sha256'], PACKAGENAME=man_data_dic['packagename'], MAINACTIVITY=man_data_dic['mainactivity'], TARGET_SDK=man_data_dic['target_sdk'], MAX_SDK=man_data_dic['max_sdk'], MIN_SDK=man_data_dic['min_sdk'], ANDROVERNAME=man_data_dic['androvername'], ANDROVER=man_data_dic['androver'], MANIFEST_ANAL=man_an_dic['manifest_anal'], PERMISSIONS=man_an_dic['permissons'], BIN_ANALYSIS=elf_an_buff, FILES=app_dic['files'], CERTZ=app_dic['certz'], ACTIVITIES=man_data_dic['activities'], RECEIVERS=man_data_dic['receivers'], PROVIDERS=man_data_dic['providers'], SERVICES=man_data_dic['services'], LIBRARIES=man_data_dic['libraries'], BROWSABLE=man_an_dic['browsable_activities'], CNT_ACT=man_an_dic['cnt_act'], CNT_PRO=man_an_dic['cnt_pro'], CNT_SER=man_an_dic['cnt_ser'], CNT_BRO=man_an_dic['cnt_bro'], CERT_INFO=cert_dic['cert_info'], ISSUED=cert_dic['issued'], NATIVE=code_an_dic['native'], DYNAMIC=code_an_dic['dynamic'], REFLECT=code_an_dic['reflect'], CRYPTO=code_an_dic['crypto'], OBFUS=code_an_dic['obfus'], API=code_an_dic['api'], DANG=code_an_dic['dang'], URLS=code_an_dic['urls'], DOMAINS=code_an_dic['domains'], EMAILS=code_an_dic['emails'], STRINGS=app_dic['strings'], ZIPPED=app_dic['zipped'], MANI=app_dic['mani'], EXPORTED_ACT=man_an_dic['exported_act'], E_ACT=man_an_dic['exported_cnt']['act'], E_SER=man_an_dic['exported_cnt']['ser'], E_BRO=man_an_dic['exported_cnt']['bro'], E_CNT=man_an_dic['exported_cnt']['cnt']) \n\t \tstatic_db.save() \n\texcept: \n\t \tPrintException('[ERROR] \tSaving \tto \tDB')\n", 
" \thosts = models.Host.smart_get_bulk(hosts_to_reserve) \n\tif (not hosts): \n\t \traise Exception('At \tleast \tone \thost \tmust \tbe \tspecified') \n\tuser = get_user(username) \n\tmodels.AclGroup.check_for_acl_violation_hosts(hosts, user.login) \n\t(user_acl, created) = models.AclGroup.objects.get_or_create(name=user.login) \n\tif created: \n\t \tuser_acl.users = [user] \n\t \tuser_acl.save() \n\tfor host in hosts: \n\t \thost.aclgroup_set.add(user_acl) \n\t \tuser_acl.hosts.add(*hosts) \n\t \tuser_acl.on_host_membership_change()\n", 
" \tfor archivableObject in xmlObject.archivableObjects: \n\t \tpaths += archivableObject.getPaths() \n\treturn paths\n", 
" \tif (call != 'function'): \n\t \traise SaltCloudSystemExit('The \tlist_vlans \tfunction \tmust \tbe \tcalled \twith \t-f \tor \t--function.') \n\tconn = get_conn(service='SoftLayer_Account') \n\treturn conn.getNetworkVlans()\n", 
" \tif (call != 'function'): \n\t \traise SaltCloudSystemExit('The \tlist_vlans \tfunction \tmust \tbe \tcalled \twith \t-f \tor \t--function.') \n\tconn = get_conn(service='SoftLayer_Account') \n\treturn conn.getNetworkVlans()\n", 
" \tif (call != 'function'): \n\t \traise SaltCloudSystemExit('The \tlist_vlans \tfunction \tmust \tbe \tcalled \twith \t-f \tor \t--function.') \n\tconn = get_conn(service='SoftLayer_Account') \n\treturn conn.getNetworkVlans()\n", 
" \tif (call != 'function'): \n\t \traise SaltCloudSystemExit('The \tlist_vlans \tfunction \tmust \tbe \tcalled \twith \t-f \tor \t--function.') \n\tconn = get_conn(service='SoftLayer_Account') \n\treturn conn.getNetworkVlans()\n", 
" \tif (call != 'function'): \n\t \traise SaltCloudSystemExit('The \tlist_vlans \tfunction \tmust \tbe \tcalled \twith \t-f \tor \t--function.') \n\tconn = get_conn(service='SoftLayer_Account') \n\treturn conn.getNetworkVlans()\n", 
" \ttry: \n\t \ttypes = objects.VolumeTypeList.get_all_types_for_qos(context, qos_specs_id) \n\texcept db_exc.DBError: \n\t \tLOG.exception(_LE('DB \terror:')) \n\t \tmsg = (_('Failed \tto \tget \tall \tassociations \tof \tqos \tspecs \t%s') % qos_specs_id) \n\t \tLOG.warning(msg) \n\t \traise exception.CinderException(message=msg) \n\tresult = [] \n\tfor vol_type in types: \n\t \tresult.append({'association_type': 'volume_type', 'name': vol_type.name, 'id': vol_type.id}) \n\treturn result\n", 
" \ttry: \n\t \ttypes = objects.VolumeTypeList.get_all_types_for_qos(context, qos_specs_id) \n\texcept db_exc.DBError: \n\t \tLOG.exception(_LE('DB \terror:')) \n\t \tmsg = (_('Failed \tto \tget \tall \tassociations \tof \tqos \tspecs \t%s') % qos_specs_id) \n\t \tLOG.warning(msg) \n\t \traise exception.CinderException(message=msg) \n\tresult = [] \n\tfor vol_type in types: \n\t \tresult.append({'association_type': 'volume_type', 'name': vol_type.name, 'id': vol_type.id}) \n\treturn result\n", 
" \ttry: \n\t \ttypes = objects.VolumeTypeList.get_all_types_for_qos(context, qos_specs_id) \n\texcept db_exc.DBError: \n\t \tLOG.exception(_LE('DB \terror:')) \n\t \tmsg = (_('Failed \tto \tget \tall \tassociations \tof \tqos \tspecs \t%s') % qos_specs_id) \n\t \tLOG.warning(msg) \n\t \traise exception.CinderException(message=msg) \n\tresult = [] \n\tfor vol_type in types: \n\t \tresult.append({'association_type': 'volume_type', 'name': vol_type.name, 'id': vol_type.id}) \n\treturn result\n", 
" \treturn volume_type_qos_disassociate_all(context, qos_specs_id)\n", 
" \ttry: \n\t \ttypes = objects.VolumeTypeList.get_all_types_for_qos(context, qos_specs_id) \n\texcept db_exc.DBError: \n\t \tLOG.exception(_LE('DB \terror:')) \n\t \tmsg = (_('Failed \tto \tget \tall \tassociations \tof \tqos \tspecs \t%s') % qos_specs_id) \n\t \tLOG.warning(msg) \n\t \traise exception.CinderException(message=msg) \n\tresult = [] \n\tfor vol_type in types: \n\t \tresult.append({'association_type': 'volume_type', 'name': vol_type.name, 'id': vol_type.id}) \n\treturn result\n", 
" \tconn = _auth(profile) \n\treturn conn.list_floatingips()\n", 
" \tif (':' in description): \n\t \t(authType, argstring) = description.split(':', 1) \n\telse: \n\t \tauthType = description \n\t \targstring = '' \n\treturn findCheckerFactory(authType).generateChecker(argstring)\n", 
" \tif (':' in description): \n\t \t(authType, argstring) = description.split(':', 1) \n\telse: \n\t \tauthType = description \n\t \targstring = '' \n\treturn findCheckerFactory(authType).generateChecker(argstring)\n", 
" \ttry: \n\t \ttypes = objects.VolumeTypeList.get_all_types_for_qos(context, qos_specs_id) \n\texcept db_exc.DBError: \n\t \tLOG.exception(_LE('DB \terror:')) \n\t \tmsg = (_('Failed \tto \tget \tall \tassociations \tof \tqos \tspecs \t%s') % qos_specs_id) \n\t \tLOG.warning(msg) \n\t \traise exception.CinderException(message=msg) \n\tresult = [] \n\tfor vol_type in types: \n\t \tresult.append({'association_type': 'volume_type', 'name': vol_type.name, 'id': vol_type.id}) \n\treturn result\n", 
" \tkstone = auth(profile, **connection_args) \n\tif (project_id and (not tenant_id)): \n\t \ttenant_id = project_id \n\telif (project_name and (not tenant)): \n\t \ttenant = project_name \n\tif user: \n\t \tuser_id = user_get(name=user, profile=profile, **connection_args)[user].get('id') \n\telse: \n\t \tuser = next(six.iterkeys(user_get(user_id, profile=profile, **connection_args)))['name'] \n\tif (not user_id): \n\t \treturn {'Error': 'Unable \tto \tresolve \tuser \tid'} \n\tif tenant: \n\t \ttenant_id = tenant_get(name=tenant, profile=profile, **connection_args)[tenant].get('id') \n\telse: \n\t \ttenant = next(six.iterkeys(tenant_get(tenant_id, profile=profile, **connection_args)))['name'] \n\tif (not tenant_id): \n\t \treturn {'Error': 'Unable \tto \tresolve \ttenant/project \tid'} \n\tif role: \n\t \trole_id = role_get(name=role, profile=profile, **connection_args)[role]['id'] \n\telse: \n\t \trole = next(six.iterkeys(role_get(role_id)))['name'] \n\tif (not role_id): \n\t \treturn {'Error': 'Unable \tto \tresolve \trole \tid'} \n\tif (_OS_IDENTITY_API_VERSION > 2): \n\t \tkstone.roles.revoke(role_id, user=user_id, project=tenant_id) \n\telse: \n\t \tkstone.roles.remove_user_role(user_id, role_id, tenant_id) \n\tret_msg = '\"{0}\" \trole \tremoved \tfor \tuser \t\"{1}\" \tunder \t\"{2}\" \ttenant' \n\treturn ret_msg.format(role, user, tenant)\n", 
" \t_quota_update(cs.quotas, args.tenant, args)\n", 
" \tconnection = boto3.session.Session(aws_access_key_id=access_key_id, aws_secret_access_key=secret_access_key, aws_session_token=session_token) \n\tconnection._session.set_config_variable('metadata_service_num_attempts', BOTO_NUM_RETRIES) \n\tec2_resource = connection.resource('ec2', region_name=region) \n\tif validate_region: \n\t \ttry: \n\t \t \tzones = ec2_resource.meta.client.describe_availability_zones() \n\t \texcept EndpointConnectionError: \n\t \t \traise InvalidRegionError(region) \n\t \tavailable_zones = [available_zone['ZoneName'] for available_zone in zones['AvailabilityZones']] \n\t \tif (zone not in available_zones): \n\t \t \traise InvalidZoneError(zone, available_zones) \n\treturn _EC2(zone=zone, connection=ec2_resource)\n", 
" \tfrom inbox.models.base import MailSyncBase \n\tfrom sqlalchemy import event, DDL \n\tincrement = ((key << 48) + 1) \n\tfor table in MailSyncBase.metadata.tables.values(): \n\t \tevent.listen(table, 'after_create', DDL('ALTER \tTABLE \t{tablename} \tAUTO_INCREMENT={increment}'.format(tablename=table, increment=increment))) \n\twith disabled_dubiously_many_queries_warning(): \n\t \tMailSyncBase.metadata.create_all(engine)\n", 
" \tfor archivableObject in xmlObject.archivableObjects: \n\t \tpaths += archivableObject.getPaths() \n\treturn paths\n", 
" \tparent = node.parent \n\twhile (parent is not None): \n\t \tif isinstance(parent, astroid.Decorators): \n\t \t \treturn True \n\t \tif (parent.is_statement or isinstance(parent, astroid.Lambda) or isinstance(parent, (scoped_nodes.ComprehensionScope, scoped_nodes.ListComp))): \n\t \t \tbreak \n\t \tparent = parent.parent \n\treturn False\n", 
" \tif ((request is not None) and (user is not None) and (request.get(u'type', u'') == u'reset')): \n\t \tuser.set_unusable_password() \n\t \tuser.save()\n", 
" \ttry: \n\t \tstatic_db = StaticAnalyzerAndroid(TITLE='Static \tAnalysis', APP_NAME=app_dic['app_name'], SIZE=app_dic['size'], MD5=app_dic['md5'], SHA1=app_dic['sha1'], SHA256=app_dic['sha256'], PACKAGENAME=man_data_dic['packagename'], MAINACTIVITY=man_data_dic['mainactivity'], TARGET_SDK=man_data_dic['target_sdk'], MAX_SDK=man_data_dic['max_sdk'], MIN_SDK=man_data_dic['min_sdk'], ANDROVERNAME=man_data_dic['androvername'], ANDROVER=man_data_dic['androver'], MANIFEST_ANAL=man_an_dic['manifest_anal'], PERMISSIONS=man_an_dic['permissons'], BIN_ANALYSIS=elf_an_buff, FILES=app_dic['files'], CERTZ=app_dic['certz'], ACTIVITIES=man_data_dic['activities'], RECEIVERS=man_data_dic['receivers'], PROVIDERS=man_data_dic['providers'], SERVICES=man_data_dic['services'], LIBRARIES=man_data_dic['libraries'], BROWSABLE=man_an_dic['browsable_activities'], CNT_ACT=man_an_dic['cnt_act'], CNT_PRO=man_an_dic['cnt_pro'], CNT_SER=man_an_dic['cnt_ser'], CNT_BRO=man_an_dic['cnt_bro'], CERT_INFO=cert_dic['cert_info'], ISSUED=cert_dic['issued'], NATIVE=code_an_dic['native'], DYNAMIC=code_an_dic['dynamic'], REFLECT=code_an_dic['reflect'], CRYPTO=code_an_dic['crypto'], OBFUS=code_an_dic['obfus'], API=code_an_dic['api'], DANG=code_an_dic['dang'], URLS=code_an_dic['urls'], DOMAINS=code_an_dic['domains'], EMAILS=code_an_dic['emails'], STRINGS=app_dic['strings'], ZIPPED=app_dic['zipped'], MANI=app_dic['mani'], EXPORTED_ACT=man_an_dic['exported_act'], E_ACT=man_an_dic['exported_cnt']['act'], E_SER=man_an_dic['exported_cnt']['ser'], E_BRO=man_an_dic['exported_cnt']['bro'], E_CNT=man_an_dic['exported_cnt']['cnt']) \n\t \tstatic_db.save() \n\texcept: \n\t \tPrintException('[ERROR] \tSaving \tto \tDB')\n", 
" \thosts = models.Host.smart_get_bulk(hosts_to_reserve) \n\tif (not hosts): \n\t \traise Exception('At \tleast \tone \thost \tmust \tbe \tspecified') \n\tuser = get_user(username) \n\tmodels.AclGroup.check_for_acl_violation_hosts(hosts, user.login) \n\t(user_acl, created) = models.AclGroup.objects.get_or_create(name=user.login) \n\tif created: \n\t \tuser_acl.users = [user] \n\t \tuser_acl.save() \n\tfor host in hosts: \n\t \thost.aclgroup_set.add(user_acl) \n\t \tuser_acl.hosts.add(*hosts) \n\t \tuser_acl.on_host_membership_change()\n", 
" \tfor archivableObject in xmlObject.archivableObjects: \n\t \tpaths += archivableObject.getPaths() \n\treturn paths\n", 
" \tif (call != 'function'): \n\t \traise SaltCloudSystemExit('The \tlist_vlans \tfunction \tmust \tbe \tcalled \twith \t-f \tor \t--function.') \n\tconn = get_conn(service='SoftLayer_Account') \n\treturn conn.getNetworkVlans()\n", 
" \tif (call != 'function'): \n\t \traise SaltCloudSystemExit('The \tlist_vlans \tfunction \tmust \tbe \tcalled \twith \t-f \tor \t--function.') \n\tconn = get_conn(service='SoftLayer_Account') \n\treturn conn.getNetworkVlans()\n", 
" \tif (call != 'function'): \n\t \traise SaltCloudSystemExit('The \tlist_vlans \tfunction \tmust \tbe \tcalled \twith \t-f \tor \t--function.') \n\tconn = get_conn(service='SoftLayer_Account') \n\treturn conn.getNetworkVlans()\n", 
" \tif (call != 'function'): \n\t \traise SaltCloudSystemExit('The \tlist_vlans \tfunction \tmust \tbe \tcalled \twith \t-f \tor \t--function.') \n\tconn = get_conn(service='SoftLayer_Account') \n\treturn conn.getNetworkVlans()\n", 
" \tif (call != 'function'): \n\t \traise SaltCloudSystemExit('The \tlist_vlans \tfunction \tmust \tbe \tcalled \twith \t-f \tor \t--function.') \n\tconn = get_conn(service='SoftLayer_Account') \n\treturn conn.getNetworkVlans()\n", 
" \ttry: \n\t \ttypes = objects.VolumeTypeList.get_all_types_for_qos(context, qos_specs_id) \n\texcept db_exc.DBError: \n\t \tLOG.exception(_LE('DB \terror:')) \n\t \tmsg = (_('Failed \tto \tget \tall \tassociations \tof \tqos \tspecs \t%s') % qos_specs_id) \n\t \tLOG.warning(msg) \n\t \traise exception.CinderException(message=msg) \n\tresult = [] \n\tfor vol_type in types: \n\t \tresult.append({'association_type': 'volume_type', 'name': vol_type.name, 'id': vol_type.id}) \n\treturn result\n", 
" \ttry: \n\t \ttypes = objects.VolumeTypeList.get_all_types_for_qos(context, qos_specs_id) \n\texcept db_exc.DBError: \n\t \tLOG.exception(_LE('DB \terror:')) \n\t \tmsg = (_('Failed \tto \tget \tall \tassociations \tof \tqos \tspecs \t%s') % qos_specs_id) \n\t \tLOG.warning(msg) \n\t \traise exception.CinderException(message=msg) \n\tresult = [] \n\tfor vol_type in types: \n\t \tresult.append({'association_type': 'volume_type', 'name': vol_type.name, 'id': vol_type.id}) \n\treturn result\n", 
" \ttry: \n\t \ttypes = objects.VolumeTypeList.get_all_types_for_qos(context, qos_specs_id) \n\texcept db_exc.DBError: \n\t \tLOG.exception(_LE('DB \terror:')) \n\t \tmsg = (_('Failed \tto \tget \tall \tassociations \tof \tqos \tspecs \t%s') % qos_specs_id) \n\t \tLOG.warning(msg) \n\t \traise exception.CinderException(message=msg) \n\tresult = [] \n\tfor vol_type in types: \n\t \tresult.append({'association_type': 'volume_type', 'name': vol_type.name, 'id': vol_type.id}) \n\treturn result\n", 
" \treturn volume_type_qos_disassociate_all(context, qos_specs_id)\n", 
" \ttry: \n\t \ttypes = objects.VolumeTypeList.get_all_types_for_qos(context, qos_specs_id) \n\texcept db_exc.DBError: \n\t \tLOG.exception(_LE('DB \terror:')) \n\t \tmsg = (_('Failed \tto \tget \tall \tassociations \tof \tqos \tspecs \t%s') % qos_specs_id) \n\t \tLOG.warning(msg) \n\t \traise exception.CinderException(message=msg) \n\tresult = [] \n\tfor vol_type in types: \n\t \tresult.append({'association_type': 'volume_type', 'name': vol_type.name, 'id': vol_type.id}) \n\treturn result\n", 
" \tconn = _auth(profile) \n\treturn conn.list_floatingips()\n", 
" \tif (':' in description): \n\t \t(authType, argstring) = description.split(':', 1) \n\telse: \n\t \tauthType = description \n\t \targstring = '' \n\treturn findCheckerFactory(authType).generateChecker(argstring)\n", 
" \tif (':' in description): \n\t \t(authType, argstring) = description.split(':', 1) \n\telse: \n\t \tauthType = description \n\t \targstring = '' \n\treturn findCheckerFactory(authType).generateChecker(argstring)\n", 
" \ttry: \n\t \ttypes = objects.VolumeTypeList.get_all_types_for_qos(context, qos_specs_id) \n\texcept db_exc.DBError: \n\t \tLOG.exception(_LE('DB \terror:')) \n\t \tmsg = (_('Failed \tto \tget \tall \tassociations \tof \tqos \tspecs \t%s') % qos_specs_id) \n\t \tLOG.warning(msg) \n\t \traise exception.CinderException(message=msg) \n\tresult = [] \n\tfor vol_type in types: \n\t \tresult.append({'association_type': 'volume_type', 'name': vol_type.name, 'id': vol_type.id}) \n\treturn result\n", 
" \tkstone = auth(profile, **connection_args) \n\tif (project_id and (not tenant_id)): \n\t \ttenant_id = project_id \n\telif (project_name and (not tenant)): \n\t \ttenant = project_name \n\tif user: \n\t \tuser_id = user_get(name=user, profile=profile, **connection_args)[user].get('id') \n\telse: \n\t \tuser = next(six.iterkeys(user_get(user_id, profile=profile, **connection_args)))['name'] \n\tif (not user_id): \n\t \treturn {'Error': 'Unable \tto \tresolve \tuser \tid'} \n\tif tenant: \n\t \ttenant_id = tenant_get(name=tenant, profile=profile, **connection_args)[tenant].get('id') \n\telse: \n\t \ttenant = next(six.iterkeys(tenant_get(tenant_id, profile=profile, **connection_args)))['name'] \n\tif (not tenant_id): \n\t \treturn {'Error': 'Unable \tto \tresolve \ttenant/project \tid'} \n\tif role: \n\t \trole_id = role_get(name=role, profile=profile, **connection_args)[role]['id'] \n\telse: \n\t \trole = next(six.iterkeys(role_get(role_id)))['name'] \n\tif (not role_id): \n\t \treturn {'Error': 'Unable \tto \tresolve \trole \tid'} \n\tif (_OS_IDENTITY_API_VERSION > 2): \n\t \tkstone.roles.revoke(role_id, user=user_id, project=tenant_id) \n\telse: \n\t \tkstone.roles.remove_user_role(user_id, role_id, tenant_id) \n\tret_msg = '\"{0}\" \trole \tremoved \tfor \tuser \t\"{1}\" \tunder \t\"{2}\" \ttenant' \n\treturn ret_msg.format(role, user, tenant)\n", 
" \t_quota_update(cs.quotas, args.tenant, args)\n", 
" \tif ('sqlite' not in IMPL.get_engine().name): \n\t \treturn IMPL.dispose_engine() \n\telse: \n\t \treturn\n", 
" \treturn context_manager.get_legacy_facade().get_session(autocommit=autocommit, expire_on_commit=expire_on_commit, use_slave=use_slave)\n", 
" \tdef decorated(func): \n\t \t'\\n \t \t \t \t \t \t \t \tDecorator \tfor \tthe \tcreation \tfunction.\\n \t \t \t \t \t \t \t \t' \n\t \t_WRITE_MODEL[model] = func \n\t \treturn func \n\treturn decorated\n", 
" \tfrom airflow import models \n\tlogging.info(u'Dropping \ttables \tthat \texist') \n\tmodels.Base.metadata.drop_all(settings.engine) \n\tmc = MigrationContext.configure(settings.engine) \n\tif mc._version.exists(settings.engine): \n\t \tmc._version.drop(settings.engine) \n\tinitdb()\n", 
" \tpbs = XML.SubElement(xml_parent, 'hudson.plugins.templateproject.ProxyBuilder') \n\tXML.SubElement(pbs, 'projectName').text = data\n", 
" \tpbs = XML.SubElement(xml_parent, 'hudson.plugins.templateproject.ProxyBuilder') \n\tXML.SubElement(pbs, 'projectName').text = data\n", 
" \traise InvalidBSON((\"Detected \tunknown \tBSON \ttype \t%r \tfor \tfieldname \t'%s'. \tAre \tyou \tusing \tthe \tlatest \tdriver \tversion?\" % (element_type, element_name)))\n", 
" \tLOG.debug(('port_create(): \tnetid=%s, \tkwargs=%s' % (network_id, kwargs))) \n\tif ('policy_profile_id' in kwargs): \n\t \tkwargs['n1kv:profile'] = kwargs.pop('policy_profile_id') \n\tkwargs = unescape_port_kwargs(**kwargs) \n\tbody = {'port': {'network_id': network_id}} \n\tif ('tenant_id' not in kwargs): \n\t \tkwargs['tenant_id'] = request.user.project_id \n\tbody['port'].update(kwargs) \n\tport = neutronclient(request).create_port(body=body).get('port') \n\treturn Port(port)\n", 
" \tcs.tenant_networks.delete(args.network_id)\n", 
" \tsrc_dev = p_utils.get_interface_name(src_dev, max_len=(n_const.DEVICE_NAME_MAX_LEN - MAX_VLAN_POSTFIX_LEN)) \n\treturn ('%s.%s' % (src_dev, vlan))\n", 
" \tmodel_class = (model_class or model_instance.__class__) \n\treturn MODEL_DATA_SPECS[model_class]\n", 
" \thost_mor = vm_util.get_host_ref(session, cluster) \n\tport_grps_on_host_ret = session._call_method(vutil, 'get_object_property', host_mor, 'config.network.portgroup') \n\tif (not port_grps_on_host_ret): \n\t \tmsg = _('ESX \tSOAP \tserver \treturned \tan \tempty \tport \tgroup \tfor \tthe \thost \tsystem \tin \tits \tresponse') \n\t \tLOG.error(msg) \n\t \traise exception.NovaException(msg) \n\tport_grps_on_host = port_grps_on_host_ret.HostPortGroup \n\tfor p_gp in port_grps_on_host: \n\t \tif (p_gp.spec.name == pg_name): \n\t \t \tp_grp_vswitch_name = p_gp.spec.vswitchName \n\t \t \treturn (p_gp.spec.vlanId, p_grp_vswitch_name) \n\treturn (None, None)\n", 
" \tcmd = '--zone={0} \t--remove-port={1}'.format(zone, port) \n\tif permanent: \n\t \tcmd += ' \t--permanent' \n\treturn __firewall_cmd(cmd)\n", 
" \tcmd = '--zone={0} \t--remove-port={1}'.format(zone, port) \n\tif permanent: \n\t \tcmd += ' \t--permanent' \n\treturn __firewall_cmd(cmd)\n", 
" \tparam_if_exists = _param_if_exists(if_exists) \n\tif (port and (not br)): \n\t \tcmd = 'ovs-vsctl \t{1}del-port \t{0}'.format(port, param_if_exists) \n\telse: \n\t \tcmd = 'ovs-vsctl \t{2}del-port \t{0} \t{1}'.format(br, port, param_if_exists) \n\tresult = __salt__['cmd.run_all'](cmd) \n\tretcode = result['retcode'] \n\treturn _retcode_to_bool(retcode)\n", 
" \tcommands = [] \n\tbrew_taps_path = (brew_path_prefix + TAP_PATH) \n\tfor user in _get_directory_names_only(brew_taps_path): \n\t \ttaps = _get_directory_names_only((brew_taps_path + ('/%s' % user))) \n\t \ttaps = (tap for tap in taps if tap.startswith('homebrew-')) \n\t \tfor tap in taps: \n\t \t \ttap_cmd_path = (brew_taps_path + (TAP_CMD_PATH % (user, tap))) \n\t \t \tif os.path.isdir(tap_cmd_path): \n\t \t \t \tcommands += (name.replace('brew-', '').replace('.rb', '') for name in os.listdir(tap_cmd_path) if _is_brew_tap_cmd_naming(name)) \n\treturn commands\n", 
" \tconn = _auth(profile) \n\treturn conn.update_port(port, name, admin_state_up)\n", 
" \tfprint('Rsync \tthirdparty/haproxy \t~/haproxy') \n\trsync_project(local_dir='third_party/haproxy/', remote_dir='~/haproxy/', ssh_opts='-o \tStrictHostKeyChecking=no') \n\tfprint('Building \thaproxy') \n\trun('haproxy/build.sh \t~/') \n\tfprint('Generating \tviewfinder.pem \tfor \thaproxy') \n\tvf_passphrase = load_passphrase_from_file() \n\tlocal(('scripts/generate_haproxy_certificate.sh \tviewfinder.co \t%s \tviewfinder.pem' % vf_passphrase)) \n\trun('mkdir \t-p \t~/conf') \n\trun('rm \t-f \t~/conf/viewfinder.pem') \n\tput('viewfinder.pem', '~/conf/viewfinder.pem') \n\trun('chmod \t400 \t~/conf/viewfinder.pem') \n\tlocal('rm \t-f \tviewfinder.pem') \n\tfprint('Pushing \thaproxy \tconfigs') \n\tassert env.nodetype, 'no \tnodetype \tspecified' \n\trun('ln \t-f \t-s \t~/viewfinder/scripts/haproxy.conf \t~/conf/haproxy.conf') \n\trun(('ln \t-f \t-s \t~/viewfinder/scripts/haproxy.redirect.%s.conf \t~/conf/haproxy.redirect.conf' % env.nodetype.lower()))\n", 
" \tif (call != 'function'): \n\t \traise SaltCloudSystemExit('The \tshow_hc \tfunction \tmust \tbe \tcalled \twith \t-f \tor \t--function.') \n\tif ((not kwargs) or ('name' not in kwargs)): \n\t \tlog.error('Must \tspecify \tname \tof \thealth \tcheck.') \n\t \treturn False \n\tconn = get_conn() \n\treturn _expand_item(conn.ex_get_healthcheck(kwargs['name']))\n", 
" \treturn set((text.strip() for text in string.split(u',') if text.strip()))\n", 
" \tif (call == 'action'): \n\t \traise SaltCloudSystemExit('The \tget_secgroup_id \tfunction \tmust \tbe \tcalled \twith \t-f \tor \t--function.') \n\tif (kwargs is None): \n\t \tkwargs = {} \n\tname = kwargs.get('name', None) \n\tif (name is None): \n\t \traise SaltCloudSystemExit(\"The \tget_secgroup_id \tfunction \trequires \ta \t'name'.\") \n\ttry: \n\t \tret = list_security_groups()[name]['id'] \n\texcept KeyError: \n\t \traise SaltCloudSystemExit(\"The \tsecurity \tgroup \t'{0}' \tcould \tnot \tbe \tfound.\".format(name)) \n\treturn ret\n", 
" \tbandwidth_items = ['vif_inbound_average', 'vif_inbound_peak', 'vif_inbound_burst', 'vif_outbound_average', 'vif_outbound_peak', 'vif_outbound_burst'] \n\tfor (key, value) in inst_type.get('extra_specs', {}).items(): \n\t \tscope = key.split(':') \n\t \tif ((len(scope) > 1) and (scope[0] == 'quota')): \n\t \t \tif (scope[1] in bandwidth_items): \n\t \t \t \tsetattr(conf, scope[1], value)\n", 
" \tpaths = config['pluginpath'].as_str_seq(split=False) \n\tpaths = [util.normpath(p) for p in paths] \n\tlog.debug(u'plugin \tpaths: \t{0}', util.displayable_path(paths)) \n\tpaths = [util.py3_path(p) for p in paths] \n\timport beetsplug \n\tbeetsplug.__path__ = (paths + beetsplug.__path__) \n\tsys.path += paths \n\tplugins.load_plugins(config['plugins'].as_str_seq()) \n\tplugins.send('pluginload') \n\treturn plugins\n", 
" \treturn _group_get_all(context, filters, marker, limit, offset, sort_keys, sort_dirs)\n", 
" \tconn = _get_conn(region=region, key=key, keyid=keyid, profile=profile) \n\tif (not conn): \n\t \treturn None \n\ttry: \n\t \tcc = conn.describe_cache_clusters(name, show_cache_node_info=True) \n\texcept boto.exception.BotoServerError as e: \n\t \tmsg = 'Failed \tto \tget \tconfig \tfor \tcache \tcluster \t{0}.'.format(name) \n\t \tlog.error(msg) \n\t \tlog.debug(e) \n\t \treturn {} \n\tcc = cc['DescribeCacheClustersResponse']['DescribeCacheClustersResult'] \n\tcc = cc['CacheClusters'][0] \n\tret = odict.OrderedDict() \n\tattrs = ['engine', 'cache_parameter_group', 'cache_cluster_id', 'cache_security_groups', 'replication_group_id', 'auto_minor_version_upgrade', 'num_cache_nodes', 'preferred_availability_zone', 'security_groups', 'cache_subnet_group_name', 'engine_version', 'cache_node_type', 'notification_configuration', 'preferred_maintenance_window', 'configuration_endpoint', 'cache_cluster_status', 'cache_nodes'] \n\tfor (key, val) in six.iteritems(cc): \n\t \t_key = boto.utils.pythonize_name(key) \n\t \tif (_key not in attrs): \n\t \t \tcontinue \n\t \tif (_key == 'cache_parameter_group'): \n\t \t \tif val: \n\t \t \t \tret[_key] = val['CacheParameterGroupName'] \n\t \t \telse: \n\t \t \t \tret[_key] = None \n\t \telif (_key == 'cache_nodes'): \n\t \t \tif val: \n\t \t \t \tret[_key] = [k for k in val] \n\t \t \telse: \n\t \t \t \tret[_key] = [] \n\t \telif (_key == 'cache_security_groups'): \n\t \t \tif val: \n\t \t \t \tret[_key] = [k['CacheSecurityGroupName'] for k in val] \n\t \t \telse: \n\t \t \t \tret[_key] = [] \n\t \telif (_key == 'configuration_endpoint'): \n\t \t \tif val: \n\t \t \t \tret['port'] = val['Port'] \n\t \t \t \tret['address'] = val['Address'] \n\t \t \telse: \n\t \t \t \tret['port'] = None \n\t \t \t \tret['address'] = None \n\t \telif (_key == 'notification_configuration'): \n\t \t \tif val: \n\t \t \t \tret['notification_topic_arn'] = val['TopicArn'] \n\t \t \telse: \n\t \t \t \tret['notification_topic_arn'] = None \n\t \telse: \n\t \t \tret[_key] = val \n\treturn ret\n", 
" \treturn list((list(releases) for (_, releases) in itertools.groupby(versions, operator.attrgetter(u'major_minor'))))\n", 
" \trequest = RequestFactory().get('/') \n\tif (user is not None): \n\t \trequest.user = user \n\telse: \n\t \trequest.user = AnonymousUser() \n\trequest.is_secure = (lambda : True) \n\trequest.get_host = (lambda : 'edx.org') \n\tcrum.set_current_request(request) \n\treturn request\n", 
" \tissue_counts = {BaseComment.OPEN: 0, BaseComment.RESOLVED: 0, BaseComment.DROPPED: 0} \n\tq = (Q(public=True) & Q(base_reply_to__isnull=True)) \n\tif extra_query: \n\t \tq = (q & extra_query) \n\tissue_statuses = review_request.reviews.filter(q).values(u'comments__pk', u'comments__issue_opened', u'comments__issue_status', u'file_attachment_comments__pk', u'file_attachment_comments__issue_opened', u'file_attachment_comments__issue_status', u'general_comments__pk', u'general_comments__issue_opened', u'general_comments__issue_status', u'screenshot_comments__pk', u'screenshot_comments__issue_opened', u'screenshot_comments__issue_status') \n\tif issue_statuses: \n\t \tcomment_fields = {u'comments': set(), u'file_attachment_comments': set(), u'general_comments': set(), u'screenshot_comments': set()} \n\t \tfor issue_fields in issue_statuses: \n\t \t \tfor (key, comments) in six.iteritems(comment_fields): \n\t \t \t \tissue_opened = issue_fields[(key + u'__issue_opened')] \n\t \t \t \tcomment_pk = issue_fields[(key + u'__pk')] \n\t \t \t \tif (issue_opened and (comment_pk not in comments)): \n\t \t \t \t \tcomments.add(comment_pk) \n\t \t \t \t \tissue_status = issue_fields[(key + u'__issue_status')] \n\t \t \t \t \tif issue_status: \n\t \t \t \t \t \tissue_counts[issue_status] += 1 \n\t \tlogging.debug(u'Calculated \tissue \tcounts \tfor \treview \trequest \tID \t%s \tacross \t%s \treview(s): \tResulting \tcounts \t= \t%r; \tDB \tvalues \t= \t%r; \tField \tIDs \t= \t%r', review_request.pk, len(issue_statuses), issue_counts, issue_statuses, comment_fields) \n\treturn issue_counts\n", 
" \treturn IMPL.group_get(context, group_id)\n", 
" \tnetwork = Network() \n\tnetwork.addRegion('sensor', 'py.RecordSensor', json.dumps({'verbosity': _VERBOSITY})) \n\tsensor = network.regions['sensor'].getSelf() \n\tsensor.encoder = createEncoder() \n\tsensor.dataSource = dataSource \n\tSP_PARAMS['inputWidth'] = sensor.encoder.getWidth() \n\tnetwork.addRegion('spatialPoolerRegion', 'py.SPRegion', json.dumps(SP_PARAMS)) \n\tnetwork.link('sensor', 'spatialPoolerRegion', 'UniformLink', '') \n\tnetwork.link('sensor', 'spatialPoolerRegion', 'UniformLink', '', srcOutput='resetOut', destInput='resetIn') \n\tnetwork.link('spatialPoolerRegion', 'sensor', 'UniformLink', '', srcOutput='spatialTopDownOut', destInput='spatialTopDownIn') \n\tnetwork.link('spatialPoolerRegion', 'sensor', 'UniformLink', '', srcOutput='temporalTopDownOut', destInput='temporalTopDownIn') \n\tif enableTP: \n\t \tTP_PARAMS['temporalImp'] = temporalImp \n\t \tnetwork.addRegion('temporalPoolerRegion', 'py.TPRegion', json.dumps(TP_PARAMS)) \n\t \tnetwork.link('spatialPoolerRegion', 'temporalPoolerRegion', 'UniformLink', '') \n\t \tnetwork.link('temporalPoolerRegion', 'spatialPoolerRegion', 'UniformLink', '', srcOutput='topDownOut', destInput='topDownIn') \n\tspatialPoolerRegion = network.regions['spatialPoolerRegion'] \n\tspatialPoolerRegion.setParameter('learningMode', True) \n\tspatialPoolerRegion.setParameter('anomalyMode', False) \n\tif enableTP: \n\t \ttemporalPoolerRegion = network.regions['temporalPoolerRegion'] \n\t \ttemporalPoolerRegion.setParameter('topDownMode', True) \n\t \ttemporalPoolerRegion.setParameter('learningMode', True) \n\t \ttemporalPoolerRegion.setParameter('inferenceMode', True) \n\t \ttemporalPoolerRegion.setParameter('anomalyMode', True) \n\treturn network\n", 
" \tconn = _auth(profile) \n\treturn conn.create_router(name, ext_network, admin_state_up)\n", 
" \treturn uuid.uuid4().hex\n", 
" \treturn ((((cloud_tmp_dir + 'locks/') + cluster_id) + '/') + str(step_num))\n", 
" \tcmd = 'ovs-vsctl \tget \tport \t{0} \ttag'.format(port) \n\tresult = __salt__['cmd.run_all'](cmd) \n\tretcode = result['retcode'] \n\tstdout = result['stdout'] \n\treturn _stdout_list_split(retcode, stdout)\n", 
" \tinfo = sendline('show \tuser-account \t{0}'.format(username)) \n\troles = re.search('^\\\\s*roles:(.*)$', info, re.MULTILINE) \n\tif roles: \n\t \troles = roles.group(1).strip().split(' \t') \n\telse: \n\t \troles = [] \n\treturn roles\n", 
" \tconn = _auth(profile) \n\treturn conn.create_router(name, ext_network, admin_state_up)\n", 
" \tconn = _auth(profile) \n\treturn conn.update_router(router, name, admin_state_up, **kwargs)\n", 
" \tconn = _auth(profile) \n\treturn conn.create_router(name, ext_network, admin_state_up)\n", 
" \tconn = _auth(profile) \n\treturn conn.add_gateway_router(router, ext_network)\n", 
" \treturn _is_owner_router_interface(port['device_owner'])\n", 
" \tif (not (1 <= int(port) <= 65535)): \n\t \treturn False \n\tsock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) \n\tout = sock.connect_ex((sanitize_host(host), int(port))) \n\treturn out\n", 
" \tif (not port_dev.namespace): \n\t \ttools.fail('tests \tshould \tnot \tchange \ttest \tmachine \tgateway') \n\tport_dev.route.add_gateway(gateway_ip)\n", 
" \tpass\n", 
" \t(cla, exc) = sys.exc_info()[:2] \n\treturn (cla.__name__, str(exc))\n", 
" \ttf = TimeFormat(value) \n\treturn tf.format(format_string)\n", 
" \treturn str(RawNginxDumper(blocks.spaced))\n", 
" \tresult = collections.defaultdict(list) \n\tfor n in range(taglist.n_tags()): \n\t \ttag = taglist.nth_tag_name(n) \n\t \tfor i in range(taglist.get_tag_size(tag)): \n\t \t \tvalue = taglist.get_value_index(tag, i) \n\t \t \tif isinstance(value, GLib.Date): \n\t \t \t \ttry: \n\t \t \t \t \tdate = datetime.date(value.get_year(), value.get_month(), value.get_day()) \n\t \t \t \t \tresult[tag].append(date.isoformat().decode(u'utf-8')) \n\t \t \t \texcept ValueError: \n\t \t \t \t \tlogger.debug(u'Ignoring \tdodgy \tdate \tvalue: \t%d-%d-%d', value.get_year(), value.get_month(), value.get_day()) \n\t \t \telif isinstance(value, Gst.DateTime): \n\t \t \t \tresult[tag].append(value.to_iso8601_string().decode(u'utf-8')) \n\t \t \telif isinstance(value, bytes): \n\t \t \t \tresult[tag].append(value.decode(u'utf-8', u'replace')) \n\t \t \telif isinstance(value, (compat.text_type, bool, numbers.Number)): \n\t \t \t \tresult[tag].append(value) \n\t \t \telif isinstance(value, Gst.Sample): \n\t \t \t \tdata = _extract_sample_data(value) \n\t \t \t \tif data: \n\t \t \t \t \tresult[tag].append(data) \n\t \t \telse: \n\t \t \t \tlogger.log(log.TRACE_LOG_LEVEL, u'Ignoring \tunknown \ttag \tdata: \t%r \t= \t%r', tag, value) \n\treturn result\n", 
" \tif (value not in settings.NODE_CATEGORY_MAP.keys()): \n\t \traise ValidationValueError('Invalid \tvalue \tfor \tcategory.') \n\treturn True\n", 
" \tconfig = OrderedDict() \n\tfor (key, device) in value.items(): \n\t \tif ('packetid' in device.keys()): \n\t \t \tmsg = (('You \tare \tusing \tan \toutdated \tconfiguration \tof \tthe \trfxtrx \t' + 'device, \t{}.'.format(key)) + ' \tYour \tnew \tconfig \tshould \tbe:\\n \t \t \t \t{}: \t\\n \t \t \t \t \t \t \t \tname: \t{}'.format(device.get('packetid'), device.get(ATTR_NAME, 'deivce_name'))) \n\t \t \t_LOGGER.warning(msg) \n\t \t \tkey = device.get('packetid') \n\t \t \tdevice.pop('packetid') \n\t \tkey = str(key) \n\t \tif (not ((len(key) % 2) == 0)): \n\t \t \tkey = ('0' + key) \n\t \tif (get_rfx_object(key) is None): \n\t \t \traise vol.Invalid('Rfxtrx \tdevice \t{} \tis \tinvalid: \tInvalid \tdevice \tid \tfor \t{}'.format(key, value)) \n\t \tif (device_type == 'sensor'): \n\t \t \tconfig[key] = DEVICE_SCHEMA_SENSOR(device) \n\t \telif (device_type == 'light_switch'): \n\t \t \tconfig[key] = DEVICE_SCHEMA(device) \n\t \telse: \n\t \t \traise vol.Invalid('Rfxtrx \tdevice \tis \tinvalid') \n\t \tif (not config[key][ATTR_NAME]): \n\t \t \tconfig[key][ATTR_NAME] = key \n\treturn config\n", 
" \taddresses = [] \n\tfor interface in netifaces.interfaces(): \n\t \ttry: \n\t \t \tiface_data = netifaces.ifaddresses(interface) \n\t \t \tfor family in iface_data: \n\t \t \t \tif (family not in (netifaces.AF_INET, netifaces.AF_INET6)): \n\t \t \t \t \tcontinue \n\t \t \t \tfor address in iface_data[family]: \n\t \t \t \t \taddr = address['addr'] \n\t \t \t \t \tif (family == netifaces.AF_INET6): \n\t \t \t \t \t \taddr = addr.split('%')[0] \n\t \t \t \t \taddresses.append(addr) \n\t \texcept ValueError: \n\t \t \tpass \n\treturn addresses\n", 
" \tif (len(port_ids) > MAX_PORTS_PER_QUERY): \n\t \tLOG.debug('Number \tof \tports \t%(pcount)s \texceeds \tthe \tmaximum \tper \tquery \t%(maxp)s. \tPartitioning \tqueries.', {'pcount': len(port_ids), 'maxp': MAX_PORTS_PER_QUERY}) \n\t \treturn (get_ports_and_sgs(context, port_ids[:MAX_PORTS_PER_QUERY]) + get_ports_and_sgs(context, port_ids[MAX_PORTS_PER_QUERY:])) \n\tLOG.debug('get_ports_and_sgs() \tcalled \tfor \tport_ids \t%s', port_ids) \n\tif (not port_ids): \n\t \treturn [] \n\tports_to_sg_ids = get_sg_ids_grouped_by_port(context, port_ids) \n\treturn [make_port_dict_with_security_groups(port, sec_groups) for (port, sec_groups) in six.iteritems(ports_to_sg_ids)]\n", 
" \treverse_value_map = {'admin_state': {'down': 'shutdown', 'up': 'no \tshutdown'}} \n\tif vlan.get('admin_state'): \n\t \tvlan = apply_value_map(reverse_value_map, vlan) \n\tVLAN_ARGS = {'name': 'name \t{0}', 'vlan_state': 'state \t{0}', 'admin_state': '{0}', 'mode': 'mode \t{0}', 'mapped_vni': 'vn-segment \t{0}'} \n\tcommands = [] \n\tfor (param, value) in vlan.iteritems(): \n\t \tif ((param == 'mapped_vni') and (value == 'default')): \n\t \t \tcommand = 'no \tvn-segment' \n\t \telse: \n\t \t \tcommand = VLAN_ARGS.get(param).format(vlan.get(param)) \n\t \tif command: \n\t \t \tcommands.append(command) \n\tcommands.insert(0, ('vlan \t' + vid)) \n\tcommands.append('exit') \n\treturn commands\n", 
" \tconn = _auth(profile) \n\treturn conn.delete_network(network)\n", 
" \tif (len(port_ids) > MAX_PORTS_PER_QUERY): \n\t \tLOG.debug('Number \tof \tports \t%(pcount)s \texceeds \tthe \tmaximum \tper \tquery \t%(maxp)s. \tPartitioning \tqueries.', {'pcount': len(port_ids), 'maxp': MAX_PORTS_PER_QUERY}) \n\t \treturn (get_ports_and_sgs(context, port_ids[:MAX_PORTS_PER_QUERY]) + get_ports_and_sgs(context, port_ids[MAX_PORTS_PER_QUERY:])) \n\tLOG.debug('get_ports_and_sgs() \tcalled \tfor \tport_ids \t%s', port_ids) \n\tif (not port_ids): \n\t \treturn [] \n\tports_to_sg_ids = get_sg_ids_grouped_by_port(context, port_ids) \n\treturn [make_port_dict_with_security_groups(port, sec_groups) for (port, sec_groups) in six.iteritems(ports_to_sg_ids)]\n", 
" \tif (not (1 <= int(port) <= 65535)): \n\t \treturn False \n\tsock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) \n\tout = sock.connect_ex((sanitize_host(host), int(port))) \n\treturn out\n", 
" \tsession = (session or get_session()) \n\treturn metadef_resource_type_api.delete(context, resource_type_name, session)\n", 
" \tdef validate_name(name): \n\t \tif (len(name) > db_const.NAME_FIELD_SIZE): \n\t \t \traise n_exc.Invalid((_('Provider \tname \t%(name)s \tis \tlimited \tby \t%(len)s \tcharacters') % {'name': name, 'len': db_const.NAME_FIELD_SIZE})) \n\tneutron_mod = NeutronModule(service_module) \n\tsvc_providers_opt = neutron_mod.service_providers() \n\tLOG.debug('Service \tproviders \t= \t%s', svc_providers_opt) \n\tres = [] \n\tfor prov_def in svc_providers_opt: \n\t \tsplit = prov_def.split(':') \n\t \ttry: \n\t \t \t(svc_type, name, driver) = split[:3] \n\t \texcept ValueError: \n\t \t \traise n_exc.Invalid(_('Invalid \tservice \tprovider \tformat')) \n\t \tvalidate_name(name) \n\t \tname = normalize_provider_name(name) \n\t \tdefault = False \n\t \tif ((len(split) == 4) and split[3]): \n\t \t \tif (split[3] == 'default'): \n\t \t \t \tdefault = True \n\t \t \telse: \n\t \t \t \tmsg = (_(\"Invalid \tprovider \tformat. \tLast \tpart \tshould \tbe \t'default' \tor \tempty: \t%s\") % prov_def) \n\t \t \t \tLOG.error(msg) \n\t \t \t \traise n_exc.Invalid(msg) \n\t \tdriver = get_provider_driver_class(driver) \n\t \tres.append({'service_type': svc_type, 'name': name, 'driver': driver, 'default': default}) \n\treturn res\n", 
" \tif ('id' not in sort_keys): \n\t \tLOG.warning(_LW('Id \tnot \tin \tsort_keys; \tis \tsort_keys \tunique?')) \n\tassert (not (sort_dir and sort_dirs)) \n\tif ((sort_dirs is None) and (sort_dir is None)): \n\t \tsort_dir = 'asc' \n\tif (sort_dirs is None): \n\t \tsort_dirs = [sort_dir for _sort_key in sort_keys] \n\tassert (len(sort_dirs) == len(sort_keys)) \n\tfor (current_sort_key, current_sort_dir) in zip(sort_keys, sort_dirs): \n\t \tsort_dir_func = {'asc': sqlalchemy.asc, 'desc': sqlalchemy.desc}[current_sort_dir] \n\t \ttry: \n\t \t \tsort_key_attr = getattr(model, current_sort_key) \n\t \texcept AttributeError: \n\t \t \traise exception.InvalidInput(reason='Invalid \tsort \tkey') \n\t \tif (not api.is_orm_value(sort_key_attr)): \n\t \t \traise exception.InvalidInput(reason='Invalid \tsort \tkey') \n\t \tquery = query.order_by(sort_dir_func(sort_key_attr)) \n\tif (marker is not None): \n\t \tmarker_values = [] \n\t \tfor sort_key in sort_keys: \n\t \t \tv = getattr(marker, sort_key) \n\t \t \tmarker_values.append(v) \n\t \tcriteria_list = [] \n\t \tfor i in range(0, len(sort_keys)): \n\t \t \tcrit_attrs = [] \n\t \t \tfor j in range(0, i): \n\t \t \t \tmodel_attr = getattr(model, sort_keys[j]) \n\t \t \t \tcrit_attrs.append((model_attr == marker_values[j])) \n\t \t \tmodel_attr = getattr(model, sort_keys[i]) \n\t \t \tif (sort_dirs[i] == 'desc'): \n\t \t \t \tcrit_attrs.append((model_attr < marker_values[i])) \n\t \t \telif (sort_dirs[i] == 'asc'): \n\t \t \t \tcrit_attrs.append((model_attr > marker_values[i])) \n\t \t \telse: \n\t \t \t \traise ValueError(_(\"Unknown \tsort \tdirection, \tmust \tbe \t'desc' \tor \t'asc'\")) \n\t \t \tcriteria = sqlalchemy.sql.and_(*crit_attrs) \n\t \t \tcriteria_list.append(criteria) \n\t \tf = sqlalchemy.sql.or_(*criteria_list) \n\t \tquery = query.filter(f) \n\tif (limit is not None): \n\t \tquery = query.limit(limit) \n\tif offset: \n\t \tquery = query.offset(offset) \n\treturn query\n", 
" \turl = config.get_main_option('sqlalchemy.url') \n\tcontext.configure(url=url, target_metadata=target_metadata, literal_binds=True) \n\twith context.begin_transaction(): \n\t \tcontext.run_migrations()\n", 
" \tconnectable = engine_from_config(config.get_section(config.config_ini_section), prefix='sqlalchemy.', poolclass=pool.NullPool) \n\twith connectable.connect() as connection: \n\t \tcontext.configure(connection=connection, target_metadata=target_metadata) \n\t \twith context.begin_transaction(): \n\t \t \tcontext.run_migrations()\n", 
" \tpass\n", 
" \tpass\n", 
" \tif ('sqlite' not in IMPL.get_engine().name): \n\t \treturn IMPL.dispose_engine() \n\telse: \n\t \treturn\n", 
" \treturn context_manager.get_legacy_facade().get_session(autocommit=autocommit, expire_on_commit=expire_on_commit, use_slave=use_slave)\n", 
" \tdef decorated(func): \n\t \t'\\n \t \t \t \t \t \t \t \tDecorator \tfor \tthe \tcreation \tfunction.\\n \t \t \t \t \t \t \t \t' \n\t \t_WRITE_MODEL[model] = func \n\t \treturn func \n\treturn decorated\n", 
" \tfrom airflow import models \n\tlogging.info(u'Dropping \ttables \tthat \texist') \n\tmodels.Base.metadata.drop_all(settings.engine) \n\tmc = MigrationContext.configure(settings.engine) \n\tif mc._version.exists(settings.engine): \n\t \tmc._version.drop(settings.engine) \n\tinitdb()\n", 
" \tmsg = 'N340: \tUse \tnova.utils.%(spawn)s() \trather \tthan \tgreenthread.%(spawn)s() \tand \teventlet.%(spawn)s()' \n\tif (('nova/utils.py' in filename) or ('nova/tests/' in filename)): \n\t \treturn \n\tmatch = re.match(spawn_re, logical_line) \n\tif match: \n\t \t(yield (0, (msg % {'spawn': match.group('spawn_part')})))\n", 
" \tpipeline = local_conf[cfg.CONF.auth_strategy] \n\tpipeline = pipeline.split() \n\tfilters = [loader.get_filter(n) for n in pipeline[:(-1)]] \n\tapp = loader.get_app(pipeline[(-1)]) \n\tfilters.reverse() \n\tfor filter in filters: \n\t \tapp = filter(app) \n\treturn app\n", 
" \treturn (_service_get(s_name, **connection_args) is not None)\n", 
" \tconfig = OrderedDict() \n\tfor (key, device) in value.items(): \n\t \tif ('packetid' in device.keys()): \n\t \t \tmsg = (('You \tare \tusing \tan \toutdated \tconfiguration \tof \tthe \trfxtrx \t' + 'device, \t{}.'.format(key)) + ' \tYour \tnew \tconfig \tshould \tbe:\\n \t \t \t \t{}: \t\\n \t \t \t \t \t \t \t \tname: \t{}'.format(device.get('packetid'), device.get(ATTR_NAME, 'deivce_name'))) \n\t \t \t_LOGGER.warning(msg) \n\t \t \tkey = device.get('packetid') \n\t \t \tdevice.pop('packetid') \n\t \tkey = str(key) \n\t \tif (not ((len(key) % 2) == 0)): \n\t \t \tkey = ('0' + key) \n\t \tif (get_rfx_object(key) is None): \n\t \t \traise vol.Invalid('Rfxtrx \tdevice \t{} \tis \tinvalid: \tInvalid \tdevice \tid \tfor \t{}'.format(key, value)) \n\t \tif (device_type == 'sensor'): \n\t \t \tconfig[key] = DEVICE_SCHEMA_SENSOR(device) \n\t \telif (device_type == 'light_switch'): \n\t \t \tconfig[key] = DEVICE_SCHEMA(device) \n\t \telse: \n\t \t \traise vol.Invalid('Rfxtrx \tdevice \tis \tinvalid') \n\t \tif (not config[key][ATTR_NAME]): \n\t \t \tconfig[key][ATTR_NAME] = key \n\treturn config\n", 
" \treturn {'id': __opts__.get('id', '')}\n", 
" \ttarget_room = None \n\tif (__opts__.get('__role') == 'master'): \n\t \tfire_master = salt.utils.event.get_master_event(__opts__, __opts__['sock_dir']).fire_event \n\telse: \n\t \tfire_master = None \n\tdef fire(tag, msg): \n\t \t'\\n \t \t \t \t \t \t \t \tfire \tevent \tto \tsalt \tbus\\n \t \t \t \t \t \t \t \t' \n\t \tif fire_master: \n\t \t \tfire_master(msg, tag) \n\t \telse: \n\t \t \t__salt__['event.send'](tag, msg) \n\tdef _eval_bot_mentions(all_messages, trigger): \n\t \t' \tyield \tpartner \tmessage \t' \n\t \tfor message in all_messages: \n\t \t \tmessage_text = message['message'] \n\t \t \tif message_text.startswith(trigger): \n\t \t \t \tfire(tag, message) \n\t \t \t \ttext = message_text.replace(trigger, '').strip() \n\t \t \t \t(yield (message['from']['mention_name'], text)) \n\ttoken = (token or api_key) \n\tif (not token): \n\t \traise UserWarning('Hipchat \ttoken \tnot \tfound') \n\trunner_functions = sorted(salt.runner.Runner(__opts__).functions) \n\tif (not api_url): \n\t \tapi_url = _DEFAULT_API_URL \n\thipc = hypchat.HypChat(token, endpoint=api_url) \n\tif (not hipc): \n\t \traise UserWarning('Unable \tto \tconnect \tto \thipchat') \n\tlog.debug('Connected \tto \tHipchat') \n\trooms_kwargs = {} \n\tif (max_rooms is None): \n\t \tmax_rooms = _DEFAULT_MAX_ROOMS \n\t \trooms_kwargs['max_results'] = max_rooms \n\telif (max_rooms > 0): \n\t \trooms_kwargs['max_results'] = max_rooms \n\tall_rooms = hipc.rooms(**rooms_kwargs)['items'] \n\tfor a_room in all_rooms: \n\t \tif (a_room['name'] == room): \n\t \t \ttarget_room = a_room \n\tif (not target_room): \n\t \tlog.debug('Unable \tto \tconnect \tto \troom \t{0}'.format(room)) \n\t \ttime.sleep(30) \n\t \traise UserWarning('Unable \tto \tconnect \tto \troom \t{0}'.format(room)) \n\tafter_message_id = target_room.latest(maxResults=1)['items'][0]['id'] \n\twhile True: \n\t \ttry: \n\t \t \tnew_messages = target_room.latest(not_before=after_message_id)['items'] \n\t \texcept hypchat.requests.HttpServiceUnavailable: \n\t \t \ttime.sleep(15) \n\t \t \tcontinue \n\t \tafter_message_id = new_messages[(-1)]['id'] \n\t \tfor (partner, text) in _eval_bot_mentions(new_messages[1:], trigger): \n\t \t \tif (not control): \n\t \t \t \tlog.debug('Engine \tnot \tconfigured \tfor \tcontrol') \n\t \t \t \treturn \n\t \t \tif valid_users: \n\t \t \t \tif (partner not in valid_users): \n\t \t \t \t \ttarget_room.message('{0} \tnot \tauthorized \tto \trun \tSalt \tcommands'.format(partner)) \n\t \t \t \t \treturn \n\t \t \targs = [] \n\t \t \tkwargs = {} \n\t \t \tcmdline = salt.utils.shlex_split(text) \n\t \t \tcmd = cmdline[0] \n\t \t \tif (aliases and isinstance(aliases, dict) and (cmd in aliases.keys())): \n\t \t \t \tcmdline = aliases[cmd].get('cmd') \n\t \t \t \tcmdline = salt.utils.shlex_split(cmdline) \n\t \t \t \tcmd = cmdline[0] \n\t \t \tif (len(cmdline) > 1): \n\t \t \t \tfor item in cmdline[1:]: \n\t \t \t \t \tif ('=' in item): \n\t \t \t \t \t \t(key, value) = item.split('=', 1) \n\t \t \t \t \t \tkwargs[key] = value \n\t \t \t \t \telse: \n\t \t \t \t \t \targs.append(item) \n\t \t \tif ('target' not in kwargs): \n\t \t \t \ttarget = '*' \n\t \t \telse: \n\t \t \t \ttarget = kwargs['target'] \n\t \t \t \tdel kwargs['target'] \n\t \t \tif ('tgt_type' not in kwargs): \n\t \t \t \ttgt_type = 'glob' \n\t \t \telse: \n\t \t \t \ttgt_type = kwargs['tgt_type'] \n\t \t \t \tdel kwargs['tgt_type'] \n\t \t \tif valid_commands: \n\t \t \t \tif (cmd not in valid_commands): \n\t \t \t \t \ttarget_room.message('Using \t{0} \tis \tnot \tallowed.'.format(cmd)) \n\t \t \t \t \treturn \n\t \t \tret = {} \n\t \t \tif (cmd in runner_functions): \n\t \t \t \trunner = salt.runner.RunnerClient(__opts__) \n\t \t \t \tret = runner.cmd(cmd, arg=args, kwarg=kwargs) \n\t \t \telse: \n\t \t \t \tlocal = salt.client.LocalClient() \n\t \t \t \tret = local.cmd('{0}'.format(target), cmd, args, kwargs, tgt_type='{0}'.format(tgt_type)) \n\t \t \ttmp_path_fn = salt.utils.files.mkstemp() \n\t \t \twith salt.utils.fopen(tmp_path_fn, 'w+') as fp_: \n\t \t \t \tfp_.write(json.dumps(ret, sort_keys=True, indent=4)) \n\t \t \tmessage_string = '@{0} \tResults \tfor: \t{1} \t{2} \t{3} \ton \t{4}'.format(partner, cmd, args, kwargs, target) \n\t \t \t_publish_file(token, room, tmp_path_fn, message=message_string, api_url=api_url) \n\t \t \tsalt.utils.safe_rm(tmp_path_fn) \n\t \ttime.sleep((wait_time or _DEFAULT_SLEEP))\n", 
" \tif (__opts__['__role'] == 'master'): \n\t \tevent_bus = salt.utils.event.get_master_event(__opts__, __opts__['sock_dir'], listen=True) \n\telse: \n\t \tevent_bus = salt.utils.event.get_event('minion', transport=__opts__['transport'], opts=__opts__, sock_dir=__opts__['sock_dir'], listen=True) \n\t \tlog.debug('test \tengine \tstarted') \n\twhile True: \n\t \tevent = event_bus.get_event() \n\t \tjevent = json.dumps(event) \n\t \tif event: \n\t \t \tlog.debug(jevent)\n", 
" \twhile True: \n\t \ttry: \n\t \t \titem = queue.get(timeout=0.1) \n\t \texcept Empty: \n\t \t \t(yield None) \n\t \t \tcontinue \n\t \texcept thread.error: \n\t \t \traise ShutdownException() \n\t \tif item.exc: \n\t \t \traise item.exc \n\t \tif item.is_stop: \n\t \t \tif cascade_stop: \n\t \t \t \traise StopIteration \n\t \t \telse: \n\t \t \t \tcontinue \n\t \t(yield item.item)\n", 
" \tflatten_result = {} \n\tsources = [] \n\tmeters = [] \n\tmetadata_flattened = {} \n\tfor (k, v) in entry.items(): \n\t \tif k.startswith('f:s_'): \n\t \t \tsources.append(decode_unicode(k[4:])) \n\t \telif k.startswith('f:r_metadata.'): \n\t \t \tqualifier = decode_unicode(k[len('f:r_metadata.'):]) \n\t \t \tmetadata_flattened[qualifier] = load(v) \n\t \telif k.startswith('f:m_'): \n\t \t \tmeter = ([unquote(i) for i in k[4:].split(':')], load(v)) \n\t \t \tmeters.append(meter) \n\t \telse: \n\t \t \tif (':' in k[2:]): \n\t \t \t \tkey = tuple([unquote(i) for i in k[2:].split(':')]) \n\t \t \telse: \n\t \t \t \tkey = unquote(k[2:]) \n\t \t \tflatten_result[key] = load(v) \n\tif get_raw_meta: \n\t \tmetadata = flatten_result.get('resource_metadata', {}) \n\telse: \n\t \tmetadata = metadata_flattened \n\treturn (flatten_result, meters, metadata)\n", 
" \tflatten_result = {} \n\tsources = [] \n\tmeters = [] \n\tmetadata_flattened = {} \n\tfor (k, v) in entry.items(): \n\t \tif k.startswith('f:s_'): \n\t \t \tsources.append(decode_unicode(k[4:])) \n\t \telif k.startswith('f:r_metadata.'): \n\t \t \tqualifier = decode_unicode(k[len('f:r_metadata.'):]) \n\t \t \tmetadata_flattened[qualifier] = load(v) \n\t \telif k.startswith('f:m_'): \n\t \t \tmeter = ([unquote(i) for i in k[4:].split(':')], load(v)) \n\t \t \tmeters.append(meter) \n\t \telse: \n\t \t \tif (':' in k[2:]): \n\t \t \t \tkey = tuple([unquote(i) for i in k[2:].split(':')]) \n\t \t \telse: \n\t \t \t \tkey = unquote(k[2:]) \n\t \t \tflatten_result[key] = load(v) \n\tif get_raw_meta: \n\t \tmetadata = flatten_result.get('resource_metadata', {}) \n\telse: \n\t \tmetadata = metadata_flattened \n\treturn (flatten_result, meters, metadata)\n", 
" \tcolumns = ['f:message', 'f:recorded_at'] \n\tcolumns.extend((('f:%s' % k) for (k, v) in kwargs.items() if (v is not None))) \n\tif metaquery: \n\t \tcolumns.extend((('f:r_%s' % k) for (k, v) in metaquery.items() if (v is not None))) \n\tsource = kwargs.get('source') \n\tif source: \n\t \tcolumns.append(('f:s_%s' % source)) \n\tif need_timestamp: \n\t \tcolumns.extend(['f:rts', 'f:timestamp']) \n\treturn columns\n", 
" \tflatten_result = {} \n\tsources = [] \n\tmeters = [] \n\tmetadata_flattened = {} \n\tfor (k, v) in entry.items(): \n\t \tif k.startswith('f:s_'): \n\t \t \tsources.append(decode_unicode(k[4:])) \n\t \telif k.startswith('f:r_metadata.'): \n\t \t \tqualifier = decode_unicode(k[len('f:r_metadata.'):]) \n\t \t \tmetadata_flattened[qualifier] = load(v) \n\t \telif k.startswith('f:m_'): \n\t \t \tmeter = ([unquote(i) for i in k[4:].split(':')], load(v)) \n\t \t \tmeters.append(meter) \n\t \telse: \n\t \t \tif (':' in k[2:]): \n\t \t \t \tkey = tuple([unquote(i) for i in k[2:].split(':')]) \n\t \t \telse: \n\t \t \t \tkey = unquote(k[2:]) \n\t \t \tflatten_result[key] = load(v) \n\tif get_raw_meta: \n\t \tmetadata = flatten_result.get('resource_metadata', {}) \n\telse: \n\t \tmetadata = metadata_flattened \n\treturn (flatten_result, meters, metadata)\n", 
" \tflatten_result = {} \n\tsources = [] \n\tmeters = [] \n\tmetadata_flattened = {} \n\tfor (k, v) in entry.items(): \n\t \tif k.startswith('f:s_'): \n\t \t \tsources.append(decode_unicode(k[4:])) \n\t \telif k.startswith('f:r_metadata.'): \n\t \t \tqualifier = decode_unicode(k[len('f:r_metadata.'):]) \n\t \t \tmetadata_flattened[qualifier] = load(v) \n\t \telif k.startswith('f:m_'): \n\t \t \tmeter = ([unquote(i) for i in k[4:].split(':')], load(v)) \n\t \t \tmeters.append(meter) \n\t \telse: \n\t \t \tif (':' in k[2:]): \n\t \t \t \tkey = tuple([unquote(i) for i in k[2:].split(':')]) \n\t \t \telse: \n\t \t \t \tkey = unquote(k[2:]) \n\t \t \tflatten_result[key] = load(v) \n\tif get_raw_meta: \n\t \tmetadata = flatten_result.get('resource_metadata', {}) \n\telse: \n\t \tmetadata = metadata_flattened \n\treturn (flatten_result, meters, metadata)\n", 
" \tcontext = req.environ['placement.context'] \n\trcs = objects.ResourceClassList.get_all(context) \n\tresponse = req.response \n\tresponse.body = encodeutils.to_utf8(jsonutils.dumps(_serialize_resource_classes(req.environ, rcs))) \n\tresponse.content_type = 'application/json' \n\treturn response\n", 
" \tfunc_name = '{0}.get_group'.format(__virtualname__) \n\tif (__opts__.get('fun', '') == func_name): \n\t \tlog.info('The \tfunction \t{0} \tshould \tnot \tbe \tused \ton \tWindows \tsystems; \tsee \tfunction \tdocs \tfor \tdetails. \tThe \tvalue \treturned \tis \tthe \tuser \t(owner).'.format(func_name)) \n\treturn get_user(path, follow_symlinks)\n", 
" \tquery = session.query(models.MetadefResourceType) \n\tresource_types = query.all() \n\tresource_types_list = [] \n\tfor rt in resource_types: \n\t \tresource_types_list.append(rt.to_dict()) \n\treturn resource_types_list\n", 
" \tsubproject = get_subproject(request, project, subproject) \n\tunits = Unit.objects.filter(checksum=checksum, translation__subproject=subproject) \n\ttry: \n\t \tsource = units[0].source_info \n\texcept IndexError: \n\t \traise Http404('Non \texisting \tunit!') \n\tcheck_flags = [(CHECKS[x].ignore_string, CHECKS[x].name) for x in CHECKS] \n\textra_flags = [(x, EXTRA_FLAGS[x]) for x in EXTRA_FLAGS] \n\treturn render(request, 'js/detail.html', {'units': units, 'source': source, 'project': subproject.project, 'next': request.GET.get('next', ''), 'priority_form': PriorityForm(initial={'priority': source.priority}), 'check_flags_form': CheckFlagsForm(initial={'flags': source.check_flags}), 'screenshot_form': ScreenshotUploadForm(instance=source), 'extra_flags': extra_flags, 'check_flags': check_flags})\n", 
" \tif (not filename): \n\t \treturn None \n\tif (filename[:8] == 'atlas://'): \n\t \treturn filename \n\tif exists(abspath(filename)): \n\t \treturn abspath(filename) \n\tfor path in reversed(resource_paths): \n\t \toutput = abspath(join(path, filename)) \n\t \tif exists(output): \n\t \t \treturn output \n\tif (filename[:5] == 'data:'): \n\t \treturn filename \n\treturn None\n", 
" \treturn uid_to_user(get_uid(path, follow_symlinks))\n", 
" \treturn [n.strip() for n in codecs.open(os.path.join('data', 'names.txt'), 'rb', 'utf8').readlines()]\n", 
" \treturn sorted([user.pw_name for user in pwd.getpwall()])\n", 
" \treturn sorted([user.pw_name for user in pwd.getpwall()])\n", 
" \tif ('term' in request.GET): \n\t \tterm = request.GET['term'] \n\telse: \n\t \traise Http404 \n\tqueryset = Project.objects.public(request.user).filter(name__icontains=term)[:20] \n\tret_list = [] \n\tfor project in queryset: \n\t \tret_list.append({'label': project.name, 'value': project.slug}) \n\tjson_response = json.dumps(ret_list) \n\treturn HttpResponse(json_response, content_type='text/javascript')\n", 
" \tif (not is_comprehensive_theming_enabled()): \n\t \treturn [] \n\tthemes_dirs = ([Path(themes_dir)] if themes_dir else get_theme_base_dirs()) \n\tthemes = [] \n\tfor themes_dir in themes_dirs: \n\t \tthemes.extend([Theme(name, name, themes_dir) for name in get_theme_dirs(themes_dir)]) \n\treturn themes\n", 
" \tif ('term' in request.GET): \n\t \tterm = request.GET['term'] \n\telse: \n\t \traise Http404 \n\tqueryset = Project.objects.public(request.user).filter(name__icontains=term)[:20] \n\tret_list = [] \n\tfor project in queryset: \n\t \tret_list.append({'label': project.name, 'value': project.slug}) \n\tjson_response = json.dumps(ret_list) \n\treturn HttpResponse(json_response, content_type='text/javascript')\n", 
" \treturn MODELS\n", 
" \tif ('term' in request.GET): \n\t \tterm = request.GET['term'] \n\telse: \n\t \traise Http404 \n\tqueryset = Project.objects.public(request.user).filter(name__icontains=term)[:20] \n\tret_list = [] \n\tfor project in queryset: \n\t \tret_list.append({'label': project.name, 'value': project.slug}) \n\tjson_response = json.dumps(ret_list) \n\treturn HttpResponse(json_response, content_type='text/javascript')\n", 
" \tcontext = req.environ['placement.context'] \n\trcs = objects.ResourceClassList.get_all(context) \n\tresponse = req.response \n\tresponse.body = encodeutils.to_utf8(jsonutils.dumps(_serialize_resource_classes(req.environ, rcs))) \n\tresponse.content_type = 'application/json' \n\treturn response\n", 
" \ti = 0 \n\tparams = {} \n\tfor item in params_str: \n\t \t(param, __, value) = item.partition('=') \n\t \tif value: \n\t \t \tparams[param] = value \n\t \telse: \n\t \t \ti = (i + 1) \n\t \t \tparams[str(i)] = param \n\treturn params\n", 
" \treturn [get_user(email=e) for e in admins]\n", 
" \tnow = time.localtime(t) \n\tntime = (now[0], now[1], now[2], 0, 0, 0, now[6], now[7], now[8]) \n\treturn (time.mktime(ntime) + DAY)\n", 
" \tflatten_result = {} \n\tsources = [] \n\tmeters = [] \n\tmetadata_flattened = {} \n\tfor (k, v) in entry.items(): \n\t \tif k.startswith('f:s_'): \n\t \t \tsources.append(decode_unicode(k[4:])) \n\t \telif k.startswith('f:r_metadata.'): \n\t \t \tqualifier = decode_unicode(k[len('f:r_metadata.'):]) \n\t \t \tmetadata_flattened[qualifier] = load(v) \n\t \telif k.startswith('f:m_'): \n\t \t \tmeter = ([unquote(i) for i in k[4:].split(':')], load(v)) \n\t \t \tmeters.append(meter) \n\t \telse: \n\t \t \tif (':' in k[2:]): \n\t \t \t \tkey = tuple([unquote(i) for i in k[2:].split(':')]) \n\t \t \telse: \n\t \t \t \tkey = unquote(k[2:]) \n\t \t \tflatten_result[key] = load(v) \n\tif get_raw_meta: \n\t \tmetadata = flatten_result.get('resource_metadata', {}) \n\telse: \n\t \tmetadata = metadata_flattened \n\treturn (flatten_result, meters, metadata)\n", 
" \tpmf = Pmf((RandomSum(dists) for i in range(n))) \n\treturn pmf\n", 
" \tflatten_result = {} \n\tsources = [] \n\tmeters = [] \n\tmetadata_flattened = {} \n\tfor (k, v) in entry.items(): \n\t \tif k.startswith('f:s_'): \n\t \t \tsources.append(decode_unicode(k[4:])) \n\t \telif k.startswith('f:r_metadata.'): \n\t \t \tqualifier = decode_unicode(k[len('f:r_metadata.'):]) \n\t \t \tmetadata_flattened[qualifier] = load(v) \n\t \telif k.startswith('f:m_'): \n\t \t \tmeter = ([unquote(i) for i in k[4:].split(':')], load(v)) \n\t \t \tmeters.append(meter) \n\t \telse: \n\t \t \tif (':' in k[2:]): \n\t \t \t \tkey = tuple([unquote(i) for i in k[2:].split(':')]) \n\t \t \telse: \n\t \t \t \tkey = unquote(k[2:]) \n\t \t \tflatten_result[key] = load(v) \n\tif get_raw_meta: \n\t \tmetadata = flatten_result.get('resource_metadata', {}) \n\telse: \n\t \tmetadata = metadata_flattened \n\treturn (flatten_result, meters, metadata)\n", 
" \tflatten_result = {} \n\tsources = [] \n\tmeters = [] \n\tmetadata_flattened = {} \n\tfor (k, v) in entry.items(): \n\t \tif k.startswith('f:s_'): \n\t \t \tsources.append(decode_unicode(k[4:])) \n\t \telif k.startswith('f:r_metadata.'): \n\t \t \tqualifier = decode_unicode(k[len('f:r_metadata.'):]) \n\t \t \tmetadata_flattened[qualifier] = load(v) \n\t \telif k.startswith('f:m_'): \n\t \t \tmeter = ([unquote(i) for i in k[4:].split(':')], load(v)) \n\t \t \tmeters.append(meter) \n\t \telse: \n\t \t \tif (':' in k[2:]): \n\t \t \t \tkey = tuple([unquote(i) for i in k[2:].split(':')]) \n\t \t \telse: \n\t \t \t \tkey = unquote(k[2:]) \n\t \t \tflatten_result[key] = load(v) \n\tif get_raw_meta: \n\t \tmetadata = flatten_result.get('resource_metadata', {}) \n\telse: \n\t \tmetadata = metadata_flattened \n\treturn (flatten_result, meters, metadata)\n", 
" \treturn (os.name == osname)\n", 
" \tif (args is None): \n\t \treturn {} \n\tsearch_offset = int(args.get('search_offset', 0)) \n\tdef _parse_timestamp(timestamp): \n\t \tif (not timestamp): \n\t \t \treturn None \n\t \ttry: \n\t \t \tiso_timestamp = timeutils.parse_isotime(timestamp) \n\t \t \tiso_timestamp = iso_timestamp.replace(tzinfo=None) \n\t \texcept ValueError: \n\t \t \traise wsme.exc.InvalidInput('timestamp', timestamp, 'invalid \ttimestamp \tformat') \n\t \treturn iso_timestamp \n\tstart_timestamp = _parse_timestamp(args.get('start_timestamp')) \n\tend_timestamp = _parse_timestamp(args.get('end_timestamp')) \n\tstart_timestamp = ((start_timestamp - datetime.timedelta(minutes=search_offset)) if start_timestamp else None) \n\tend_timestamp = ((end_timestamp + datetime.timedelta(minutes=search_offset)) if end_timestamp else None) \n\treturn {'start_timestamp': start_timestamp, 'end_timestamp': end_timestamp, 'start_timestamp_op': args.get('start_timestamp_op'), 'end_timestamp_op': args.get('end_timestamp_op')}\n", 
" \tif metadata: \n\t \treturn dict(((k.replace('.', ':').replace(':', '.', 1), six.text_type(v)) for (k, v) in utils.recursive_keypairs(metadata, separator='.') if (type(v) is not set))) \n\treturn {}\n", 
" \tfor cloud in OPENSTACK_CLOUDS: \n\t \ttry: \n\t \t \tcloud_config = os_client_config.get_config(cloud=cloud, identity_api_version=version) \n\t \t \treturn cloud_config.get_legacy_client(service_key=IDENTITY_CLIENT, constructor=client.Client) \n\t \texcept os_client_config.exceptions.OpenStackConfigException: \n\t \t \tpass \n\traise Exception('Could \tnot \tfind \tany \tcloud \tdefinition \tfor \tclouds \tnamed \tfunctional_admin \tor \tdevstack-admin. \tCheck \tyour \tclouds.yaml \tfile \tor \tyour \tenvvars \tand \ttry \tagain.')\n", 
" \tif (acl_type == 'moz_contact'): \n\t \ttry: \n\t \t \treturn (user.email in obj.addon.get_mozilla_contacts()) \n\t \texcept AttributeError: \n\t \t \ttry: \n\t \t \t \treturn (user.email in obj.thread.addon.get_mozilla_contacts()) \n\t \t \texcept AttributeError: \n\t \t \t \treturn False \n\telif (acl_type == 'admin'): \n\t \treturn acl.action_allowed_user(user, 'Admin', '%') \n\telif (acl_type == 'reviewer'): \n\t \treturn (acl.action_allowed_user(user, 'Apps', 'Review') or acl.action_allowed_user(user, 'ContentTools', 'AddonReview')) \n\telif (acl_type == 'senior_reviewer'): \n\t \treturn acl.action_allowed_user(user, 'Apps', 'ReviewEscalated') \n\telse: \n\t \traise Exception('Invalid \tACL \tlookup.') \n\treturn False\n", 
" \treturn get_limited_to(headers)[1]\n", 
" \tsys.stdout.flush() \n\tsys.stderr.flush() \n\tclose_fds = (sys.platform != 'win32') \n\tshell = isinstance(cmd, str) \n\texecutable = None \n\tif (shell and (os.name == 'posix') and ('SHELL' in os.environ)): \n\t \texecutable = os.environ['SHELL'] \n\tp = subprocess.Popen(cmd, shell=shell, executable=executable, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=stderr, close_fds=close_fds) \n\ttry: \n\t \tout = callback(p) \n\texcept KeyboardInterrupt: \n\t \tprint '^C' \n\t \tsys.stdout.flush() \n\t \tsys.stderr.flush() \n\t \tout = None \n\tfinally: \n\t \tif (p.returncode is None): \n\t \t \ttry: \n\t \t \t \tp.terminate() \n\t \t \t \tp.poll() \n\t \t \texcept OSError: \n\t \t \t \tpass \n\t \tif (p.returncode is None): \n\t \t \ttry: \n\t \t \t \tp.kill() \n\t \t \texcept OSError: \n\t \t \t \tpass \n\treturn out\n", 
" \terrback = (errback or _ensure_errback) \n\twith pool.acquire(block=True) as conn: \n\t \tconn.ensure_connection(errback=errback) \n\t \tchannel = conn.default_channel \n\t \trevive = partial(revive_connection, conn, on_revive=on_revive) \n\t \tinsured = conn.autoretry(fun, channel, errback=errback, on_revive=revive, **opts) \n\t \t(retval, _) = insured(*args, **dict(kwargs, connection=conn)) \n\t \treturn retval\n", 
" \ttry: \n\t \tuuid.UUID(value) \n\t \treturn value \n\texcept (ValueError, AttributeError): \n\t \treturn int(value)\n", 
" \tusers = get_group_members(group_name=group_name, region=region, key=key, keyid=keyid, profile=profile) \n\tif users: \n\t \tfor _user in users: \n\t \t \tif (user_name == _user['user_name']): \n\t \t \t \tmsg = 'Username \t: \t{0} \tis \talready \tin \tgroup \t{1}.' \n\t \t \t \tlog.info(msg.format(user_name, group_name)) \n\t \t \t \treturn True \n\treturn False\n", 
" \treturn getattr(settings, 'CMS_CACHE_DURATIONS', {'menus': (60 * 60), 'content': 60, 'permissions': (60 * 60)})\n", 
" \t(orig_exc_type, orig_exc_value, orig_exc_traceback) = sys.exc_info() \n\tif isinstance(new_exc, six.string_types): \n\t \tnew_exc = orig_exc_type(new_exc) \n\tif hasattr(new_exc, 'args'): \n\t \tif (len(new_exc.args) > 0): \n\t \t \tnew_message = ', \t'.join((str(arg) for arg in new_exc.args)) \n\t \telse: \n\t \t \tnew_message = '' \n\t \tnew_message += ('\\n\\nOriginal \texception:\\n DCTB ' + orig_exc_type.__name__) \n\t \tif (hasattr(orig_exc_value, 'args') and (len(orig_exc_value.args) > 0)): \n\t \t \tif getattr(orig_exc_value, 'reraised', False): \n\t \t \t \tnew_message += (': \t' + str(orig_exc_value.args[0])) \n\t \t \telse: \n\t \t \t \tnew_message += (': \t' + ', \t'.join((str(arg) for arg in orig_exc_value.args))) \n\t \tnew_exc.args = ((new_message,) + new_exc.args[1:]) \n\tnew_exc.__cause__ = orig_exc_value \n\tnew_exc.reraised = True \n\tsix.reraise(type(new_exc), new_exc, orig_exc_traceback)\n", 
" \tglobal _translations \n\tif (language not in _translations): \n\t \t_translations[language] = DjangoTranslation(language) \n\treturn _translations[language]\n", 
" \treturn _TranslationProxy(mygettext, string)\n", 
" \tret = _ConvertToList(arg) \n\tfor element in ret: \n\t \tif (not isinstance(element, element_type)): \n\t \t \traise TypeError(('%s \tshould \tbe \tsingle \telement \tor \tlist \tof \ttype \t%s' % (arg_name, element_type))) \n\treturn ret\n", 
" \tlogger = logging.getLogger() \n\tloglevel = get_loglevel((loglevel or u'ERROR')) \n\tlogfile = (logfile if logfile else sys.__stderr__) \n\tif (not logger.handlers): \n\t \tif hasattr(logfile, u'write'): \n\t \t \thandler = logging.StreamHandler(logfile) \n\t \telse: \n\t \t \thandler = WatchedFileHandler(logfile) \n\t \tlogger.addHandler(handler) \n\t \tlogger.setLevel(loglevel) \n\treturn logger\n", 
" \tdef decorator(fx): \n\t \tfx.__cached = None \n\t \tfx.__cached_at = 0 \n\t \tdef wrapper(*args, **kwargs): \n\t \t \tdt = (time.time() - fx.__cached_at) \n\t \t \tif (((dt > duration) and (duration is not None)) or ((fx.__cached_at == 0) and (duration is None))): \n\t \t \t \tval = fx(*args, **kwargs) \n\t \t \t \tfx.__cached = val \n\t \t \t \tfx.__cached_at = time.time() \n\t \t \telse: \n\t \t \t \tval = fx.__cached \n\t \t \treturn val \n\t \twrapper.__doc__ = fx.__doc__ \n\t \treturn wrapper \n\treturn decorator\n", 
" \treturn datetime.datetime.strptime(d['isostr'], _DATETIME_FORMAT)\n", 
" \thost = node \n\tport = 27017 \n\tidx = node.rfind(':') \n\tif (idx != (-1)): \n\t \t(host, port) = (node[:idx], int(node[(idx + 1):])) \n\tif host.startswith('['): \n\t \thost = host[1:(-1)] \n\treturn (host, port)\n", 
" \ttry: \n\t \ttree_gen = parse(file, format, **kwargs) \n\t \ttree = next(tree_gen) \n\texcept StopIteration: \n\t \traise ValueError('There \tare \tno \ttrees \tin \tthis \tfile.') \n\ttry: \n\t \tnext(tree_gen) \n\texcept StopIteration: \n\t \treturn tree \n\telse: \n\t \traise ValueError('There \tare \tmultiple \ttrees \tin \tthis \tfile; \tuse \tparse() \tinstead.')\n", 
" \tglobal _translations \n\tt = _translations.get(language, None) \n\tif (t is not None): \n\t \treturn t \n\tfrom google.appengine._internal.django.conf import settings \n\tglobalpath = os.path.join(os.path.dirname(sys.modules[settings.__module__].__file__), 'locale') \n\tif (settings.SETTINGS_MODULE is not None): \n\t \tparts = settings.SETTINGS_MODULE.split('.') \n\t \tproject = import_module(parts[0]) \n\t \tprojectpath = os.path.join(os.path.dirname(project.__file__), 'locale') \n\telse: \n\t \tprojectpath = None \n\tdef _fetch(lang, fallback=None): \n\t \tglobal _translations \n\t \tloc = to_locale(lang) \n\t \tres = _translations.get(lang, None) \n\t \tif (res is not None): \n\t \t \treturn res \n\t \tdef _translation(path): \n\t \t \ttry: \n\t \t \t \tt = gettext_module.translation('django', path, [loc], DjangoTranslation) \n\t \t \t \tt.set_language(lang) \n\t \t \t \treturn t \n\t \t \texcept IOError as e: \n\t \t \t \treturn None \n\t \tres = _translation(globalpath) \n\t \tbase_lang = (lambda x: x.split('-', 1)[0]) \n\t \tif (base_lang(lang) in [base_lang(trans) for trans in _translations]): \n\t \t \tres._info = res._info.copy() \n\t \t \tres._catalog = res._catalog.copy() \n\t \tdef _merge(path): \n\t \t \tt = _translation(path) \n\t \t \tif (t is not None): \n\t \t \t \tif (res is None): \n\t \t \t \t \treturn t \n\t \t \t \telse: \n\t \t \t \t \tres.merge(t) \n\t \t \treturn res \n\t \tfor localepath in settings.LOCALE_PATHS: \n\t \t \tif os.path.isdir(localepath): \n\t \t \t \tres = _merge(localepath) \n\t \tfor appname in settings.INSTALLED_APPS: \n\t \t \tapp = import_module(appname) \n\t \t \tapppath = os.path.join(os.path.dirname(app.__file__), 'locale') \n\t \t \tif os.path.isdir(apppath): \n\t \t \t \tres = _merge(apppath) \n\t \tif (projectpath and os.path.isdir(projectpath)): \n\t \t \tres = _merge(projectpath) \n\t \tif (res is None): \n\t \t \tif (fallback is not None): \n\t \t \t \tres = fallback \n\t \t \telse: \n\t \t \t \treturn gettext_module.NullTranslations() \n\t \t_translations[lang] = res \n\t \treturn res \n\tdefault_translation = _fetch(settings.LANGUAGE_CODE) \n\tcurrent_translation = _fetch(language, fallback=default_translation) \n\treturn current_translation\n", 
" \tret = {} \n\tcmd = __execute_kadmin('list_policies') \n\tif ((cmd['retcode'] != 0) or cmd['stderr']): \n\t \tret['comment'] = cmd['stderr'].splitlines()[(-1)] \n\t \tret['result'] = False \n\t \treturn ret \n\tret = {'policies': []} \n\tfor i in cmd['stdout'].splitlines()[1:]: \n\t \tret['policies'].append(i) \n\treturn ret\n", 
" \tname = ('_' + func.__name__) \n\tdef wrapper(self): \n\t \ttry: \n\t \t \treturn getattr(self, name) \n\t \texcept AttributeError: \n\t \t \tresult = func(self) \n\t \t \tif inspect.isgenerator(result): \n\t \t \t \tresult = list(result) \n\t \t \tsetattr(self, name, result) \n\t \t \treturn result \n\treturn wrapper\n", 
" \tclass DoctypeSafeCallbackTarget(ElementTree.TreeBuilder, ): \n\t \tdef doctype(*args): \n\t \t \tpass \n\ttree = ElementTree.ElementTree() \n\ttry: \n\t \troot = tree.parse(fname, parser=ElementTree.XMLParser(target=DoctypeSafeCallbackTarget())) \n\texcept ParseError: \n\t \tlog.exception('Error \tparsing \tfile \t%s', fname) \n\t \traise \n\tElementInclude.include(root) \n\treturn tree\n", 
" \ttb = treebuilders.getTreeBuilder(treebuilder) \n\tp = HTMLParser(tb, namespaceHTMLElements=namespaceHTMLElements) \n\treturn p.parse(doc, encoding=encoding)\n", 
" \tfrom config import get_os, get_checks_places, get_valid_check_class \n\tosname = get_os() \n\tchecks_places = get_checks_places(osname, agentConfig) \n\tfor check_path_builder in checks_places: \n\t \tcheck_path = check_path_builder(check_name) \n\t \tif (not os.path.exists(check_path)): \n\t \t \tcontinue \n\t \t(check_is_valid, check_class, load_failure) = get_valid_check_class(check_name, check_path) \n\t \tif check_is_valid: \n\t \t \treturn check_class \n\tlog.warning(('Failed \tto \tload \tthe \tcheck \tclass \tfor \t%s.' % check_name)) \n\treturn None\n", 
" \tmsg = 'N340: \tUse \tnova.utils.%(spawn)s() \trather \tthan \tgreenthread.%(spawn)s() \tand \teventlet.%(spawn)s()' \n\tif (('nova/utils.py' in filename) or ('nova/tests/' in filename)): \n\t \treturn \n\tmatch = re.match(spawn_re, logical_line) \n\tif match: \n\t \t(yield (0, (msg % {'spawn': match.group('spawn_part')})))\n", 
" \tdef wrapper(func): \n\t \t@functools.wraps(func) \n\t \tdef inner(*args, **kwds): \n\t \t \tlock.acquire() \n\t \t \ttry: \n\t \t \t \treturn func(*args, **kwds) \n\t \t \tfinally: \n\t \t \t \tlock.release() \n\t \treturn inner \n\treturn wrapper\n", 
" \tlock = kwds.pop('lock', None) \n\tif kwds: \n\t \traise ValueError(('unrecognized \tkeyword \targument(s): \t%s' % kwds.keys())) \n\tobj = RawValue(typecode_or_type, *args) \n\tif (lock is False): \n\t \treturn obj \n\tif (lock in (True, None)): \n\t \tlock = RLock() \n\tif (not hasattr(lock, 'acquire')): \n\t \traise AttributeError((\"'%r' \thas \tno \tmethod \t'acquire'\" % lock)) \n\treturn synchronized(obj, lock)\n", 
" \tkey = key.replace('-', '_') \n\tif ((key.upper() == 'ALL') and (val.upper() == 'DEFAULT')): \n\t \tfor ci in config: \n\t \t \tconfig[ci].value = config[ci].default \n\t \tconfig.save() \n\t \tmessage = 'Default \tconfiguration \treinstated' \n\telif (not (key.upper() in config)): \n\t \tmessage = ('Unknown \tconfig \titem: \t%s%s%s' % (c.r, key, c.w)) \n\telif (val.upper() == 'DEFAULT'): \n\t \tatt = config[key.upper()] \n\t \tatt.value = att.default \n\t \tmessage = '%s%s%s \tset \tto \t%s%s%s \t(default)' \n\t \tdispval = (att.display or 'None') \n\t \tmessage = (message % (c.y, key, c.w, c.y, dispval, c.w)) \n\t \tconfig.save() \n\telse: \n\t \tmessage = config[key.upper()].set(val) \n\tshowconfig() \n\tg.message = message\n", 
" \tsession = Session.object_session(series) \n\treleases = session.query(Episode).join(Episode.releases, Episode.series).filter((Series.id == series.id)) \n\tif downloaded: \n\t \treleases = releases.filter((Release.downloaded == True)) \n\tif (season is not None): \n\t \treleases = releases.filter((Episode.season == season)) \n\tif (series.identified_by and (series.identified_by != u'auto')): \n\t \treleases = releases.filter((Episode.identified_by == series.identified_by)) \n\tif (series.identified_by in [u'ep', u'sequence']): \n\t \tlatest_release = releases.order_by(desc(Episode.season), desc(Episode.number)).first() \n\telif (series.identified_by == u'date'): \n\t \tlatest_release = releases.order_by(desc(Episode.identifier)).first() \n\telse: \n\t \tlatest_release = releases.order_by(desc(Episode.first_seen.label(u'ep_first_seen'))).first() \n\tif (not latest_release): \n\t \tlog.debug(u'get_latest_release \treturning \tNone, \tno \tdownloaded \tepisodes \tfound \tfor: \t%s', series.name) \n\t \treturn \n\treturn latest_release\n", 
" \tpackages = {'pack_1': {'input_select': {'ib1': None}}} \n\tconfig = {config_util.CONF_CORE: {config_util.CONF_PACKAGES: packages}, 'input_select': {'ib1': None}} \n\tconfig_util.merge_packages_config(config, packages) \n\tassert (merge_log_err.call_count == 1) \n\tassert (len(config) == 2) \n\tassert (len(config['input_select']) == 1)\n", 
" \traise exception\n", 
" \tif hasattr(obj, 'bind'): \n\t \tconn = obj.bind \n\telse: \n\t \ttry: \n\t \t \tconn = object_session(obj).bind \n\t \texcept UnmappedInstanceError: \n\t \t \tconn = obj \n\tif (not hasattr(conn, 'execute')): \n\t \traise TypeError('This \tmethod \taccepts \tonly \tSession, \tEngine, \tConnection \tand \tdeclarative \tmodel \tobjects.') \n\treturn conn\n", 
" \tqueue_dir = __opts__['sqlite_queue_dir'] \n\tdb = os.path.join(queue_dir, '{0}.db'.format(queue)) \n\tlog.debug('Connecting \tto: \t \t{0}'.format(db)) \n\tcon = lite.connect(db) \n\ttables = _list_tables(con) \n\tif (queue not in tables): \n\t \t_create_table(con, queue) \n\treturn con\n", 
" \tqueue_dir = __opts__['sqlite_queue_dir'] \n\tdb = os.path.join(queue_dir, '{0}.db'.format(queue)) \n\tlog.debug('Connecting \tto: \t \t{0}'.format(db)) \n\tcon = lite.connect(db) \n\ttables = _list_tables(con) \n\tif (queue not in tables): \n\t \t_create_table(con, queue) \n\treturn con\n", 
" \tmsg = 'N340: \tUse \tnova.utils.%(spawn)s() \trather \tthan \tgreenthread.%(spawn)s() \tand \teventlet.%(spawn)s()' \n\tif (('nova/utils.py' in filename) or ('nova/tests/' in filename)): \n\t \treturn \n\tmatch = re.match(spawn_re, logical_line) \n\tif match: \n\t \t(yield (0, (msg % {'spawn': match.group('spawn_part')})))\n", 
" \tpid = get_pid_from_file(program_name, pid_files_dir) \n\tif (pid is None): \n\t \treturn False \n\treturn pid_is_alive(pid)\n", 
" \treturn (file_path in _db_content.get('files'))\n", 
" \tstrategy = kwargs.pop('strategy', default_strategy) \n\tstrategy = strategies.strategies[strategy] \n\treturn strategy.create(*args, **kwargs)\n", 
" \tif hasattr(obj, 'bind'): \n\t \tconn = obj.bind \n\telse: \n\t \ttry: \n\t \t \tconn = object_session(obj).bind \n\t \texcept UnmappedInstanceError: \n\t \t \tconn = obj \n\tif (not hasattr(conn, 'execute')): \n\t \traise TypeError('This \tmethod \taccepts \tonly \tSession, \tEngine, \tConnection \tand \tdeclarative \tmodel \tobjects.') \n\treturn conn\n", 
" \treturn ''.join(traceback.format_stack())\n", 
" \tdb_read_url_parts = cli.parse_db_config('ckan.datastore.write_url') \n\tdb_ckan_url_parts = cli.parse_db_config('sqlalchemy.url') \n\tsame_db = (db_read_url_parts['db_name'] == db_ckan_url_parts['db_name']) \n\tif same_db: \n\t \tmodel.repo.tables_created_and_initialised = False \n\tclear_db(Session) \n\tmodel.repo.rebuild_db()\n", 
" \tif ('id' not in sort_keys): \n\t \tLOG.warning(_LW('Id \tnot \tin \tsort_keys; \tis \tsort_keys \tunique?')) \n\tassert (not (sort_dir and sort_dirs)) \n\tif ((sort_dirs is None) and (sort_dir is None)): \n\t \tsort_dir = 'asc' \n\tif (sort_dirs is None): \n\t \tsort_dirs = [sort_dir for _sort_key in sort_keys] \n\tassert (len(sort_dirs) == len(sort_keys)) \n\tfor (current_sort_key, current_sort_dir) in zip(sort_keys, sort_dirs): \n\t \tsort_dir_func = {'asc': sqlalchemy.asc, 'desc': sqlalchemy.desc}[current_sort_dir] \n\t \ttry: \n\t \t \tsort_key_attr = getattr(model, current_sort_key) \n\t \texcept AttributeError: \n\t \t \traise exception.InvalidInput(reason='Invalid \tsort \tkey') \n\t \tif (not api.is_orm_value(sort_key_attr)): \n\t \t \traise exception.InvalidInput(reason='Invalid \tsort \tkey') \n\t \tquery = query.order_by(sort_dir_func(sort_key_attr)) \n\tif (marker is not None): \n\t \tmarker_values = [] \n\t \tfor sort_key in sort_keys: \n\t \t \tv = getattr(marker, sort_key) \n\t \t \tmarker_values.append(v) \n\t \tcriteria_list = [] \n\t \tfor i in range(0, len(sort_keys)): \n\t \t \tcrit_attrs = [] \n\t \t \tfor j in range(0, i): \n\t \t \t \tmodel_attr = getattr(model, sort_keys[j]) \n\t \t \t \tcrit_attrs.append((model_attr == marker_values[j])) \n\t \t \tmodel_attr = getattr(model, sort_keys[i]) \n\t \t \tif (sort_dirs[i] == 'desc'): \n\t \t \t \tcrit_attrs.append((model_attr < marker_values[i])) \n\t \t \telif (sort_dirs[i] == 'asc'): \n\t \t \t \tcrit_attrs.append((model_attr > marker_values[i])) \n\t \t \telse: \n\t \t \t \traise ValueError(_(\"Unknown \tsort \tdirection, \tmust \tbe \t'desc' \tor \t'asc'\")) \n\t \t \tcriteria = sqlalchemy.sql.and_(*crit_attrs) \n\t \t \tcriteria_list.append(criteria) \n\t \tf = sqlalchemy.sql.or_(*criteria_list) \n\t \tquery = query.filter(f) \n\tif (limit is not None): \n\t \tquery = query.limit(limit) \n\tif offset: \n\t \tquery = query.offset(offset) \n\treturn query\n", 
" \treturn (0 if (not run_horcmstart_returns_error2) else 3)\n", 
" \tlevel = notification.get_default_level() \n\tif (stack.status == stack.IN_PROGRESS): \n\t \tsuffix = 'start' \n\telif (stack.status == stack.COMPLETE): \n\t \tsuffix = 'end' \n\telse: \n\t \tsuffix = 'error' \n\t \tlevel = notification.ERROR \n\tevent_type = ('%s.%s.%s' % ('stack', stack.action.lower(), suffix)) \n\tnotification.notify(stack.context, event_type, level, engine_api.format_notification_body(stack))\n", 
" \tlevel = notification.get_default_level() \n\tif (stack.status == stack.IN_PROGRESS): \n\t \tsuffix = 'start' \n\telif (stack.status == stack.COMPLETE): \n\t \tsuffix = 'end' \n\telse: \n\t \tsuffix = 'error' \n\t \tlevel = notification.ERROR \n\tevent_type = ('%s.%s.%s' % ('stack', stack.action.lower(), suffix)) \n\tnotification.notify(stack.context, event_type, level, engine_api.format_notification_body(stack))\n", 
" \tglobal _notifier \n\tif (_notifier is None): \n\t \thost = (CONF.default_publisher_id or socket.gethostname()) \n\t \ttry: \n\t \t \ttransport = oslo_messaging.get_notification_transport(CONF) \n\t \t \t_notifier = oslo_messaging.Notifier(transport, ('identity.%s' % host)) \n\t \texcept Exception: \n\t \t \tLOG.exception(_LE('Failed \tto \tconstruct \tnotifier')) \n\t \t \t_notifier = False \n\treturn _notifier\n", 
" \tseed = (pseed or config.unittests.rseed) \n\tif (seed == 'random'): \n\t \tseed = None \n\ttry: \n\t \tif seed: \n\t \t \tseed = int(seed) \n\t \telse: \n\t \t \tseed = None \n\texcept ValueError: \n\t \tprint('Error: \tconfig.unittests.rseed \tcontains \tinvalid \tseed, \tusing \tNone \tinstead', file=sys.stderr) \n\t \tseed = None \n\treturn seed\n", 
" \treturn auth_is_loggedin_user()\n", 
" \tif ((r.representation == 'html') and (r.name == 'shelter') and r.id and (not r.component)): \n\t \tT = current.T \n\t \tmsg = current.msg \n\t \ts3db = current.s3db \n\t \trecord = r.record \n\t \tctable = s3db.pr_contact \n\t \tstable = s3db.cr_shelter \n\t \tmessage = '' \n\t \ttext = '' \n\t \ts_id = record.id \n\t \ts_name = record.name \n\t \ts_phone = record.phone \n\t \ts_email = record.email \n\t \ts_status = record.status \n\t \tif (s_phone in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_email in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_status in ('', None)): \n\t \t \ts_status = T('Not \tDefined') \n\t \telif (s_status == 1): \n\t \t \ts_status = 'Open' \n\t \telif (s_status == 2): \n\t \t \ts_status = 'Close' \n\t \telse: \n\t \t \ts_status = 'Unassigned \tShelter \tStatus' \n\t \ttext += '************************************************' \n\t \ttext += ('\\n%s \t' % T('Automatic \tMessage')) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Shelter \tID'), s_id)) \n\t \ttext += (' \t%s: \t%s' % (T('Shelter \tname'), s_name)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Email'), s_email)) \n\t \ttext += (' \t%s: \t%s' % (T('Phone'), s_phone)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Working \tStatus'), s_status)) \n\t \ttext += '\\n************************************************\\n' \n\t \turl = URL(c='cr', f='shelter', args=r.id) \n\t \topts = dict(type='SMS', subject=T('Deployment \tRequest'), message=(message + text), url=url) \n\t \toutput = msg.compose(**opts) \n\t \tif attr.get('rheader'): \n\t \t \trheader = attr['rheader'](r) \n\t \t \tif rheader: \n\t \t \t \toutput['rheader'] = rheader \n\t \toutput['title'] = T('Send \tNotification') \n\t \tcurrent.response.view = 'msg/compose.html' \n\t \treturn output \n\telse: \n\t \traise HTTP(501, current.messages.BADMETHOD)\n", 
" \tcan_users_receive_email = email_manager.can_users_receive_thread_email(recipient_list, exploration_id, has_suggestion) \n\tfor (index, recipient_id) in enumerate(recipient_list): \n\t \tif can_users_receive_email[index]: \n\t \t \ttransaction_services.run_in_transaction(_enqueue_feedback_thread_status_change_email_task, recipient_id, feedback_message_reference, old_status, new_status)\n", 
" \tglobal _notifier \n\tif (_notifier is None): \n\t \thost = (CONF.default_publisher_id or socket.gethostname()) \n\t \ttry: \n\t \t \ttransport = oslo_messaging.get_notification_transport(CONF) \n\t \t \t_notifier = oslo_messaging.Notifier(transport, ('identity.%s' % host)) \n\t \texcept Exception: \n\t \t \tLOG.exception(_LE('Failed \tto \tconstruct \tnotifier')) \n\t \t \t_notifier = False \n\treturn _notifier\n", 
" \tcan_users_receive_email = email_manager.can_users_receive_thread_email(recipient_list, exploration_id, has_suggestion) \n\tfor (index, recipient_id) in enumerate(recipient_list): \n\t \tif can_users_receive_email[index]: \n\t \t \ttransaction_services.run_in_transaction(_enqueue_feedback_thread_status_change_email_task, recipient_id, feedback_message_reference, old_status, new_status)\n", 
" \tdef wrapped_func(*args, **kwarg): \n\t \tCALLED_FUNCTION.append(name) \n\t \treturn function(*args, **kwarg) \n\treturn wrapped_func\n", 
" \tlevel = notification.get_default_level() \n\tif (stack.status == stack.IN_PROGRESS): \n\t \tsuffix = 'start' \n\telif (stack.status == stack.COMPLETE): \n\t \tsuffix = 'end' \n\telse: \n\t \tsuffix = 'error' \n\t \tlevel = notification.ERROR \n\tevent_type = ('%s.%s.%s' % ('stack', stack.action.lower(), suffix)) \n\tnotification.notify(stack.context, event_type, level, engine_api.format_notification_body(stack))\n", 
" \ttry: \n\t \tdriver = _DRIVERS[driver_name] \n\texcept KeyError: \n\t \traise DriverNotFoundError(('No \tdriver \tfor \t%s' % driver_name)) \n\treturn driver(*args, **kwargs)\n", 
" \tlevel = notification.get_default_level() \n\tif (stack.status == stack.IN_PROGRESS): \n\t \tsuffix = 'start' \n\telif (stack.status == stack.COMPLETE): \n\t \tsuffix = 'end' \n\telse: \n\t \tsuffix = 'error' \n\t \tlevel = notification.ERROR \n\tevent_type = ('%s.%s.%s' % ('stack', stack.action.lower(), suffix)) \n\tnotification.notify(stack.context, event_type, level, engine_api.format_notification_body(stack))\n", 
" \tseed = (pseed or config.unittests.rseed) \n\tif (seed == 'random'): \n\t \tseed = None \n\ttry: \n\t \tif seed: \n\t \t \tseed = int(seed) \n\t \telse: \n\t \t \tseed = None \n\texcept ValueError: \n\t \tprint('Error: \tconfig.unittests.rseed \tcontains \tinvalid \tseed, \tusing \tNone \tinstead', file=sys.stderr) \n\t \tseed = None \n\treturn seed\n", 
" \tif ((r.representation == 'html') and (r.name == 'shelter') and r.id and (not r.component)): \n\t \tT = current.T \n\t \tmsg = current.msg \n\t \ts3db = current.s3db \n\t \trecord = r.record \n\t \tctable = s3db.pr_contact \n\t \tstable = s3db.cr_shelter \n\t \tmessage = '' \n\t \ttext = '' \n\t \ts_id = record.id \n\t \ts_name = record.name \n\t \ts_phone = record.phone \n\t \ts_email = record.email \n\t \ts_status = record.status \n\t \tif (s_phone in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_email in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_status in ('', None)): \n\t \t \ts_status = T('Not \tDefined') \n\t \telif (s_status == 1): \n\t \t \ts_status = 'Open' \n\t \telif (s_status == 2): \n\t \t \ts_status = 'Close' \n\t \telse: \n\t \t \ts_status = 'Unassigned \tShelter \tStatus' \n\t \ttext += '************************************************' \n\t \ttext += ('\\n%s \t' % T('Automatic \tMessage')) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Shelter \tID'), s_id)) \n\t \ttext += (' \t%s: \t%s' % (T('Shelter \tname'), s_name)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Email'), s_email)) \n\t \ttext += (' \t%s: \t%s' % (T('Phone'), s_phone)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Working \tStatus'), s_status)) \n\t \ttext += '\\n************************************************\\n' \n\t \turl = URL(c='cr', f='shelter', args=r.id) \n\t \topts = dict(type='SMS', subject=T('Deployment \tRequest'), message=(message + text), url=url) \n\t \toutput = msg.compose(**opts) \n\t \tif attr.get('rheader'): \n\t \t \trheader = attr['rheader'](r) \n\t \t \tif rheader: \n\t \t \t \toutput['rheader'] = rheader \n\t \toutput['title'] = T('Send \tNotification') \n\t \tcurrent.response.view = 'msg/compose.html' \n\t \treturn output \n\telse: \n\t \traise HTTP(501, current.messages.BADMETHOD)\n", 
" \ttrunc = 20 \n\targspec = inspect.getargspec(fobj) \n\targ_list = [] \n\tif argspec.args: \n\t \tfor arg in argspec.args: \n\t \t \targ_list.append(str(arg)) \n\targ_list.reverse() \n\tif argspec.defaults: \n\t \tfor i in range(len(argspec.defaults)): \n\t \t \targ_list[i] = ((str(arg_list[i]) + '=') + str(argspec.defaults[(- i)])) \n\targ_list.reverse() \n\tif argspec.varargs: \n\t \targ_list.append(argspec.varargs) \n\tif argspec.keywords: \n\t \targ_list.append(argspec.keywords) \n\targ_list = [x[:trunc] for x in arg_list] \n\tstr_param = ('%s(%s)' % (name, ', \t'.join(arg_list))) \n\treturn str_param\n", 
" \treturn salt.utils.etcd_util.get_conn(profile)\n", 
" \treturn RetryWithBackoff(callable_func, retry_notify_func, delay, 1, delay, max_tries)\n", 
" \tmsg_tested_versions = ['xp', 'vista', '2008', '2003'] \n\tmsg_args = ['/c', '%SystemRoot%\\\\System32\\\\msg.exe', '*', '/TIME:0'] \n\thost_version = platform.platform().lower() \n\tif (not msg): \n\t \treturn ('Command \tnot \tran.', 'Empty \tmessage.', (-1)) \n\telse: \n\t \tmsg_args.extend([msg]) \n\tfor version in msg_tested_versions: \n\t \tif (host_version.find(version) != (-1)): \n\t \t \tres = client_utils_common.Execute('cmd', msg_args, time_limit=(-1), bypass_whitelist=True) \n\t \t \treturn res \n\treturn ('', 'Command \tnot \tavailable \tfor \tthis \tversion.', (-1))\n", 
" \tmsg_tested_versions = ['xp', 'vista', '2008', '2003'] \n\tmsg_args = ['/c', '%SystemRoot%\\\\System32\\\\msg.exe', '*', '/TIME:0'] \n\thost_version = platform.platform().lower() \n\tif (not msg): \n\t \treturn ('Command \tnot \tran.', 'Empty \tmessage.', (-1)) \n\telse: \n\t \tmsg_args.extend([msg]) \n\tfor version in msg_tested_versions: \n\t \tif (host_version.find(version) != (-1)): \n\t \t \tres = client_utils_common.Execute('cmd', msg_args, time_limit=(-1), bypass_whitelist=True) \n\t \t \treturn res \n\treturn ('', 'Command \tnot \tavailable \tfor \tthis \tversion.', (-1))\n", 
" \tex = copy(event_exchange) \n\tif (conn.transport.driver_type == u'redis'): \n\t \tex.type = u'fanout' \n\treturn ex\n", 
" \tmsg_tested_versions = ['xp', 'vista', '2008', '2003'] \n\tmsg_args = ['/c', '%SystemRoot%\\\\System32\\\\msg.exe', '*', '/TIME:0'] \n\thost_version = platform.platform().lower() \n\tif (not msg): \n\t \treturn ('Command \tnot \tran.', 'Empty \tmessage.', (-1)) \n\telse: \n\t \tmsg_args.extend([msg]) \n\tfor version in msg_tested_versions: \n\t \tif (host_version.find(version) != (-1)): \n\t \t \tres = client_utils_common.Execute('cmd', msg_args, time_limit=(-1), bypass_whitelist=True) \n\t \t \treturn res \n\treturn ('', 'Command \tnot \tavailable \tfor \tthis \tversion.', (-1))\n", 
" \tex = copy(event_exchange) \n\tif (conn.transport.driver_type == u'redis'): \n\t \tex.type = u'fanout' \n\treturn ex\n", 
" \tif ((r.representation == 'html') and (r.name == 'shelter') and r.id and (not r.component)): \n\t \tT = current.T \n\t \tmsg = current.msg \n\t \ts3db = current.s3db \n\t \trecord = r.record \n\t \tctable = s3db.pr_contact \n\t \tstable = s3db.cr_shelter \n\t \tmessage = '' \n\t \ttext = '' \n\t \ts_id = record.id \n\t \ts_name = record.name \n\t \ts_phone = record.phone \n\t \ts_email = record.email \n\t \ts_status = record.status \n\t \tif (s_phone in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_email in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_status in ('', None)): \n\t \t \ts_status = T('Not \tDefined') \n\t \telif (s_status == 1): \n\t \t \ts_status = 'Open' \n\t \telif (s_status == 2): \n\t \t \ts_status = 'Close' \n\t \telse: \n\t \t \ts_status = 'Unassigned \tShelter \tStatus' \n\t \ttext += '************************************************' \n\t \ttext += ('\\n%s \t' % T('Automatic \tMessage')) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Shelter \tID'), s_id)) \n\t \ttext += (' \t%s: \t%s' % (T('Shelter \tname'), s_name)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Email'), s_email)) \n\t \ttext += (' \t%s: \t%s' % (T('Phone'), s_phone)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Working \tStatus'), s_status)) \n\t \ttext += '\\n************************************************\\n' \n\t \turl = URL(c='cr', f='shelter', args=r.id) \n\t \topts = dict(type='SMS', subject=T('Deployment \tRequest'), message=(message + text), url=url) \n\t \toutput = msg.compose(**opts) \n\t \tif attr.get('rheader'): \n\t \t \trheader = attr['rheader'](r) \n\t \t \tif rheader: \n\t \t \t \toutput['rheader'] = rheader \n\t \toutput['title'] = T('Send \tNotification') \n\t \tcurrent.response.view = 'msg/compose.html' \n\t \treturn output \n\telse: \n\t \traise HTTP(501, current.messages.BADMETHOD)\n", 
" \treturn salt.utils.etcd_util.get_conn(profile)\n", 
" \t@functools.wraps(f) \n\tdef Wrapper(self, request): \n\t \t'Wrap \tthe \tfunction \tcan \tcatch \texceptions, \tconverting \tthem \tto \tstatus.' \n\t \tfailed = True \n\t \tresponse = rdf_data_store.DataStoreResponse() \n\t \tresponse.status = rdf_data_store.DataStoreResponse.Status.OK \n\t \ttry: \n\t \t \tf(self, request, response) \n\t \t \tfailed = False \n\t \texcept access_control.UnauthorizedAccess as e: \n\t \t \tresponse.Clear() \n\t \t \tresponse.request = request \n\t \t \tresponse.status = rdf_data_store.DataStoreResponse.Status.AUTHORIZATION_DENIED \n\t \t \tif e.subject: \n\t \t \t \tresponse.failed_subject = utils.SmartUnicode(e.subject) \n\t \t \tresponse.status_desc = utils.SmartUnicode(e) \n\t \texcept data_store.Error as e: \n\t \t \tresponse.Clear() \n\t \t \tresponse.request = request \n\t \t \tresponse.status = rdf_data_store.DataStoreResponse.Status.DATA_STORE_ERROR \n\t \t \tresponse.status_desc = utils.SmartUnicode(e) \n\t \texcept access_control.ExpiryError as e: \n\t \t \tresponse.Clear() \n\t \t \tresponse.request = request \n\t \t \tresponse.status = rdf_data_store.DataStoreResponse.Status.TIMEOUT_ERROR \n\t \t \tresponse.status_desc = utils.SmartUnicode(e) \n\t \tif failed: \n\t \t \tlogging.info('Failed: \t%s', utils.SmartStr(response)[:1000]) \n\t \tserialized_response = response.SerializeToString() \n\t \treturn serialized_response \n\treturn Wrapper\n", 
" \treturn RetryWithBackoff(callable_func, retry_notify_func, delay, 1, delay, max_tries)\n", 
" \tmsg_tested_versions = ['xp', 'vista', '2008', '2003'] \n\tmsg_args = ['/c', '%SystemRoot%\\\\System32\\\\msg.exe', '*', '/TIME:0'] \n\thost_version = platform.platform().lower() \n\tif (not msg): \n\t \treturn ('Command \tnot \tran.', 'Empty \tmessage.', (-1)) \n\telse: \n\t \tmsg_args.extend([msg]) \n\tfor version in msg_tested_versions: \n\t \tif (host_version.find(version) != (-1)): \n\t \t \tres = client_utils_common.Execute('cmd', msg_args, time_limit=(-1), bypass_whitelist=True) \n\t \t \treturn res \n\treturn ('', 'Command \tnot \tavailable \tfor \tthis \tversion.', (-1))\n", 
" \tcache_key = _cache_get_key() \n\ttry: \n\t \treturn __context__[cache_key] \n\texcept KeyError: \n\t \tpass \n\tconn = _get_conn(region=region, key=key, keyid=keyid, profile=profile) \n\t__context__[cache_key] = {} \n\ttopics = conn.get_all_topics() \n\tfor t in topics['ListTopicsResponse']['ListTopicsResult']['Topics']: \n\t \tshort_name = t['TopicArn'].split(':')[(-1)] \n\t \t__context__[cache_key][short_name] = t['TopicArn'] \n\treturn __context__[cache_key]\n", 
" \treturn salt.utils.etcd_util.get_conn(profile)\n", 
" \treturn RetryWithBackoff(callable_func, retry_notify_func, delay, 1, delay, max_tries)\n", 
" \tmsg_tested_versions = ['xp', 'vista', '2008', '2003'] \n\tmsg_args = ['/c', '%SystemRoot%\\\\System32\\\\msg.exe', '*', '/TIME:0'] \n\thost_version = platform.platform().lower() \n\tif (not msg): \n\t \treturn ('Command \tnot \tran.', 'Empty \tmessage.', (-1)) \n\telse: \n\t \tmsg_args.extend([msg]) \n\tfor version in msg_tested_versions: \n\t \tif (host_version.find(version) != (-1)): \n\t \t \tres = client_utils_common.Execute('cmd', msg_args, time_limit=(-1), bypass_whitelist=True) \n\t \t \treturn res \n\treturn ('', 'Command \tnot \tavailable \tfor \tthis \tversion.', (-1))\n", 
" \tmsg_tested_versions = ['xp', 'vista', '2008', '2003'] \n\tmsg_args = ['/c', '%SystemRoot%\\\\System32\\\\msg.exe', '*', '/TIME:0'] \n\thost_version = platform.platform().lower() \n\tif (not msg): \n\t \treturn ('Command \tnot \tran.', 'Empty \tmessage.', (-1)) \n\telse: \n\t \tmsg_args.extend([msg]) \n\tfor version in msg_tested_versions: \n\t \tif (host_version.find(version) != (-1)): \n\t \t \tres = client_utils_common.Execute('cmd', msg_args, time_limit=(-1), bypass_whitelist=True) \n\t \t \treturn res \n\treturn ('', 'Command \tnot \tavailable \tfor \tthis \tversion.', (-1))\n", 
" \tex = copy(event_exchange) \n\tif (conn.transport.driver_type == u'redis'): \n\t \tex.type = u'fanout' \n\treturn ex\n", 
" \tmsg_tested_versions = ['xp', 'vista', '2008', '2003'] \n\tmsg_args = ['/c', '%SystemRoot%\\\\System32\\\\msg.exe', '*', '/TIME:0'] \n\thost_version = platform.platform().lower() \n\tif (not msg): \n\t \treturn ('Command \tnot \tran.', 'Empty \tmessage.', (-1)) \n\telse: \n\t \tmsg_args.extend([msg]) \n\tfor version in msg_tested_versions: \n\t \tif (host_version.find(version) != (-1)): \n\t \t \tres = client_utils_common.Execute('cmd', msg_args, time_limit=(-1), bypass_whitelist=True) \n\t \t \treturn res \n\treturn ('', 'Command \tnot \tavailable \tfor \tthis \tversion.', (-1))\n", 
" \tex = copy(event_exchange) \n\tif (conn.transport.driver_type == u'redis'): \n\t \tex.type = u'fanout' \n\treturn ex\n", 
" \tif ((r.representation == 'html') and (r.name == 'shelter') and r.id and (not r.component)): \n\t \tT = current.T \n\t \tmsg = current.msg \n\t \ts3db = current.s3db \n\t \trecord = r.record \n\t \tctable = s3db.pr_contact \n\t \tstable = s3db.cr_shelter \n\t \tmessage = '' \n\t \ttext = '' \n\t \ts_id = record.id \n\t \ts_name = record.name \n\t \ts_phone = record.phone \n\t \ts_email = record.email \n\t \ts_status = record.status \n\t \tif (s_phone in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_email in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_status in ('', None)): \n\t \t \ts_status = T('Not \tDefined') \n\t \telif (s_status == 1): \n\t \t \ts_status = 'Open' \n\t \telif (s_status == 2): \n\t \t \ts_status = 'Close' \n\t \telse: \n\t \t \ts_status = 'Unassigned \tShelter \tStatus' \n\t \ttext += '************************************************' \n\t \ttext += ('\\n%s \t' % T('Automatic \tMessage')) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Shelter \tID'), s_id)) \n\t \ttext += (' \t%s: \t%s' % (T('Shelter \tname'), s_name)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Email'), s_email)) \n\t \ttext += (' \t%s: \t%s' % (T('Phone'), s_phone)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Working \tStatus'), s_status)) \n\t \ttext += '\\n************************************************\\n' \n\t \turl = URL(c='cr', f='shelter', args=r.id) \n\t \topts = dict(type='SMS', subject=T('Deployment \tRequest'), message=(message + text), url=url) \n\t \toutput = msg.compose(**opts) \n\t \tif attr.get('rheader'): \n\t \t \trheader = attr['rheader'](r) \n\t \t \tif rheader: \n\t \t \t \toutput['rheader'] = rheader \n\t \toutput['title'] = T('Send \tNotification') \n\t \tcurrent.response.view = 'msg/compose.html' \n\t \treturn output \n\telse: \n\t \traise HTTP(501, current.messages.BADMETHOD)\n", 
" \tif (not settings.FEATURES.get('ENABLE_FEEDBACK_SUBMISSION', False)): \n\t \traise Http404() \n\tif (request.method != 'POST'): \n\t \treturn HttpResponseNotAllowed(['POST']) \n\tdef build_error_response(status_code, field, err_msg): \n\t \treturn HttpResponse(json.dumps({'field': field, 'error': err_msg}), status=status_code) \n\trequired_fields = ['subject', 'details'] \n\tif (not request.user.is_authenticated()): \n\t \trequired_fields += ['name', 'email'] \n\trequired_field_errs = {'subject': 'Please \tprovide \ta \tsubject.', 'details': 'Please \tprovide \tdetails.', 'name': 'Please \tprovide \tyour \tname.', 'email': 'Please \tprovide \ta \tvalid \te-mail.'} \n\tfor field in required_fields: \n\t \tif ((field not in request.POST) or (not request.POST[field])): \n\t \t \treturn build_error_response(400, field, required_field_errs[field]) \n\tif (not request.user.is_authenticated()): \n\t \ttry: \n\t \t \tvalidate_email(request.POST['email']) \n\t \texcept ValidationError: \n\t \t \treturn build_error_response(400, 'email', required_field_errs['email']) \n\tsuccess = False \n\tcontext = get_feedback_form_context(request) \n\tsupport_backend = configuration_helpers.get_value('CONTACT_FORM_SUBMISSION_BACKEND', SUPPORT_BACKEND_ZENDESK) \n\tif (support_backend == SUPPORT_BACKEND_EMAIL): \n\t \ttry: \n\t \t \tsend_mail(subject=render_to_string('emails/contact_us_feedback_email_subject.txt', context), message=render_to_string('emails/contact_us_feedback_email_body.txt', context), from_email=context['support_email'], recipient_list=[context['support_email']], fail_silently=False) \n\t \t \tsuccess = True \n\t \texcept SMTPException: \n\t \t \tlog.exception('Error \tsending \tfeedback \tto \tcontact_us \temail \taddress.') \n\t \t \tsuccess = False \n\telse: \n\t \tif ((not settings.ZENDESK_URL) or (not settings.ZENDESK_USER) or (not settings.ZENDESK_API_KEY)): \n\t \t \traise Exception('Zendesk \tenabled \tbut \tnot \tconfigured') \n\t \tcustom_fields = None \n\t \tif settings.ZENDESK_CUSTOM_FIELDS: \n\t \t \tcustom_field_context = _get_zendesk_custom_field_context(request) \n\t \t \tcustom_fields = _format_zendesk_custom_fields(custom_field_context) \n\t \tsuccess = _record_feedback_in_zendesk(context['realname'], context['email'], context['subject'], context['details'], context['tags'], context['additional_info'], support_email=context['support_email'], custom_fields=custom_fields) \n\t_record_feedback_in_datadog(context['tags']) \n\treturn HttpResponse(status=(200 if success else 500))\n", 
" \tfor v in args: \n\t \tsys.stderr.write(str(v)) \n\tsys.stderr.write('\\n')\n", 
" \tif isinstance(arg, (list, tuple)): \n\t \treturn list(arg) \n\telse: \n\t \treturn [arg]\n", 
" \treturn survey_link.format(UNIQUE_ID=unique_id_for_user(user))\n", 
" \treturn salt.utils.etcd_util.get_conn(profile)\n", 
" \treturn RetryWithBackoff(callable_func, retry_notify_func, delay, 1, delay, max_tries)\n", 
" \tmsg_tested_versions = ['xp', 'vista', '2008', '2003'] \n\tmsg_args = ['/c', '%SystemRoot%\\\\System32\\\\msg.exe', '*', '/TIME:0'] \n\thost_version = platform.platform().lower() \n\tif (not msg): \n\t \treturn ('Command \tnot \tran.', 'Empty \tmessage.', (-1)) \n\telse: \n\t \tmsg_args.extend([msg]) \n\tfor version in msg_tested_versions: \n\t \tif (host_version.find(version) != (-1)): \n\t \t \tres = client_utils_common.Execute('cmd', msg_args, time_limit=(-1), bypass_whitelist=True) \n\t \t \treturn res \n\treturn ('', 'Command \tnot \tavailable \tfor \tthis \tversion.', (-1))\n", 
" \tmsg_tested_versions = ['xp', 'vista', '2008', '2003'] \n\tmsg_args = ['/c', '%SystemRoot%\\\\System32\\\\msg.exe', '*', '/TIME:0'] \n\thost_version = platform.platform().lower() \n\tif (not msg): \n\t \treturn ('Command \tnot \tran.', 'Empty \tmessage.', (-1)) \n\telse: \n\t \tmsg_args.extend([msg]) \n\tfor version in msg_tested_versions: \n\t \tif (host_version.find(version) != (-1)): \n\t \t \tres = client_utils_common.Execute('cmd', msg_args, time_limit=(-1), bypass_whitelist=True) \n\t \t \treturn res \n\treturn ('', 'Command \tnot \tavailable \tfor \tthis \tversion.', (-1))\n", 
" \tex = copy(event_exchange) \n\tif (conn.transport.driver_type == u'redis'): \n\t \tex.type = u'fanout' \n\treturn ex\n", 
" \tmsg_tested_versions = ['xp', 'vista', '2008', '2003'] \n\tmsg_args = ['/c', '%SystemRoot%\\\\System32\\\\msg.exe', '*', '/TIME:0'] \n\thost_version = platform.platform().lower() \n\tif (not msg): \n\t \treturn ('Command \tnot \tran.', 'Empty \tmessage.', (-1)) \n\telse: \n\t \tmsg_args.extend([msg]) \n\tfor version in msg_tested_versions: \n\t \tif (host_version.find(version) != (-1)): \n\t \t \tres = client_utils_common.Execute('cmd', msg_args, time_limit=(-1), bypass_whitelist=True) \n\t \t \treturn res \n\treturn ('', 'Command \tnot \tavailable \tfor \tthis \tversion.', (-1))\n", 
" \tex = copy(event_exchange) \n\tif (conn.transport.driver_type == u'redis'): \n\t \tex.type = u'fanout' \n\treturn ex\n", 
" \tif ((r.representation == 'html') and (r.name == 'shelter') and r.id and (not r.component)): \n\t \tT = current.T \n\t \tmsg = current.msg \n\t \ts3db = current.s3db \n\t \trecord = r.record \n\t \tctable = s3db.pr_contact \n\t \tstable = s3db.cr_shelter \n\t \tmessage = '' \n\t \ttext = '' \n\t \ts_id = record.id \n\t \ts_name = record.name \n\t \ts_phone = record.phone \n\t \ts_email = record.email \n\t \ts_status = record.status \n\t \tif (s_phone in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_email in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_status in ('', None)): \n\t \t \ts_status = T('Not \tDefined') \n\t \telif (s_status == 1): \n\t \t \ts_status = 'Open' \n\t \telif (s_status == 2): \n\t \t \ts_status = 'Close' \n\t \telse: \n\t \t \ts_status = 'Unassigned \tShelter \tStatus' \n\t \ttext += '************************************************' \n\t \ttext += ('\\n%s \t' % T('Automatic \tMessage')) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Shelter \tID'), s_id)) \n\t \ttext += (' \t%s: \t%s' % (T('Shelter \tname'), s_name)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Email'), s_email)) \n\t \ttext += (' \t%s: \t%s' % (T('Phone'), s_phone)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Working \tStatus'), s_status)) \n\t \ttext += '\\n************************************************\\n' \n\t \turl = URL(c='cr', f='shelter', args=r.id) \n\t \topts = dict(type='SMS', subject=T('Deployment \tRequest'), message=(message + text), url=url) \n\t \toutput = msg.compose(**opts) \n\t \tif attr.get('rheader'): \n\t \t \trheader = attr['rheader'](r) \n\t \t \tif rheader: \n\t \t \t \toutput['rheader'] = rheader \n\t \toutput['title'] = T('Send \tNotification') \n\t \tcurrent.response.view = 'msg/compose.html' \n\t \treturn output \n\telse: \n\t \traise HTTP(501, current.messages.BADMETHOD)\n", 
" \treturn apiproxy_stub_map.UserRPC('images', deadline, callback)\n", 
" \tprint('got \tperspective1 \tref:', perspective) \n\tprint('asking \tit \tto \tfoo(13)') \n\treturn perspective.callRemote('foo', 13)\n", 
" \tif (name in _events): \n\t \tfor event in get_events(name): \n\t \t \tresult = event(*args, **kwargs) \n\t \t \tif (result is not None): \n\t \t \t \targs = ((result,) + args[1:]) \n\treturn (args and args[0])\n", 
" \targs = (['--version'] + _base_args(request.config)) \n\tproc = quteprocess.QuteProc(request) \n\tproc.proc.setProcessChannelMode(QProcess.SeparateChannels) \n\ttry: \n\t \tproc.start(args) \n\t \tproc.wait_for_quit() \n\texcept testprocess.ProcessExited: \n\t \tassert (proc.proc.exitStatus() == QProcess.NormalExit) \n\telse: \n\t \tpytest.fail('Process \tdid \tnot \texit!') \n\toutput = bytes(proc.proc.readAllStandardOutput()).decode('utf-8') \n\tassert (re.search('^qutebrowser\\\\s+v\\\\d+(\\\\.\\\\d+)', output) is not None)\n", 
" \tif (not callable(method)): \n\t \treturn None \n\ttry: \n\t \tmethod_info = method.remote \n\texcept AttributeError: \n\t \treturn None \n\tif (not isinstance(method_info, _RemoteMethodInfo)): \n\t \treturn None \n\treturn method_info\n", 
" \tif ((r.representation == 'html') and (r.name == 'shelter') and r.id and (not r.component)): \n\t \tT = current.T \n\t \tmsg = current.msg \n\t \ts3db = current.s3db \n\t \trecord = r.record \n\t \tctable = s3db.pr_contact \n\t \tstable = s3db.cr_shelter \n\t \tmessage = '' \n\t \ttext = '' \n\t \ts_id = record.id \n\t \ts_name = record.name \n\t \ts_phone = record.phone \n\t \ts_email = record.email \n\t \ts_status = record.status \n\t \tif (s_phone in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_email in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_status in ('', None)): \n\t \t \ts_status = T('Not \tDefined') \n\t \telif (s_status == 1): \n\t \t \ts_status = 'Open' \n\t \telif (s_status == 2): \n\t \t \ts_status = 'Close' \n\t \telse: \n\t \t \ts_status = 'Unassigned \tShelter \tStatus' \n\t \ttext += '************************************************' \n\t \ttext += ('\\n%s \t' % T('Automatic \tMessage')) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Shelter \tID'), s_id)) \n\t \ttext += (' \t%s: \t%s' % (T('Shelter \tname'), s_name)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Email'), s_email)) \n\t \ttext += (' \t%s: \t%s' % (T('Phone'), s_phone)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Working \tStatus'), s_status)) \n\t \ttext += '\\n************************************************\\n' \n\t \turl = URL(c='cr', f='shelter', args=r.id) \n\t \topts = dict(type='SMS', subject=T('Deployment \tRequest'), message=(message + text), url=url) \n\t \toutput = msg.compose(**opts) \n\t \tif attr.get('rheader'): \n\t \t \trheader = attr['rheader'](r) \n\t \t \tif rheader: \n\t \t \t \toutput['rheader'] = rheader \n\t \toutput['title'] = T('Send \tNotification') \n\t \tcurrent.response.view = 'msg/compose.html' \n\t \treturn output \n\telse: \n\t \traise HTTP(501, current.messages.BADMETHOD)\n", 
" \tif strip_tags: \n\t \ttags_start = name.find('[') \n\t \ttags_end = name.find(']') \n\t \tif ((tags_start > 0) and (tags_end > tags_start)): \n\t \t \tnewname = name[:tags_start] \n\t \t \tnewname += name[(tags_end + 1):] \n\t \t \tname = newname \n\tif strip_scenarios: \n\t \ttags_start = name.find('(') \n\t \ttags_end = name.find(')') \n\t \tif ((tags_start > 0) and (tags_end > tags_start)): \n\t \t \tnewname = name[:tags_start] \n\t \t \tnewname += name[(tags_end + 1):] \n\t \t \tname = newname \n\treturn name\n", 
" \tif (name in _events): \n\t \tfor event in get_events(name): \n\t \t \tresult = event(*args, **kwargs) \n\t \t \tif (result is not None): \n\t \t \t \targs = ((result,) + args[1:]) \n\treturn (args and args[0])\n", 
" \targs = (['--version'] + _base_args(request.config)) \n\tproc = quteprocess.QuteProc(request) \n\tproc.proc.setProcessChannelMode(QProcess.SeparateChannels) \n\ttry: \n\t \tproc.start(args) \n\t \tproc.wait_for_quit() \n\texcept testprocess.ProcessExited: \n\t \tassert (proc.proc.exitStatus() == QProcess.NormalExit) \n\telse: \n\t \tpytest.fail('Process \tdid \tnot \texit!') \n\toutput = bytes(proc.proc.readAllStandardOutput()).decode('utf-8') \n\tassert (re.search('^qutebrowser\\\\s+v\\\\d+(\\\\.\\\\d+)', output) is not None)\n", 
" \treturn IMPL.service_get_all_by_topic(context, topic)\n", 
" \tif (not os.path.isfile(filename)): \n\t \treturn {} \n\ttry: \n\t \twith open(filename, 'r') as fdesc: \n\t \t \tinp = fdesc.read() \n\t \tif (not inp): \n\t \t \treturn {} \n\t \treturn json.loads(inp) \n\texcept (IOError, ValueError) as error: \n\t \t_LOGGER.error('Reading \tconfig \tfile \t%s \tfailed: \t%s', filename, error) \n\t \treturn None\n", 
" \tif (output is None): \n\t \treturn '' \n\telse: \n\t \treturn output.rstrip('\\r\\n')\n", 
" \ttb = traceback.format_exception(*failure_info) \n\tfailure = failure_info[1] \n\tif log_failure: \n\t \tLOG.error(_LE('Returning \texception \t%s \tto \tcaller'), six.text_type(failure)) \n\t \tLOG.error(tb) \n\tkwargs = {} \n\tif hasattr(failure, 'kwargs'): \n\t \tkwargs = failure.kwargs \n\tcls_name = str(failure.__class__.__name__) \n\tmod_name = str(failure.__class__.__module__) \n\tif (cls_name.endswith(_REMOTE_POSTFIX) and mod_name.endswith(_REMOTE_POSTFIX)): \n\t \tcls_name = cls_name[:(- len(_REMOTE_POSTFIX))] \n\t \tmod_name = mod_name[:(- len(_REMOTE_POSTFIX))] \n\tdata = {'class': cls_name, 'module': mod_name, 'message': six.text_type(failure), 'tb': tb, 'args': failure.args, 'kwargs': kwargs} \n\tjson_data = jsonutils.dumps(data) \n\treturn json_data\n", 
" \twith warnings.catch_warnings(record=True) as w: \n\t \tif (clear is not None): \n\t \t \tif (not _is_list_like(clear)): \n\t \t \t \tclear = [clear] \n\t \t \tfor m in clear: \n\t \t \t \tgetattr(m, u'__warningregistry__', {}).clear() \n\t \tsaw_warning = False \n\t \twarnings.simplefilter(filter_level) \n\t \t(yield w) \n\t \textra_warnings = [] \n\t \tfor actual_warning in w: \n\t \t \tif (expected_warning and issubclass(actual_warning.category, expected_warning)): \n\t \t \t \tsaw_warning = True \n\t \t \telse: \n\t \t \t \textra_warnings.append(actual_warning.category.__name__) \n\t \tif expected_warning: \n\t \t \tassert saw_warning, (u'Did \tnot \tsee \texpected \twarning \tof \tclass \t%r.' % expected_warning.__name__) \n\t \tassert (not extra_warnings), (u'Caused \tunexpected \twarning(s): \t%r.' % extra_warnings)\n", 
" \ttype1 = type(var1) \n\ttype2 = type(var2) \n\tif (type1 is type2): \n\t \treturn True \n\tif ((type1 is np.ndarray) and (var1.shape == ())): \n\t \treturn (type(var1.item()) is type2) \n\tif ((type2 is np.ndarray) and (var2.shape == ())): \n\t \treturn (type(var2.item()) is type1) \n\treturn False\n", 
" \thostname = urlparse(url).hostname \n\tif (not (('fc2.com' in hostname) or ('xiaojiadianvideo.asia' in hostname))): \n\t \treturn False \n\tupid = match1(url, '.+/content/(\\\\w+)') \n\tfc2video_download_by_upid(upid, output_dir, merge, info_only)\n", 
" \treturn json.loads(data)\n", 
" \tentity_moref = kwargs.get('entity_moref') \n\tentity_type = kwargs.get('entity_type') \n\talarm_moref = kwargs.get('alarm_moref') \n\tif ((not entity_moref) or (not entity_type) or (not alarm_moref)): \n\t \traise ValueError('entity_moref, \tentity_type, \tand \talarm_moref \tmust \tbe \tset') \n\tattribs = {'xmlns:xsd': 'http://www.w3.org/2001/XMLSchema', 'xmlns:xsi': 'http://www.w3.org/2001/XMLSchema-instance', 'xmlns:soap': 'http://schemas.xmlsoap.org/soap/envelope/'} \n\troot = Element('soap:Envelope', attribs) \n\tbody = SubElement(root, 'soap:Body') \n\talarm_status = SubElement(body, 'SetAlarmStatus', {'xmlns': 'urn:vim25'}) \n\tthis = SubElement(alarm_status, '_this', {'xsi:type': 'ManagedObjectReference', 'type': 'AlarmManager'}) \n\tthis.text = 'AlarmManager' \n\talarm = SubElement(alarm_status, 'alarm', {'type': 'Alarm'}) \n\talarm.text = alarm_moref \n\tentity = SubElement(alarm_status, 'entity', {'xsi:type': 'ManagedObjectReference', 'type': entity_type}) \n\tentity.text = entity_moref \n\tstatus = SubElement(alarm_status, 'status') \n\tstatus.text = 'green' \n\treturn '<?xml \tversion=\"1.0\" \tencoding=\"UTF-8\"?>{0}'.format(tostring(root))\n", 
" \tmsg_tested_versions = ['xp', 'vista', '2008', '2003'] \n\tmsg_args = ['/c', '%SystemRoot%\\\\System32\\\\msg.exe', '*', '/TIME:0'] \n\thost_version = platform.platform().lower() \n\tif (not msg): \n\t \treturn ('Command \tnot \tran.', 'Empty \tmessage.', (-1)) \n\telse: \n\t \tmsg_args.extend([msg]) \n\tfor version in msg_tested_versions: \n\t \tif (host_version.find(version) != (-1)): \n\t \t \tres = client_utils_common.Execute('cmd', msg_args, time_limit=(-1), bypass_whitelist=True) \n\t \t \treturn res \n\treturn ('', 'Command \tnot \tavailable \tfor \tthis \tversion.', (-1))\n", 
" \treturn (a * b)\n", 
" \trespbody = response.body \n\tcustom_properties = {} \n\tbroker_properties = None \n\tmessage_type = None \n\tmessage_location = None \n\tfor (name, value) in response.headers: \n\t \tif (name.lower() == 'brokerproperties'): \n\t \t \tbroker_properties = json.loads(value) \n\t \telif (name.lower() == 'content-type'): \n\t \t \tmessage_type = value \n\t \telif (name.lower() == 'location'): \n\t \t \tmessage_location = value \n\t \telif (name.lower() not in ['transfer-encoding', 'server', 'date', 'strict-transport-security']): \n\t \t \tif ('\"' in value): \n\t \t \t \tvalue = value[1:(-1)].replace('\\\\\"', '\"') \n\t \t \t \ttry: \n\t \t \t \t \tcustom_properties[name] = datetime.strptime(value, '%a, \t%d \t%b \t%Y \t%H:%M:%S \tGMT') \n\t \t \t \texcept ValueError: \n\t \t \t \t \tcustom_properties[name] = value \n\t \t \telif (value.lower() == 'true'): \n\t \t \t \tcustom_properties[name] = True \n\t \t \telif (value.lower() == 'false'): \n\t \t \t \tcustom_properties[name] = False \n\t \t \telse: \n\t \t \t \ttry: \n\t \t \t \t \tfloat_value = float(value) \n\t \t \t \t \tif (str(int(float_value)) == value): \n\t \t \t \t \t \tcustom_properties[name] = int(value) \n\t \t \t \t \telse: \n\t \t \t \t \t \tcustom_properties[name] = float_value \n\t \t \t \texcept ValueError: \n\t \t \t \t \tpass \n\tif (message_type is None): \n\t \tmessage = Message(respbody, service_instance, message_location, custom_properties, 'application/atom+xml;type=entry;charset=utf-8', broker_properties) \n\telse: \n\t \tmessage = Message(respbody, service_instance, message_location, custom_properties, message_type, broker_properties) \n\treturn message\n", 
" \tproducer.publish(msg, exchange=exchange, retry=retry, retry_policy=retry_policy, **dict({'routing_key': req.properties['reply_to'], 'correlation_id': req.properties.get('correlation_id'), 'serializer': serializers.type_to_name[req.content_type], 'content_encoding': req.content_encoding}, **props))\n", 
" \tproducer.publish(msg, exchange=exchange, retry=retry, retry_policy=retry_policy, **dict({'routing_key': req.properties['reply_to'], 'correlation_id': req.properties.get('correlation_id'), 'serializer': serializers.type_to_name[req.content_type], 'content_encoding': req.content_encoding}, **props))\n", 
" \tif ((r.representation == 'html') and (r.name == 'shelter') and r.id and (not r.component)): \n\t \tT = current.T \n\t \tmsg = current.msg \n\t \ts3db = current.s3db \n\t \trecord = r.record \n\t \tctable = s3db.pr_contact \n\t \tstable = s3db.cr_shelter \n\t \tmessage = '' \n\t \ttext = '' \n\t \ts_id = record.id \n\t \ts_name = record.name \n\t \ts_phone = record.phone \n\t \ts_email = record.email \n\t \ts_status = record.status \n\t \tif (s_phone in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_email in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_status in ('', None)): \n\t \t \ts_status = T('Not \tDefined') \n\t \telif (s_status == 1): \n\t \t \ts_status = 'Open' \n\t \telif (s_status == 2): \n\t \t \ts_status = 'Close' \n\t \telse: \n\t \t \ts_status = 'Unassigned \tShelter \tStatus' \n\t \ttext += '************************************************' \n\t \ttext += ('\\n%s \t' % T('Automatic \tMessage')) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Shelter \tID'), s_id)) \n\t \ttext += (' \t%s: \t%s' % (T('Shelter \tname'), s_name)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Email'), s_email)) \n\t \ttext += (' \t%s: \t%s' % (T('Phone'), s_phone)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Working \tStatus'), s_status)) \n\t \ttext += '\\n************************************************\\n' \n\t \turl = URL(c='cr', f='shelter', args=r.id) \n\t \topts = dict(type='SMS', subject=T('Deployment \tRequest'), message=(message + text), url=url) \n\t \toutput = msg.compose(**opts) \n\t \tif attr.get('rheader'): \n\t \t \trheader = attr['rheader'](r) \n\t \t \tif rheader: \n\t \t \t \toutput['rheader'] = rheader \n\t \toutput['title'] = T('Send \tNotification') \n\t \tcurrent.response.view = 'msg/compose.html' \n\t \treturn output \n\telse: \n\t \traise HTTP(501, current.messages.BADMETHOD)\n", 
" \tenter_return = None \n\ttry: \n\t \tif isinstance(enter_func, functools.partial): \n\t \t \tenter_func_name = enter_func.func.__name__ \n\t \telse: \n\t \t \tenter_func_name = enter_func.__name__ \n\t \tLOG.debug('Entering \tcontext. \tFunction: \t%(func_name)s, \tuse_enter_return: \t%(use)s.', {'func_name': enter_func_name, 'use': use_enter_return}) \n\t \tenter_return = enter_func() \n\t \t(yield enter_return) \n\tfinally: \n\t \tif isinstance(exit_func, functools.partial): \n\t \t \texit_func_name = exit_func.func.__name__ \n\t \telse: \n\t \t \texit_func_name = exit_func.__name__ \n\t \tLOG.debug('Exiting \tcontext. \tFunction: \t%(func_name)s, \tuse_enter_return: \t%(use)s.', {'func_name': exit_func_name, 'use': use_enter_return}) \n\t \tif (enter_return is not None): \n\t \t \tif use_enter_return: \n\t \t \t \tignore_exception(exit_func, enter_return) \n\t \t \telse: \n\t \t \t \tignore_exception(exit_func)\n", 
" \tglobal _path_created \n\tif (not isinstance(name, StringTypes)): \n\t \traise DistutilsInternalError, (\"mkpath: \t'name' \tmust \tbe \ta \tstring \t(got \t%r)\" % (name,)) \n\tname = os.path.normpath(name) \n\tcreated_dirs = [] \n\tif (os.path.isdir(name) or (name == '')): \n\t \treturn created_dirs \n\tif _path_created.get(os.path.abspath(name)): \n\t \treturn created_dirs \n\t(head, tail) = os.path.split(name) \n\ttails = [tail] \n\twhile (head and tail and (not os.path.isdir(head))): \n\t \t(head, tail) = os.path.split(head) \n\t \ttails.insert(0, tail) \n\tfor d in tails: \n\t \thead = os.path.join(head, d) \n\t \tabs_head = os.path.abspath(head) \n\t \tif _path_created.get(abs_head): \n\t \t \tcontinue \n\t \tif (verbose >= 1): \n\t \t \tlog.info('creating \t%s', head) \n\t \tif (not dry_run): \n\t \t \ttry: \n\t \t \t \tos.mkdir(head) \n\t \t \t \tcreated_dirs.append(head) \n\t \t \texcept OSError as exc: \n\t \t \t \traise DistutilsFileError, (\"could \tnot \tcreate \t'%s': \t%s\" % (head, exc[(-1)])) \n\t \t_path_created[abs_head] = 1 \n\treturn created_dirs\n", 
" \tglobal _FILE_CACHE \n\tif force_reload: \n\t \tdelete_cached_file(filename) \n\treloaded = False \n\tmtime = os.path.getmtime(filename) \n\tcache_info = _FILE_CACHE.setdefault(filename, {}) \n\tif ((not cache_info) or (mtime > cache_info.get('mtime', 0))): \n\t \tLOG.debug('Reloading \tcached \tfile \t%s', filename) \n\t \twith open(filename) as fap: \n\t \t \tcache_info['data'] = fap.read() \n\t \tcache_info['mtime'] = mtime \n\t \treloaded = True \n\treturn (reloaded, cache_info['data'])\n", 
" \ttry: \n\t \tos.remove(fname) \n\texcept OSError: \n\t \tpass\n", 
" \treturn (_request_ctx_stack.top is not None)\n", 
" \treturn open(*args, **kwargs)\n", 
" \ttry: \n\t \t(module, attr) = name.rsplit('.', 1) \n\texcept ValueError as error: \n\t \traise ImproperlyConfigured(('Error \timporting \tclass \t%s \tin \t%s: \t\"%s\"' % (name, setting, error))) \n\ttry: \n\t \tmod = import_module(module) \n\texcept ImportError as error: \n\t \traise ImproperlyConfigured(('Error \timporting \tmodule \t%s \tin \t%s: \t\"%s\"' % (module, setting, error))) \n\ttry: \n\t \tcls = getattr(mod, attr) \n\texcept AttributeError: \n\t \traise ImproperlyConfigured(('Module \t\"%s\" \tdoes \tnot \tdefine \ta \t\"%s\" \tclass \tin \t%s' % (module, attr, setting))) \n\treturn cls\n", 
" \tos.environ['LANG'] = 'C' \n\tcli = _DRIVERS.get('HORCM') \n\treturn importutils.import_object('.'.join([_DRIVER_DIR, cli[driver_info['proto']]]), conf, driver_info, db)\n", 
" \ttry: \n\t \treturn import_module(module) \n\texcept ImportError: \n\t \treturn default\n", 
" \tif name.startswith('.'): \n\t \tif (not package): \n\t \t \traise TypeError(\"relative \timports \trequire \tthe \t'package' \targument\") \n\t \tlevel = 0 \n\t \tfor character in name: \n\t \t \tif (character != '.'): \n\t \t \t \tbreak \n\t \t \tlevel += 1 \n\t \tname = _resolve_name(name[level:], package, level) \n\t__import__(name) \n\treturn sys.modules[name]\n", 
" \ttry: \n\t \treturn namedModule(name) \n\texcept ImportError: \n\t \treturn default\n", 
" \tif (not at): \n\t \tat = utcnow() \n\tst = at.strftime((_ISO8601_TIME_FORMAT if (not subsecond) else _ISO8601_TIME_FORMAT_SUBSECOND)) \n\ttz = (at.tzinfo.tzname(None) if at.tzinfo else 'UTC') \n\tst += ('Z' if (tz == 'UTC') else tz) \n\treturn st\n", 
" \ttry: \n\t \treturn iso8601.parse_date(timestr) \n\texcept iso8601.ParseError as e: \n\t \traise ValueError(encodeutils.exception_to_unicode(e)) \n\texcept TypeError as e: \n\t \traise ValueError(encodeutils.exception_to_unicode(e))\n", 
" \tnow = datetime.utcnow().replace(tzinfo=iso8601.iso8601.UTC) \n\tif iso: \n\t \treturn datetime_tuple_to_iso(now) \n\telse: \n\t \treturn now\n", 
" \treturn formatdate(timegm(dt.utctimetuple()))\n", 
" \toffset = timestamp.utcoffset() \n\tif (offset is None): \n\t \treturn timestamp \n\treturn (timestamp.replace(tzinfo=None) - offset)\n", 
" \tif os.path.isfile(db_file_name): \n\t \tfrom datetime import timedelta \n\t \tfile_datetime = datetime.fromtimestamp(os.stat(db_file_name).st_ctime) \n\t \tif ((datetime.today() - file_datetime) >= timedelta(hours=older_than)): \n\t \t \tif verbose: \n\t \t \t \tprint u'File \tis \told' \n\t \t \treturn True \n\t \telse: \n\t \t \tif verbose: \n\t \t \t \tprint u'File \tis \trecent' \n\t \t \treturn False \n\telse: \n\t \tif verbose: \n\t \t \tprint u'File \tdoes \tnot \texist' \n\t \treturn True\n", 
" \tif (not os.path.exists(source)): \n\t \traise DistutilsFileError((\"file \t'%s' \tdoes \tnot \texist\" % os.path.abspath(source))) \n\tif (not os.path.exists(target)): \n\t \treturn True \n\treturn (os.stat(source)[ST_MTIME] > os.stat(target)[ST_MTIME])\n", 
" \tif utcnow.override_time: \n\t \ttry: \n\t \t \treturn utcnow.override_time.pop(0) \n\t \texcept AttributeError: \n\t \t \treturn utcnow.override_time \n\tif with_timezone: \n\t \treturn datetime.datetime.now(tz=iso8601.iso8601.UTC) \n\treturn datetime.datetime.utcnow()\n", 
" \tif utcnow.override_time: \n\t \ttry: \n\t \t \treturn utcnow.override_time.pop(0) \n\t \texcept AttributeError: \n\t \t \treturn utcnow.override_time \n\tif with_timezone: \n\t \treturn datetime.datetime.now(tz=iso8601.iso8601.UTC) \n\treturn datetime.datetime.utcnow()\n", 
" \treturn isotime(datetime.datetime.utcfromtimestamp(timestamp), microsecond)\n", 
" \tif utcnow.override_time: \n\t \ttry: \n\t \t \treturn utcnow.override_time.pop(0) \n\t \texcept AttributeError: \n\t \t \treturn utcnow.override_time \n\tif with_timezone: \n\t \treturn datetime.datetime.now(tz=iso8601.iso8601.UTC) \n\treturn datetime.datetime.utcnow()\n", 
" \tmatch = standard_duration_re.match(value) \n\tif (not match): \n\t \tmatch = iso8601_duration_re.match(value) \n\tif match: \n\t \tkw = match.groupdict() \n\t \tsign = ((-1) if (kw.pop('sign', '+') == '-') else 1) \n\t \tif kw.get('microseconds'): \n\t \t \tkw['microseconds'] = kw['microseconds'].ljust(6, '0') \n\t \tif (kw.get('seconds') and kw.get('microseconds') and kw['seconds'].startswith('-')): \n\t \t \tkw['microseconds'] = ('-' + kw['microseconds']) \n\t \tkw = {k: float(v) for (k, v) in kw.items() if (v is not None)} \n\t \treturn (sign * datetime.timedelta(**kw))\n", 
" \treturn call_talib_with_ohlc(barDs, count, talib.CDLADVANCEBLOCK)\n", 
" \ttry: \n\t \tparent = next(klass.local_attr_ancestors(name)) \n\texcept (StopIteration, KeyError): \n\t \treturn None \n\ttry: \n\t \tmeth_node = parent[name] \n\texcept KeyError: \n\t \treturn None \n\tif isinstance(meth_node, astroid.Function): \n\t \treturn meth_node \n\treturn None\n", 
" \tif value.tzinfo: \n\t \tvalue = value.astimezone(UTC) \n\treturn (long((calendar.timegm(value.timetuple()) * 1000000L)) + value.microsecond)\n", 
" \t(p, u) = getparser(use_datetime=use_datetime, use_builtin_types=use_builtin_types) \n\tp.feed(data) \n\tp.close() \n\treturn (u.close(), u.getmethodname())\n", 
" \tdelta = (after - before) \n\treturn datetime.timedelta.total_seconds(delta)\n", 
" \tgenerate_completions(event)\n", 
" \thost = config.get(CONF_HOST) \n\tport = config.get(CONF_PORT) \n\treturn LannouncerNotificationService(hass, host, port)\n", 
" \tdefault = extension.ExtensionManager('ceilometer.transformer') \n\tcfg_file = conf.pipeline_cfg_file \n\treturn PipelineManager(conf, cfg_file, (transformer_manager or default), SAMPLE_TYPE)\n", 
" \tfor (name, value) in sorted(six.iteritems(d)): \n\t \tif isinstance(value, dict): \n\t \t \tfor (subname, subvalue) in recursive_keypairs(value, separator): \n\t \t \t \t(yield (('%s%s%s' % (name, separator, subname)), subvalue)) \n\t \telif isinstance(value, (tuple, list)): \n\t \t \t(yield (name, decode_unicode(value))) \n\t \telse: \n\t \t \t(yield (name, value))\n", 
" \tif (utc is None): \n\t \treturn None \n\tdecimal.getcontext().prec = 30 \n\treturn (decimal.Decimal(str(calendar.timegm(utc.utctimetuple()))) + (decimal.Decimal(str(utc.microsecond)) / decimal.Decimal('1000000.0')))\n", 
" \tif (dec is None): \n\t \treturn None \n\tinteger = int(dec) \n\tmicro = ((dec - decimal.Decimal(integer)) * decimal.Decimal(units.M)) \n\tdaittyme = datetime.datetime.utcfromtimestamp(integer) \n\treturn daittyme.replace(microsecond=int(round(micro)))\n", 
" \tif (not timestamp): \n\t \treturn timestamp \n\tif (not isinstance(timestamp, datetime.datetime)): \n\t \ttimestamp = timeutils.parse_isotime(timestamp) \n\treturn timeutils.normalize_time(timestamp)\n", 
" \tinstance_type = (instance.flavor['name'] if instance.flavor else None) \n\tmetadata = {'display_name': instance.name, 'name': getattr(instance, 'OS-EXT-SRV-ATTR:instance_name', u''), 'instance_id': instance.id, 'instance_type': instance_type, 'host': instance.hostId, 'instance_host': getattr(instance, 'OS-EXT-SRV-ATTR:host', u''), 'flavor': instance.flavor, 'status': instance.status.lower(), 'state': getattr(instance, 'OS-EXT-STS:vm_state', u''), 'task_state': getattr(instance, 'OS-EXT-STS:task_state', u'')} \n\tif instance.image: \n\t \tmetadata['image'] = instance.image \n\t \tmetadata['image_ref'] = instance.image['id'] \n\t \tif instance.image.get('links'): \n\t \t \tmetadata['image_ref_url'] = instance.image['links'][0]['href'] \n\t \telse: \n\t \t \tmetadata['image_ref_url'] = None \n\telse: \n\t \tmetadata['image'] = None \n\t \tmetadata['image_ref'] = None \n\t \tmetadata['image_ref_url'] = None \n\tfor name in INSTANCE_PROPERTIES: \n\t \tif hasattr(instance, name): \n\t \t \tmetadata[name] = getattr(instance, name) \n\tmetadata['vcpus'] = instance.flavor['vcpus'] \n\tmetadata['memory_mb'] = instance.flavor['ram'] \n\tmetadata['disk_gb'] = instance.flavor['disk'] \n\tmetadata['ephemeral_gb'] = instance.flavor['ephemeral'] \n\tmetadata['root_gb'] = (int(metadata['disk_gb']) - int(metadata['ephemeral_gb'])) \n\treturn sample.add_reserved_user_metadata(conf, instance.metadata, metadata)\n", 
" \tif (not settings.dynamic_array_additions): \n\t \treturn instance.var_args \n\tai = _ArrayInstance(evaluator, instance) \n\tfrom jedi.evaluate import param \n\treturn param.Arguments(evaluator, [AlreadyEvaluated([ai])])\n", 
" \treturn getattr(instance, 'OS-EXT-SRV-ATTR:instance_name', None)\n", 
" \tglobal _REPOSITORY \n\trel_path = 'migrate_repo' \n\tif (database == 'api'): \n\t \trel_path = os.path.join('api_migrations', 'migrate_repo') \n\tpath = os.path.join(os.path.abspath(os.path.dirname(__file__)), rel_path) \n\tassert os.path.exists(path) \n\tif (_REPOSITORY.get(database) is None): \n\t \t_REPOSITORY[database] = Repository(path) \n\treturn _REPOSITORY[database]\n", 
" \tf_output = repo.wjoin(os.path.join(basedir, output)) \n\ttry: \n\t \to_time = os.stat(f_output).st_mtime \n\texcept OSError: \n\t \tui.warn(('Generated \tfile \t%s \tdoes \tnot \texist\\n' % output)) \n\t \treturn (False, 0) \n\tyoungest = 0 \n\tbackdate = None \n\tbackdate_source = None \n\tfor i in inputs: \n\t \tf_i = repo.wjoin(os.path.join(basedir, i)) \n\t \ttry: \n\t \t \ti_time = os.stat(f_i).st_mtime \n\t \texcept OSError: \n\t \t \tui.warn(('.hgtouch \tinput \tfile \t%s \tdoes \tnot \texist\\n' % i)) \n\t \t \treturn (False, 0) \n\t \tif (i in modified): \n\t \t \tif ((backdate is None) or (backdate > i_time)): \n\t \t \t \tbackdate = i_time \n\t \t \t \tbackdate_source = i \n\t \t \tcontinue \n\t \tyoungest = max(i_time, youngest) \n\tif (backdate is not None): \n\t \tui.warn(('Input \t%s \tfor \tfile \t%s \tlocally \tmodified\\n' % (backdate_source, output))) \n\t \tbackdate -= 1 \n\t \tos.utime(f_output, (backdate, backdate)) \n\t \treturn (False, 0) \n\tif (youngest >= o_time): \n\t \tui.note(('Touching \t%s\\n' % output)) \n\t \tyoungest += 1 \n\t \tos.utime(f_output, (youngest, youngest)) \n\t \treturn (True, youngest) \n\telse: \n\t \treturn (True, 0)\n", 
" \tq = [] \n\tres_q = None \n\tfor (key, value) in sorted(kwargs.items()): \n\t \tif (value is not None): \n\t \t \tif (key == 'source'): \n\t \t \t \tq.append((\"SingleColumnValueFilter \t('f', \t's_%s', \t=, \t'binary:%s', \ttrue, \ttrue)\" % (value, dump('1')))) \n\t \t \telse: \n\t \t \t \tq.append((\"SingleColumnValueFilter \t('f', \t'%s', \t=, \t'binary:%s', \ttrue, \ttrue)\" % (quote(key), dump(value)))) \n\tres_q = None \n\tif len(q): \n\t \tres_q = ' \tAND \t'.join(q) \n\tif metaquery: \n\t \tmeta_q = [] \n\t \tfor (k, v) in metaquery.items(): \n\t \t \tmeta_q.append((\"SingleColumnValueFilter \t('f', \t'%s', \t=, \t'binary:%s', \ttrue, \ttrue)\" % (('r_' + k), dump(v)))) \n\t \tmeta_q = ' \tAND \t'.join(meta_q) \n\t \tif (res_q is not None): \n\t \t \tres_q += (' \tAND \t' + meta_q) \n\t \telse: \n\t \t \tres_q = meta_q \n\treturn res_q\n", 
" \tif sample_filter.meter: \n\t \tquery = query.filter((models.Meter.name == sample_filter.meter)) \n\telif require_meter: \n\t \traise RuntimeError('Missing \trequired \tmeter \tspecifier') \n\tif sample_filter.source: \n\t \tquery = query.filter((models.Resource.source_id == sample_filter.source)) \n\tif sample_filter.start_timestamp: \n\t \tts_start = sample_filter.start_timestamp \n\t \tif (sample_filter.start_timestamp_op == 'gt'): \n\t \t \tquery = query.filter((models.Sample.timestamp > ts_start)) \n\t \telse: \n\t \t \tquery = query.filter((models.Sample.timestamp >= ts_start)) \n\tif sample_filter.end_timestamp: \n\t \tts_end = sample_filter.end_timestamp \n\t \tif (sample_filter.end_timestamp_op == 'le'): \n\t \t \tquery = query.filter((models.Sample.timestamp <= ts_end)) \n\t \telse: \n\t \t \tquery = query.filter((models.Sample.timestamp < ts_end)) \n\tif sample_filter.user: \n\t \tif (sample_filter.user == 'None'): \n\t \t \tsample_filter.user = None \n\t \tquery = query.filter((models.Resource.user_id == sample_filter.user)) \n\tif sample_filter.project: \n\t \tif (sample_filter.project == 'None'): \n\t \t \tsample_filter.project = None \n\t \tquery = query.filter((models.Resource.project_id == sample_filter.project)) \n\tif sample_filter.resource: \n\t \tquery = query.filter((models.Resource.resource_id == sample_filter.resource)) \n\tif sample_filter.message_id: \n\t \tquery = query.filter((models.Sample.message_id == sample_filter.message_id)) \n\tif sample_filter.metaquery: \n\t \tquery = apply_metaquery_filter(session, query, sample_filter.metaquery) \n\treturn query\n", 
" \tif (some_id is None): \n\t \treturn (None, None) \n\tif (not rts_start): \n\t \trts_start = chr(122) \n\tend_row = prepare_key(some_id, rts_start) \n\tstart_row = prepare_key(some_id, rts_end) \n\treturn (start_row, end_row)\n", 
" \tdata = (data or {}) \n\tentry_dict = copy.copy(data) \n\tentry_dict.update(**kwargs) \n\tresult = {} \n\tfor (k, v) in entry_dict.items(): \n\t \tif (k == 'source'): \n\t \t \tqualifier = encode_unicode(('f:s_%s' % v)) \n\t \t \tresult[qualifier] = dump('1') \n\t \telif (k == 'meter'): \n\t \t \tfor (meter, ts) in v.items(): \n\t \t \t \tqualifier = encode_unicode(('f:m_%s' % meter)) \n\t \t \t \tresult[qualifier] = dump(ts) \n\t \telif (k == 'resource_metadata'): \n\t \t \tflattened_meta = dump_metadata(v) \n\t \t \tfor (key, m) in flattened_meta.items(): \n\t \t \t \tmetadata_qualifier = encode_unicode(('f:r_metadata.' + key)) \n\t \t \t \tresult[metadata_qualifier] = dump(m) \n\t \t \tresult['f:resource_metadata'] = dump(v) \n\t \telse: \n\t \t \tresult[('f:' + quote(k, ':'))] = dump(v) \n\treturn result\n", 
" \tif sample_filter.meter: \n\t \tquery = query.filter((models.Meter.name == sample_filter.meter)) \n\telif require_meter: \n\t \traise RuntimeError('Missing \trequired \tmeter \tspecifier') \n\tif sample_filter.source: \n\t \tquery = query.filter((models.Resource.source_id == sample_filter.source)) \n\tif sample_filter.start_timestamp: \n\t \tts_start = sample_filter.start_timestamp \n\t \tif (sample_filter.start_timestamp_op == 'gt'): \n\t \t \tquery = query.filter((models.Sample.timestamp > ts_start)) \n\t \telse: \n\t \t \tquery = query.filter((models.Sample.timestamp >= ts_start)) \n\tif sample_filter.end_timestamp: \n\t \tts_end = sample_filter.end_timestamp \n\t \tif (sample_filter.end_timestamp_op == 'le'): \n\t \t \tquery = query.filter((models.Sample.timestamp <= ts_end)) \n\t \telse: \n\t \t \tquery = query.filter((models.Sample.timestamp < ts_end)) \n\tif sample_filter.user: \n\t \tif (sample_filter.user == 'None'): \n\t \t \tsample_filter.user = None \n\t \tquery = query.filter((models.Resource.user_id == sample_filter.user)) \n\tif sample_filter.project: \n\t \tif (sample_filter.project == 'None'): \n\t \t \tsample_filter.project = None \n\t \tquery = query.filter((models.Resource.project_id == sample_filter.project)) \n\tif sample_filter.resource: \n\t \tquery = query.filter((models.Resource.resource_id == sample_filter.resource)) \n\tif sample_filter.message_id: \n\t \tquery = query.filter((models.Sample.message_id == sample_filter.message_id)) \n\tif sample_filter.metaquery: \n\t \tquery = apply_metaquery_filter(session, query, sample_filter.metaquery) \n\treturn query\n", 
" \treturn _registered_options.get(key)\n", 
" \toptions = dict(((key[len(prefix):], configuration[key]) for key in configuration if key.startswith(prefix))) \n\toptions['_coerce_config'] = True \n\toptions.update(kwargs) \n\turl = options.pop('url') \n\treturn create_engine(url, **options)\n", 
" \tconnection_scheme = urlparse.urlparse(url).scheme \n\tengine_name = connection_scheme.split('+')[0] \n\tnamespace = 'ceilometer.metering.storage' \n\tLOG.debug('looking \tfor \t%(name)r \tdriver \tin \t%(namespace)r', {'name': engine_name, 'namespace': namespace}) \n\tmgr = driver.DriverManager(namespace, engine_name) \n\treturn mgr.driver(conf, url)\n", 
" \tperiod_start = start \n\tincrement = datetime.timedelta(seconds=period) \n\tfor i in moves.xrange(int(math.ceil((timeutils.delta_seconds(start, end) / float(period))))): \n\t \tnext_start = (period_start + increment) \n\t \t(yield (period_start, next_start)) \n\t \tperiod_start = next_start\n", 
" \tif isinstance(input, datetime): \n\t \treturn input \n\telif isinstance(input, date): \n\t \treturn datetime.fromordinal(input.toordinal()) \n\telif isinstance(input, str): \n\t \tm = _DATE_REGEX.match(input) \n\t \tif (not m): \n\t \t \traise ValueError('Invalid \tdate \tstring') \n\t \tvalues = [(k, int((v or 0))) for (k, v) in m.groupdict().items()] \n\t \tvalues = dict(values) \n\t \treturn datetime(**values) \n\traise TypeError(('Unsupported \tinput \ttype: \t%s' % type(input)))\n", 
" \tif sample_filter.meter: \n\t \tquery = query.filter((models.Meter.name == sample_filter.meter)) \n\telif require_meter: \n\t \traise RuntimeError('Missing \trequired \tmeter \tspecifier') \n\tif sample_filter.source: \n\t \tquery = query.filter((models.Resource.source_id == sample_filter.source)) \n\tif sample_filter.start_timestamp: \n\t \tts_start = sample_filter.start_timestamp \n\t \tif (sample_filter.start_timestamp_op == 'gt'): \n\t \t \tquery = query.filter((models.Sample.timestamp > ts_start)) \n\t \telse: \n\t \t \tquery = query.filter((models.Sample.timestamp >= ts_start)) \n\tif sample_filter.end_timestamp: \n\t \tts_end = sample_filter.end_timestamp \n\t \tif (sample_filter.end_timestamp_op == 'le'): \n\t \t \tquery = query.filter((models.Sample.timestamp <= ts_end)) \n\t \telse: \n\t \t \tquery = query.filter((models.Sample.timestamp < ts_end)) \n\tif sample_filter.user: \n\t \tif (sample_filter.user == 'None'): \n\t \t \tsample_filter.user = None \n\t \tquery = query.filter((models.Resource.user_id == sample_filter.user)) \n\tif sample_filter.project: \n\t \tif (sample_filter.project == 'None'): \n\t \t \tsample_filter.project = None \n\t \tquery = query.filter((models.Resource.project_id == sample_filter.project)) \n\tif sample_filter.resource: \n\t \tquery = query.filter((models.Resource.resource_id == sample_filter.resource)) \n\tif sample_filter.message_id: \n\t \tquery = query.filter((models.Sample.message_id == sample_filter.message_id)) \n\tif sample_filter.metaquery: \n\t \tquery = apply_metaquery_filter(session, query, sample_filter.metaquery) \n\treturn query\n", 
" \twith _ignore_deprecated_imports(deprecated): \n\t \ttry: \n\t \t \treturn importlib.import_module(name) \n\t \texcept ImportError as msg: \n\t \t \traise unittest.SkipTest(str(msg))\n", 
" \tmsg = {'source': sample.source, 'counter_name': sample.name, 'counter_type': sample.type, 'counter_unit': sample.unit, 'counter_volume': sample.volume, 'user_id': sample.user_id, 'project_id': sample.project_id, 'resource_id': sample.resource_id, 'timestamp': sample.timestamp, 'resource_metadata': sample.resource_metadata, 'message_id': sample.id} \n\tmsg['message_signature'] = compute_signature(msg, secret) \n\treturn msg\n", 
" \tif (not secret): \n\t \treturn '' \n\tif isinstance(secret, six.text_type): \n\t \tsecret = secret.encode('utf-8') \n\tdigest_maker = hmac.new(secret, '', hashlib.sha256) \n\tfor (name, value) in utils.recursive_keypairs(message): \n\t \tif (name == 'message_signature'): \n\t \t \tcontinue \n\t \tdigest_maker.update(six.text_type(name).encode('utf-8')) \n\t \tdigest_maker.update(six.text_type(value).encode('utf-8')) \n\treturn digest_maker.hexdigest()\n", 
" \tif (not secret): \n\t \treturn True \n\told_sig = message.get('message_signature', '') \n\tnew_sig = compute_signature(message, secret) \n\tif isinstance(old_sig, six.text_type): \n\t \ttry: \n\t \t \told_sig = old_sig.encode('ascii') \n\t \texcept UnicodeDecodeError: \n\t \t \treturn False \n\tif six.PY3: \n\t \tnew_sig = new_sig.encode('ascii') \n\treturn secretutils.constant_time_compare(new_sig, old_sig)\n", 
" \tmsg = {'source': sample.source, 'counter_name': sample.name, 'counter_type': sample.type, 'counter_unit': sample.unit, 'counter_volume': sample.volume, 'user_id': sample.user_id, 'project_id': sample.project_id, 'resource_id': sample.resource_id, 'timestamp': sample.timestamp, 'resource_metadata': sample.resource_metadata, 'message_id': sample.id} \n\tmsg['message_signature'] = compute_signature(msg, secret) \n\treturn msg\n", 
" \tparse_result = netutils.urlsplit(url) \n\tloaded_driver = driver.DriverManager(namespace, parse_result.scheme) \n\tif issubclass(loaded_driver.driver, ConfigPublisherBase): \n\t \treturn loaded_driver.driver(conf, parse_result) \n\telse: \n\t \treturn loaded_driver.driver(parse_result)\n", 
" \tif (auth_url or os.getenv('OS_AUTH_URL')): \n\t \tforce_strategy = 'keystone' \n\telse: \n\t \tforce_strategy = None \n\tcreds = {'username': (username or os.getenv('OS_AUTH_USER', os.getenv('OS_USERNAME'))), 'password': (password or os.getenv('OS_AUTH_KEY', os.getenv('OS_PASSWORD'))), 'tenant': (tenant or os.getenv('OS_AUTH_TENANT', os.getenv('OS_TENANT_NAME'))), 'auth_url': (auth_url or os.getenv('OS_AUTH_URL')), 'strategy': (force_strategy or auth_strategy or os.getenv('OS_AUTH_STRATEGY', 'noauth')), 'region': (region or os.getenv('OS_REGION_NAME'))} \n\tif ((creds['strategy'] == 'keystone') and (not creds['auth_url'])): \n\t \tmsg = _('--os_auth_url \toption \tor \tOS_AUTH_URL \tenvironment \tvariable \trequired \twhen \tkeystone \tauthentication \tstrategy \tis \tenabled\\n') \n\t \traise exception.ClientConfigurationError(msg) \n\treturn CacheClient(host=host, port=port, timeout=timeout, use_ssl=use_ssl, auth_token=(auth_token or os.getenv('OS_TOKEN')), creds=creds, insecure=insecure, configure_via_auth=False)\n", 
" \tnamespaced_key = '.'.join([namespace, key]) \n\treturn namespaced_key\n", 
" \tnamespaced_key = _make_namespaced_xattr_key(key) \n\ttry: \n\t \treturn xattr.getxattr(path, namespaced_key) \n\texcept IOError: \n\t \tif ('default' in kwargs): \n\t \t \treturn kwargs['default'] \n\t \telse: \n\t \t \traise\n", 
" \tnamespaced_key = _make_namespaced_xattr_key(key) \n\tif (not isinstance(value, six.binary_type)): \n\t \tvalue = str(value) \n\t \tif six.PY3: \n\t \t \tvalue = value.encode('utf-8') \n\txattr.setxattr(path, namespaced_key, value)\n", 
" \tcount = int(get_xattr(path, key)) \n\tcount += n \n\tset_xattr(path, key, str(count))\n", 
" \tdeserializer = wsgi.JSONRequestDeserializer() \n\tserializer = wsgi.JSONResponseSerializer() \n\treturn wsgi.Resource(Controller(), deserializer, serializer)\n", 
" \tif copy_dict: \n\t \tnew_image_meta = copy.copy(image_meta) \n\telse: \n\t \tnew_image_meta = image_meta \n\tnew_image_meta.pop('location', None) \n\tnew_image_meta.pop('location_data', None) \n\treturn new_image_meta\n", 
" \tdeserializer = rpc.RPCJSONDeserializer() \n\tserializer = rpc.RPCJSONSerializer() \n\treturn wsgi.Resource(Controller(), deserializer, serializer)\n", 
" \tfilename = 'schema-image.json' \n\tmatch = CONF.find_file(filename) \n\tif match: \n\t \twith open(match, 'r') as schema_file: \n\t \t \tschema_data = schema_file.read() \n\t \treturn json.loads(schema_data) \n\telse: \n\t \tmsg = (_LW('Could \tnot \tfind \tschema \tproperties \tfile \t%s. \tContinuing \twithout \tcustom \tproperties') % filename) \n\t \tLOG.warn(msg) \n\t \treturn {}\n", 
" \tdeserializer = rpc.RPCJSONDeserializer() \n\tserializer = rpc.RPCJSONSerializer() \n\treturn wsgi.Resource(Controller(), deserializer, serializer)\n", 
" \tdeserializer = wsgi.JSONRequestDeserializer() \n\tserializer = wsgi.JSONResponseSerializer() \n\treturn wsgi.Resource(Controller(), deserializer, serializer)\n", 
" \tdeserializer = RequestDeserializer() \n\tserializer = ResponseSerializer() \n\tcontroller = ImageDataController() \n\treturn wsgi.Resource(controller, deserializer, serializer)\n", 
" \tdeserializer = rpc.RPCJSONDeserializer() \n\tserializer = rpc.RPCJSONSerializer() \n\treturn wsgi.Resource(Controller(), deserializer, serializer)\n", 
" \tdeserializer = CachedImageDeserializer() \n\tserializer = CachedImageSerializer() \n\treturn wsgi.Resource(Controller(), deserializer, serializer)\n", 
" \ttry: \n\t \tcontext = request.context \n\t \tpayload = {'bytes_sent': bytes_written, 'image_id': image_meta['id'], 'owner_id': image_meta['owner'], 'receiver_tenant_id': context.tenant, 'receiver_user_id': context.user, 'destination_ip': request.remote_addr} \n\t \tif (bytes_written != expected_size): \n\t \t \tnotify = notifier.error \n\t \telse: \n\t \t \tnotify = notifier.info \n\t \tnotify('image.send', payload) \n\texcept Exception as err: \n\t \tmsg = (_LE('An \terror \toccurred \tduring \timage.send \tnotification: \t%(err)s') % {'err': err}) \n\t \tLOG.error(msg)\n", 
" \tif context.is_admin: \n\t \treturn True \n\tif ((image['owner'] is None) or (context.owner is None)): \n\t \treturn False \n\treturn (image['owner'] == context.owner)\n", 
" \tif context.is_admin: \n\t \treturn True \n\tif ((image['owner'] is None) or (context.owner is None)): \n\t \treturn False \n\treturn (image['owner'] == context.owner)\n", 
" \ttry: \n\t \tuuid.UUID(value) \n\t \treturn value \n\texcept (ValueError, AttributeError): \n\t \treturn int(value)\n", 
" \tlogger = logging.getLogger() \n\tloglevel = get_loglevel((loglevel or u'ERROR')) \n\tlogfile = (logfile if logfile else sys.__stderr__) \n\tif (not logger.handlers): \n\t \tif hasattr(logfile, u'write'): \n\t \t \thandler = logging.StreamHandler(logfile) \n\t \telse: \n\t \t \thandler = WatchedFileHandler(logfile) \n\t \tlogger.addHandler(handler) \n\t \tlogger.setLevel(loglevel) \n\treturn logger\n", 
" \treturn datetime.datetime.strptime(d['isostr'], _DATETIME_FORMAT)\n", 
" \treturn _aliases.get(alias.lower(), alias)\n", 
" \twith open(CHANGELOG) as f: \n\t \tcur_index = 0 \n\t \tfor line in f: \n\t \t \tmatch = re.search('^\\\\d+\\\\.\\\\d+\\\\.\\\\d+', line) \n\t \t \tif match: \n\t \t \t \tif (cur_index == index): \n\t \t \t \t \treturn match.group(0) \n\t \t \t \telse: \n\t \t \t \t \tcur_index += 1\n", 
" \tgitstr = _git_str() \n\tif (gitstr is None): \n\t \tgitstr = '' \n\tpath = os.path.join(BASEDIR, 'qutebrowser', 'git-commit-id') \n\twith _open(path, 'w', encoding='ascii') as f: \n\t \tf.write(gitstr)\n", 
" \tif (len(sys.argv) < 2): \n\t \treturn True \n\tinfo_commands = ['--help-commands', '--name', '--version', '-V', '--fullname', '--author', '--author-email', '--maintainer', '--maintainer-email', '--contact', '--contact-email', '--url', '--license', '--description', '--long-description', '--platforms', '--classifiers', '--keywords', '--provides', '--requires', '--obsoletes'] \n\tinfo_commands.extend(['egg_info', 'install_egg_info', 'rotate']) \n\tfor command in info_commands: \n\t \tif (command in sys.argv[1:]): \n\t \t \treturn False \n\treturn True\n", 
" \trefs = list_refs(refnames=[refname], repo_dir=repo_dir, limit_to_heads=True) \n\tl = tuple(islice(refs, 2)) \n\tif l: \n\t \tassert (len(l) == 1) \n\t \treturn l[0][1] \n\telse: \n\t \treturn None\n", 
" \tif isinstance(fileIshObject, TextIOBase): \n\t \treturn unicode \n\tif isinstance(fileIshObject, IOBase): \n\t \treturn bytes \n\tencoding = getattr(fileIshObject, 'encoding', None) \n\timport codecs \n\tif isinstance(fileIshObject, (codecs.StreamReader, codecs.StreamWriter)): \n\t \tif encoding: \n\t \t \treturn unicode \n\t \telse: \n\t \t \treturn bytes \n\tif (not _PY3): \n\t \tif isinstance(fileIshObject, file): \n\t \t \tif (encoding is not None): \n\t \t \t \treturn basestring \n\t \t \telse: \n\t \t \t \treturn bytes \n\t \tfrom cStringIO import InputType, OutputType \n\t \tfrom StringIO import StringIO \n\t \tif isinstance(fileIshObject, (StringIO, InputType, OutputType)): \n\t \t \treturn bytes \n\treturn default\n", 
" \tcmd = (_pkg(jail, chroot, root) + ['--version']) \n\treturn __salt__['cmd.run'](cmd).strip()\n", 
" \tcmd = (_pkg(jail, chroot, root) + ['--version']) \n\treturn __salt__['cmd.run'](cmd).strip()\n", 
" \tconn = _get_conn(region=region, key=key, keyid=keyid, profile=profile) \n\tpolicy_arn = _get_policy_arn(policy_name, region, key, keyid, profile) \n\ttry: \n\t \tconn.set_default_policy_version(policy_arn, version_id) \n\t \tlog.info('Set \t{0} \tpolicy \tto \tversion \t{1}.'.format(policy_name, version_id)) \n\texcept boto.exception.BotoServerError as e: \n\t \taws = __utils__['boto.get_error'](e) \n\t \tlog.debug(aws) \n\t \tmsg = 'Failed \tto \tset \t{0} \tpolicy \tto \tversion \t{1}: \t{2}' \n\t \tlog.error(msg.format(policy_name, version_id, aws.get('message'))) \n\t \treturn False \n\treturn True\n", 
" \tvalidate = attr['validate'] \n\tkey = [k for k in validate.keys() if k.startswith('type:dict')] \n\tif (not key): \n\t \tLOG.warning(_LW('Unable \tto \tfind \tdata \ttype \tdescriptor \tfor \tattribute \t%s'), attr_name) \n\t \treturn \n\tdata = validate[key[0]] \n\tif (not isinstance(data, dict)): \n\t \tLOG.debug('Attribute \ttype \tdescriptor \tis \tnot \ta \tdict. \tUnable \tto \tgenerate \tany \tsub-attr \tpolicy \trule \tfor \t%s.', attr_name) \n\t \treturn \n\tsub_attr_rules = [policy.RuleCheck('rule', ('%s:%s:%s' % (action, attr_name, sub_attr_name))) for sub_attr_name in data if (sub_attr_name in target[attr_name])] \n\treturn policy.AndCheck(sub_attr_rules)\n", 
" \trule_method = ('telemetry:' + policy_name) \n\theaders = request.headers \n\tpolicy_dict = dict() \n\tpolicy_dict['roles'] = headers.get('X-Roles', '').split(',') \n\tpolicy_dict['user_id'] = headers.get('X-User-Id') \n\tpolicy_dict['project_id'] = headers.get('X-Project-Id') \n\tif ((_has_rule('default') or _has_rule(rule_method)) and (not pecan.request.enforcer.enforce(rule_method, {}, policy_dict))): \n\t \tpecan.core.abort(status_code=403, detail='RBAC \tAuthorization \tFailed')\n", 
" \ttry: \n\t \ttree_gen = parse(file, format, **kwargs) \n\t \ttree = next(tree_gen) \n\texcept StopIteration: \n\t \traise ValueError('There \tare \tno \ttrees \tin \tthis \tfile.') \n\ttry: \n\t \tnext(tree_gen) \n\texcept StopIteration: \n\t \treturn tree \n\telse: \n\t \traise ValueError('There \tare \tmultiple \ttrees \tin \tthis \tfile; \tuse \tparse() \tinstead.')\n", 
" \tfrom django.contrib.syndication.feeds import Feed as LegacyFeed \n\timport warnings \n\twarnings.warn('The \tsyndication \tfeed() \tview \tis \tdeprecated. \tPlease \tuse \tthe \tnew \tclass \tbased \tview \tAPI.', category=PendingDeprecationWarning) \n\tif (not feed_dict): \n\t \traise Http404('No \tfeeds \tare \tregistered.') \n\ttry: \n\t \t(slug, param) = url.split('/', 1) \n\texcept ValueError: \n\t \t(slug, param) = (url, '') \n\ttry: \n\t \tf = feed_dict[slug] \n\texcept KeyError: \n\t \traise Http404((\"Slug \t%r \tisn't \tregistered.\" % slug)) \n\tif (not issubclass(f, LegacyFeed)): \n\t \tinstance = f() \n\t \tinstance.feed_url = (getattr(f, 'feed_url', None) or request.path) \n\t \tinstance.title_template = (f.title_template or ('feeds/%s_title.html' % slug)) \n\t \tinstance.description_template = (f.description_template or ('feeds/%s_description.html' % slug)) \n\t \treturn instance(request) \n\ttry: \n\t \tfeedgen = f(slug, request).get_feed(param) \n\texcept FeedDoesNotExist: \n\t \traise Http404(('Invalid \tfeed \tparameters. \tSlug \t%r \tis \tvalid, \tbut \tother \tparameters, \tor \tlack \tthereof, \tare \tnot.' % slug)) \n\tresponse = HttpResponse(mimetype=feedgen.mime_type) \n\tfeedgen.write(response, 'utf-8') \n\treturn response\n", 
" \tret = {} \n\tcmd = __execute_kadmin('list_policies') \n\tif ((cmd['retcode'] != 0) or cmd['stderr']): \n\t \tret['comment'] = cmd['stderr'].splitlines()[(-1)] \n\t \tret['result'] = False \n\t \treturn ret \n\tret = {'policies': []} \n\tfor i in cmd['stdout'].splitlines()[1:]: \n\t \tret['policies'].append(i) \n\treturn ret\n", 
" \tdef call_and_assert(arg, context=None): \n\t \tif (context is None): \n\t \t \tcontext = {} \n\t \tresult = function(arg, context=context) \n\t \tassert (result == arg), 'Should \treturn \tthe \targument \tthat \twas \tpassed \tto \tit, \tunchanged \t({arg})'.format(arg=repr(arg)) \n\t \treturn result \n\treturn call_and_assert\n", 
" \tif (not app_messages): \n\t \tapp_messages = get_messages_for_app(app) \n\tif (not app_messages): \n\t \treturn \n\ttpath = frappe.get_pymodule_path(app, u'translations') \n\tfrappe.create_folder(tpath) \n\twrite_csv_file(os.path.join(tpath, (lang + u'.csv')), app_messages, (full_dict or get_full_dict(lang)))\n", 
" \ttb = treebuilders.getTreeBuilder(treebuilder) \n\tp = HTMLParser(tb, namespaceHTMLElements=namespaceHTMLElements) \n\treturn p.parse(doc, encoding=encoding)\n", 
" \tfrom config import get_os, get_checks_places, get_valid_check_class \n\tosname = get_os() \n\tchecks_places = get_checks_places(osname, agentConfig) \n\tfor check_path_builder in checks_places: \n\t \tcheck_path = check_path_builder(check_name) \n\t \tif (not os.path.exists(check_path)): \n\t \t \tcontinue \n\t \t(check_is_valid, check_class, load_failure) = get_valid_check_class(check_name, check_path) \n\t \tif check_is_valid: \n\t \t \treturn check_class \n\tlog.warning(('Failed \tto \tload \tthe \tcheck \tclass \tfor \t%s.' % check_name)) \n\treturn None\n", 
" \treturn auth_is_loggedin_user()\n", 
" \tif ((r.representation == 'html') and (r.name == 'shelter') and r.id and (not r.component)): \n\t \tT = current.T \n\t \tmsg = current.msg \n\t \ts3db = current.s3db \n\t \trecord = r.record \n\t \tctable = s3db.pr_contact \n\t \tstable = s3db.cr_shelter \n\t \tmessage = '' \n\t \ttext = '' \n\t \ts_id = record.id \n\t \ts_name = record.name \n\t \ts_phone = record.phone \n\t \ts_email = record.email \n\t \ts_status = record.status \n\t \tif (s_phone in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_email in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_status in ('', None)): \n\t \t \ts_status = T('Not \tDefined') \n\t \telif (s_status == 1): \n\t \t \ts_status = 'Open' \n\t \telif (s_status == 2): \n\t \t \ts_status = 'Close' \n\t \telse: \n\t \t \ts_status = 'Unassigned \tShelter \tStatus' \n\t \ttext += '************************************************' \n\t \ttext += ('\\n%s \t' % T('Automatic \tMessage')) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Shelter \tID'), s_id)) \n\t \ttext += (' \t%s: \t%s' % (T('Shelter \tname'), s_name)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Email'), s_email)) \n\t \ttext += (' \t%s: \t%s' % (T('Phone'), s_phone)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Working \tStatus'), s_status)) \n\t \ttext += '\\n************************************************\\n' \n\t \turl = URL(c='cr', f='shelter', args=r.id) \n\t \topts = dict(type='SMS', subject=T('Deployment \tRequest'), message=(message + text), url=url) \n\t \toutput = msg.compose(**opts) \n\t \tif attr.get('rheader'): \n\t \t \trheader = attr['rheader'](r) \n\t \t \tif rheader: \n\t \t \t \toutput['rheader'] = rheader \n\t \toutput['title'] = T('Send \tNotification') \n\t \tcurrent.response.view = 'msg/compose.html' \n\t \treturn output \n\telse: \n\t \traise HTTP(501, current.messages.BADMETHOD)\n", 
" \tcan_users_receive_email = email_manager.can_users_receive_thread_email(recipient_list, exploration_id, has_suggestion) \n\tfor (index, recipient_id) in enumerate(recipient_list): \n\t \tif can_users_receive_email[index]: \n\t \t \ttransaction_services.run_in_transaction(_enqueue_feedback_thread_status_change_email_task, recipient_id, feedback_message_reference, old_status, new_status)\n", 
" \tglobal _notifier \n\tif (_notifier is None): \n\t \thost = (CONF.default_publisher_id or socket.gethostname()) \n\t \ttry: \n\t \t \ttransport = oslo_messaging.get_notification_transport(CONF) \n\t \t \t_notifier = oslo_messaging.Notifier(transport, ('identity.%s' % host)) \n\t \texcept Exception: \n\t \t \tLOG.exception(_LE('Failed \tto \tconstruct \tnotifier')) \n\t \t \t_notifier = False \n\treturn _notifier\n", 
" \tcan_users_receive_email = email_manager.can_users_receive_thread_email(recipient_list, exploration_id, has_suggestion) \n\tfor (index, recipient_id) in enumerate(recipient_list): \n\t \tif can_users_receive_email[index]: \n\t \t \ttransaction_services.run_in_transaction(_enqueue_feedback_thread_status_change_email_task, recipient_id, feedback_message_reference, old_status, new_status)\n", 
" \tdef wrapped_func(*args, **kwarg): \n\t \tCALLED_FUNCTION.append(name) \n\t \treturn function(*args, **kwarg) \n\treturn wrapped_func\n", 
" \tlevel = notification.get_default_level() \n\tif (stack.status == stack.IN_PROGRESS): \n\t \tsuffix = 'start' \n\telif (stack.status == stack.COMPLETE): \n\t \tsuffix = 'end' \n\telse: \n\t \tsuffix = 'error' \n\t \tlevel = notification.ERROR \n\tevent_type = ('%s.%s.%s' % ('stack', stack.action.lower(), suffix)) \n\tnotification.notify(stack.context, event_type, level, engine_api.format_notification_body(stack))\n", 
" \ttry: \n\t \tdriver = _DRIVERS[driver_name] \n\texcept KeyError: \n\t \traise DriverNotFoundError(('No \tdriver \tfor \t%s' % driver_name)) \n\treturn driver(*args, **kwargs)\n", 
" \tlevel = notification.get_default_level() \n\tif (stack.status == stack.IN_PROGRESS): \n\t \tsuffix = 'start' \n\telif (stack.status == stack.COMPLETE): \n\t \tsuffix = 'end' \n\telse: \n\t \tsuffix = 'error' \n\t \tlevel = notification.ERROR \n\tevent_type = ('%s.%s.%s' % ('stack', stack.action.lower(), suffix)) \n\tnotification.notify(stack.context, event_type, level, engine_api.format_notification_body(stack))\n", 
" \tseed = (pseed or config.unittests.rseed) \n\tif (seed == 'random'): \n\t \tseed = None \n\ttry: \n\t \tif seed: \n\t \t \tseed = int(seed) \n\t \telse: \n\t \t \tseed = None \n\texcept ValueError: \n\t \tprint('Error: \tconfig.unittests.rseed \tcontains \tinvalid \tseed, \tusing \tNone \tinstead', file=sys.stderr) \n\t \tseed = None \n\treturn seed\n", 
" \tif ((r.representation == 'html') and (r.name == 'shelter') and r.id and (not r.component)): \n\t \tT = current.T \n\t \tmsg = current.msg \n\t \ts3db = current.s3db \n\t \trecord = r.record \n\t \tctable = s3db.pr_contact \n\t \tstable = s3db.cr_shelter \n\t \tmessage = '' \n\t \ttext = '' \n\t \ts_id = record.id \n\t \ts_name = record.name \n\t \ts_phone = record.phone \n\t \ts_email = record.email \n\t \ts_status = record.status \n\t \tif (s_phone in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_email in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_status in ('', None)): \n\t \t \ts_status = T('Not \tDefined') \n\t \telif (s_status == 1): \n\t \t \ts_status = 'Open' \n\t \telif (s_status == 2): \n\t \t \ts_status = 'Close' \n\t \telse: \n\t \t \ts_status = 'Unassigned \tShelter \tStatus' \n\t \ttext += '************************************************' \n\t \ttext += ('\\n%s \t' % T('Automatic \tMessage')) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Shelter \tID'), s_id)) \n\t \ttext += (' \t%s: \t%s' % (T('Shelter \tname'), s_name)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Email'), s_email)) \n\t \ttext += (' \t%s: \t%s' % (T('Phone'), s_phone)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Working \tStatus'), s_status)) \n\t \ttext += '\\n************************************************\\n' \n\t \turl = URL(c='cr', f='shelter', args=r.id) \n\t \topts = dict(type='SMS', subject=T('Deployment \tRequest'), message=(message + text), url=url) \n\t \toutput = msg.compose(**opts) \n\t \tif attr.get('rheader'): \n\t \t \trheader = attr['rheader'](r) \n\t \t \tif rheader: \n\t \t \t \toutput['rheader'] = rheader \n\t \toutput['title'] = T('Send \tNotification') \n\t \tcurrent.response.view = 'msg/compose.html' \n\t \treturn output \n\telse: \n\t \traise HTTP(501, current.messages.BADMETHOD)\n", 
" \ttry: \n\t \t(module, attr) = name.rsplit('.', 1) \n\texcept ValueError as error: \n\t \traise ImproperlyConfigured(('Error \timporting \tclass \t%s \tin \t%s: \t\"%s\"' % (name, setting, error))) \n\ttry: \n\t \tmod = import_module(module) \n\texcept ImportError as error: \n\t \traise ImproperlyConfigured(('Error \timporting \tmodule \t%s \tin \t%s: \t\"%s\"' % (module, setting, error))) \n\ttry: \n\t \tcls = getattr(mod, attr) \n\texcept AttributeError: \n\t \traise ImproperlyConfigured(('Module \t\"%s\" \tdoes \tnot \tdefine \ta \t\"%s\" \tclass \tin \t%s' % (module, attr, setting))) \n\treturn cls\n", 
" \tos.environ['LANG'] = 'C' \n\tcli = _DRIVERS.get('HORCM') \n\treturn importutils.import_object('.'.join([_DRIVER_DIR, cli[driver_info['proto']]]), conf, driver_info, db)\n", 
" \tos.environ['LANG'] = 'C' \n\tcli = _DRIVERS.get('HORCM') \n\treturn importutils.import_object('.'.join([_DRIVER_DIR, cli[driver_info['proto']]]), conf, driver_info, db)\n", 
" \tif name.startswith('.'): \n\t \tif (not package): \n\t \t \traise TypeError(\"relative \timports \trequire \tthe \t'package' \targument\") \n\t \tlevel = 0 \n\t \tfor character in name: \n\t \t \tif (character != '.'): \n\t \t \t \tbreak \n\t \t \tlevel += 1 \n\t \tname = _resolve_name(name[level:], package, level) \n\t__import__(name) \n\treturn sys.modules[name]\n", 
" \ttry: \n\t \treturn namedModule(name) \n\texcept ImportError: \n\t \treturn default\n", 
" \tif (not at): \n\t \tat = utcnow() \n\tst = at.strftime((_ISO8601_TIME_FORMAT if (not subsecond) else _ISO8601_TIME_FORMAT_SUBSECOND)) \n\ttz = (at.tzinfo.tzname(None) if at.tzinfo else 'UTC') \n\tst += ('Z' if (tz == 'UTC') else tz) \n\treturn st\n", 
" \ttry: \n\t \treturn iso8601.parse_date(timestr) \n\texcept iso8601.ParseError as e: \n\t \traise ValueError(encodeutils.exception_to_unicode(e)) \n\texcept TypeError as e: \n\t \traise ValueError(encodeutils.exception_to_unicode(e))\n", 
" \tnow = datetime.utcnow().replace(tzinfo=iso8601.iso8601.UTC) \n\tif iso: \n\t \treturn datetime_tuple_to_iso(now) \n\telse: \n\t \treturn now\n", 
" \treturn formatdate(timegm(dt.utctimetuple()))\n", 
" \toffset = timestamp.utcoffset() \n\tif (offset is None): \n\t \treturn timestamp \n\treturn (timestamp.replace(tzinfo=None) - offset)\n", 
" \tif os.path.isfile(db_file_name): \n\t \tfrom datetime import timedelta \n\t \tfile_datetime = datetime.fromtimestamp(os.stat(db_file_name).st_ctime) \n\t \tif ((datetime.today() - file_datetime) >= timedelta(hours=older_than)): \n\t \t \tif verbose: \n\t \t \t \tprint u'File \tis \told' \n\t \t \treturn True \n\t \telse: \n\t \t \tif verbose: \n\t \t \t \tprint u'File \tis \trecent' \n\t \t \treturn False \n\telse: \n\t \tif verbose: \n\t \t \tprint u'File \tdoes \tnot \texist' \n\t \treturn True\n", 
" \tif (not os.path.exists(source)): \n\t \traise DistutilsFileError((\"file \t'%s' \tdoes \tnot \texist\" % os.path.abspath(source))) \n\tif (not os.path.exists(target)): \n\t \treturn True \n\treturn (os.stat(source)[ST_MTIME] > os.stat(target)[ST_MTIME])\n", 
" \tif utcnow.override_time: \n\t \ttry: \n\t \t \treturn utcnow.override_time.pop(0) \n\t \texcept AttributeError: \n\t \t \treturn utcnow.override_time \n\tif with_timezone: \n\t \treturn datetime.datetime.now(tz=iso8601.iso8601.UTC) \n\treturn datetime.datetime.utcnow()\n", 
" \tif utcnow.override_time: \n\t \ttry: \n\t \t \treturn utcnow.override_time.pop(0) \n\t \texcept AttributeError: \n\t \t \treturn utcnow.override_time \n\tif with_timezone: \n\t \treturn datetime.datetime.now(tz=iso8601.iso8601.UTC) \n\treturn datetime.datetime.utcnow()\n", 
" \treturn isotime(datetime.datetime.utcfromtimestamp(timestamp), microsecond)\n", 
" \tif utcnow.override_time: \n\t \ttry: \n\t \t \treturn utcnow.override_time.pop(0) \n\t \texcept AttributeError: \n\t \t \treturn utcnow.override_time \n\tif with_timezone: \n\t \treturn datetime.datetime.now(tz=iso8601.iso8601.UTC) \n\treturn datetime.datetime.utcnow()\n", 
" \tmatch = standard_duration_re.match(value) \n\tif (not match): \n\t \tmatch = iso8601_duration_re.match(value) \n\tif match: \n\t \tkw = match.groupdict() \n\t \tsign = ((-1) if (kw.pop('sign', '+') == '-') else 1) \n\t \tif kw.get('microseconds'): \n\t \t \tkw['microseconds'] = kw['microseconds'].ljust(6, '0') \n\t \tif (kw.get('seconds') and kw.get('microseconds') and kw['seconds'].startswith('-')): \n\t \t \tkw['microseconds'] = ('-' + kw['microseconds']) \n\t \tkw = {k: float(v) for (k, v) in kw.items() if (v is not None)} \n\t \treturn (sign * datetime.timedelta(**kw))\n", 
" \treturn call_talib_with_ohlc(barDs, count, talib.CDLADVANCEBLOCK)\n", 
" \ttry: \n\t \tparent = next(klass.local_attr_ancestors(name)) \n\texcept (StopIteration, KeyError): \n\t \treturn None \n\ttry: \n\t \tmeth_node = parent[name] \n\texcept KeyError: \n\t \treturn None \n\tif isinstance(meth_node, astroid.Function): \n\t \treturn meth_node \n\treturn None\n", 
" \tif value.tzinfo: \n\t \tvalue = value.astimezone(UTC) \n\treturn (long((calendar.timegm(value.timetuple()) * 1000000L)) + value.microsecond)\n", 
" \t(p, u) = getparser(use_datetime=use_datetime, use_builtin_types=use_builtin_types) \n\tp.feed(data) \n\tp.close() \n\treturn (u.close(), u.getmethodname())\n", 
" \tlater = (mktime(date1.timetuple()) + (date1.microsecond / 1000000.0)) \n\tearlier = (mktime(date2.timetuple()) + (date2.microsecond / 1000000.0)) \n\treturn (later - earlier)\n", 
" \tgenerate_completions(event)\n", 
" \t@functools.wraps(func) \n\tdef wrapped(self, *args, **kwargs): \n\t \ttry: \n\t \t \treturn func(self, *args, **kwargs) \n\t \texcept exception.NotAuthenticated: \n\t \t \tself._authenticate(force_reauth=True) \n\t \t \treturn func(self, *args, **kwargs) \n\treturn wrapped\n", 
" \tMAX_REDIRECTS = 5 \n\t@functools.wraps(func) \n\tdef wrapped(self, method, url, body, headers): \n\t \tfor i in range(MAX_REDIRECTS): \n\t \t \ttry: \n\t \t \t \treturn func(self, method, url, body, headers) \n\t \t \texcept exception.RedirectException as redirect: \n\t \t \t \tif (redirect.url is None): \n\t \t \t \t \traise exception.InvalidRedirect() \n\t \t \t \turl = redirect.url \n\t \traise exception.MaxRedirectsExceeded(redirects=MAX_REDIRECTS) \n\treturn wrapped\n", 
" \treturn (conf.bind_host, (conf.bind_port or default_port))\n", 
" \tbind_addr = get_bind_addr(conf, default_port) \n\taddress_family = [addr[0] for addr in socket.getaddrinfo(bind_addr[0], bind_addr[1], socket.AF_UNSPEC, socket.SOCK_STREAM) if (addr[0] in (socket.AF_INET, socket.AF_INET6))][0] \n\tcert_file = conf.cert_file \n\tkey_file = conf.key_file \n\tuse_ssl = (cert_file or key_file) \n\tif (use_ssl and ((not cert_file) or (not key_file))): \n\t \traise RuntimeError(_('When \trunning \tserver \tin \tSSL \tmode, \tyou \tmust \tspecify \tboth \ta \tcert_file \tand \tkey_file \toption \tvalue \tin \tyour \tconfiguration \tfile')) \n\tsock = None \n\tretry_until = (time.time() + 30) \n\twhile ((not sock) and (time.time() < retry_until)): \n\t \ttry: \n\t \t \tsock = eventlet.listen(bind_addr, backlog=conf.backlog, family=address_family) \n\t \texcept socket.error as err: \n\t \t \tif (err.args[0] != errno.EADDRINUSE): \n\t \t \t \traise \n\t \t \teventlet.sleep(0.1) \n\tif (not sock): \n\t \traise RuntimeError((_('Could \tnot \tbind \tto \t%(bind_addr)safter \ttrying \tfor \t30 \tseconds') % {'bind_addr': bind_addr})) \n\treturn sock\n", 
" \treturn (chunkiter(iter, chunk_size) if hasattr(iter, 'read') else iter)\n", 
" \twhile True: \n\t \tchunk = fp.read(chunk_size) \n\t \tif chunk: \n\t \t \t(yield chunk) \n\t \telse: \n\t \t \tbreak\n", 
" \ttry: \n\t \tfor chunk in iter: \n\t \t \tsleep(0) \n\t \t \t(yield chunk) \n\texcept Exception as err: \n\t \twith excutils.save_and_reraise_exception(): \n\t \t \tmsg = (_LE('Error: \tcooperative_iter \texception \t%s') % err) \n\t \t \tLOG.error(msg)\n", 
" \tdef readfn(*args): \n\t \tresult = fd.read(*args) \n\t \tsleep(0) \n\t \treturn result \n\treturn readfn\n", 
" \theaders = {} \n\tfor (k, v) in image_meta.items(): \n\t \tif (v is not None): \n\t \t \tif (k == 'properties'): \n\t \t \t \tfor (pk, pv) in v.items(): \n\t \t \t \t \tif (pv is not None): \n\t \t \t \t \t \theaders[('x-image-meta-property-%s' % pk.lower())] = six.text_type(pv) \n\t \t \telse: \n\t \t \t \theaders[('x-image-meta-%s' % k.lower())] = six.text_type(v) \n\treturn headers\n", 
" \tdb = current.db \n\ts3db = current.s3db \n\tcache = s3db.cache \n\trequest = current.request \n\tcontroller = request.controller \n\tfunction = request.function \n\tfqtable = s3db.gis_feature_query \n\tmtable = s3db.gis_marker \n\tauth = current.auth \n\tauth_user = auth.user \n\tif auth_user: \n\t \tcreated_by = auth_user.id \n\t \ts3_make_session_owner = auth.s3_make_session_owner \n\telse: \n\t \tcreated_by = None \n\tlayers_feature_query = [] \n\tappend = layers_feature_query.append \n\tfor layer in feature_queries: \n\t \tname = str(layer['name']) \n\t \t_layer = dict(name=name) \n\t \tname_safe = re.sub('\\\\W', '_', name) \n\t \ttry: \n\t \t \tlayer['query'][0].gis_location.lat \n\t \t \tjoin = True \n\t \texcept: \n\t \t \tjoin = False \n\t \tcname = ('%s_%s_%s' % (name_safe, controller, function)) \n\t \tquery = ((fqtable.name == cname) & (fqtable.created_by == created_by)) \n\t \tdb(query).delete() \n\t \tfor row in layer['query']: \n\t \t \trowdict = {'name': cname} \n\t \t \tif join: \n\t \t \t \trowdict['lat'] = row.gis_location.lat \n\t \t \t \trowdict['lon'] = row.gis_location.lon \n\t \t \telse: \n\t \t \t \trowdict['lat'] = row['lat'] \n\t \t \t \trowdict['lon'] = row['lon'] \n\t \t \tif ('popup_url' in row): \n\t \t \t \trowdict['popup_url'] = row['popup_url'] \n\t \t \tif ('popup_label' in row): \n\t \t \t \trowdict['popup_label'] = row['popup_label'] \n\t \t \tif ('marker' in row): \n\t \t \t \trowdict['marker_url'] = URL(c='static', f='img', args=['markers', row['marker'].image]) \n\t \t \t \trowdict['marker_height'] = row['marker'].height \n\t \t \t \trowdict['marker_width'] = row['marker'].width \n\t \t \telse: \n\t \t \t \tif ('marker_url' in row): \n\t \t \t \t \trowdict['marker_url'] = row['marker_url'] \n\t \t \t \tif ('marker_height' in row): \n\t \t \t \t \trowdict['marker_height'] = row['marker_height'] \n\t \t \t \tif ('marker_width' in row): \n\t \t \t \t \trowdict['marker_width'] = row['marker_width'] \n\t \t \tif ('shape' in row): \n\t \t \t \trowdict['shape'] = row['shape'] \n\t \t \tif ('size' in row): \n\t \t \t \trowdict['size'] = row['size'] \n\t \t \tif ('colour' in row): \n\t \t \t \trowdict['colour'] = row['colour'] \n\t \t \tif ('opacity' in row): \n\t \t \t \trowdict['opacity'] = row['opacity'] \n\t \t \trecord_id = fqtable.insert(**rowdict) \n\t \t \tif (not created_by): \n\t \t \t \ts3_make_session_owner(fqtable, record_id) \n\t \turl = ('%s.geojson?feature_query.name=%s&feature_query.created_by=%s' % (URL(c='gis', f='feature_query'), cname, created_by)) \n\t \t_layer['url'] = url \n\t \tif (('active' in layer) and (not layer['active'])): \n\t \t \t_layer['visibility'] = False \n\t \tif ('marker' in layer): \n\t \t \tmarker = layer['marker'] \n\t \t \tif isinstance(marker, int): \n\t \t \t \tmarker = db((mtable.id == marker)).select(mtable.image, mtable.height, mtable.width, limitby=(0, 1), cache=cache).first() \n\t \t \tif marker: \n\t \t \t \t_layer['marker_url'] = marker['image'] \n\t \t \t \t_layer['marker_height'] = marker['height'] \n\t \t \t \t_layer['marker_width'] = marker['width'] \n\t \tif (('opacity' in layer) and (layer['opacity'] != 1)): \n\t \t \t_layer['opacity'] = ('%.1f' % layer['opacity']) \n\t \tif (('cluster_attribute' in layer) and (layer['cluster_attribute'] != CLUSTER_ATTRIBUTE)): \n\t \t \t_layer['cluster_attribute'] = layer['cluster_attribute'] \n\t \tif (('cluster_distance' in layer) and (layer['cluster_distance'] != CLUSTER_DISTANCE)): \n\t \t \t_layer['cluster_distance'] = layer['cluster_distance'] \n\t \tif (('cluster_threshold' in layer) and (layer['cluster_threshold'] != CLUSTER_THRESHOLD)): \n\t \t \t_layer['cluster_threshold'] = layer['cluster_threshold'] \n\t \tappend(_layer) \n\treturn layers_feature_query\n", 
" \tresult = {} \n\tproperties = {} \n\tif hasattr(response, 'getheaders'): \n\t \theaders = response.getheaders() \n\telse: \n\t \theaders = response.headers.items() \n\tfor (key, value) in headers: \n\t \tkey = str(key.lower()) \n\t \tif key.startswith('x-image-meta-property-'): \n\t \t \tfield_name = key[len('x-image-meta-property-'):].replace('-', '_') \n\t \t \tproperties[field_name] = (value or None) \n\t \telif key.startswith('x-image-meta-'): \n\t \t \tfield_name = key[len('x-image-meta-'):].replace('-', '_') \n\t \t \tif (('x-image-meta-' + field_name) not in IMAGE_META_HEADERS): \n\t \t \t \tmsg = (_('Bad \theader: \t%(header_name)s') % {'header_name': key}) \n\t \t \t \traise exc.HTTPBadRequest(msg, content_type='text/plain') \n\t \t \tresult[field_name] = (value or None) \n\tresult['properties'] = properties \n\tfor (key, nullable) in [('size', False), ('min_disk', False), ('min_ram', False), ('virtual_size', True)]: \n\t \tif (key in result): \n\t \t \ttry: \n\t \t \t \tresult[key] = int(result[key]) \n\t \t \texcept ValueError: \n\t \t \t \tif (nullable and (result[key] == str(None))): \n\t \t \t \t \tresult[key] = None \n\t \t \t \telse: \n\t \t \t \t \textra = (_(\"Cannot \tconvert \timage \t%(key)s \t'%(value)s' \tto \tan \tinteger.\") % {'key': key, 'value': result[key]}) \n\t \t \t \t \traise exception.InvalidParameterValue(value=result[key], param=key, extra_msg=extra) \n\t \t \tif ((result[key] is not None) and (result[key] < 0)): \n\t \t \t \textra = _('Cannot \tbe \ta \tnegative \tvalue.') \n\t \t \t \traise exception.InvalidParameterValue(value=result[key], param=key, extra_msg=extra) \n\tfor key in ('is_public', 'deleted', 'protected'): \n\t \tif (key in result): \n\t \t \tresult[key] = strutils.bool_from_string(result[key]) \n\treturn result\n", 
" \tif isinstance(obj, str): \n\t \tobj = obj.strip().lower() \n\t \tif (obj in ('true', 'yes', 'on', 'y', 't', '1')): \n\t \t \treturn True \n\t \tif (obj in ('false', 'no', 'off', 'n', 'f', '0')): \n\t \t \treturn False \n\t \traise ValueError(('Unable \tto \tinterpret \tvalue \t\"%s\" \tas \tboolean' % obj)) \n\treturn bool(obj)\n", 
" \t@functools.wraps(func) \n\tdef wrapped(self, req, *args, **kwargs): \n\t \tif req.context.read_only: \n\t \t \tmsg = 'Read-only \taccess' \n\t \t \tLOG.debug(msg) \n\t \t \traise exc.HTTPForbidden(msg, request=req, content_type='text/plain') \n\t \treturn func(self, req, *args, **kwargs) \n\treturn wrapped\n", 
" \tdef pad(text): \n\t \t'\\n \t \t \t \t \t \t \t \tPads \ttext \tto \tbe \tencrypted\\n \t \t \t \t \t \t \t \t' \n\t \tpad_length = (blocksize - (len(text) % blocksize)) \n\t \tsr = random.StrongRandom() \n\t \tpad = ''.join((six.int2byte(sr.randint(1, 255)) for i in range((pad_length - 1)))) \n\t \treturn ((text + '\\x00') + pad) \n\tplaintext = encodeutils.to_utf8(plaintext) \n\tkey = encodeutils.to_utf8(key) \n\tinit_vector = Random.get_random_bytes(16) \n\tcypher = AES.new(key, AES.MODE_CBC, init_vector) \n\tpadded = cypher.encrypt(pad(six.binary_type(plaintext))) \n\tencoded = base64.urlsafe_b64encode((init_vector + padded)) \n\tif six.PY3: \n\t \tencoded = encoded.decode('ascii') \n\treturn encoded\n", 
" \tciphertext = encodeutils.to_utf8(ciphertext) \n\tkey = encodeutils.to_utf8(key) \n\tciphertext = base64.urlsafe_b64decode(ciphertext) \n\tcypher = AES.new(key, AES.MODE_CBC, ciphertext[:16]) \n\tpadded = cypher.decrypt(ciphertext[16:]) \n\ttext = padded[:padded.rfind('\\x00')] \n\tif six.PY3: \n\t \ttext = text.decode('utf-8') \n\treturn text\n", 
" \tendpoints = ks_service_catalog.ServiceCatalogV2({'serviceCatalog': service_catalog}).get_urls(service_type=service_type, region_name=endpoint_region, endpoint_type=endpoint_type) \n\tif (endpoints is None): \n\t \traise exception.NoServiceEndpoint() \n\telif (len(endpoints) == 1): \n\t \treturn endpoints[0] \n\telse: \n\t \traise exception.RegionAmbiguity(region=endpoint_region)\n", 
" \tproduct_name = 'neutron' \n\tlogging.set_defaults(default_log_levels=(logging.get_default_log_levels() + EXTRA_LOG_LEVEL_DEFAULTS)) \n\tlogging.setup(cfg.CONF, product_name) \n\tLOG.info(_LI('Logging \tenabled!')) \n\tLOG.info(_LI('%(prog)s \tversion \t%(version)s'), {'prog': sys.argv[0], 'version': version.version_info.release_string()}) \n\tLOG.debug('command \tline: \t%s', ' \t'.join(sys.argv))\n", 
" \tif (not flavor): \n\t \tflavor = CONF.paste_deploy.flavor \n\treturn ('' if (not flavor) else ('-' + flavor))\n", 
" \tpath = CONF.paste_deploy.config_file \n\tif (not path): \n\t \tpath = _get_paste_config_path() \n\tif (not path): \n\t \tmsg = (_('Unable \tto \tlocate \tpaste \tconfig \tfile \tfor \t%s.') % CONF.prog) \n\t \traise RuntimeError(msg) \n\treturn os.path.abspath(path)\n", 
" \tloader = wsgi.Loader(cfg.CONF) \n\tapp = loader.load_app(app_name) \n\treturn app\n", 
" \tdef _fetch_memb(memb, attr_map): \n\t \treturn {k: memb[v] for (k, v) in attr_map.items() if (v in memb.keys())} \n\treturn [_fetch_memb(memb, attr_map) for memb in members]\n", 
" \tdeserializer = wsgi.JSONRequestDeserializer() \n\tserializer = wsgi.JSONResponseSerializer() \n\treturn wsgi.Resource(Controller(), deserializer, serializer)\n", 
" \tdef _fetch_attrs(d, attrs): \n\t \treturn {a: d[a] for a in attrs if (a in d.keys())} \n\tproperties = {p['name']: p['value'] for p in image['properties'] if (not p['deleted'])} \n\timage_dict = _fetch_attrs(image, glance.db.IMAGE_ATTRS) \n\timage_dict['properties'] = properties \n\t_limit_locations(image_dict) \n\treturn image_dict\n", 
" \tdeserializer = rpc.RPCJSONDeserializer() \n\tserializer = rpc.RPCJSONSerializer() \n\treturn wsgi.Resource(Controller(), deserializer, serializer)\n", 
" \tglobal _CLIENT_KWARGS, _CLIENT_HOST, _CLIENT_PORT \n\ttry: \n\t \t(host, port) = (CONF.registry_host, CONF.registry_port) \n\texcept cfg.ConfigFileValueError: \n\t \tmsg = _('Configuration \toption \twas \tnot \tvalid') \n\t \tLOG.error(msg) \n\t \traise exception.BadRegistryConnectionConfiguration(msg) \n\texcept IndexError: \n\t \tmsg = _('Could \tnot \tfind \trequired \tconfiguration \toption') \n\t \tLOG.error(msg) \n\t \traise exception.BadRegistryConnectionConfiguration(msg) \n\t_CLIENT_HOST = host \n\t_CLIENT_PORT = port \n\t_CLIENT_KWARGS = {'use_ssl': (CONF.registry_client_protocol.lower() == 'https'), 'key_file': CONF.registry_client_key_file, 'cert_file': CONF.registry_client_cert_file, 'ca_file': CONF.registry_client_ca_file, 'insecure': CONF.registry_client_insecure, 'timeout': CONF.registry_client_timeout} \n\tif (not CONF.use_user_token): \n\t \tconfigure_registry_admin_creds()\n", 
" \tmodels = (Artifact, ArtifactTag, ArtifactProperty, ArtifactBlob, ArtifactBlobLocation, ArtifactDependency) \n\tfor model in models: \n\t \tmodel.metadata.create_all(engine)\n", 
" \tmodels = (ArtifactDependency, ArtifactBlobLocation, ArtifactBlob, ArtifactProperty, ArtifactTag, Artifact) \n\tfor model in models: \n\t \tmodel.metadata.drop_all(engine)\n", 
" \tmodule_path = ('glance.db.sqlalchemy.migrate_repo.versions.%s' % module_name) \n\tmodule = __import__(module_path, globals(), locals(), fromlist, 0) \n\treturn [getattr(module, item) for item in fromlist]\n", 
" \t(get_images_table,) = from_migration_import('008_add_image_members_table', ['get_images_table']) \n\timages = get_images_table(meta) \n\treturn images\n", 
" \t(get_images_table,) = from_migration_import('004_add_checksum', ['get_images_table']) \n\timages = get_images_table(meta) \n\treturn images\n", 
" \t(get_images_table,) = from_migration_import('008_add_image_members_table', ['get_images_table']) \n\timages = get_images_table(meta) \n\treturn images\n", 
" \timages = Table('images', meta, Column('id', Integer(), primary_key=True, nullable=False), Column('name', String(255)), Column('disk_format', String(20)), Column('container_format', String(20)), Column('size', BigInteger()), Column('status', String(30), nullable=False), Column('is_public', Boolean(), nullable=False, default=False, index=True), Column('location', Text()), Column('created_at', DateTime(), nullable=False), Column('updated_at', DateTime()), Column('deleted_at', DateTime()), Column('deleted', Boolean(), nullable=False, default=False, index=True), Column('checksum', String(32)), Column('owner', String(255)), mysql_engine='InnoDB', extend_existing=True) \n\treturn images\n", 
" \t(get_images_table,) = from_migration_import('004_add_checksum', ['get_images_table']) \n\timages = get_images_table(meta) \n\treturn images\n", 
" \ts3_conn = boto3.client(u's3', u'us-west-2') \n\ts3_conn.create_bucket(Bucket=u'test-bucket') \n\tzip_content = get_test_zip_file2() \n\ts3_conn.put_object(Bucket=u'test-bucket', Key=u'test.zip', Body=zip_content) \n\tconn = boto3.client(u'lambda', u'us-west-2') \n\tconn.list_functions()[u'Functions'].should.have.length_of(0) \n\tconn.create_function(FunctionName=u'testFunction', Runtime=u'python2.7', Role=u'test-iam-role', Handler=u'lambda_function.handler', Code={u'S3Bucket': u'test-bucket', u'S3Key': u'test.zip'}, Description=u'test \tlambda \tfunction', Timeout=3, MemorySize=128, Publish=True) \n\texpected_function_result = {u'Code': {u'Location': u's3://lambda-functions.aws.amazon.com/test.zip', u'RepositoryType': u'S3'}, u'Configuration': {u'CodeSha256': hashlib.sha256(zip_content).hexdigest(), u'CodeSize': len(zip_content), u'Description': u'test \tlambda \tfunction', u'FunctionArn': u'arn:aws:lambda:123456789012:function:testFunction', u'FunctionName': u'testFunction', u'Handler': u'lambda_function.handler', u'LastModified': u'2015-01-01 \t00:00:00', u'MemorySize': 128, u'Role': u'test-iam-role', u'Runtime': u'python2.7', u'Timeout': 3, u'Version': u'$LATEST', u'VpcConfig': {u'SecurityGroupIds': [], u'SubnetIds': []}}, u'ResponseMetadata': {u'HTTPStatusCode': 200}} \n\tconn.list_functions()[u'Functions'].should.equal([expected_function_result[u'Configuration']]) \n\tfunc = conn.get_function(FunctionName=u'testFunction') \n\tfunc[u'ResponseMetadata'].pop(u'HTTPHeaders', None) \n\tfunc[u'ResponseMetadata'].pop(u'RetryAttempts', None) \n\tfunc.should.equal(expected_function_result) \n\tconn.delete_function(FunctionName=u'testFunction') \n\tconn.list_functions()[u'Functions'].should.have.length_of(0)\n", 
" \tif (not CONF.metadata_encryption_key): \n\t \tmsg = _LI(\"'metadata_encryption_key' \twas \tnot \tspecified \tin \tthe \tconfig \tfile \tor \ta \tconfig \tfile \twas \tnot \tspecified. \tThis \tmeans \tthat \tthis \tmigration \tis \ta \tNOOP.\") \n\t \tLOG.info(msg) \n\t \treturn \n\tmeta = sqlalchemy.schema.MetaData() \n\tmeta.bind = migrate_engine \n\timages_table = sqlalchemy.Table('images', meta, autoload=True) \n\timages = list(images_table.select().execute()) \n\tfor image in images: \n\t \ttry: \n\t \t \tfixed_uri = fix_uri_credentials(image['location'], to_quoted) \n\t \t \timages_table.update().where((images_table.c.id == image['id'])).values(location=fixed_uri).execute() \n\t \texcept exception.Invalid: \n\t \t \tmsg = (_LW('Failed \tto \tdecrypt \tlocation \tvalue \tfor \timage \t%(image_id)s') % {'image_id': image['id']}) \n\t \t \tLOG.warn(msg) \n\t \texcept exception.BadStoreUri as e: \n\t \t \treason = encodeutils.exception_to_unicode(e) \n\t \t \tmsg = (_LE('Invalid \tstore \turi \tfor \timage: \t%(image_id)s. \tDetails: \t%(reason)s') % {'image_id': image.id, 'reason': reason}) \n\t \t \tLOG.exception(msg) \n\t \t \traise\n", 
" \tif (not uri): \n\t \treturn \n\ttry: \n\t \tdecrypted_uri = decrypt_location(uri) \n\texcept (TypeError, ValueError) as e: \n\t \traise exception.Invalid(str(e)) \n\treturn legacy_parse_uri(decrypted_uri, to_quoted)\n", 
" \tif (uri.count('://') != 1): \n\t \treason = _('URI \tcannot \tcontain \tmore \tthan \tone \toccurrence \tof \ta \tscheme.If \tyou \thave \tspecified \ta \tURI \tlike \tswift://user:pass@http://authurl.com/v1/container/obj, \tyou \tneed \tto \tchange \tit \tto \tuse \tthe \tswift+http:// \tscheme, \tlike \tso: \tswift+http://user:pass@authurl.com/v1/container/obj') \n\t \traise exception.BadStoreUri(message=reason) \n\tpieces = urlparse.urlparse(uri) \n\tif (pieces.scheme not in ('swift', 'swift+http', 'swift+https')): \n\t \traise exception.BadStoreUri(message=(\"Unacceptable \tscheme: \t'%s'\" % pieces.scheme)) \n\tscheme = pieces.scheme \n\tnetloc = pieces.netloc \n\tpath = pieces.path.lstrip('/') \n\tif (netloc != ''): \n\t \tif ('@' in netloc): \n\t \t \t(creds, netloc) = netloc.split('@') \n\t \telse: \n\t \t \tcreds = None \n\telse: \n\t \tif ('@' in path): \n\t \t \t(creds, path) = path.split('@') \n\t \telse: \n\t \t \tcreds = None \n\t \tnetloc = path[0:path.find('/')].strip('/') \n\t \tpath = path[path.find('/'):].strip('/') \n\tif creds: \n\t \tcred_parts = creds.split(':') \n\t \tif to_quote: \n\t \t \tif (len(cred_parts) == 1): \n\t \t \t \treason = (_(\"Badly \tformed \tcredentials \t'%(creds)s' \tin \tSwift \tURI\") % {'creds': creds}) \n\t \t \t \traise exception.BadStoreUri(message=reason) \n\t \t \telif (len(cred_parts) == 3): \n\t \t \t \tuser = ':'.join(cred_parts[0:2]) \n\t \t \telse: \n\t \t \t \tuser = cred_parts[0] \n\t \t \tkey = cred_parts[(-1)] \n\t \t \tuser = user \n\t \t \tkey = key \n\t \telse: \n\t \t \tif (len(cred_parts) != 2): \n\t \t \t \treason = _('Badly \tformed \tcredentials \tin \tSwift \tURI.') \n\t \t \t \traise exception.BadStoreUri(message=reason) \n\t \t \t(user, key) = cred_parts \n\t \t \tuser = urlparse.unquote(user) \n\t \t \tkey = urlparse.unquote(key) \n\telse: \n\t \tuser = None \n\t \tkey = None \n\tpath_parts = path.split('/') \n\ttry: \n\t \tobj = path_parts.pop() \n\t \tcontainer = path_parts.pop() \n\t \tif (not netloc.startswith('http')): \n\t \t \tpath_parts.insert(0, netloc) \n\t \t \tauth_or_store_url = '/'.join(path_parts) \n\texcept IndexError: \n\t \treason = (_('Badly \tformed \tS3 \tURI: \t%(uri)s') % {'uri': uri}) \n\t \traise exception.BadStoreUri(message=reason) \n\tif auth_or_store_url.startswith('http://'): \n\t \tauth_or_store_url = auth_or_store_url[len('http://'):] \n\telif auth_or_store_url.startswith('https://'): \n\t \tauth_or_store_url = auth_or_store_url[len('https://'):] \n\tcredstring = '' \n\tif (user and key): \n\t \tif to_quote: \n\t \t \tquote_user = urlparse.quote(user) \n\t \t \tquote_key = urlparse.quote(key) \n\t \telse: \n\t \t \tquote_user = user \n\t \t \tquote_key = key \n\t \tcredstring = ('%s:%s@' % (quote_user, quote_key)) \n\tauth_or_store_url = auth_or_store_url.strip('/') \n\tcontainer = container.strip('/') \n\tobj = obj.strip('/') \n\treturn ('%s://%s%s/%s/%s' % (scheme, credstring, auth_or_store_url, container, obj))\n", 
" \timages = Table('images', meta, Column('id', Integer(), primary_key=True, nullable=False), Column('name', String(255)), Column('disk_format', String(20)), Column('container_format', String(20)), Column('size', Integer()), Column('status', String(30), nullable=False), Column('is_public', Boolean(), nullable=False, default=False, index=True), Column('location', Text()), Column('created_at', DateTime(), nullable=False), Column('updated_at', DateTime()), Column('deleted_at', DateTime()), Column('deleted', Boolean(), nullable=False, default=False, index=True), mysql_engine='InnoDB', extend_existing=True) \n\treturn images\n", 
" \t(get_images_table,) = from_migration_import('004_add_checksum', ['get_images_table']) \n\timages = get_images_table(meta) \n\treturn images\n", 
" \t(get_images_table,) = from_migration_import('004_add_checksum', ['get_images_table']) \n\timages = get_images_table(meta) \n\treturn images\n", 
" \torig_properties = {} \n\tfor prop_ref in image_ref.properties: \n\t \torig_properties[prop_ref.name] = prop_ref \n\tfor (name, value) in six.iteritems(properties): \n\t \tprop_values = {'image_id': image_ref.id, 'name': name, 'value': value} \n\t \tif (name in orig_properties): \n\t \t \tprop_ref = orig_properties[name] \n\t \t \t_image_property_update(context, prop_ref, prop_values, session=session) \n\t \telse: \n\t \t \timage_property_create(context, prop_values, session=session) \n\tif purge_props: \n\t \tfor key in orig_properties.keys(): \n\t \t \tif (key not in properties): \n\t \t \t \tprop_ref = orig_properties[key] \n\t \t \t \timage_property_delete(context, prop_ref.name, image_ref.id, session=session)\n", 
" \timages = Table('images', meta, Column('id', Integer(), primary_key=True, nullable=False), Column('name', String(255)), Column('disk_format', String(20)), Column('container_format', String(20)), Column('size', Integer()), Column('status', String(30), nullable=False), Column('is_public', Boolean(), nullable=False, default=False, index=True), Column('location', Text()), Column('created_at', DateTime(), nullable=False), Column('updated_at', DateTime()), Column('deleted_at', DateTime()), Column('deleted', Boolean(), nullable=False, default=False, index=True), mysql_engine='InnoDB', extend_existing=True) \n\treturn images\n", 
" \t(get_images_table,) = from_migration_import('004_add_checksum', ['get_images_table']) \n\timages = get_images_table(meta) \n\treturn images\n", 
" \t(get_images_table,) = from_migration_import('007_add_owner', ['get_images_table']) \n\timages = get_images_table(meta) \n\treturn images\n", 
" \t(get_images_table,) = from_migration_import('007_add_owner', ['get_images_table']) \n\timages = get_images_table(meta) \n\treturn images\n", 
" \timages = Table('images', meta, Column('id', Integer(), primary_key=True, nullable=False), Column('name', String(255)), Column('disk_format', String(20)), Column('container_format', String(20)), Column('size', Integer()), Column('status', String(30), nullable=False), Column('is_public', Boolean(), nullable=False, default=False, index=True), Column('location', Text()), Column('created_at', DateTime(), nullable=False), Column('updated_at', DateTime()), Column('deleted_at', DateTime()), Column('deleted', Boolean(), nullable=False, default=False, index=True), mysql_engine='InnoDB', extend_existing=True) \n\treturn images\n", 
" \t(get_images_table,) = from_migration_import('004_add_checksum', ['get_images_table']) \n\timages = get_images_table(meta) \n\treturn images\n", 
" \timages = Table('images', meta, Column('id', Integer(), primary_key=True, nullable=False), Column('name', String(255)), Column('disk_format', String(20)), Column('container_format', String(20)), Column('size', Integer()), Column('status', String(30), nullable=False), Column('is_public', Boolean(), nullable=False, default=False, index=True), Column('location', Text()), Column('created_at', DateTime(), nullable=False), Column('updated_at', DateTime()), Column('deleted_at', DateTime()), Column('deleted', Boolean(), nullable=False, default=False, index=True), mysql_engine='InnoDB', extend_existing=True) \n\treturn images\n", 
" \t(get_images_table,) = from_migration_import('004_add_checksum', ['get_images_table']) \n\timages = get_images_table(meta) \n\treturn images\n", 
" \tmeta = sqlalchemy.MetaData() \n\tmeta.bind = migrate_engine \n\tt_images = _get_table('images', meta) \n\tt_image_members = _get_table('image_members', meta) \n\tt_image_properties = _get_table('image_properties', meta) \n\tdialect = migrate_engine.url.get_dialect().name \n\tif (dialect == 'sqlite'): \n\t \t_upgrade_sqlite(meta, t_images, t_image_members, t_image_properties) \n\t \t_update_all_ids_to_uuids(t_images, t_image_members, t_image_properties) \n\telif (dialect == 'ibm_db_sa'): \n\t \t_upgrade_db2(meta, t_images, t_image_members, t_image_properties) \n\t \t_update_all_ids_to_uuids(t_images, t_image_members, t_image_properties) \n\t \t_add_db2_constraints(meta) \n\telse: \n\t \t_upgrade_other(t_images, t_image_members, t_image_properties, dialect)\n", 
" \tmeta = sqlalchemy.MetaData() \n\tmeta.bind = migrate_engine \n\tt_images = _get_table('images', meta) \n\tt_image_members = _get_table('image_members', meta) \n\tt_image_properties = _get_table('image_properties', meta) \n\tdialect = migrate_engine.url.get_dialect().name \n\tif (dialect == 'sqlite'): \n\t \t_upgrade_sqlite(meta, t_images, t_image_members, t_image_properties) \n\t \t_update_all_ids_to_uuids(t_images, t_image_members, t_image_properties) \n\telif (dialect == 'ibm_db_sa'): \n\t \t_upgrade_db2(meta, t_images, t_image_members, t_image_properties) \n\t \t_update_all_ids_to_uuids(t_images, t_image_members, t_image_properties) \n\t \t_add_db2_constraints(meta) \n\telse: \n\t \t_upgrade_other(t_images, t_image_members, t_image_properties, dialect)\n", 
" \tsql_commands = ['CREATE \tTABLE \timages_backup \t(\\n \t \t \t \t \t \t \t \t \t \t \tid \tVARCHAR(36) \tNOT \tNULL,\\n \t \t \t \t \t \t \t \t \t \t \tname \tVARCHAR(255),\\n \t \t \t \t \t \t \t \t \t \t \tsize \tINTEGER,\\n \t \t \t \t \t \t \t \t \t \t \tstatus \tVARCHAR(30) \tNOT \tNULL,\\n \t \t \t \t \t \t \t \t \t \t \tis_public \tBOOLEAN \tNOT \tNULL,\\n \t \t \t \t \t \t \t \t \t \t \tlocation \tTEXT,\\n \t \t \t \t \t \t \t \t \t \t \tcreated_at \tDATETIME \tNOT \tNULL,\\n \t \t \t \t \t \t \t \t \t \t \tupdated_at \tDATETIME,\\n \t \t \t \t \t \t \t \t \t \t \tdeleted_at \tDATETIME,\\n \t \t \t \t \t \t \t \t \t \t \tdeleted \tBOOLEAN \tNOT \tNULL,\\n \t \t \t \t \t \t \t \t \t \t \tdisk_format \tVARCHAR(20),\\n \t \t \t \t \t \t \t \t \t \t \tcontainer_format \tVARCHAR(20),\\n \t \t \t \t \t \t \t \t \t \t \tchecksum \tVARCHAR(32),\\n \t \t \t \t \t \t \t \t \t \t \towner \tVARCHAR(255),\\n \t \t \t \t \t \t \t \t \t \t \tmin_disk \tINTEGER \tNOT \tNULL,\\n \t \t \t \t \t \t \t \t \t \t \tmin_ram \tINTEGER \tNOT \tNULL,\\n \t \t \t \t \t \t \t \t \t \t \tPRIMARY \tKEY \t(id),\\n \t \t \t \t \t \t \t \t \t \t \tCHECK \t(is_public \tIN \t(0, \t1)),\\n \t \t \t \t \t \t \t \t \t \t \tCHECK \t(deleted \tIN \t(0, \t1))\\n \t \t \t \t \t \t \t \t);', 'INSERT \tINTO \timages_backup\\n \t \t \t \t \t \t \t \t \t \t \tSELECT \t* \tFROM \timages;', 'CREATE \tTABLE \timage_members_backup \t(\\n \t \t \t \t \t \t \t \t \t \t \t \tid \tINTEGER \tNOT \tNULL,\\n \t \t \t \t \t \t \t \t \t \t \t \timage_id \tVARCHAR(36) \tNOT \tNULL,\\n \t \t \t \t \t \t \t \t \t \t \t \tmember \tVARCHAR(255) \tNOT \tNULL,\\n \t \t \t \t \t \t \t \t \t \t \t \tcan_share \tBOOLEAN \tNOT \tNULL,\\n \t \t \t \t \t \t \t \t \t \t \t \tcreated_at \tDATETIME \tNOT \tNULL,\\n \t \t \t \t \t \t \t \t \t \t \t \tupdated_at \tDATETIME,\\n \t \t \t \t \t \t \t \t \t \t \t \tdeleted_at \tDATETIME,\\n \t \t \t \t \t \t \t \t \t \t \t \tdeleted \tBOOLEAN \tNOT \tNULL,\\n \t \t \t \t \t \t \t \t \t \t \t \tPRIMARY \tKEY \t(id),\\n \t \t \t \t \t \t \t \t \t \t \t \tUNIQUE \t(image_id, \tmember),\\n \t \t \t \t \t \t \t \t \t \t \t \tCHECK \t(can_share \tIN \t(0, \t1)),\\n \t \t \t \t \t \t \t \t \t \t \t \tCHECK \t(deleted \tIN \t(0, \t1)),\\n \t \t \t \t \t \t \t \t \t \t \t \tFOREIGN \tKEY(image_id) \tREFERENCES \timages \t(id)\\n \t \t \t \t \t \t \t \t);', 'INSERT \tINTO \timage_members_backup\\n \t \t \t \t \t \t \t \t \t \t \t \tSELECT \t* \tFROM \timage_members;', 'CREATE \tTABLE \timage_properties_backup \t(\\n \t \t \t \t \t \t \t \t \t \t \t \tid \tINTEGER \tNOT \tNULL,\\n \t \t \t \t \t \t \t \t \t \t \t \timage_id \tVARCHAR(36) \tNOT \tNULL,\\n \t \t \t \t \t \t \t \t \t \t \t \tname \tVARCHAR(255) \tNOT \tNULL,\\n \t \t \t \t \t \t \t \t \t \t \t \tvalue \tTEXT,\\n \t \t \t \t \t \t \t \t \t \t \t \tcreated_at \tDATETIME \tNOT \tNULL,\\n \t \t \t \t \t \t \t \t \t \t \t \tupdated_at \tDATETIME,\\n \t \t \t \t \t \t \t \t \t \t \t \tdeleted_at \tDATETIME,\\n \t \t \t \t \t \t \t \t \t \t \t \tdeleted \tBOOLEAN \tNOT \tNULL,\\n \t \t \t \t \t \t \t \t \t \t \t \tPRIMARY \tKEY \t(id),\\n \t \t \t \t \t \t \t \t \t \t \t \tCHECK \t(deleted \tIN \t(0, \t1)),\\n \t \t \t \t \t \t \t \t \t \t \t \tUNIQUE \t(image_id, \tname),\\n \t \t \t \t \t \t \t \t \t \t \t \tFOREIGN \tKEY(image_id) \tREFERENCES \timages \t(id)\\n \t \t \t \t \t \t \t \t);', 'INSERT \tINTO \timage_properties_backup\\n \t \t \t \t \t \t \t \t \t \t \t \tSELECT \t* \tFROM \timage_properties;'] \n\tfor command in sql_commands: \n\t \tmeta.bind.execute(command) \n\t_sqlite_table_swap(meta, t_image_members, t_image_properties, t_images)\n", 
" \tsql_commands = ['CREATE \tTABLE \timages_backup \t(\\n \t \t \t \t \t \t \t \t \t \t \tid \tVARCHAR(36) \tNOT \tNULL,\\n \t \t \t \t \t \t \t \t \t \t \tname \tVARCHAR(255),\\n \t \t \t \t \t \t \t \t \t \t \tsize \tINTEGER,\\n \t \t \t \t \t \t \t \t \t \t \tstatus \tVARCHAR(30) \tNOT \tNULL,\\n \t \t \t \t \t \t \t \t \t \t \tis_public \tBOOLEAN \tNOT \tNULL,\\n \t \t \t \t \t \t \t \t \t \t \tlocation \tTEXT,\\n \t \t \t \t \t \t \t \t \t \t \tcreated_at \tDATETIME \tNOT \tNULL,\\n \t \t \t \t \t \t \t \t \t \t \tupdated_at \tDATETIME,\\n \t \t \t \t \t \t \t \t \t \t \tdeleted_at \tDATETIME,\\n \t \t \t \t \t \t \t \t \t \t \tdeleted \tBOOLEAN \tNOT \tNULL,\\n \t \t \t \t \t \t \t \t \t \t \tdisk_format \tVARCHAR(20),\\n \t \t \t \t \t \t \t \t \t \t \tcontainer_format \tVARCHAR(20),\\n \t \t \t \t \t \t \t \t \t \t \tchecksum \tVARCHAR(32),\\n \t \t \t \t \t \t \t \t \t \t \towner \tVARCHAR(255),\\n \t \t \t \t \t \t \t \t \t \t \tmin_disk \tINTEGER \tNOT \tNULL,\\n \t \t \t \t \t \t \t \t \t \t \tmin_ram \tINTEGER \tNOT \tNULL,\\n \t \t \t \t \t \t \t \t \t \t \tPRIMARY \tKEY \t(id),\\n \t \t \t \t \t \t \t \t \t \t \tCHECK \t(is_public \tIN \t(0, \t1)),\\n \t \t \t \t \t \t \t \t \t \t \tCHECK \t(deleted \tIN \t(0, \t1))\\n \t \t \t \t \t \t \t \t);', 'INSERT \tINTO \timages_backup\\n \t \t \t \t \t \t \t \t \t \t \tSELECT \t* \tFROM \timages;', 'CREATE \tTABLE \timage_members_backup \t(\\n \t \t \t \t \t \t \t \t \t \t \t \tid \tINTEGER \tNOT \tNULL,\\n \t \t \t \t \t \t \t \t \t \t \t \timage_id \tVARCHAR(36) \tNOT \tNULL,\\n \t \t \t \t \t \t \t \t \t \t \t \tmember \tVARCHAR(255) \tNOT \tNULL,\\n \t \t \t \t \t \t \t \t \t \t \t \tcan_share \tBOOLEAN \tNOT \tNULL,\\n \t \t \t \t \t \t \t \t \t \t \t \tcreated_at \tDATETIME \tNOT \tNULL,\\n \t \t \t \t \t \t \t \t \t \t \t \tupdated_at \tDATETIME,\\n \t \t \t \t \t \t \t \t \t \t \t \tdeleted_at \tDATETIME,\\n \t \t \t \t \t \t \t \t \t \t \t \tdeleted \tBOOLEAN \tNOT \tNULL,\\n \t \t \t \t \t \t \t \t \t \t \t \tPRIMARY \tKEY \t(id),\\n \t \t \t \t \t \t \t \t \t \t \t \tUNIQUE \t(image_id, \tmember),\\n \t \t \t \t \t \t \t \t \t \t \t \tCHECK \t(can_share \tIN \t(0, \t1)),\\n \t \t \t \t \t \t \t \t \t \t \t \tCHECK \t(deleted \tIN \t(0, \t1)),\\n \t \t \t \t \t \t \t \t \t \t \t \tFOREIGN \tKEY(image_id) \tREFERENCES \timages \t(id)\\n \t \t \t \t \t \t \t \t);', 'INSERT \tINTO \timage_members_backup\\n \t \t \t \t \t \t \t \t \t \t \t \tSELECT \t* \tFROM \timage_members;', 'CREATE \tTABLE \timage_properties_backup \t(\\n \t \t \t \t \t \t \t \t \t \t \t \tid \tINTEGER \tNOT \tNULL,\\n \t \t \t \t \t \t \t \t \t \t \t \timage_id \tVARCHAR(36) \tNOT \tNULL,\\n \t \t \t \t \t \t \t \t \t \t \t \tname \tVARCHAR(255) \tNOT \tNULL,\\n \t \t \t \t \t \t \t \t \t \t \t \tvalue \tTEXT,\\n \t \t \t \t \t \t \t \t \t \t \t \tcreated_at \tDATETIME \tNOT \tNULL,\\n \t \t \t \t \t \t \t \t \t \t \t \tupdated_at \tDATETIME,\\n \t \t \t \t \t \t \t \t \t \t \t \tdeleted_at \tDATETIME,\\n \t \t \t \t \t \t \t \t \t \t \t \tdeleted \tBOOLEAN \tNOT \tNULL,\\n \t \t \t \t \t \t \t \t \t \t \t \tPRIMARY \tKEY \t(id),\\n \t \t \t \t \t \t \t \t \t \t \t \tCHECK \t(deleted \tIN \t(0, \t1)),\\n \t \t \t \t \t \t \t \t \t \t \t \tUNIQUE \t(image_id, \tname),\\n \t \t \t \t \t \t \t \t \t \t \t \tFOREIGN \tKEY(image_id) \tREFERENCES \timages \t(id)\\n \t \t \t \t \t \t \t \t);', 'INSERT \tINTO \timage_properties_backup\\n \t \t \t \t \t \t \t \t \t \t \t \tSELECT \t* \tFROM \timage_properties;'] \n\tfor command in sql_commands: \n\t \tmeta.bind.execute(command) \n\t_sqlite_table_swap(meta, t_image_members, t_image_properties, t_images)\n", 
" \tforeign_keys = _get_foreign_keys(t_images, t_image_members, t_image_properties, dialect) \n\tfor fk in foreign_keys: \n\t \tfk.drop() \n\tt_images.c.id.alter(sqlalchemy.String(36), primary_key=True) \n\tt_image_members.c.image_id.alter(sqlalchemy.String(36)) \n\tt_image_properties.c.image_id.alter(sqlalchemy.String(36)) \n\t_update_all_ids_to_uuids(t_images, t_image_members, t_image_properties) \n\tfor fk in foreign_keys: \n\t \tfk.create()\n", 
" \tforeign_keys = _get_foreign_keys(t_images, t_image_members, t_image_properties, dialect) \n\tfor fk in foreign_keys: \n\t \tfk.drop() \n\tt_images.c.id.alter(sqlalchemy.String(36), primary_key=True) \n\tt_image_members.c.image_id.alter(sqlalchemy.String(36)) \n\tt_image_properties.c.image_id.alter(sqlalchemy.String(36)) \n\t_update_all_ids_to_uuids(t_images, t_image_members, t_image_properties) \n\tfor fk in foreign_keys: \n\t \tfk.create()\n", 
" \treturn sqlalchemy.Table(table_name, metadata, autoload=True)\n", 
" \tforeign_keys = [] \n\tif t_image_members.foreign_keys: \n\t \timg_members_fk_name = list(t_image_members.foreign_keys)[0].name \n\t \tif (dialect == 'mysql'): \n\t \t \tfk1 = migrate.ForeignKeyConstraint([t_image_members.c.image_id], [t_images.c.id], name=img_members_fk_name) \n\t \telse: \n\t \t \tfk1 = migrate.ForeignKeyConstraint([t_image_members.c.image_id], [t_images.c.id]) \n\t \tforeign_keys.append(fk1) \n\tif t_image_properties.foreign_keys: \n\t \timg_properties_fk_name = list(t_image_properties.foreign_keys)[0].name \n\t \tif (dialect == 'mysql'): \n\t \t \tfk2 = migrate.ForeignKeyConstraint([t_image_properties.c.image_id], [t_images.c.id], name=img_properties_fk_name) \n\t \telse: \n\t \t \tfk2 = migrate.ForeignKeyConstraint([t_image_properties.c.image_id], [t_images.c.id]) \n\t \tforeign_keys.append(fk2) \n\treturn foreign_keys\n", 
" \timages = list(t_images.select().execute()) \n\tfor image in images: \n\t \told_id = image['id'] \n\t \tnew_id = str(uuid.uuid4()) \n\t \tt_images.update().where((t_images.c.id == old_id)).values(id=new_id).execute() \n\t \tt_image_members.update().where((t_image_members.c.image_id == old_id)).values(image_id=new_id).execute() \n\t \tt_image_properties.update().where((t_image_properties.c.image_id == old_id)).values(image_id=new_id).execute() \n\t \tt_image_properties.update().where(and_(or_((t_image_properties.c.name == 'kernel_id'), (t_image_properties.c.name == 'ramdisk_id')), (t_image_properties.c.value == old_id))).values(value=new_id).execute()\n", 
" \timages = list(t_images.select().execute()) \n\tfor image in images: \n\t \told_id = image['id'] \n\t \tnew_id = str(uuid.uuid4()) \n\t \tt_images.update().where((t_images.c.id == old_id)).values(id=new_id).execute() \n\t \tt_image_members.update().where((t_image_members.c.image_id == old_id)).values(image_id=new_id).execute() \n\t \tt_image_properties.update().where((t_image_properties.c.image_id == old_id)).values(image_id=new_id).execute() \n\t \tt_image_properties.update().where(and_(or_((t_image_properties.c.name == 'kernel_id'), (t_image_properties.c.name == 'ramdisk_id')), (t_image_properties.c.value == old_id))).values(value=new_id).execute()\n", 
" \tmeta = sqlalchemy.schema.MetaData() \n\tmeta.bind = migrate_engine \n\timages_table = sqlalchemy.Table('images', meta, autoload=True) \n\timages = list(images_table.select(images_table.c.location.startswith('swift')).execute()) \n\tfor image in images: \n\t \ttry: \n\t \t \tfixed_uri = legacy_parse_uri(image['location'], to_quoted) \n\t \t \timages_table.update().where((images_table.c.id == image['id'])).values(location=fixed_uri).execute() \n\t \texcept exception.BadStoreUri as e: \n\t \t \treason = encodeutils.exception_to_unicode(e) \n\t \t \tmsg = (_LE('Invalid \tstore \turi \tfor \timage: \t%(image_id)s. \tDetails: \t%(reason)s') % {'image_id': image.id, 'reason': reason}) \n\t \t \tLOG.exception(msg) \n\t \t \traise\n", 
" \tif (not uri): \n\t \treturn \n\ttry: \n\t \tdecrypted_uri = decrypt_location(uri) \n\texcept (TypeError, ValueError) as e: \n\t \traise exception.Invalid(str(e)) \n\treturn legacy_parse_uri(decrypted_uri, to_quoted)\n", 
" \tif (uri.count('://') != 1): \n\t \treason = _('URI \tcannot \tcontain \tmore \tthan \tone \toccurrence \tof \ta \tscheme.If \tyou \thave \tspecified \ta \tURI \tlike \tswift://user:pass@http://authurl.com/v1/container/obj, \tyou \tneed \tto \tchange \tit \tto \tuse \tthe \tswift+http:// \tscheme, \tlike \tso: \tswift+http://user:pass@authurl.com/v1/container/obj') \n\t \traise exception.BadStoreUri(message=reason) \n\tpieces = urlparse.urlparse(uri) \n\tif (pieces.scheme not in ('swift', 'swift+http', 'swift+https')): \n\t \traise exception.BadStoreUri(message=(\"Unacceptable \tscheme: \t'%s'\" % pieces.scheme)) \n\tscheme = pieces.scheme \n\tnetloc = pieces.netloc \n\tpath = pieces.path.lstrip('/') \n\tif (netloc != ''): \n\t \tif ('@' in netloc): \n\t \t \t(creds, netloc) = netloc.split('@') \n\t \telse: \n\t \t \tcreds = None \n\telse: \n\t \tif ('@' in path): \n\t \t \t(creds, path) = path.split('@') \n\t \telse: \n\t \t \tcreds = None \n\t \tnetloc = path[0:path.find('/')].strip('/') \n\t \tpath = path[path.find('/'):].strip('/') \n\tif creds: \n\t \tcred_parts = creds.split(':') \n\t \tif to_quote: \n\t \t \tif (len(cred_parts) == 1): \n\t \t \t \treason = (_(\"Badly \tformed \tcredentials \t'%(creds)s' \tin \tSwift \tURI\") % {'creds': creds}) \n\t \t \t \traise exception.BadStoreUri(message=reason) \n\t \t \telif (len(cred_parts) == 3): \n\t \t \t \tuser = ':'.join(cred_parts[0:2]) \n\t \t \telse: \n\t \t \t \tuser = cred_parts[0] \n\t \t \tkey = cred_parts[(-1)] \n\t \t \tuser = user \n\t \t \tkey = key \n\t \telse: \n\t \t \tif (len(cred_parts) != 2): \n\t \t \t \treason = _('Badly \tformed \tcredentials \tin \tSwift \tURI.') \n\t \t \t \traise exception.BadStoreUri(message=reason) \n\t \t \t(user, key) = cred_parts \n\t \t \tuser = urlparse.unquote(user) \n\t \t \tkey = urlparse.unquote(key) \n\telse: \n\t \tuser = None \n\t \tkey = None \n\tpath_parts = path.split('/') \n\ttry: \n\t \tobj = path_parts.pop() \n\t \tcontainer = path_parts.pop() \n\t \tif (not netloc.startswith('http')): \n\t \t \tpath_parts.insert(0, netloc) \n\t \t \tauth_or_store_url = '/'.join(path_parts) \n\texcept IndexError: \n\t \treason = (_('Badly \tformed \tS3 \tURI: \t%(uri)s') % {'uri': uri}) \n\t \traise exception.BadStoreUri(message=reason) \n\tif auth_or_store_url.startswith('http://'): \n\t \tauth_or_store_url = auth_or_store_url[len('http://'):] \n\telif auth_or_store_url.startswith('https://'): \n\t \tauth_or_store_url = auth_or_store_url[len('https://'):] \n\tcredstring = '' \n\tif (user and key): \n\t \tif to_quote: \n\t \t \tquote_user = urlparse.quote(user) \n\t \t \tquote_key = urlparse.quote(key) \n\t \telse: \n\t \t \tquote_user = user \n\t \t \tquote_key = key \n\t \tcredstring = ('%s:%s@' % (quote_user, quote_key)) \n\tauth_or_store_url = auth_or_store_url.strip('/') \n\tcontainer = container.strip('/') \n\tobj = obj.strip('/') \n\treturn ('%s://%s%s/%s/%s' % (scheme, credstring, auth_or_store_url, container, obj))\n", 
" \tprint db_api.db_version(db_api.get_engine())\n", 
" \tprint db_api.db_version(db_api.get_engine())\n", 
" \tprint db_api.db_version(db_api.get_engine())\n", 
" \tdb_api.db_sync(db_api.get_engine(), CONF.command.version)\n", 
" \tdb_api.db_sync(db_api.get_engine(), CONF.command.version)\n", 
" \tdb_api.db_sync(db_api.get_engine(), CONF.command.version)\n", 
" \tglobal _REPOSITORY \n\trel_path = 'migrate_repo' \n\tif (database == 'api'): \n\t \trel_path = os.path.join('api_migrations', 'migrate_repo') \n\tpath = os.path.join(os.path.abspath(os.path.dirname(__file__)), rel_path) \n\tassert os.path.exists(path) \n\tif (_REPOSITORY.get(database) is None): \n\t \t_REPOSITORY[database] = Repository(path) \n\treturn _REPOSITORY[database]\n", 
" \tdefaults = {'host': 'salt', 'user': 'salt', 'pass': 'salt', 'db': 'salt', 'port': 3306, 'ssl_ca': None, 'ssl_cert': None, 'ssl_key': None} \n\tattrs = {'host': 'host', 'user': 'user', 'pass': 'pass', 'db': 'db', 'port': 'port', 'ssl_ca': 'ssl_ca', 'ssl_cert': 'ssl_cert', 'ssl_key': 'ssl_key'} \n\t_options = salt.returners.get_returner_options(__virtualname__, ret, attrs, __salt__=__salt__, __opts__=__opts__, defaults=defaults) \n\tfor (k, v) in _options.iteritems(): \n\t \tif (isinstance(v, string_types) and (v.lower() == 'none')): \n\t \t \t_options[k] = None \n\t \tif (k == 'port'): \n\t \t \t_options[k] = int(v) \n\treturn _options\n", 
" \tdb.create_all() \n\t(yield db) \n\tdb.drop_all()\n", 
" \tif ('sqlite' not in IMPL.get_engine().name): \n\t \treturn IMPL.dispose_engine() \n\telse: \n\t \treturn\n", 
" \treturn context_manager.get_legacy_facade().get_session(autocommit=autocommit, expire_on_commit=expire_on_commit, use_slave=use_slave)\n", 
" \tif hasattr(obj, 'bind'): \n\t \tconn = obj.bind \n\telse: \n\t \ttry: \n\t \t \tconn = object_session(obj).bind \n\t \texcept UnmappedInstanceError: \n\t \t \tconn = obj \n\tif (not hasattr(conn, 'execute')): \n\t \traise TypeError('This \tmethod \taccepts \tonly \tSession, \tEngine, \tConnection \tand \tdeclarative \tmodel \tobjects.') \n\treturn conn\n", 
" \tfield = getattr(model, fieldname) \n\tif isinstance(field, ColumnElement): \n\t \treturn field.type \n\tif isinstance(field, AssociationProxy): \n\t \tfield = field.remote_attr \n\tif hasattr(field, 'property'): \n\t \tprop = field.property \n\t \tif isinstance(prop, RelProperty): \n\t \t \treturn None \n\t \treturn prop.columns[0].type \n\treturn None\n", 
" \treturn (file_path in _db_content.get('files'))\n", 
" \tif (('nova/virt' in filename) and (not filename.endswith('fake.py'))): \n\t \tif logical_line.startswith('from \tnova \timport \tdb'): \n\t \t \t(yield (0, 'N307: \tnova.db \timport \tnot \tallowed \tin \tnova/virt/*'))\n", 
" \treturn client.image_create(values=values, v1_mode=v1_mode)\n", 
" \treturn client.image_update(values=values, image_id=image_id, purge_props=purge_props, from_state=from_state, v1_mode=v1_mode)\n", 
" \treturn client.image_destroy(image_id=image_id)\n", 
" \t_check_image_id(image_id) \n\tsession = (session or get_session()) \n\ttry: \n\t \tquery = session.query(models.Image).options(sa_orm.joinedload(models.Image.properties)).options(sa_orm.joinedload(models.Image.locations)).filter_by(id=image_id) \n\t \tif ((not force_show_deleted) and (not context.can_see_deleted)): \n\t \t \tquery = query.filter_by(deleted=False) \n\t \timage = query.one() \n\texcept sa_orm.exc.NoResultFound: \n\t \tmsg = ('No \timage \tfound \twith \tID \t%s' % image_id) \n\t \tLOG.debug(msg) \n\t \traise exception.ImageNotFound(msg) \n\tif (not is_image_visible(context, image)): \n\t \tmsg = ('Forbidding \trequest, \timage \t%s \tnot \tvisible' % image_id) \n\t \tLOG.debug(msg) \n\t \traise exception.Forbidden(msg) \n\treturn image\n", 
" \tif context.is_admin: \n\t \treturn True \n\tif ((image['owner'] is None) or (context.owner is None)): \n\t \treturn False \n\treturn (image['owner'] == context.owner)\n", 
" \tif (not frappe.has_permission(doctype, ptype=u'share', doc=name)): \n\t \tfrappe.throw(_(u'No \tpermission \tto \t{0} \t{1} \t{2}'.format(u'share', doctype, name)), frappe.PermissionError)\n", 
" \tif context.is_admin: \n\t \treturn True \n\tif (image['owner'] is None): \n\t \treturn True \n\tif (image['visibility'] in ['public', 'community']): \n\t \treturn True \n\tif (context.owner is not None): \n\t \tif (context.owner == image['owner']): \n\t \t \treturn True \n\t \tif ('shared' == image['visibility']): \n\t \t \tmembers = image_member_find(context, image_id=image['id'], member=context.owner, status=status) \n\t \t \tif members: \n\t \t \t \treturn True \n\treturn False\n", 
" \tif ('id' not in sort_keys): \n\t \tLOG.warning(_LW('Id \tnot \tin \tsort_keys; \tis \tsort_keys \tunique?')) \n\tassert (not (sort_dir and sort_dirs)) \n\tif ((sort_dirs is None) and (sort_dir is None)): \n\t \tsort_dir = 'asc' \n\tif (sort_dirs is None): \n\t \tsort_dirs = [sort_dir for _sort_key in sort_keys] \n\tassert (len(sort_dirs) == len(sort_keys)) \n\tfor (current_sort_key, current_sort_dir) in zip(sort_keys, sort_dirs): \n\t \tsort_dir_func = {'asc': sqlalchemy.asc, 'desc': sqlalchemy.desc}[current_sort_dir] \n\t \ttry: \n\t \t \tsort_key_attr = getattr(model, current_sort_key) \n\t \texcept AttributeError: \n\t \t \traise exception.InvalidInput(reason='Invalid \tsort \tkey') \n\t \tif (not api.is_orm_value(sort_key_attr)): \n\t \t \traise exception.InvalidInput(reason='Invalid \tsort \tkey') \n\t \tquery = query.order_by(sort_dir_func(sort_key_attr)) \n\tif (marker is not None): \n\t \tmarker_values = [] \n\t \tfor sort_key in sort_keys: \n\t \t \tv = getattr(marker, sort_key) \n\t \t \tmarker_values.append(v) \n\t \tcriteria_list = [] \n\t \tfor i in range(0, len(sort_keys)): \n\t \t \tcrit_attrs = [] \n\t \t \tfor j in range(0, i): \n\t \t \t \tmodel_attr = getattr(model, sort_keys[j]) \n\t \t \t \tcrit_attrs.append((model_attr == marker_values[j])) \n\t \t \tmodel_attr = getattr(model, sort_keys[i]) \n\t \t \tif (sort_dirs[i] == 'desc'): \n\t \t \t \tcrit_attrs.append((model_attr < marker_values[i])) \n\t \t \telif (sort_dirs[i] == 'asc'): \n\t \t \t \tcrit_attrs.append((model_attr > marker_values[i])) \n\t \t \telse: \n\t \t \t \traise ValueError(_(\"Unknown \tsort \tdirection, \tmust \tbe \t'desc' \tor \t'asc'\")) \n\t \t \tcriteria = sqlalchemy.sql.and_(*crit_attrs) \n\t \t \tcriteria_list.append(criteria) \n\t \tf = sqlalchemy.sql.or_(*criteria_list) \n\t \tquery = query.filter(f) \n\tif (limit is not None): \n\t \tquery = query.limit(limit) \n\tif offset: \n\t \tquery = query.offset(offset) \n\treturn query\n", 
" \tsort_key = (['created_at'] if (not sort_key) else sort_key) \n\tsort_dir = (['desc'] if (not sort_dir) else sort_dir) \n\treturn client.image_get_all(filters=filters, marker=marker, limit=limit, sort_key=sort_key, sort_dir=sort_dir, member_status=member_status, is_public=is_public, admin_as_user=admin_as_user, return_tag=return_tag, v1_mode=v1_mode)\n", 
" \tfor attr in model_class.__protected_attributes__: \n\t \tif (attr in values): \n\t \t \tdel values[attr]\n", 
" \tif mandatory_status: \n\t \tstatus = values.get('status') \n\t \tif (not status): \n\t \t \tmsg = 'Image \tstatus \tis \trequired.' \n\t \t \traise exception.Invalid(msg) \n\t \tif (status not in STATUSES): \n\t \t \tmsg = (\"Invalid \timage \tstatus \t'%s' \tfor \timage.\" % status) \n\t \t \traise exception.Invalid(msg) \n\t_validate_db_int(min_disk=values.get('min_disk'), min_ram=values.get('min_ram')) \n\treturn values\n", 
" \tvalues = values.copy() \n\tsession = get_session() \n\twith session.begin(): \n\t \tproperties = values.pop('properties', {}) \n\t \tlocation_data = values.pop('locations', None) \n\t \tnew_status = values.get('status', None) \n\t \tif image_id: \n\t \t \timage_ref = _image_get(context, image_id, session=session) \n\t \t \tcurrent = image_ref.status \n\t \t \t_check_mutate_authorization(context, image_ref) \n\t \telse: \n\t \t \tif (values.get('size') is not None): \n\t \t \t \tvalues['size'] = int(values['size']) \n\t \t \tif ('min_ram' in values): \n\t \t \t \tvalues['min_ram'] = int((values['min_ram'] or 0)) \n\t \t \tif ('min_disk' in values): \n\t \t \t \tvalues['min_disk'] = int((values['min_disk'] or 0)) \n\t \t \tvalues['protected'] = bool(values.get('protected', False)) \n\t \t \timage_ref = models.Image() \n\t \tvalues = db_utils.ensure_image_dict_v2_compliant(values) \n\t \tif (('owner' in values) and (not values['owner'])): \n\t \t \tvalues['owner'] = None \n\t \tif image_id: \n\t \t \t_drop_protected_attrs(models.Image, values) \n\t \t \tvalues['updated_at'] = timeutils.utcnow() \n\t \tif image_id: \n\t \t \tquery = session.query(models.Image).filter_by(id=image_id) \n\t \t \tif from_state: \n\t \t \t \tquery = query.filter_by(status=from_state) \n\t \t \tmandatory_status = (True if new_status else False) \n\t \t \t_validate_image(values, mandatory_status=mandatory_status) \n\t \t \tvalues = {key: values[key] for key in values if (key in image_ref.to_dict())} \n\t \t \tupdated = query.update(values, synchronize_session='fetch') \n\t \t \tif (not updated): \n\t \t \t \tmsg = (_('cannot \ttransition \tfrom \t%(current)s \tto \t%(next)s \tin \tupdate \t(wanted \tfrom_state=%(from)s)') % {'current': current, 'next': new_status, 'from': from_state}) \n\t \t \t \traise exception.Conflict(msg) \n\t \t \timage_ref = _image_get(context, image_id, session=session) \n\t \telse: \n\t \t \timage_ref.update(values) \n\t \t \tvalues = _validate_image(image_ref.to_dict()) \n\t \t \t_update_values(image_ref, values) \n\t \t \ttry: \n\t \t \t \timage_ref.save(session=session) \n\t \t \texcept db_exception.DBDuplicateEntry: \n\t \t \t \traise exception.Duplicate(('Image \tID \t%s \talready \texists!' % values['id'])) \n\t \t_set_properties_for_image(context, image_ref, properties, purge_props, session) \n\t \tif location_data: \n\t \t \t_image_locations_set(context, image_ref.id, location_data, session=session) \n\treturn image_get(context, image_ref.id)\n", 
" \torig_properties = {} \n\tfor prop_ref in image_ref.properties: \n\t \torig_properties[prop_ref.name] = prop_ref \n\tfor (name, value) in six.iteritems(properties): \n\t \tprop_values = {'image_id': image_ref.id, 'name': name, 'value': value} \n\t \tif (name in orig_properties): \n\t \t \tprop_ref = orig_properties[name] \n\t \t \t_image_property_update(context, prop_ref, prop_values, session=session) \n\t \telse: \n\t \t \timage_property_create(context, prop_values, session=session) \n\tif purge_props: \n\t \tfor key in orig_properties.keys(): \n\t \t \tif (key not in properties): \n\t \t \t \tprop_ref = orig_properties[key] \n\t \t \t \timage_property_delete(context, prop_ref.name, image_ref.id, session=session)\n", 
" \treturn client.image_property_create(values=values)\n", 
" \tsession = (session or get_session()) \n\tprop = session.query(models.ImageProperty).filter_by(image_id=image_ref, name=prop_ref).one() \n\tprop.delete(session=session) \n\treturn prop\n", 
" \tsession = (session or get_session()) \n\tprop = session.query(models.ImageProperty).filter_by(image_id=image_ref, name=prop_ref).one() \n\tprop.delete(session=session) \n\treturn prop\n", 
" \treturn client.image_member_create(values=values)\n", 
" \treturn {'id': member_ref['id'], 'image_id': member_ref['image_id'], 'member': member_ref['member'], 'can_share': member_ref['can_share'], 'status': member_ref['status'], 'created_at': member_ref['created_at'], 'updated_at': member_ref['updated_at'], 'deleted': member_ref['deleted']}\n", 
" \treturn client.image_member_update(memb_id=memb_id, values=values)\n", 
" \t_drop_protected_attrs(models.ImageMember, values) \n\tvalues['deleted'] = False \n\tvalues.setdefault('can_share', False) \n\tmemb_ref.update(values) \n\tmemb_ref.save(session=session) \n\treturn memb_ref\n", 
" \tclient.image_member_delete(memb_id=memb_id)\n", 
" \tquery = session.query(models.ImageMember) \n\tquery = query.filter_by(id=memb_id) \n\treturn query.one()\n", 
" \treturn client.image_member_find(image_id=image_id, member=member, status=status, include_deleted=include_deleted)\n", 
" \tgyp_includes_set = set() \n\tcompiler_includes_list = [] \n\tif compiler_path: \n\t \tcommand = shlex.split(compiler_path) \n\t \tcommand.extend(['-E', '-xc++', '-v', '-']) \n\t \tproc = subprocess.Popen(args=command, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE) \n\t \toutput = proc.communicate()[1] \n\t \tin_include_list = False \n\t \tfor line in output.splitlines(): \n\t \t \tif line.startswith('#include'): \n\t \t \t \tin_include_list = True \n\t \t \t \tcontinue \n\t \t \tif line.startswith('End \tof \tsearch \tlist.'): \n\t \t \t \tbreak \n\t \t \tif in_include_list: \n\t \t \t \tinclude_dir = line.strip() \n\t \t \t \tif (include_dir not in compiler_includes_list): \n\t \t \t \t \tcompiler_includes_list.append(include_dir) \n\tflavor = gyp.common.GetFlavor(params) \n\tif (flavor == 'win'): \n\t \tgenerator_flags = params.get('generator_flags', {}) \n\tfor target_name in target_list: \n\t \ttarget = target_dicts[target_name] \n\t \tif (config_name in target['configurations']): \n\t \t \tconfig = target['configurations'][config_name] \n\t \t \tif (flavor == 'win'): \n\t \t \t \tmsvs_settings = gyp.msvs_emulation.MsvsSettings(target, generator_flags) \n\t \t \t \tcflags = msvs_settings.GetCflags(config_name) \n\t \t \telse: \n\t \t \t \tcflags = config['cflags'] \n\t \t \tfor cflag in cflags: \n\t \t \t \tif cflag.startswith('-I'): \n\t \t \t \t \tinclude_dir = cflag[2:] \n\t \t \t \t \tif (include_dir not in compiler_includes_list): \n\t \t \t \t \t \tcompiler_includes_list.append(include_dir) \n\t \t \tif config.has_key('include_dirs'): \n\t \t \t \tinclude_dirs = config['include_dirs'] \n\t \t \t \tfor shared_intermediate_dir in shared_intermediate_dirs: \n\t \t \t \t \tfor include_dir in include_dirs: \n\t \t \t \t \t \tinclude_dir = include_dir.replace('$SHARED_INTERMEDIATE_DIR', shared_intermediate_dir) \n\t \t \t \t \t \tif (not os.path.isabs(include_dir)): \n\t \t \t \t \t \t \tbase_dir = os.path.dirname(target_name) \n\t \t \t \t \t \t \tinclude_dir = ((base_dir + '/') + include_dir) \n\t \t \t \t \t \t \tinclude_dir = os.path.abspath(include_dir) \n\t \t \t \t \t \tgyp_includes_set.add(include_dir) \n\tall_includes_list = list(gyp_includes_set) \n\tall_includes_list.sort() \n\tfor compiler_include in compiler_includes_list: \n\t \tif (not (compiler_include in gyp_includes_set)): \n\t \t \tall_includes_list.append(compiler_include) \n\treturn all_includes_list\n", 
" \treturn client.image_tag_create(image_id=image_id, value=value)\n", 
" \tclient.image_tag_delete(image_id=image_id, value=value)\n", 
" \treturn client.image_tag_get_all(image_id=image_id)\n", 
" \tif context.is_admin: \n\t \treturn True \n\tif ((image['owner'] is None) or (context.owner is None)): \n\t \treturn False \n\treturn (image['owner'] == context.owner)\n", 
" \tif (not frappe.has_permission(doctype, ptype=u'share', doc=name)): \n\t \tfrappe.throw(_(u'No \tpermission \tto \t{0} \t{1} \t{2}'.format(u'share', doctype, name)), frappe.PermissionError)\n", 
" \tif context.is_admin: \n\t \treturn True \n\tif (image['owner'] is None): \n\t \treturn True \n\tif (image['visibility'] in ['public', 'community']): \n\t \treturn True \n\tif (context.owner is not None): \n\t \tif (context.owner == image['owner']): \n\t \t \treturn True \n\t \tif ('shared' == image['visibility']): \n\t \t \tmembers = image_member_find(context, image_id=image['id'], member=context.owner, status=status) \n\t \t \tif members: \n\t \t \t \treturn True \n\treturn False\n", 
" \treturn IMPL.db_sync(version=version, database=database, context=context)\n", 
" \treturn IMPL.db_version(database=database, context=context)\n", 
" \tif ('all' not in inventory): \n\t \tinventory['all'] = {} \n\tif ('vars' not in inventory['all']): \n\t \tinventory['all']['vars'] = {} \n\tinventory['all']['vars']['container_cidr'] = user_cidr \n\tif ('global_overrides' in user_defined_config): \n\t \tif isinstance(user_defined_config['global_overrides'], dict): \n\t \t \tinventory['all']['vars'].update(user_defined_config['global_overrides']) \n\t \t \tlogger.debug('Applied \tglobal_overrides') \n\t \t \tkept_vars = user_defined_config['global_overrides'].keys() \n\t \t \tkept_vars.append('container_cidr') \n\t \t \tfor key in inventory['all']['vars'].keys(): \n\t \t \t \tif (key not in kept_vars): \n\t \t \t \t \tlogger.debug('Deleting \tkey \t%s \tfrom \tinventory', key) \n\t \t \t \t \tdel inventory['all']['vars'][key]\n", 
" \tobj_type = apps.get_model('contenttypes', 'ContentType').objects.get_for_model(obj) \n\twith atomic(): \n\t \t(like, created) = Like.objects.get_or_create(content_type=obj_type, object_id=obj.id, user=user) \n\t \tif (like.project is not None): \n\t \t \tlike.project.refresh_totals() \n\treturn like\n", 
" \ttry: \n\t \tContents = [] \n\t \targs = {'Bucket': Bucket, 'FetchOwner': FetchOwner} \n\t \t(args.update({'Delimiter': Delimiter}) if Delimiter else None) \n\t \t(args.update({'EncodingType': EncodingType}) if Delimiter else None) \n\t \t(args.update({'Prefix': Prefix}) if Prefix else None) \n\t \t(args.update({'StartAfter': StartAfter}) if StartAfter else None) \n\t \tconn = _get_conn(region=region, key=key, keyid=keyid, profile=profile) \n\t \tIsTruncated = True \n\t \twhile IsTruncated: \n\t \t \tret = conn.list_objects_v2(**args) \n\t \t \tIsTruncated = ret.get('IsTruncated', False) \n\t \t \tif (IsTruncated in ('True', 'true', True)): \n\t \t \t \targs['ContinuationToken'] = ret['NextContinuationToken'] \n\t \t \tContents += ret.get('Contents', []) \n\t \treturn {'Contents': Contents} \n\texcept ClientError as e: \n\t \treturn {'error': __utils__['boto3.get_error'](e)}\n", 
" \tif isinstance(Delete, six.string_types): \n\t \tDelete = json.loads(Delete) \n\tif (not isinstance(Delete, dict)): \n\t \traise SaltInvocationError('Malformed \tDelete \trequest.') \n\tif ('Objects' not in Delete): \n\t \traise SaltInvocationError('Malformed \tDelete \trequest.') \n\tfailed = [] \n\tobjs = Delete['Objects'] \n\tfor i in range(0, len(objs), 1000): \n\t \tchunk = objs[i:(i + 1000)] \n\t \tsubset = {'Objects': chunk, 'Quiet': True} \n\t \ttry: \n\t \t \targs = {'Bucket': Bucket} \n\t \t \t(args.update({'MFA': MFA}) if MFA else None) \n\t \t \t(args.update({'RequestPayer': RequestPayer}) if RequestPayer else None) \n\t \t \targs.update({'Delete': subset}) \n\t \t \tconn = _get_conn(region=region, key=key, keyid=keyid, profile=profile) \n\t \t \tret = conn.delete_objects(**args) \n\t \t \tfailed += ret.get('Errors', []) \n\t \texcept ClientError as e: \n\t \t \treturn {'deleted': False, 'error': __utils__['boto3.get_error'](e)} \n\tif len(failed): \n\t \treturn {'deleted': False, 'failed': failed} \n\telse: \n\t \treturn {'deleted': True}\n", 
" \tmore_results = True \n\tk = None \n\twhile more_results: \n\t \trs = bucket.get_all_keys(prefix=prefix, marker=marker, delimiter=delimiter, headers=headers, encoding_type=encoding_type) \n\t \tfor k in rs: \n\t \t \t(yield k) \n\t \tif k: \n\t \t \tmarker = (rs.next_marker or k.name) \n\t \tif (marker and (encoding_type == 'url')): \n\t \t \tmarker = unquote_str(marker) \n\t \tmore_results = rs.is_truncated\n", 
" \treturn (hasattr(arg, method) and callable(getattr(arg, method)))\n", 
" \tunique = {} \n\tfor (idx, name) in enumerate(names): \n\t \tname = name.upper() \n\t \tif (name in unique): \n\t \t \tunique[name].append(idx) \n\t \telse: \n\t \t \tunique[name] = [idx] \n\treturn unique\n", 
" \treturn _default_registry.register(prefix, mapping)\n", 
" \treturn get_paths(scheme, vars, expand)[name]\n", 
" \tif (object_type_name == 'commit'): \n\t \tfrom . import commit \n\t \treturn commit.Commit \n\telif (object_type_name == 'tag'): \n\t \tfrom . import tag \n\t \treturn tag.TagObject \n\telif (object_type_name == 'blob'): \n\t \tfrom . import blob \n\t \treturn blob.Blob \n\telif (object_type_name == 'tree'): \n\t \tfrom . import tree \n\t \treturn tree.Tree \n\telse: \n\t \traise ValueError(('Cannot \thandle \tunknown \tobject \ttype: \t%s' % object_type_name))\n", 
" \tchunk_size = (chunk_size or CHUNK_SIZE) \n\tif isinstance(iterator, (file, httplib.HTTPResponse)): \n\t \tget_data = iterator.read \n\t \targs = (chunk_size,) \n\telse: \n\t \tget_data = next \n\t \targs = (iterator,) \n\tdata = b('') \n\tempty = False \n\twhile ((not empty) or (len(data) > 0)): \n\t \tif (not empty): \n\t \t \ttry: \n\t \t \t \tchunk = b(get_data(*args)) \n\t \t \t \tif (len(chunk) > 0): \n\t \t \t \t \tdata += chunk \n\t \t \t \telse: \n\t \t \t \t \tempty = True \n\t \t \texcept StopIteration: \n\t \t \t \tempty = True \n\t \tif (len(data) == 0): \n\t \t \tif (empty and yield_empty): \n\t \t \t \t(yield b('')) \n\t \t \traise StopIteration \n\t \tif fill_size: \n\t \t \tif (empty or (len(data) >= chunk_size)): \n\t \t \t \t(yield data[:chunk_size]) \n\t \t \t \tdata = data[chunk_size:] \n\t \telse: \n\t \t \t(yield data) \n\t \t \tdata = b('')\n", 
" \tstore_utils.delete_image_location_from_backend(req.context, id, location_data)\n", 
" \tavailable = get_backends().keys() \n\tvalues = {'associated': [], 'not_associated': available, 'backends': available} \n\tif (hasattr(user, 'is_authenticated') and user.is_authenticated()): \n\t \tassociated = UserSocialAuth.get_social_auth_for_user(user) \n\t \tnot_associated = list((set(available) - set((assoc.provider for assoc in associated)))) \n\t \tvalues['associated'] = associated \n\t \tvalues['not_associated'] = not_associated \n\treturn values\n", 
" \tif isinstance(path, bytes): \n\t \treturn False \n\treturn (path.split('://', 1)[0] in ['http', 'https'])\n", 
" \ttry: \n\t \tret = store_api.delete_from_backend(location['url'], context=context) \n\t \tlocation['status'] = 'deleted' \n\t \tif ('id' in location): \n\t \t \tdb_api.get_api().image_location_delete(context, image_id, location['id'], 'deleted') \n\t \treturn ret \n\texcept store_api.NotFound: \n\t \tmsg = (_LW('Failed \tto \tdelete \timage \t%s \tin \tstore \tfrom \tURI') % image_id) \n\t \tLOG.warn(msg) \n\texcept store_api.StoreDeleteNotSupported as e: \n\t \tLOG.warn(encodeutils.exception_to_unicode(e)) \n\texcept store_api.UnsupportedBackend: \n\t \texc_type = sys.exc_info()[0].__name__ \n\t \tmsg = (_LE('Failed \tto \tdelete \timage \t%(image_id)s \tfrom \tstore: \t%(exc)s') % dict(image_id=image_id, exc=exc_type)) \n\t \tLOG.error(msg)\n", 
" \tdb_queue = scrubber.get_scrub_queue() \n\tif (not CONF.use_user_token): \n\t \tcontext = None \n\tret = db_queue.add_location(image_id, location) \n\tif ret: \n\t \tlocation['status'] = 'pending_delete' \n\t \tif ('id' in location): \n\t \t \tdb_api.get_api().image_location_delete(context, image_id, location['id'], 'pending_delete') \n\t \telse: \n\t \t \tdb_api.get_api().image_location_add(context, image_id, location) \n\treturn ret\n", 
" \treturn {'id': image.image_id, 'name': image.name, 'status': image.status, 'created_at': timeutils.isotime(image.created_at), 'updated_at': timeutils.isotime(image.updated_at), 'min_disk': image.min_disk, 'min_ram': image.min_ram, 'protected': image.protected, 'checksum': image.checksum, 'owner': image.owner, 'disk_format': image.disk_format, 'container_format': image.container_format, 'size': image.size, 'virtual_size': image.virtual_size, 'is_public': (image.visibility == 'public'), 'visibility': image.visibility, 'properties': dict(image.extra_properties), 'tags': list(image.tags), 'deleted': False, 'deleted_at': None}\n", 
" \tclass FakeSocket(object, ): \n\t \tdef __init__(self, *args, **kwargs): \n\t \t \tpass \n\t \tdef fileno(self): \n\t \t \treturn 42 \n\tclass FakeSendFile(object, ): \n\t \tdef __init__(self, req): \n\t \t \tself.req = req \n\t \tdef sendfile(self, o, i, offset, nbytes): \n\t \t \tos.lseek(i, offset, os.SEEK_SET) \n\t \t \tprev_len = len(self.req.body) \n\t \t \tself.req.body += os.read(i, nbytes) \n\t \t \treturn (len(self.req.body) - prev_len) \n\tclass FakeGlanceConnection(object, ): \n\t \tdef __init__(self, *args, **kwargs): \n\t \t \tself.sock = FakeSocket() \n\t \t \tself.stub_force_sendfile = kwargs.get('stub_force_sendfile', SENDFILE_SUPPORTED) \n\t \tdef connect(self): \n\t \t \treturn True \n\t \tdef close(self): \n\t \t \treturn True \n\t \tdef _clean_url(self, url): \n\t \t \treturn (url.replace('/v1', '', 1) if url.startswith('/v1') else url) \n\t \tdef putrequest(self, method, url): \n\t \t \tself.req = webob.Request.blank(self._clean_url(url)) \n\t \t \tif self.stub_force_sendfile: \n\t \t \t \tfake_sendfile = FakeSendFile(self.req) \n\t \t \t \tstubs.Set(sendfile, 'sendfile', fake_sendfile.sendfile) \n\t \t \tself.req.method = method \n\t \tdef putheader(self, key, value): \n\t \t \tself.req.headers[key] = value \n\t \tdef endheaders(self): \n\t \t \thl = [i.lower() for i in self.req.headers.keys()] \n\t \t \tassert (not (('content-length' in hl) and ('transfer-encoding' in hl))), 'Content-Length \tand \tTransfer-Encoding \tare \tmutually \texclusive' \n\t \tdef send(self, data): \n\t \t \tself.req.body += data.split('\\r\\n')[1] \n\t \tdef request(self, method, url, body=None, headers=None): \n\t \t \tself.req = webob.Request.blank(self._clean_url(url)) \n\t \t \tself.req.method = method \n\t \t \tif headers: \n\t \t \t \tself.req.headers = headers \n\t \t \tif body: \n\t \t \t \tself.req.body = body \n\t \tdef getresponse(self): \n\t \t \tmapper = routes.Mapper() \n\t \t \tapi = context.UnauthenticatedContextMiddleware(router.API(mapper)) \n\t \t \tres = self.req.get_response(api) \n\t \t \tdef fake_reader(): \n\t \t \t \treturn res.body \n\t \t \tsetattr(res, 'read', fake_reader) \n\t \t \treturn res \n\tdef fake_get_connection_type(client): \n\t \t'Returns \tthe \tproper \tconnection \ttype.' \n\t \tDEFAULT_REGISTRY_PORT = 9191 \n\t \tDEFAULT_API_PORT = 9292 \n\t \tif ((client.port == DEFAULT_API_PORT) and (client.host == '0.0.0.0')): \n\t \t \treturn FakeGlanceConnection \n\t \telif ((client.port == DEFAULT_REGISTRY_PORT) and (client.host == '0.0.0.0')): \n\t \t \trserver = kwargs.get('registry', None) \n\t \t \treturn FakeRegistryConnection(registry=rserver) \n\tdef fake_image_iter(self): \n\t \tfor i in self.source.app_iter: \n\t \t \t(yield i) \n\tdef fake_sendable(self, body): \n\t \tforce = getattr(self, 'stub_force_sendfile', None) \n\t \tif (force is None): \n\t \t \treturn self._stub_orig_sendable(body) \n\t \telse: \n\t \t \tif force: \n\t \t \t \tassert glance.common.client.SENDFILE_SUPPORTED \n\t \t \treturn force \n\tstubs.Set(glance.common.client.BaseClient, 'get_connection_type', fake_get_connection_type) \n\tsetattr(glance.common.client.BaseClient, '_stub_orig_sendable', glance.common.client.BaseClient._sendable) \n\tstubs.Set(glance.common.client.BaseClient, '_sendable', fake_sendable)\n", 
" \tdef fake_get_connection_type(client): \n\t \t'Returns \tthe \tproper \tconnection \ttype.' \n\t \tDEFAULT_REGISTRY_PORT = 9191 \n\t \tif ((client.port == DEFAULT_REGISTRY_PORT) and (client.host == '0.0.0.0')): \n\t \t \trserver = kwargs.pop('registry', None) \n\t \t \treturn FakeRegistryConnection(registry=rserver) \n\tdef fake_image_iter(self): \n\t \tfor i in self.response.app_iter: \n\t \t \t(yield i) \n\tstubs.Set(glance.common.client.BaseClient, 'get_connection_type', fake_get_connection_type)\n", 
" \tif (not timestamp): \n\t \traise ValueError('Specify \ta \tvalid \ttimestamp \tto \tpurge.') \n\tlogger.info(('Purging \texecutions \tolder \tthan \ttimestamp: \t%s' % timestamp.strftime('%Y-%m-%dT%H:%M:%S.%fZ'))) \n\tfilters = {} \n\tif purge_incomplete: \n\t \tfilters['start_timestamp__lt'] = timestamp \n\telse: \n\t \tfilters['end_timestamp__lt'] = timestamp \n\t \tfilters['start_timestamp__lt'] = timestamp \n\t \tfilters['status'] = {'$in': DONE_STATES} \n\texec_filters = copy.copy(filters) \n\tif action_ref: \n\t \texec_filters['action__ref'] = action_ref \n\tliveaction_filters = copy.deepcopy(filters) \n\tif action_ref: \n\t \tliveaction_filters['action'] = action_ref \n\ttry: \n\t \tdeleted_count = ActionExecution.delete_by_query(**exec_filters) \n\texcept InvalidQueryError as e: \n\t \tmsg = ('Bad \tquery \t(%s) \tused \tto \tdelete \texecution \tinstances: \t%sPlease \tcontact \tsupport.' % (exec_filters, str(e))) \n\t \traise InvalidQueryError(msg) \n\texcept: \n\t \tlogger.exception('Deletion \tof \texecution \tmodels \tfailed \tfor \tquery \twith \tfilters: \t%s.', exec_filters) \n\telse: \n\t \tlogger.info(('Deleted \t%s \taction \texecution \tobjects' % deleted_count)) \n\ttry: \n\t \tdeleted_count = LiveAction.delete_by_query(**liveaction_filters) \n\texcept InvalidQueryError as e: \n\t \tmsg = ('Bad \tquery \t(%s) \tused \tto \tdelete \tliveaction \tinstances: \t%sPlease \tcontact \tsupport.' % (liveaction_filters, str(e))) \n\t \traise InvalidQueryError(msg) \n\texcept: \n\t \tlogger.exception('Deletion \tof \tliveaction \tmodels \tfailed \tfor \tquery \twith \tfilters: \t%s.', liveaction_filters) \n\telse: \n\t \tlogger.info(('Deleted \t%s \tliveaction \tobjects' % deleted_count)) \n\tzombie_execution_instances = len(ActionExecution.query(**exec_filters)) \n\tzombie_liveaction_instances = len(LiveAction.query(**liveaction_filters)) \n\tif ((zombie_execution_instances > 0) or (zombie_liveaction_instances > 0)): \n\t \tlogger.error('Zombie \texecution \tinstances \tleft: \t%d.', zombie_execution_instances) \n\t \tlogger.error('Zombie \tliveaction \tinstances \tleft: \t%s.', zombie_liveaction_instances) \n\tlogger.info('All \texecution \tmodels \tolder \tthan \ttimestamp \t%s \twere \tdeleted.', timestamp)\n", 
" \t@functools.wraps(func) \n\tdef wrapped(*a, **kwargs): \n\t \tfunc.__test__ = False \n\t \ttest_obj = a[0] \n\t \tmessage = getattr(test_obj, 'disabled_message', 'Test \tdisabled') \n\t \tif getattr(test_obj, 'disabled', False): \n\t \t \ttest_obj.skipTest(message) \n\t \tfunc(*a, **kwargs) \n\treturn wrapped\n", 
" \tenv = os.environ.copy() \n\tif (exec_env is not None): \n\t \tfor (env_name, env_val) in exec_env.items(): \n\t \t \tif callable(env_val): \n\t \t \t \tenv[env_name] = env_val(env.get(env_name)) \n\t \t \telse: \n\t \t \t \tenv[env_name] = env_val \n\tif (no_venv and ('VIRTUAL_ENV' in env)): \n\t \tenv['PATH'] = env['PATH'].split(os.pathsep, 1)[(-1)] \n\t \tdel env['VIRTUAL_ENV'] \n\tpath_ext = [os.path.join(os.getcwd(), 'bin')] \n\targs = shlex.split(cmd) \n\texecutable = args[0] \n\tif os.path.isabs(executable): \n\t \tpath_ext.append(os.path.dirname(executable)) \n\tenv['PATH'] = ((':'.join(path_ext) + ':') + env['PATH']) \n\tprocess = subprocess.Popen(args, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=env) \n\tif expect_exit: \n\t \tresult = process.communicate() \n\t \t(out, err) = result \n\t \texitcode = process.returncode \n\telse: \n\t \tout = '' \n\t \terr = '' \n\t \texitcode = 0 \n\tif ((exitcode != expected_exitcode) and raise_error): \n\t \tmsg = ('Command \t%(cmd)s \tdid \tnot \tsucceed. \tReturned \tan \texit \tcode \tof \t%(exitcode)d.\\n\\nSTDOUT: \t%(out)s\\n\\nSTDERR: \t%(err)s' % {'cmd': cmd, 'exitcode': exitcode, 'out': out, 'err': err}) \n\t \tif context: \n\t \t \tmsg += ('\\n\\nCONTEXT: \t%s' % context) \n\t \traise RuntimeError(msg) \n\treturn (exitcode, out, err)\n", 
" \tif os.path.isabs(cmdname): \n\t \treturn cmdname \n\tpath = ([os.path.join(os.getcwd(), 'bin')] + os.environ['PATH'].split(os.pathsep)) \n\tfor elem in path: \n\t \tfull_path = os.path.join(elem, cmdname) \n\t \tif os.access(full_path, os.X_OK): \n\t \t \treturn full_path \n\treturn None\n", 
" \t(port, s) = get_unused_port_and_socket() \n\ts.close() \n\treturn port\n", 
" \ttry: \n\t \timport xattr \n\texcept ImportError: \n\t \treturn False \n\tdef set_xattr(path, key, value): \n\t \txattr.setxattr(path, ('user.%s' % key), value) \n\tfake_filepath = os.path.join(path, 'testing-checkme') \n\tresult = True \n\twith open(fake_filepath, 'wb') as fake_file: \n\t \tfake_file.write('XXX') \n\t \tfake_file.flush() \n\ttry: \n\t \tset_xattr(fake_filepath, 'hits', '1') \n\texcept IOError as e: \n\t \tif (e.errno == errno.EOPNOTSUPP): \n\t \t \tresult = False \n\telse: \n\t \tif os.path.exists(fake_filepath): \n\t \t \tos.unlink(fake_filepath) \n\treturn result\n", 
" \tif (parsed_url.scheme == 'https'): \n\t \tconn = httplib.HTTPSConnection(parsed_url.netloc) \n\telse: \n\t \tconn = httplib.HTTPConnection(parsed_url.netloc) \n\treturn conn\n", 
" \tclient = _get_client() \n\tstatus = base_status.copy() \n\tret = client.search(term) \n\tif ret: \n\t \t_valid(status, out=ret, id_=term) \n\telse: \n\t \t_invalid(status) \n\treturn status\n", 
" \tconn = boto.connect_s3() \n\ttry: \n\t \tbucket = conn.get_bucket(bucket_name) \n\texcept boto.exception.S3ResponseError as e: \n\t \tconn.create_bucket(bucket_name) \n\t \tbucket = conn.get_bucket(bucket_name, validate=False) \n\tkey = boto.s3.key.Key(bucket) \n\tkey.key = key_name \n\tkey.set_contents_from_filename(file_path) \n\tkey.set_acl('public-read') \n\turl = 'https://s3.amazonaws.com/{}/{}'.format(bucket.name, key.name) \n\tprint 'URL: \t{}'.format(url) \n\treturn url\n", 
" \tif (key in data): \n\t \tdata[key].add(value) \n\telse: \n\t \tdata[key] = set([value])\n", 
" \treturn Table(name, MetaData(bind=session.bind), autoload=True)\n", 
" \tr = jsonify(message=message) \n\tr.status_code = status_code \n\treturn r\n", 
" \t(groups, referrers) = ((groups or []), (referrers or [])) \n\treferrers = [('.r:%s' % r) for r in referrers] \n\tresult = ','.join((groups + referrers)) \n\treturn (clean_acl(header_name, result) if header_name else result)\n", 
" \tparams = http_request.uri.query.copy() \n\tparams['oauth_consumer_key'] = consumer_key \n\tparams['oauth_nonce'] = nonce \n\tparams['oauth_signature_method'] = signaure_type \n\tparams['oauth_timestamp'] = str(timestamp) \n\tif (next is not None): \n\t \tparams['oauth_callback'] = str(next) \n\tif (token is not None): \n\t \tparams['oauth_token'] = token \n\tif (version is not None): \n\t \tparams['oauth_version'] = version \n\tif (verifier is not None): \n\t \tparams['oauth_verifier'] = verifier \n\tsorted_keys = None \n\ttry: \n\t \tsorted_keys = sorted(params.keys()) \n\texcept NameError: \n\t \tsorted_keys = params.keys() \n\t \tsorted_keys.sort() \n\tpairs = [] \n\tfor key in sorted_keys: \n\t \tpairs.append(('%s=%s' % (urllib.quote(key, safe='~'), urllib.quote(params[key], safe='~')))) \n\tall_parameters = urllib.quote('&'.join(pairs), safe='~') \n\tnormailzed_host = http_request.uri.host.lower() \n\tnormalized_scheme = (http_request.uri.scheme or 'http').lower() \n\tnon_default_port = None \n\tif ((http_request.uri.port is not None) and (((normalized_scheme == 'https') and (http_request.uri.port != 443)) or ((normalized_scheme == 'http') and (http_request.uri.port != 80)))): \n\t \tnon_default_port = http_request.uri.port \n\tpath = (http_request.uri.path or '/') \n\trequest_path = None \n\tif (not path.startswith('/')): \n\t \tpath = ('/%s' % path) \n\tif (non_default_port is not None): \n\t \trequest_path = urllib.quote(('%s://%s:%s%s' % (normalized_scheme, normailzed_host, non_default_port, path)), safe='~') \n\telse: \n\t \trequest_path = urllib.quote(('%s://%s%s' % (normalized_scheme, normailzed_host, path)), safe='~') \n\tbase_string = '&'.join((http_request.method.upper(), request_path, all_parameters)) \n\treturn base_string\n", 
" \tassert (not isinstance(header_values, basestring)) \n\tresult = [] \n\tfor text in header_values: \n\t \torig_text = text \n\t \tpairs = [] \n\t \twhile text: \n\t \t \tm = HEADER_TOKEN_RE.search(text) \n\t \t \tif m: \n\t \t \t \ttext = unmatched(m) \n\t \t \t \tname = m.group(1) \n\t \t \t \tm = HEADER_QUOTED_VALUE_RE.search(text) \n\t \t \t \tif m: \n\t \t \t \t \ttext = unmatched(m) \n\t \t \t \t \tvalue = m.group(1) \n\t \t \t \t \tvalue = HEADER_ESCAPE_RE.sub('\\\\1', value) \n\t \t \t \telse: \n\t \t \t \t \tm = HEADER_VALUE_RE.search(text) \n\t \t \t \t \tif m: \n\t \t \t \t \t \ttext = unmatched(m) \n\t \t \t \t \t \tvalue = m.group(1) \n\t \t \t \t \t \tvalue = value.rstrip() \n\t \t \t \t \telse: \n\t \t \t \t \t \tvalue = None \n\t \t \t \tpairs.append((name, value)) \n\t \t \telif text.lstrip().startswith(','): \n\t \t \t \ttext = text.lstrip()[1:] \n\t \t \t \tif pairs: \n\t \t \t \t \tresult.append(pairs) \n\t \t \t \tpairs = [] \n\t \t \telse: \n\t \t \t \t(non_junk, nr_junk_chars) = re.subn('^[=\\\\s;]*', '', text) \n\t \t \t \tassert (nr_junk_chars > 0), (\"split_header_words \tbug: \t'%s', \t'%s', \t%s\" % (orig_text, text, pairs)) \n\t \t \t \ttext = non_junk \n\t \tif pairs: \n\t \t \tresult.append(pairs) \n\treturn result\n", 
" \tif (not path.startswith('s3://')): \n\t \traise ValueError(('Bad \tS3 \tpath \t%s' % path)) \n\tr = path[len('s3://'):].split('/', 1) \n\tbucket = key = None \n\tif (len(r) == 2): \n\t \t(bucket, key) = (r[0], r[1]) \n\telse: \n\t \tbucket = r[0] \n\tif (not bucket): \n\t \traise ValueError(('Bad \tS3 \tpath \t%s' % path)) \n\treturn (bucket, key)\n", 
" \tconf = global_conf.copy() \n\tconf.update(local_conf) \n\tdef filter(app): \n\t \treturn ContextMiddleware(app, conf) \n\treturn filter\n", 
" \tif redirect_output: \n\t \tstdout = subprocess.PIPE \n\telse: \n\t \tstdout = None \n\tproc = subprocess.Popen(cmd, cwd=ROOT, stdout=stdout) \n\toutput = proc.communicate()[0] \n\tif (check_exit_code and (proc.returncode != 0)): \n\t \traise Exception(('Command \t\"%s\" \tfailed.\\n%s' % (' \t'.join(cmd), output))) \n\treturn output\n", 
" \tif (not HAS_VIRTUALENV): \n\t \traise Exception(('Virtualenv \tnot \tfound. \t' + 'Try \tinstalling \tpython-virtualenv')) \n\tprint 'done.'\n", 
" \tprint 'Creating \tvenv...', \n\tinstall = ['virtualenv', '-q', venv] \n\trun_command(install) \n\tprint 'done.' \n\tprint 'Installing \tpip \tin \tvirtualenv...', \n\tif (install_pip and (not run_command(['tools/with_venv.sh', 'easy_install', 'pip>1.0']))): \n\t \tdie('Failed \tto \tinstall \tpip.') \n\tprint 'done.'\n", 
" \tallparts = [] \n\twhile True: \n\t \tparts = os.path.split(path) \n\t \tif (parts[0] == path): \n\t \t \tallparts.insert(0, parts[0]) \n\t \t \tbreak \n\t \telif (parts[1] == path): \n\t \t \tallparts.insert(0, parts[1]) \n\t \t \tbreak \n\t \telse: \n\t \t \tpath = parts[0] \n\t \t \tallparts.insert(0, parts[1]) \n\treturn allparts\n", 
" \treturn re.sub(' \t+', ' \t', normalize_newlines(text).replace('\\n', '')).strip()\n", 
" \treturn shortcuts.redirect(horizon.get_user_home(request.user))\n", 
" \tif hasattr(random, 'SystemRandom'): \n\t \tlogging.info('Generating \ta \tsecure \trandom \tkey \tusing \tSystemRandom.') \n\t \tchoice = random.SystemRandom().choice \n\telse: \n\t \tmsg = 'WARNING: \tSystemRandom \tnot \tpresent. \tGenerating \ta \trandom \tkey \tusing \trandom.choice \t(NOT \tCRYPTOGRAPHICALLY \tSECURE).' \n\t \tlogging.warning(msg) \n\t \tchoice = random.choice \n\treturn ''.join(map((lambda x: choice((string.digits + string.ascii_letters))), range(key_length)))\n", 
" \tabspath = os.path.abspath(key_file) \n\tif os.path.exists(key_file): \n\t \tkey = read_from_file(key_file) \n\t \treturn key \n\tlock = lockutils.external_lock((key_file + '.lock'), lock_path=os.path.dirname(abspath)) \n\twith lock: \n\t \tif (not os.path.exists(key_file)): \n\t \t \tkey = generate_key(key_length) \n\t \t \told_umask = os.umask(127) \n\t \t \twith open(key_file, 'w') as f: \n\t \t \t \tf.write(key) \n\t \t \tos.umask(old_umask) \n\t \telse: \n\t \t \tkey = read_from_file(key_file) \n\t \treturn key\n", 
" \tcontext = {'HORIZON_CONFIG': conf.HORIZON_CONFIG, 'True': True, 'False': False} \n\treturn context\n", 
" \tif hasattr(request, '_messages'): \n\t \treturn request._messages.add(level, message, extra_tags) \n\tif (not fail_silently): \n\t \traise MessageFailure('You \tcannot \tadd \tmessages \twithout \tinstalling \tdjango.contrib.messages.middleware.MessageMiddleware')\n", 
" \tadd_message(get_request(request), constants.DEBUG, message)\n", 
" \tadd_message(get_request(request), constants.INFO, message)\n", 
" \tadd_message(get_request(request), constants.SUCCESS, message)\n", 
" \tadd_message(get_request(request), constants.WARNING, message)\n", 
" \tadd_message(get_request(request), constants.ERROR, message)\n", 
" \t@functools.wraps(view_func, assigned=available_attrs(view_func)) \n\tdef dec(request, *args, **kwargs): \n\t \tif dashboard: \n\t \t \trequest.horizon['dashboard'] = dashboard \n\t \tif panel: \n\t \t \trequest.horizon['panel'] = panel \n\t \treturn view_func(request, *args, **kwargs) \n\treturn dec\n", 
" \tfrom horizon.exceptions import NotAuthenticated \n\t@functools.wraps(view_func, assigned=available_attrs(view_func)) \n\tdef dec(request, *args, **kwargs): \n\t \tif request.user.is_authenticated(): \n\t \t \treturn view_func(request, *args, **kwargs) \n\t \traise NotAuthenticated(_('Please \tlog \tin \tto \tcontinue.')) \n\treturn dec\n", 
" \tfrom horizon.exceptions import NotAuthorized \n\tcurrent_perms = getattr(view_func, '_required_perms', set([])) \n\tview_func._required_perms = (current_perms | set(required)) \n\t@functools.wraps(view_func, assigned=available_attrs(view_func)) \n\tdef dec(request, *args, **kwargs): \n\t \tif request.user.is_authenticated(): \n\t \t \tif request.user.has_perms(view_func._required_perms): \n\t \t \t \treturn view_func(request, *args, **kwargs) \n\t \traise NotAuthorized((_('You \tare \tnot \tauthorized \tto \taccess \t%s') % request.path)) \n\tif required: \n\t \treturn dec \n\telse: \n\t \treturn view_func\n", 
" \t(exc_type, exc_value, exc_traceback) = sys.exc_info() \n\tif set(str(exc_value).split(' \t')).issuperset(set(keywords)): \n\t \texc_value.message = message \n\t \traise\n", 
" \t(exc_type, exc_value, exc_traceback) = sys.exc_info() \n\tlog_method = getattr(LOG, (log_level or 'exception')) \n\tforce_log = (force_log or os.environ.get('HORIZON_TEST_RUN', False)) \n\tforce_silence = getattr(exc_value, 'silence_logging', False) \n\thandled = issubclass(exc_type, HandledException) \n\twrap = False \n\tif handled: \n\t \t(exc_type, exc_value, exc_traceback) = exc_value.wrapped \n\t \twrap = True \n\tlog_entry = encoding.force_text(exc_value) \n\tuser_message = '' \n\tif issubclass(exc_type, HorizonException): \n\t \tuser_message = log_entry \n\telif (message and ('%(exc)s' in message)): \n\t \tuser_message = (encoding.force_text(message) % {'exc': log_entry}) \n\telif message: \n\t \tuser_message = encoding.force_text(message) \n\tfor exc_handler in HANDLE_EXC_METHODS: \n\t \tif issubclass(exc_type, exc_handler['exc']): \n\t \t \tif exc_handler['set_wrap']: \n\t \t \t \twrap = True \n\t \t \thandler = exc_handler['handler'] \n\t \t \tret = handler(request, user_message, redirect, ignore, exc_handler.get('escalate', escalate), handled, force_silence, force_log, log_method, log_entry, log_level) \n\t \t \tif ret: \n\t \t \t \treturn ret \n\tif wrap: \n\t \traise HandledException([exc_type, exc_value, exc_traceback]) \n\tif message: \n\t \tret = handle_recoverable(request, user_message, redirect, ignore, escalate, handled, force_silence, force_log, log_method, log_entry, log_level) \n\t \tif ret: \n\t \t \treturn ret \n\tsix.reraise(exc_type, exc_value, exc_traceback)\n", 
" \treturn user.has_perms(getattr(component, 'permissions', set()))\n", 
" \tif ('request' not in context): \n\t \treturn {} \n\tcurrent_dashboard = context['request'].horizon.get('dashboard', None) \n\tdashboards = [] \n\tfor dash in Horizon.get_dashboards(): \n\t \tif dash.can_access(context): \n\t \t \tif (callable(dash.nav) and dash.nav(context)): \n\t \t \t \tdashboards.append(dash) \n\t \t \telif dash.nav: \n\t \t \t \tdashboards.append(dash) \n\treturn {'components': dashboards, 'user': context['request'].user, 'current': current_dashboard, 'request': context['request']}\n", 
" \tif ('request' not in context): \n\t \treturn {} \n\tdashboard = context['request'].horizon['dashboard'] \n\tpanel_groups = dashboard.get_panel_groups() \n\tnon_empty_groups = [] \n\tfor group in panel_groups.values(): \n\t \tallowed_panels = [] \n\t \tfor panel in group: \n\t \t \tif (callable(panel.nav) and panel.nav(context) and panel.can_access(context)): \n\t \t \t \tallowed_panels.append(panel) \n\t \t \telif ((not callable(panel.nav)) and panel.nav and panel.can_access(context)): \n\t \t \t \tallowed_panels.append(panel) \n\t \tif allowed_panels: \n\t \t \tif (group.name is None): \n\t \t \t \tnon_empty_groups.append((dashboard.name, allowed_panels)) \n\t \t \telse: \n\t \t \t \tnon_empty_groups.append((group.name, allowed_panels)) \n\treturn {'components': OrderedDict(non_empty_groups), 'user': context['request'].user, 'current': context['request'].horizon['panel'].slug, 'request': context['request']}\n", 
" \tif show_progress: \n\t \treturn click.progressbar(it, **kwargs) \n\treturn CallbackManager((lambda it=it: it))\n", 
" \tnodelist = parser.parse(('endjstemplate',)) \n\tparser.delete_first_token() \n\treturn JSTemplateNode(nodelist)\n", 
" \treturn ceph_cfg.pool_add(pool_name, **kwargs)\n", 
" \treturn ceph_cfg.pool_add(pool_name, **kwargs)\n", 
" \treturn list_objects(service_instance, vim.ResourcePool)\n", 
" \tif (call != 'function'): \n\t \traise SaltCloudSystemExit('The \tattach_lb \tfunction \tmust \tbe \tcalled \twith \t-f \tor \t--function.') \n\tif ((not kwargs) or ('name' not in kwargs)): \n\t \tlog.error('A \tload-balancer \tname \tmust \tbe \tspecified.') \n\t \treturn False \n\tif ('member' not in kwargs): \n\t \tlog.error('A \tnode \tname \tname \tmust \tbe \tspecified.') \n\t \treturn False \n\tconn = get_conn() \n\tnode = conn.ex_get_node(kwargs['member']) \n\tlb_conn = get_lb_conn(conn) \n\tlb = lb_conn.get_balancer(kwargs['name']) \n\t__utils__['cloud.fire_event']('event', 'attach \tload_balancer', 'salt/cloud/loadbalancer/attaching', args=kwargs, sock_dir=__opts__['sock_dir'], transport=__opts__['transport']) \n\tresult = lb_conn.balancer_attach_compute_node(lb, node) \n\t__utils__['cloud.fire_event']('event', 'attached \tload_balancer', 'salt/cloud/loadbalancer/attached', args=kwargs, sock_dir=__opts__['sock_dir'], transport=__opts__['transport']) \n\treturn _expand_item(result)\n", 
" \thas_more_data = False \n\thas_prev_data = False \n\tif paginate: \n\t \tif reversed_order: \n\t \t \tsort_dir = ('desc' if (sort_dir == 'asc') else 'asc') \n\t \tpage_size = utils.get_page_size(request) \n\t \tflavors = novaclient(request).flavors.list(is_public=is_public, marker=marker, limit=(page_size + 1), sort_key=sort_key, sort_dir=sort_dir) \n\t \t(flavors, has_more_data, has_prev_data) = update_pagination(flavors, page_size, marker, sort_dir, sort_key, reversed_order) \n\telse: \n\t \tflavors = novaclient(request).flavors.list(is_public=is_public) \n\tif get_extras: \n\t \tfor flavor in flavors: \n\t \t \tflavor.extras = flavor_get_extras(request, flavor.id, True, flavor) \n\treturn (flavors, has_more_data, has_prev_data)\n", 
" \tif (flavor is None): \n\t \tflavor = novaclient(request).flavors.get(flavor_id) \n\textras = flavor.get_keys() \n\tif raw: \n\t \treturn extras \n\treturn [FlavorExtraSpec(flavor_id, key, value) for (key, value) in extras.items()]\n", 
" \tflavor = novaclient(request).flavors.get(flavor_id) \n\treturn flavor.unset_keys(keys)\n", 
" \tflavor = novaclient(request).flavors.get(flavor_id) \n\tif (not metadata): \n\t \treturn None \n\treturn flavor.set_keys(metadata)\n", 
" \treturn novaclient(request).servers.get_console_output(instance_id, length=tail_length)\n", 
" \tsgs = list_securitygroup() \n\tsecuritygroup = config.get_cloud_config_value('securitygroup', vm_, __opts__, search_global=False) \n\tif (not securitygroup): \n\t \traise SaltCloudNotFound('No \tsecuritygroup \tID \tspecified \tfor \tthis \tVM.') \n\tif (securitygroup and (str(securitygroup) in sgs)): \n\t \treturn sgs[securitygroup]['SecurityGroupId'] \n\traise SaltCloudNotFound(\"The \tspecified \tsecurity \tgroup, \t'{0}', \tcould \tnot \tbe \tfound.\".format(securitygroup))\n", 
" \thas_more_data = False \n\thas_prev_data = False \n\tvolumes = [] \n\tc_client = cinderclient(request) \n\tif (c_client is None): \n\t \treturn (volumes, has_more_data, has_prev_data) \n\ttransfers = {t.volume_id: t for t in transfer_list(request, search_opts=search_opts)} \n\tif ((VERSIONS.active > 1) and paginate): \n\t \tpage_size = utils.get_page_size(request) \n\t \tsort = ('created_at:' + sort_dir) \n\t \tfor v in c_client.volumes.list(search_opts=search_opts, limit=(page_size + 1), marker=marker, sort=sort): \n\t \t \tv.transfer = transfers.get(v.id) \n\t \t \tvolumes.append(Volume(v)) \n\t \t(volumes, has_more_data, has_prev_data) = update_pagination(volumes, page_size, marker, sort_dir) \n\telse: \n\t \tfor v in c_client.volumes.list(search_opts=search_opts): \n\t \t \tv.transfer = transfers.get(v.id) \n\t \t \tvolumes.append(Volume(v)) \n\treturn (volumes, has_more_data, has_prev_data)\n", 
" \timage = glanceclient(request).images.get(image_id) \n\treturn Image(image)\n", 
" \treturn IP_VERSION_DICT.get(ip_version, '')\n", 
" \tLOG.debug(('network_list_for_tenant(): \ttenant_id=%s, \tparams=%s' % (tenant_id, params))) \n\tnetworks = [] \n\tshared = params.get('shared') \n\tif (shared is not None): \n\t \tdel params['shared'] \n\tif (shared in (None, False)): \n\t \tnetworks += network_list(request, tenant_id=tenant_id, shared=False, **params) \n\tif (shared in (None, True)): \n\t \tnetworks += network_list(request, shared=True, **params) \n\tparams['router:external'] = params.get('router:external', True) \n\tif (params['router:external'] and include_external): \n\t \tif (shared is not None): \n\t \t \tparams['shared'] = shared \n\t \tfetched_net_ids = [n.id for n in networks] \n\t \text_nets = network_list(request, **params) \n\t \tnetworks += [n for n in ext_nets if (n.id not in fetched_net_ids)] \n\treturn networks\n", 
" \tLOG.debug(('subnet_create(): \tnetid=%s, \tkwargs=%s' % (network_id, kwargs))) \n\tbody = {'subnet': {'network_id': network_id}} \n\tif ('tenant_id' not in kwargs): \n\t \tkwargs['tenant_id'] = request.user.project_id \n\tbody['subnet'].update(kwargs) \n\tsubnet = neutronclient(request).create_subnet(body=body).get('subnet') \n\treturn Subnet(subnet)\n", 
" \tLOG.debug(('subnet_create(): \tnetid=%s, \tkwargs=%s' % (network_id, kwargs))) \n\tbody = {'subnet': {'network_id': network_id}} \n\tif ('tenant_id' not in kwargs): \n\t \tkwargs['tenant_id'] = request.user.project_id \n\tbody['subnet'].update(kwargs) \n\tsubnet = neutronclient(request).create_subnet(body=body).get('subnet') \n\treturn Subnet(subnet)\n", 
" \tLOG.debug(('port_create(): \tnetid=%s, \tkwargs=%s' % (network_id, kwargs))) \n\tif ('policy_profile_id' in kwargs): \n\t \tkwargs['n1kv:profile'] = kwargs.pop('policy_profile_id') \n\tkwargs = unescape_port_kwargs(**kwargs) \n\tbody = {'port': {'network_id': network_id}} \n\tif ('tenant_id' not in kwargs): \n\t \tkwargs['tenant_id'] = request.user.project_id \n\tbody['port'].update(kwargs) \n\tport = neutronclient(request).create_port(body=body).get('port') \n\treturn Port(port)\n", 
" \tapi_version = VERSIONS.get_active_version() \n\tuser = request.user \n\ttoken_id = user.token.id \n\tif is_multi_domain_enabled: \n\t \tif is_domain_admin(request): \n\t \t \tdomain_token = request.session.get('domain_token') \n\t \t \tif domain_token: \n\t \t \t \ttoken_id = getattr(domain_token, 'auth_token', None) \n\tif admin: \n\t \tif (not policy.check((('identity', 'admin_required'),), request)): \n\t \t \traise exceptions.NotAuthorized \n\t \tendpoint_type = 'adminURL' \n\telse: \n\t \tendpoint_type = getattr(settings, 'OPENSTACK_ENDPOINT_TYPE', 'internalURL') \n\tcache_attr = ('_keystoneclient_admin' if admin else backend.KEYSTONE_CLIENT_ATTR) \n\tif (hasattr(request, cache_attr) and ((not user.token.id) or (getattr(request, cache_attr).auth_token == user.token.id))): \n\t \tconn = getattr(request, cache_attr) \n\telse: \n\t \tendpoint = _get_endpoint_url(request, endpoint_type) \n\t \tinsecure = getattr(settings, 'OPENSTACK_SSL_NO_VERIFY', False) \n\t \tcacert = getattr(settings, 'OPENSTACK_SSL_CACERT', None) \n\t \tLOG.debug(('Creating \ta \tnew \tkeystoneclient \tconnection \tto \t%s.' % endpoint)) \n\t \tremote_addr = request.environ.get('REMOTE_ADDR', '') \n\t \tconn = api_version['client'].Client(token=token_id, endpoint=endpoint, original_ip=remote_addr, insecure=insecure, cacert=cacert, auth_url=endpoint, debug=settings.DEBUG) \n\t \tsetattr(request, cache_attr, conn) \n\treturn conn\n", 
" \ttenants = [x for x in keystone.tenants.list() if (x.name == name)] \n\tcount = len(tenants) \n\tif (count == 0): \n\t \traise KeyError(('No \tkeystone \ttenants \twith \tname \t%s' % name)) \n\telif (count > 1): \n\t \traise ValueError(('%d \ttenants \twith \tname \t%s' % (count, name))) \n\telse: \n\t \treturn tenants[0]\n", 
" \tmanager = keystoneclient(request, admin=True).roles \n\troles = [] \n\tkwargs = {} \n\tif (filters is not None): \n\t \tkwargs.update(filters) \n\tif ('id' in kwargs): \n\t \ttry: \n\t \t \troles = [manager.get(kwargs['id'])] \n\t \texcept keystone_exceptions.NotFound: \n\t \t \troles = [] \n\t \texcept Exception: \n\t \t \texceptions.handle(request) \n\telse: \n\t \troles = manager.list(**kwargs) \n\treturn roles\n", 
" \tmanager = keystoneclient(request, admin=True).roles \n\tif (VERSIONS.active < 3): \n\t \treturn manager.add_user_role(user, role, project) \n\telse: \n\t \treturn manager.grant(role, user=user, project=project, group=group, domain=domain)\n", 
" \tmanager = keystoneclient(request, admin=True).roles \n\tif (VERSIONS.active < 3): \n\t \treturn manager.remove_user_role(user, role, project) \n\telse: \n\t \treturn manager.revoke(role, user=user, project=project, group=group, domain=domain)\n", 
" \tclient = keystoneclient(request, admin=True) \n\troles = client.roles.roles_for_user(user, project) \n\tfor role in roles: \n\t \tremove_tenant_user_role(request, user=user, role=role.id, project=project, domain=domain)\n", 
" \tglobal DEFAULT_ROLE \n\tdefault = getattr(settings, 'OPENSTACK_KEYSTONE_DEFAULT_ROLE', None) \n\tif (default and (DEFAULT_ROLE is None)): \n\t \ttry: \n\t \t \troles = keystoneclient(request, admin=True).roles.list() \n\t \texcept Exception: \n\t \t \troles = [] \n\t \t \texceptions.handle(request) \n\t \tfor role in roles: \n\t \t \tif ((role.id == default) or (role.name == default)): \n\t \t \t \tDEFAULT_ROLE = role \n\t \t \t \tbreak \n\treturn DEFAULT_ROLE\n", 
" \tobjects = [] \n\tfor item in items: \n\t \tif (item.get('subdir', None) is not None): \n\t \t \tobject_cls = PseudoFolder \n\t \telse: \n\t \t \tobject_cls = StorageObject \n\t \tobjects.append(object_cls(item, container_name)) \n\treturn objects\n", 
" \treturn _aliases.get(alias.lower(), alias)\n", 
" \twith open(CHANGELOG) as f: \n\t \tcur_index = 0 \n\t \tfor line in f: \n\t \t \tmatch = re.search('^\\\\d+\\\\.\\\\d+\\\\.\\\\d+', line) \n\t \t \tif match: \n\t \t \t \tif (cur_index == index): \n\t \t \t \t \treturn match.group(0) \n\t \t \t \telse: \n\t \t \t \t \tcur_index += 1\n", 
" \tgitstr = _git_str() \n\tif (gitstr is None): \n\t \tgitstr = '' \n\tpath = os.path.join(BASEDIR, 'qutebrowser', 'git-commit-id') \n\twith _open(path, 'w', encoding='ascii') as f: \n\t \tf.write(gitstr)\n", 
" \tif (len(sys.argv) < 2): \n\t \treturn True \n\tinfo_commands = ['--help-commands', '--name', '--version', '-V', '--fullname', '--author', '--author-email', '--maintainer', '--maintainer-email', '--contact', '--contact-email', '--url', '--license', '--description', '--long-description', '--platforms', '--classifiers', '--keywords', '--provides', '--requires', '--obsoletes'] \n\tinfo_commands.extend(['egg_info', 'install_egg_info', 'rotate']) \n\tfor command in info_commands: \n\t \tif (command in sys.argv[1:]): \n\t \t \treturn False \n\treturn True\n", 
" \trefs = list_refs(refnames=[refname], repo_dir=repo_dir, limit_to_heads=True) \n\tl = tuple(islice(refs, 2)) \n\tif l: \n\t \tassert (len(l) == 1) \n\t \treturn l[0][1] \n\telse: \n\t \treturn None\n", 
" \tif isinstance(fileIshObject, TextIOBase): \n\t \treturn unicode \n\tif isinstance(fileIshObject, IOBase): \n\t \treturn bytes \n\tencoding = getattr(fileIshObject, 'encoding', None) \n\timport codecs \n\tif isinstance(fileIshObject, (codecs.StreamReader, codecs.StreamWriter)): \n\t \tif encoding: \n\t \t \treturn unicode \n\t \telse: \n\t \t \treturn bytes \n\tif (not _PY3): \n\t \tif isinstance(fileIshObject, file): \n\t \t \tif (encoding is not None): \n\t \t \t \treturn basestring \n\t \t \telse: \n\t \t \t \treturn bytes \n\t \tfrom cStringIO import InputType, OutputType \n\t \tfrom StringIO import StringIO \n\t \tif isinstance(fileIshObject, (StringIO, InputType, OutputType)): \n\t \t \treturn bytes \n\treturn default\n", 
" \tcmd = (_pkg(jail, chroot, root) + ['--version']) \n\treturn __salt__['cmd.run'](cmd).strip()\n", 
" \tcmd = (_pkg(jail, chroot, root) + ['--version']) \n\treturn __salt__['cmd.run'](cmd).strip()\n", 
" \ttry: \n\t \tuuid.UUID(value) \n\t \treturn value \n\texcept (ValueError, AttributeError): \n\t \treturn int(value)\n", 
" \tcontext = {} \n\tcontext.setdefault('authorized_tenants', []) \n\tif request.user.is_authenticated(): \n\t \tcontext['authorized_tenants'] = [tenant for tenant in request.user.authorized_tenants if tenant.enabled] \n\tavailable_regions = getattr(settings, 'AVAILABLE_REGIONS', []) \n\tregions = {'support': (len(available_regions) > 1), 'current': {'endpoint': request.session.get('region_endpoint'), 'name': request.session.get('region_name')}, 'available': [{'endpoint': region[0], 'name': region[1]} for region in available_regions]} \n\tcontext['regions'] = regions \n\tcontext['WEBROOT'] = getattr(settings, 'WEBROOT', '/') \n\tprofiler_settings = getattr(settings, 'OPENSTACK_PROFILER', {}) \n\tprofiler_enabled = profiler_settings.get('enabled', False) \n\tcontext['profiler_enabled'] = profiler_enabled \n\tif (profiler_enabled and ('profile_page' in request.COOKIES)): \n\t \tindex_view_id = request.META.get(profiler.ROOT_HEADER, '') \n\t \thmac_keys = profiler_settings.get('keys', []) \n\t \tcontext['x_trace_info'] = profiler.update_trace_headers(hmac_keys, parent_id=index_view_id) \n\tjs_catalog = ['horizon', 'openstack_dashboard'] \n\tregex = re.compile('^openstack_dashboard') \n\tall_plugins = conf.HORIZON_CONFIG['plugins'] \n\tjs_catalog.extend((p for p in all_plugins if (not regex.search(p)))) \n\tcontext['JS_CATALOG'] = '+'.join(js_catalog) \n\treturn context\n", 
" \timport cPickle \n\tdata = [1, 1.0, 2j, 2L, System.Int64(1), System.UInt64(1), System.UInt32(1), System.Int16(1), System.UInt16(1), System.Byte(1), System.SByte(1), System.Decimal(1), System.Char.MaxValue, System.DBNull.Value, System.Single(1.0), System.DateTime.Now, None, {}, (), [], {'a': 2}, (42,), [42], System.StringSplitOptions.RemoveEmptyEntries] \n\tdata.append(list(data)) \n\tdata.append(tuple(data)) \n\tclass X: \n\t \tdef __init__(self): \n\t \t \tself.abc = 3 \n\tclass Y(object, ): \n\t \tdef __init__(self): \n\t \t \tself.abc = 3 \n\tdata.append(X().__dict__) \n\tdata.append(Y().__dict__) \n\tl = [] \n\tl.append(l) \n\tdata.append(l) \n\td = {} \n\tcnt = 100 \n\tfor x in data: \n\t \td[cnt] = x \n\t \tcnt += 1 \n\tdata.append(d) \n\td1 = {} \n\td2 = {} \n\td1['abc'] = d2 \n\td1['foo'] = 'baz' \n\td2['abc'] = d1 \n\tdata.append(d1) \n\tdata.append(d2) \n\tfor value in data: \n\t \tfor newVal in (cPickle.loads(cPickle.dumps(value)), clr.Deserialize(*clr.Serialize(value))): \n\t \t \tAreEqual(type(newVal), type(value)) \n\t \t \ttry: \n\t \t \t \tAreEqual(newVal, value) \n\t \t \texcept RuntimeError as e: \n\t \t \t \tAreEqual(e.message, 'maximum \trecursion \tdepth \texceeded \tin \tcmp') \n\t \t \t \tAssert(((type(newVal) is list) or (type(newVal) is dict))) \n\tAssertError(ValueError, clr.Deserialize, 'unknown', 'foo') \n\tal = System.Collections.ArrayList() \n\tal.Add(2) \n\tgl = System.Collections.Generic.List[int]() \n\tgl.Add(2) \n\tfor value in (al, gl): \n\t \tfor newX in (cPickle.loads(cPickle.dumps(value)), clr.Deserialize(*clr.Serialize(value))): \n\t \t \tAreEqual(value.Count, newX.Count) \n\t \t \tfor i in xrange(value.Count): \n\t \t \t \tAreEqual(value[i], newX[i]) \n\tht = System.Collections.Hashtable() \n\tht['foo'] = 'bar' \n\tgd = System.Collections.Generic.Dictionary[(str, str)]() \n\tgd['foo'] = 'bar' \n\tfor value in (ht, gd): \n\t \tfor newX in (cPickle.loads(cPickle.dumps(value)), clr.Deserialize(*clr.Serialize(value))): \n\t \t \tAreEqual(value.Count, newX.Count) \n\t \t \tfor key in value.Keys: \n\t \t \t \tAreEqual(value[key], newX[key]) \n\tfor tempX in [System.Exception('some \tmessage')]: \n\t \tfor newX in (cPickle.loads(cPickle.dumps(tempX)), clr.Deserialize(*clr.Serialize(tempX))): \n\t \t \tAreEqual(newX.Message, tempX.Message) \n\ttry: \n\t \texec ' \tprint \t1' \n\texcept Exception as tempX: \n\t \tpass \n\tnewX = cPickle.loads(cPickle.dumps(tempX)) \n\tfor attr in ['args', 'filename', 'text', 'lineno', 'msg', 'offset', 'print_file_and_line', 'message']: \n\t \tAreEqual(eval(('newX.%s' % attr)), eval(('tempX.%s' % attr))) \n\tclass K(System.Exception, ): \n\t \tother = 'something \telse' \n\ttempX = K() \n\ttempX = System.Exception\n", 
" \ttry: \n\t \tfunc(*args) \n\texcept exc: \n\t \treturn True \n\telse: \n\t \treturn False\n", 
" \ttry: \n\t \tfn(*args, **kwargs) \n\texcept Exception as e: \n\t \tassert (e.__class__ == cls), ('got \t%s, \texpected \t%s' % (e.__class__.__name__, cls.__name__)) \n\telse: \n\t \traise AssertionError(('%s \tnot \traised' % cls))\n", 
" \treturn s3_rest_controller()\n", 
" \ttry: \n\t \t__import__(module_name) \n\texcept ImportError: \n\t \treturn False \n\telse: \n\t \treturn True\n", 
" \tresult = [] \n\tchecks = get_qualitychecks() \n\tfor (check, cat) in checks.items(): \n\t \tresult.append({'code': check, 'is_critical': (cat == Category.CRITICAL), 'title': (u'%s' % check_names.get(check, check)), 'url': path_obj.get_translate_url(check=check)}) \n\tdef alphabetical_critical_first(item): \n\t \tcritical_first = (0 if item['is_critical'] else 1) \n\t \treturn (critical_first, item['title'].lower()) \n\tresult = sorted(result, key=alphabetical_critical_first) \n\treturn result\n", 
" \tcount_failed = count_all = 0 \n\treport = BaseReport(options) \n\tcounters = report.counters \n\tchecks = (options.physical_checks + options.logical_checks) \n\tfor (name, check, argument_names) in checks: \n\t \tfor line in check.__doc__.splitlines(): \n\t \t \tline = line.lstrip() \n\t \t \tmatch = SELFTEST_REGEX.match(line) \n\t \t \tif (match is None): \n\t \t \t \tcontinue \n\t \t \t(code, source) = match.groups() \n\t \t \tlines = [(part.replace('\\\\t', ' DCTB ') + '\\n') for part in source.split('\\\\n')] \n\t \t \tchecker = Checker(lines=lines, options=options, report=report) \n\t \t \tchecker.check_all() \n\t \t \terror = None \n\t \t \tif (code == 'Okay'): \n\t \t \t \tif (len(counters) > len(options.benchmark_keys)): \n\t \t \t \t \tcodes = [key for key in counters if (key not in options.benchmark_keys)] \n\t \t \t \t \terror = ('incorrectly \tfound \t%s' % ', \t'.join(codes)) \n\t \t \telif (not counters.get(code)): \n\t \t \t \terror = ('failed \tto \tfind \t%s' % code) \n\t \t \tfor key in (set(counters) - set(options.benchmark_keys)): \n\t \t \t \tdel counters[key] \n\t \t \tcount_all += 1 \n\t \t \tif (not error): \n\t \t \t \tif options.verbose: \n\t \t \t \t \tprint ('%s: \t%s' % (code, source)) \n\t \t \telse: \n\t \t \t \tcount_failed += 1 \n\t \t \t \tprint ('pycodestyle.py: \t%s:' % error) \n\t \t \t \tfor line in checker.lines: \n\t \t \t \t \tprint line.rstrip() \n\treturn (count_failed, count_all)\n", 
" \tif (sys.platform == 'win32'): \n\t \traise SkipTest('Skipping \tline \tendings \tcheck \ton \tWindows') \n\treport = list() \n\tgood_exts = ('.py', '.dat', '.sel', '.lout', '.css', '.js', '.lay', '.txt', '.elc', '.csd', '.sfp', '.json', '.hpts', '.vmrk', '.vhdr', '.head', '.eve', '.ave', '.cov', '.label') \n\tfor (dirpath, dirnames, filenames) in os.walk(dir_): \n\t \tfor fname in filenames: \n\t \t \tif ((op.splitext(fname)[1] not in good_exts) or (fname in skip_files)): \n\t \t \t \tcontinue \n\t \t \tfilename = op.join(dirpath, fname) \n\t \t \trelfilename = op.relpath(filename, dir_) \n\t \t \ttry: \n\t \t \t \twith open(filename, 'rb') as fid: \n\t \t \t \t \ttext = fid.read().decode('utf-8') \n\t \t \texcept UnicodeDecodeError: \n\t \t \t \treport.append(('In \t%s \tfound \tnon-decodable \tbytes' % relfilename)) \n\t \t \telse: \n\t \t \t \tcrcount = text.count('\\r') \n\t \t \t \tif crcount: \n\t \t \t \t \treport.append(('In \t%s \tfound \t%i/%i \tCR/LF' % (relfilename, crcount, text.count('\\n')))) \n\tif (len(report) > 0): \n\t \traise AssertionError(('Found \t%s \tfiles \twith \tincorrect \tendings:\\n%s' % (len(report), '\\n'.join(report))))\n", 
" \tglobal fr, st \n\tfr = inspect.currentframe() \n\tst = inspect.stack() \n\tp = x \n\tq = (y / 0)\n", 
" \turl = api_url(host, port, '/Users/AuthenticateByName') \n\tr = requests.post(url, headers=headers, data=auth_data) \n\treturn r.json().get('AccessToken')\n", 
" \treturn (not re.match('[a-z]+://', file_location))\n", 
" \tif (not _state): \n\t \traise RuntimeError('no \tactive \tinput()') \n\treturn _state.filename()\n", 
" \tfrom spyder.utils.programs import is_module_installed \n\tif is_module_installed('jedi', '=0.9.0'): \n\t \timport jedi \n\telse: \n\t \traise ImportError((\"jedi \t%s \tcan't \tbe \tpatched\" % jedi.__version__)) \n\tfrom spyder.utils.introspection import docstrings \n\tjedi.evaluate.representation.docstrings = docstrings \n\tfrom jedi.evaluate.compiled import CompiledObject, builtin, _create_from_name, debug \n\tclass CompiledObject(CompiledObject, ): \n\t \tdef _execute_function(self, evaluator, params): \n\t \t \tif (self.type != 'funcdef'): \n\t \t \t \treturn \n\t \t \tfrom spyder.utils.introspection import docstrings \n\t \t \ttypes = docstrings.find_return_types(evaluator, self) \n\t \t \tif types: \n\t \t \t \tfor result in types: \n\t \t \t \t \tdebug.dbg('docstrings \ttype \treturn: \t%s \tin \t%s', result, self) \n\t \t \t \t \t(yield result) \n\t \t \tfor name in self._parse_function_doc()[1].split(): \n\t \t \t \ttry: \n\t \t \t \t \tbltn_obj = _create_from_name(builtin, builtin, name) \n\t \t \t \texcept AttributeError: \n\t \t \t \t \tcontinue \n\t \t \t \telse: \n\t \t \t \t \tif (isinstance(bltn_obj, CompiledObject) and (bltn_obj.obj is None)): \n\t \t \t \t \t \tcontinue \n\t \t \t \t \tfor result in evaluator.execute(bltn_obj, params): \n\t \t \t \t \t \t(yield result) \n\t \t@property \n\t \tdef raw_doc(self): \n\t \t \ttry: \n\t \t \t \tdoc = unicode(self.doc) \n\t \t \texcept NameError: \n\t \t \t \tdoc = self.doc \n\t \t \treturn doc \n\tjedi.evaluate.compiled.CompiledObject = CompiledObject \n\tfrom jedi.evaluate.precedence import tree, calculate \n\tdef calculate_children(evaluator, children): \n\t \t'\\n \t \t \t \t \t \t \t \tCalculate \ta \tlist \tof \tchildren \twith \toperators.\\n \t \t \t \t \t \t \t \t' \n\t \titerator = iter(children) \n\t \ttypes = evaluator.eval_element(next(iterator)) \n\t \tfor operator in iterator: \n\t \t \ttry: \n\t \t \t \tright = next(iterator) \n\t \t \t \tif tree.is_node(operator, 'comp_op'): \n\t \t \t \t \toperator = ' \t'.join((str(c.value) for c in operator.children)) \n\t \t \t \tif (operator in ('and', 'or')): \n\t \t \t \t \tleft_bools = set([left.py__bool__() for left in types]) \n\t \t \t \t \tif (left_bools == set([True])): \n\t \t \t \t \t \tif (operator == 'and'): \n\t \t \t \t \t \t \ttypes = evaluator.eval_element(right) \n\t \t \t \t \telif (left_bools == set([False])): \n\t \t \t \t \t \tif (operator != 'and'): \n\t \t \t \t \t \t \ttypes = evaluator.eval_element(right) \n\t \t \t \telse: \n\t \t \t \t \ttypes = calculate(evaluator, types, operator, evaluator.eval_element(right)) \n\t \t \texcept StopIteration: \n\t \t \t \tdebug.warning('calculate_children \tStopIteration \t%s', types) \n\t \tdebug.dbg('calculate_children \ttypes \t%s', types) \n\t \treturn types \n\tjedi.evaluate.precedence.calculate_children = calculate_children \n\tfrom jedi.evaluate.representation import tree, InstanceName, Instance, compiled, FunctionExecution, InstanceElement \n\tdef get_instance_el(evaluator, instance, var, is_class_var=False): \n\t \t'\\n \t \t \t \t \t \t \t \tReturns \tan \tInstanceElement \tif \tit \tmakes \tsense, \totherwise \tleaves \tthe \tobject\\n \t \t \t \t \t \t \t \tuntouched.\\n\\n \t \t \t \t \t \t \t \tBasically \thaving \tan \tInstanceElement \tis \tcontext \tinformation. \tThat \tis \tneeded\\n \t \t \t \t \t \t \t \tin \tquite \ta \tlot \tof \tcases, \twhich \tincludes \tNodes \tlike \t``power``, \tthat \tneed \tto\\n \t \t \t \t \t \t \t \tknow \twhere \ta \tself \tname \tcomes \tfrom \tfor \texample.\\n \t \t \t \t \t \t \t \t' \n\t \tif isinstance(var, tree.Name): \n\t \t \tparent = get_instance_el(evaluator, instance, var.parent, is_class_var) \n\t \t \treturn InstanceName(var, parent) \n\t \telif (var is None): \n\t \t \treturn var \n\t \telif ((var.type != 'funcdef') and isinstance(var, (Instance, compiled.CompiledObject, tree.Leaf, tree.Module, FunctionExecution))): \n\t \t \treturn var \n\t \tvar = evaluator.wrap(var) \n\t \treturn InstanceElement(evaluator, instance, var, is_class_var) \n\tjedi.evaluate.representation.get_instance_el = get_instance_el \n\treturn jedi\n", 
" \treturn getattr(settings, 'CMS_CACHE_DURATIONS', {'menus': (60 * 60), 'content': 60, 'permissions': (60 * 60)})\n", 
" \td = _translate_snapshot_summary_view(context, vol) \n\treturn d\n", 
" \td = {} \n\td['id'] = snapshot.id \n\td['status'] = snapshot.status \n\td['progress'] = snapshot.progress \n\td['size'] = snapshot.size \n\td['created_at'] = snapshot.created_at \n\td['display_name'] = snapshot.name \n\td['display_description'] = snapshot.description \n\td['volume_id'] = snapshot.volume_id \n\td['project_id'] = snapshot.project_id \n\td['volume_size'] = snapshot.size \n\treturn d\n", 
" \td = _translate_attachment_summary_view(volume_id, instance_uuid, mountpoint) \n\treturn d\n", 
" \td = {} \n\td['id'] = volume_id \n\td['volumeId'] = volume_id \n\td['serverId'] = instance_uuid \n\tif mountpoint: \n\t \td['device'] = mountpoint \n\treturn d\n", 
" \td = _translate_volume_summary_view(context, vol) \n\treturn d\n", 
" \td = {} \n\td['id'] = vol.id \n\td['status'] = vol.status \n\td['size'] = vol.size \n\td['availability_zone'] = vol.availability_zone \n\td['created_at'] = vol.created_at \n\td['attach_time'] = '' \n\td['mountpoint'] = '' \n\td['multiattach'] = getattr(vol, 'multiattach', False) \n\tif vol.attachments: \n\t \td['attachments'] = collections.OrderedDict() \n\t \tfor attachment in vol.attachments: \n\t \t \ta = {attachment['server_id']: {'attachment_id': attachment.get('attachment_id'), 'mountpoint': attachment.get('device')}} \n\t \t \td['attachments'].update(a.items()) \n\t \td['attach_status'] = 'attached' \n\telse: \n\t \td['attach_status'] = 'detached' \n\td['display_name'] = vol.name \n\td['display_description'] = vol.description \n\td['volume_type_id'] = vol.volume_type \n\td['snapshot_id'] = vol.snapshot_id \n\td['bootable'] = strutils.bool_from_string(vol.bootable) \n\td['volume_metadata'] = {} \n\tfor (key, value) in vol.metadata.items(): \n\t \td['volume_metadata'][key] = value \n\tif hasattr(vol, 'volume_image_metadata'): \n\t \td['volume_image_metadata'] = copy.deepcopy(vol.volume_image_metadata) \n\treturn d\n", 
" \tif context.is_admin: \n\t \tfor key in ('sort_key', 'sort_dir', 'limit', 'marker'): \n\t \t \tsearch_options.pop(key, None) \n\t \treturn \n\tunknown_options = [opt for opt in search_options if (opt not in allowed_search_options)] \n\tif unknown_options: \n\t \tLOG.debug(\"Removing \toptions \t'%s' \tfrom \tquery\", ', \t'.join(unknown_options)) \n\t \tfor opt in unknown_options: \n\t \t \tsearch_options.pop(opt, None)\n", 
" \tdef decorator(func): \n\t \tif (not hasattr(func, 'wsgi_serializers')): \n\t \t \tfunc.wsgi_serializers = {} \n\t \tfunc.wsgi_serializers.update(serializers) \n\t \treturn func \n\treturn decorator\n", 
" \tdef decorator(func): \n\t \tif (not hasattr(func, 'wsgi_deserializers')): \n\t \t \tfunc.wsgi_deserializers = {} \n\t \tfunc.wsgi_deserializers.update(deserializers) \n\t \treturn func \n\treturn decorator\n", 
" \tdef decorator(func): \n\t \tfunc.wsgi_code = code \n\t \treturn func \n\treturn decorator\n", 
" \ttry: \n\t \tdecoded = jsonutils.loads(body) \n\texcept ValueError: \n\t \tmsg = _('cannot \tunderstand \tJSON') \n\t \traise exception.MalformedRequestBody(reason=msg) \n\tif (len(decoded) != 1): \n\t \tmsg = _('too \tmany \tbody \tkeys') \n\t \traise exception.MalformedRequestBody(reason=msg) \n\treturn list(decoded.keys())[0]\n", 
" \ttry: \n\t \tdecoded = jsonutils.loads(body) \n\texcept ValueError: \n\t \tmsg = _('cannot \tunderstand \tJSON') \n\t \traise exception.MalformedRequestBody(reason=msg) \n\tif (len(decoded) != 1): \n\t \tmsg = _('too \tmany \tbody \tkeys') \n\t \traise exception.MalformedRequestBody(reason=msg) \n\treturn list(decoded.keys())[0]\n", 
" \tdef decorator(func): \n\t \tfunc.wsgi_action = name \n\t \treturn func \n\treturn decorator\n", 
" \tdef decorator(func): \n\t \tfunc.wsgi_extends = (func.__name__, kwargs.get('action')) \n\t \treturn func \n\tif args: \n\t \treturn decorator(*args) \n\treturn decorator\n", 
" \td = _translate_snapshot_summary_view(context, vol) \n\treturn d\n", 
" \td = {} \n\td['id'] = snapshot.id \n\td['status'] = snapshot.status \n\td['progress'] = snapshot.progress \n\td['size'] = snapshot.size \n\td['created_at'] = snapshot.created_at \n\td['display_name'] = snapshot.name \n\td['display_description'] = snapshot.description \n\td['volume_id'] = snapshot.volume_id \n\td['project_id'] = snapshot.project_id \n\td['volume_size'] = snapshot.size \n\treturn d\n", 
" \tif context.is_admin: \n\t \tfor key in ('sort_key', 'sort_dir', 'limit', 'marker'): \n\t \t \tsearch_options.pop(key, None) \n\t \treturn \n\tunknown_options = [opt for opt in search_options if (opt not in allowed_search_options)] \n\tif unknown_options: \n\t \tLOG.debug(\"Removing \toptions \t'%s' \tfrom \tquery\", ', \t'.join(unknown_options)) \n\t \tfor opt in unknown_options: \n\t \t \tsearch_options.pop(opt, None)\n", 
" \tcurr_time = timeutils.utcnow(with_timezone=True) \n\tcontext = req.environ['cinder.context'] \n\tfilters = {'disabled': False} \n\tservices = objects.ServiceList.get_all(context, filters) \n\tzone = '' \n\tif ('zone' in req.GET): \n\t \tzone = req.GET['zone'] \n\tif zone: \n\t \tservices = [s for s in services if (s['availability_zone'] == zone)] \n\thosts = [] \n\tfor host in services: \n\t \tdelta = (curr_time - (host.updated_at or host.created_at)) \n\t \talive = (abs(delta.total_seconds()) <= CONF.service_down_time) \n\t \tstatus = ((alive and 'available') or 'unavailable') \n\t \tactive = 'enabled' \n\t \tif host.disabled: \n\t \t \tactive = 'disabled' \n\t \tLOG.debug('status, \tactive \tand \tupdate: \t%s, \t%s, \t%s', status, active, host.updated_at) \n\t \tupdated_at = host.updated_at \n\t \tif updated_at: \n\t \t \tupdated_at = timeutils.normalize_time(updated_at) \n\t \thosts.append({'host_name': host.host, 'service': host.topic, 'zone': host.availability_zone, 'service-status': status, 'service-state': active, 'last-update': updated_at}) \n\tif service: \n\t \thosts = [host for host in hosts if (host['service'] == service)] \n\treturn hosts\n", 
" \tdef wrapped(self, req, id, service=None, *args, **kwargs): \n\t \tlisted_hosts = _list_hosts(req, service) \n\t \thosts = [h['host_name'] for h in listed_hosts] \n\t \tif (id in hosts): \n\t \t \treturn fn(self, req, id, *args, **kwargs) \n\t \traise exception.HostNotFound(host=id) \n\treturn wrapped\n", 
" \treturn _load_pipeline(loader, local_conf[CONF.api.auth_strategy].split())\n", 
" \tif (not isinstance(default, int)): \n\t \tmsg = (\"'%s' \tobject \tcannot \tbe \tinterpreted \tas \tan \tinteger\" % type(default).__name__) \n\t \traise TypeError(msg) \n\ttry: \n\t \treturn len(obj) \n\texcept TypeError: \n\t \tpass \n\ttry: \n\t \thint = type(obj).__length_hint__ \n\texcept AttributeError: \n\t \treturn default \n\ttry: \n\t \tval = hint(obj) \n\texcept TypeError: \n\t \treturn default \n\tif (val is NotImplemented): \n\t \treturn default \n\tif (not isinstance(val, int)): \n\t \tmsg = ('__length_hint__ \tmust \tbe \tinteger, \tnot \t%s' % type(val).__name__) \n\t \traise TypeError(msg) \n\tif (val < 0): \n\t \tmsg = '__length_hint__() \tshould \treturn \t>= \t0' \n\t \traise ValueError(msg) \n\treturn val\n", 
" \treturn itertools.chain(element.iterfind((_OLD_NAMESPACE_PREFIX + tag)), element.iterfind((_NEW_NAMESPACE_PREFIX + tag)))\n", 
" \tfor (key, value) in sub_dict.items(): \n\t \tif isinstance(value, list): \n\t \t \tfor repeated_element in value: \n\t \t \t \tsub_element = ET.SubElement(parent, key) \n\t \t \t \t_add_element_attrs(sub_element, repeated_element.get('attrs', {})) \n\t \t \t \tchildren = repeated_element.get('children', None) \n\t \t \t \tif isinstance(children, dict): \n\t \t \t \t \t_add_sub_elements_from_dict(sub_element, children) \n\t \t \t \telif isinstance(children, str): \n\t \t \t \t \tsub_element.text = children \n\t \telse: \n\t \t \tsub_element = ET.SubElement(parent, key) \n\t \t \t_add_element_attrs(sub_element, value.get('attrs', {})) \n\t \t \tchildren = value.get('children', None) \n\t \t \tif isinstance(children, dict): \n\t \t \t \t_add_sub_elements_from_dict(sub_element, children) \n\t \t \telif isinstance(children, str): \n\t \t \t \tsub_element.text = children\n", 
" \treturn ''.join([t for t in educate_tokens(tokenize(text), attr, language)])\n", 
" \tour_dir = path[0] \n\tfor (dirpath, dirnames, filenames) in os.walk(our_dir): \n\t \trelpath = os.path.relpath(dirpath, our_dir) \n\t \tif (relpath == '.'): \n\t \t \trelpkg = '' \n\t \telse: \n\t \t \trelpkg = ('.%s' % '.'.join(relpath.split(os.sep))) \n\t \tfor fname in filenames: \n\t \t \t(root, ext) = os.path.splitext(fname) \n\t \t \tif ((ext not in ('.py', '.pyc')) or (root == '__init__') or (fname in FILES_TO_SKIP)): \n\t \t \t \tcontinue \n\t \t \tif ((ext == '.pyc') and ((root + '.py') in filenames)): \n\t \t \t \tcontinue \n\t \t \tclassname = ('%s%s' % (root[0].upper(), root[1:])) \n\t \t \tclasspath = ('%s%s.%s.%s' % (package, relpkg, root, classname)) \n\t \t \tif ((ext_list is not None) and (classname not in ext_list)): \n\t \t \t \tlogger.debug(('Skipping \textension: \t%s' % classpath)) \n\t \t \t \tcontinue \n\t \t \ttry: \n\t \t \t \text_mgr.load_extension(classpath) \n\t \t \texcept Exception as exc: \n\t \t \t \tlogger.warning(_LW('Failed \tto \tload \textension \t%(classpath)s: \t%(exc)s'), {'classpath': classpath, 'exc': exc}) \n\t \tsubdirs = [] \n\t \tfor dname in dirnames: \n\t \t \tif (not os.path.exists(os.path.join(dirpath, dname, '__init__.py'))): \n\t \t \t \tcontinue \n\t \t \text_name = ('%s%s.%s.extension' % (package, relpkg, dname)) \n\t \t \ttry: \n\t \t \t \text = importutils.import_class(ext_name) \n\t \t \texcept ImportError: \n\t \t \t \tsubdirs.append(dname) \n\t \t \telse: \n\t \t \t \ttry: \n\t \t \t \t \text(ext_mgr) \n\t \t \t \texcept Exception as exc: \n\t \t \t \t \tlogger.warning(_LW('Failed \tto \tload \textension \t%(ext_name)s: \t%(exc)s'), {'ext_name': ext_name, 'exc': exc}) \n\t \tdirnames[:] = subdirs\n", 
" \tmax_limit = _get_pagination_max_limit() \n\tlimit = _get_limit_param(request) \n\tif (max_limit > 0): \n\t \tlimit = (min(max_limit, limit) or max_limit) \n\tif (not limit): \n\t \treturn (None, None) \n\tmarker = request.GET.get('marker', None) \n\treturn (limit, marker)\n", 
" \tlimit = request.GET.get('limit', 0) \n\ttry: \n\t \tlimit = int(limit) \n\t \tif (limit >= 0): \n\t \t \treturn limit \n\texcept ValueError: \n\t \tpass \n\tmsg = (_(\"Limit \tmust \tbe \tan \tinteger \t0 \tor \tgreater \tand \tnot \t'%s'\") % limit) \n\traise exceptions.BadRequest(resource='limit', msg=msg)\n", 
" \treturn request.GET['marker']\n", 
" \tparams = get_pagination_params(request) \n\toffset = params.get('offset', 0) \n\tlimit = CONF.api.max_limit \n\tlimit = min(limit, (params.get('limit') or limit)) \n\treturn items[offset:(offset + limit)]\n", 
" \tmax_limit = (max_limit or CONF.osapi_max_limit) \n\t(marker, limit, __) = get_pagination_params(request.GET.copy(), max_limit) \n\tstart_index = 0 \n\tif marker: \n\t \tstart_index = (-1) \n\t \tfor (i, item) in enumerate(items): \n\t \t \tif ('flavorid' in item): \n\t \t \t \tif (item['flavorid'] == marker): \n\t \t \t \t \tstart_index = (i + 1) \n\t \t \t \t \tbreak \n\t \t \telif ((item['id'] == marker) or (item.get('uuid') == marker)): \n\t \t \t \tstart_index = (i + 1) \n\t \t \t \tbreak \n\t \tif (start_index < 0): \n\t \t \tmsg = (_('marker \t[%s] \tnot \tfound') % marker) \n\t \t \traise webob.exc.HTTPBadRequest(explanation=msg) \n\trange_end = (start_index + limit) \n\treturn items[start_index:range_end]\n", 
" \tparsed_url = urllib.parse.urlsplit(href) \n\turl_parts = parsed_url.path.split('/', 2) \n\texpression = re.compile('^v([0-9]+|[0-9]+\\\\.[0-9]+)(/.*|$)') \n\tfor x in range(len(url_parts)): \n\t \tif expression.match(url_parts[x]): \n\t \t \tdel url_parts[x] \n\t \t \tbreak \n\tnew_path = '/'.join(url_parts) \n\tif (new_path == parsed_url.path): \n\t \tmsg = ('href \t%s \tdoes \tnot \tcontain \tversion' % href) \n\t \tLOG.debug(msg) \n\t \traise ValueError(msg) \n\tparsed_url = list(parsed_url) \n\tparsed_url[2] = new_path \n\treturn urllib.parse.urlunsplit(parsed_url)\n", 
" \tif (value and (value[0] == value[(-1)] == '\"')): \n\t \tvalue = value[1:(-1)] \n\treturn value\n", 
" \tresult = [] \n\tfor item in _parse_list_header(value): \n\t \tif (item[:1] == item[(-1):] == '\"'): \n\t \t \titem = unquote_header_value(item[1:(-1)]) \n\t \tresult.append(item) \n\treturn result\n", 
" \tif (not value): \n\t \treturn ('', {}) \n\tresult = [] \n\tvalue = (',' + value.replace('\\n', ',')) \n\twhile value: \n\t \tmatch = _option_header_start_mime_type.match(value) \n\t \tif (not match): \n\t \t \tbreak \n\t \tresult.append(match.group(1)) \n\t \toptions = {} \n\t \trest = match.group(2) \n\t \twhile rest: \n\t \t \toptmatch = _option_header_piece_re.match(rest) \n\t \t \tif (not optmatch): \n\t \t \t \tbreak \n\t \t \t(option, encoding, _, option_value) = optmatch.groups() \n\t \t \toption = unquote_header_value(option) \n\t \t \tif (option_value is not None): \n\t \t \t \toption_value = unquote_header_value(option_value, (option == 'filename')) \n\t \t \t \tif (encoding is not None): \n\t \t \t \t \toption_value = _unquote(option_value).decode(encoding) \n\t \t \toptions[option] = option_value \n\t \t \trest = rest[optmatch.end():] \n\t \tresult.append(options) \n\t \tif (multiple is False): \n\t \t \treturn tuple(result) \n\t \tvalue = rest \n\treturn (tuple(result) if result else ('', {}))\n", 
" \ttry: \n\t \tuuid.UUID(value) \n\t \treturn value \n\texcept (ValueError, AttributeError): \n\t \treturn int(value)\n", 
" \t(orig_exc_type, orig_exc_value, orig_exc_traceback) = sys.exc_info() \n\tif isinstance(new_exc, six.string_types): \n\t \tnew_exc = orig_exc_type(new_exc) \n\tif hasattr(new_exc, 'args'): \n\t \tif (len(new_exc.args) > 0): \n\t \t \tnew_message = ', \t'.join((str(arg) for arg in new_exc.args)) \n\t \telse: \n\t \t \tnew_message = '' \n\t \tnew_message += ('\\n\\nOriginal \texception:\\n DCTB ' + orig_exc_type.__name__) \n\t \tif (hasattr(orig_exc_value, 'args') and (len(orig_exc_value.args) > 0)): \n\t \t \tif getattr(orig_exc_value, 'reraised', False): \n\t \t \t \tnew_message += (': \t' + str(orig_exc_value.args[0])) \n\t \t \telse: \n\t \t \t \tnew_message += (': \t' + ', \t'.join((str(arg) for arg in orig_exc_value.args))) \n\t \tnew_exc.args = ((new_message,) + new_exc.args[1:]) \n\tnew_exc.__cause__ = orig_exc_value \n\tnew_exc.reraised = True \n\tsix.reraise(type(new_exc), new_exc, orig_exc_traceback)\n", 
" \tret = _ConvertToList(arg) \n\tfor element in ret: \n\t \tif (not isinstance(element, element_type)): \n\t \t \traise TypeError(('%s \tshould \tbe \tsingle \telement \tor \tlist \tof \ttype \t%s' % (arg_name, element_type))) \n\treturn ret\n", 
" \tlogger = logging.getLogger() \n\tloglevel = get_loglevel((loglevel or u'ERROR')) \n\tlogfile = (logfile if logfile else sys.__stderr__) \n\tif (not logger.handlers): \n\t \tif hasattr(logfile, u'write'): \n\t \t \thandler = logging.StreamHandler(logfile) \n\t \telse: \n\t \t \thandler = WatchedFileHandler(logfile) \n\t \tlogger.addHandler(handler) \n\t \tlogger.setLevel(loglevel) \n\treturn logger\n", 
" \treturn ((module in sys.modules) and isinstance(obj, getattr(import_module(module), class_name)))\n", 
" \tvalidate_config_version(config_details.config_files) \n\tprocessed_files = [process_config_file(config_file, config_details.environment) for config_file in config_details.config_files] \n\tconfig_details = config_details._replace(config_files=processed_files) \n\tmain_file = config_details.config_files[0] \n\tvolumes = load_mapping(config_details.config_files, u'get_volumes', u'Volume') \n\tnetworks = load_mapping(config_details.config_files, u'get_networks', u'Network') \n\tservice_dicts = load_services(config_details, main_file) \n\tif (main_file.version != V1): \n\t \tfor service_dict in service_dicts: \n\t \t \tmatch_named_volumes(service_dict, volumes) \n\tservices_using_deploy = [s for s in service_dicts if s.get(u'deploy')] \n\tif services_using_deploy: \n\t \tlog.warn(u\"Some \tservices \t({}) \tuse \tthe \t'deploy' \tkey, \twhich \twill \tbe \tignored. \tCompose \tdoes \tnot \tsupport \tdeploy \tconfiguration \t- \tuse \t`docker \tstack \tdeploy` \tto \tdeploy \tto \ta \tswarm.\".format(u', \t'.join(sorted((s[u'name'] for s in services_using_deploy))))) \n\treturn Config(main_file.version, service_dicts, volumes, networks)\n", 
" \ttry: \n\t \tvalue = args_list.pop(0) \n\texcept IndexError: \n\t \traise BadCommandUsage(msg) \n\tif ((expected_size_after is not None) and (len(args_list) > expected_size_after)): \n\t \traise BadCommandUsage('too \tmany \targuments') \n\treturn value\n", 
" \treturn datetime.datetime.strptime(d['isostr'], _DATETIME_FORMAT)\n", 
" \treturn _aliases.get(alias.lower(), alias)\n", 
" \twith open(CHANGELOG) as f: \n\t \tcur_index = 0 \n\t \tfor line in f: \n\t \t \tmatch = re.search('^\\\\d+\\\\.\\\\d+\\\\.\\\\d+', line) \n\t \t \tif match: \n\t \t \t \tif (cur_index == index): \n\t \t \t \t \treturn match.group(0) \n\t \t \t \telse: \n\t \t \t \t \tcur_index += 1\n", 
" \tgitstr = _git_str() \n\tif (gitstr is None): \n\t \tgitstr = '' \n\tpath = os.path.join(BASEDIR, 'qutebrowser', 'git-commit-id') \n\twith _open(path, 'w', encoding='ascii') as f: \n\t \tf.write(gitstr)\n", 
" \tif (len(sys.argv) < 2): \n\t \treturn True \n\tinfo_commands = ['--help-commands', '--name', '--version', '-V', '--fullname', '--author', '--author-email', '--maintainer', '--maintainer-email', '--contact', '--contact-email', '--url', '--license', '--description', '--long-description', '--platforms', '--classifiers', '--keywords', '--provides', '--requires', '--obsoletes'] \n\tinfo_commands.extend(['egg_info', 'install_egg_info', 'rotate']) \n\tfor command in info_commands: \n\t \tif (command in sys.argv[1:]): \n\t \t \treturn False \n\treturn True\n", 
" \trefs = list_refs(refnames=[refname], repo_dir=repo_dir, limit_to_heads=True) \n\tl = tuple(islice(refs, 2)) \n\tif l: \n\t \tassert (len(l) == 1) \n\t \treturn l[0][1] \n\telse: \n\t \treturn None\n", 
" \tif isinstance(fileIshObject, TextIOBase): \n\t \treturn unicode \n\tif isinstance(fileIshObject, IOBase): \n\t \treturn bytes \n\tencoding = getattr(fileIshObject, 'encoding', None) \n\timport codecs \n\tif isinstance(fileIshObject, (codecs.StreamReader, codecs.StreamWriter)): \n\t \tif encoding: \n\t \t \treturn unicode \n\t \telse: \n\t \t \treturn bytes \n\tif (not _PY3): \n\t \tif isinstance(fileIshObject, file): \n\t \t \tif (encoding is not None): \n\t \t \t \treturn basestring \n\t \t \telse: \n\t \t \t \treturn bytes \n\t \tfrom cStringIO import InputType, OutputType \n\t \tfrom StringIO import StringIO \n\t \tif isinstance(fileIshObject, (StringIO, InputType, OutputType)): \n\t \t \treturn bytes \n\treturn default\n", 
" \tcmd = (_pkg(jail, chroot, root) + ['--version']) \n\treturn __salt__['cmd.run'](cmd).strip()\n", 
" \tcmd = (_pkg(jail, chroot, root) + ['--version']) \n\treturn __salt__['cmd.run'](cmd).strip()\n", 
" \thost = node \n\tport = 27017 \n\tidx = node.rfind(':') \n\tif (idx != (-1)): \n\t \t(host, port) = (node[:idx], int(node[(idx + 1):])) \n\tif host.startswith('['): \n\t \thost = host[1:(-1)] \n\treturn (host, port)\n", 
" \tif isinstance(mode, six.string_types): \n\t \tif (mode.lower() == 'enforcing'): \n\t \t \tmode = '1' \n\t \t \tmodestring = 'Enforcing' \n\t \telif (mode.lower() == 'permissive'): \n\t \t \tmode = '0' \n\t \t \tmodestring = 'Permissive' \n\t \telif (mode.lower() == 'disabled'): \n\t \t \tmode = '0' \n\t \t \tmodestring = 'Disabled' \n\t \telse: \n\t \t \treturn 'Invalid \tmode \t{0}'.format(mode) \n\telif isinstance(mode, int): \n\t \tif mode: \n\t \t \tmode = '1' \n\t \telse: \n\t \t \tmode = '0' \n\telse: \n\t \treturn 'Invalid \tmode \t{0}'.format(mode) \n\tif (getenforce() != 'Disabled'): \n\t \tenforce = os.path.join(selinux_fs_path(), 'enforce') \n\t \ttry: \n\t \t \twith salt.utils.fopen(enforce, 'w') as _fp: \n\t \t \t \t_fp.write(mode) \n\t \texcept (IOError, OSError) as exc: \n\t \t \tmsg = 'Could \tnot \twrite \tSELinux \tenforce \tfile: \t{0}' \n\t \t \traise CommandExecutionError(msg.format(str(exc))) \n\tconfig = '/etc/selinux/config' \n\ttry: \n\t \twith salt.utils.fopen(config, 'r') as _cf: \n\t \t \tconf = _cf.read() \n\t \ttry: \n\t \t \twith salt.utils.fopen(config, 'w') as _cf: \n\t \t \t \tconf = re.sub('\\\\nSELINUX=.*\\\\n', (('\\nSELINUX=' + modestring) + '\\n'), conf) \n\t \t \t \t_cf.write(conf) \n\t \texcept (IOError, OSError) as exc: \n\t \t \tmsg = 'Could \tnot \twrite \tSELinux \tconfig \tfile: \t{0}' \n\t \t \traise CommandExecutionError(msg.format(str(exc))) \n\texcept (IOError, OSError) as exc: \n\t \tmsg = 'Could \tnot \tread \tSELinux \tconfig \tfile: \t{0}' \n\t \traise CommandExecutionError(msg.format(str(exc))) \n\treturn getenforce()\n", 
" \t'\\n \t \t \t \tInitialize \tthe \tworkflow\\n \t \t \t \t' \n\tgetmask = pe.Workflow(name=name) \n\t'\\n \t \t \t \tDefine \tthe \tinputs \tto \tthe \tworkflow.\\n \t \t \t \t' \n\tinputnode = pe.Node(niu.IdentityInterface(fields=['source_file', 'subject_id', 'subjects_dir', 'contrast_type']), name='inputspec') \n\t'\\n \t \t \t \tDefine \tall \tthe \tnodes \tof \tthe \tworkflow:\\n\\n \t \t \t \tfssource: \tused \tto \tretrieve \taseg.mgz\\n \t \t \t \tthreshold \t: \tbinarize \taseg\\n \t \t \t \tregister \t: \tcoregister \tsource \tfile \tto \tfreesurfer \tspace\\n \t \t \t \tvoltransform: \tconvert \tbinarized \taseg \tto \tsource \tfile \tspace\\n \t \t \t \t' \n\tfssource = pe.Node(nio.FreeSurferSource(), name='fssource') \n\tthreshold = pe.Node(fs.Binarize(min=0.5, out_type='nii'), name='threshold') \n\tregister = pe.MapNode(fs.BBRegister(init='fsl'), iterfield=['source_file'], name='register') \n\tvoltransform = pe.MapNode(fs.ApplyVolTransform(inverse=True), iterfield=['source_file', 'reg_file'], name='transform') \n\t'\\n \t \t \t \tConnect \tthe \tnodes\\n \t \t \t \t' \n\tgetmask.connect([(inputnode, fssource, [('subject_id', 'subject_id'), ('subjects_dir', 'subjects_dir')]), (inputnode, register, [('source_file', 'source_file'), ('subject_id', 'subject_id'), ('subjects_dir', 'subjects_dir'), ('contrast_type', 'contrast_type')]), (inputnode, voltransform, [('subjects_dir', 'subjects_dir'), ('source_file', 'source_file')]), (fssource, threshold, [(('aparc_aseg', get_aparc_aseg), 'in_file')]), (register, voltransform, [('out_reg_file', 'reg_file')]), (threshold, voltransform, [('binary_file', 'target_file')])]) \n\t'\\n \t \t \t \tAdd \tremaining \tnodes \tand \tconnections\\n\\n \t \t \t \tdilate \t: \tdilate \tthe \ttransformed \tfile \tin \tsource \tspace\\n \t \t \t \tthreshold2 \t: \tbinarize \ttransformed \tfile\\n \t \t \t \t' \n\tthreshold2 = pe.MapNode(fs.Binarize(min=0.5, out_type='nii'), iterfield=['in_file'], name='threshold2') \n\tif dilate_mask: \n\t \tthreshold2.inputs.dilate = 1 \n\tgetmask.connect([(voltransform, threshold2, [('transformed_file', 'in_file')])]) \n\t'\\n \t \t \t \tSetup \tan \toutputnode \tthat \tdefines \trelevant \tinputs \tof \tthe \tworkflow.\\n \t \t \t \t' \n\toutputnode = pe.Node(niu.IdentityInterface(fields=['mask_file', 'reg_file', 'reg_cost']), name='outputspec') \n\tgetmask.connect([(register, outputnode, [('out_reg_file', 'reg_file')]), (register, outputnode, [('min_cost_file', 'reg_cost')]), (threshold2, outputnode, [('binary_file', 'mask_file')])]) \n\treturn getmask\n", 
" \trule_method = ('telemetry:' + policy_name) \n\theaders = request.headers \n\tpolicy_dict = dict() \n\tpolicy_dict['roles'] = headers.get('X-Roles', '').split(',') \n\tpolicy_dict['user_id'] = headers.get('X-User-Id') \n\tpolicy_dict['project_id'] = headers.get('X-Project-Id') \n\tif ((_has_rule('default') or _has_rule(rule_method)) and (not pecan.request.enforcer.enforce(rule_method, {}, policy_dict))): \n\t \tpecan.core.abort(status_code=403, detail='RBAC \tAuthorization \tFailed')\n", 
" \tdef wrapped(f): \n\t \tviews[name] = f \n\t \treturn f \n\treturn wrapped\n", 
" \tif (target is None): \n\t \ttarget = {} \n\tmatch_rule = _build_match_rule(action, target, pluralized) \n\tcredentials = context.to_policy_values() \n\treturn (match_rule, target, credentials)\n", 
" \ttry: \n\t \tlength = len(value) \n\texcept TypeError: \n\t \traise VdtTypeError(value) \n\tif (length < len(args)): \n\t \traise VdtValueTooShortError(value) \n\telif (length > len(args)): \n\t \traise VdtValueTooLongError(value) \n\ttry: \n\t \treturn [fun_dict[arg](val) for (arg, val) in zip(args, value)] \n\texcept KeyError as e: \n\t \traise VdtParamError('mixed_list', e)\n", 
" \treq = get('http://{host}:{port}'.format(host=host, port=port), timeout=SOCKET_TIMEOUT_FOR_POLLING, persistent=False) \n\tdef failed(failure): \n\t \treturn False \n\tdef succeeded(result): \n\t \treturn True \n\treq.addCallbacks(succeeded, failed) \n\treturn req\n", 
" \tfeasible_ind = numpy.array(individual) \n\tfeasible_ind = numpy.maximum(MIN_BOUND, feasible_ind) \n\tfeasible_ind = numpy.minimum(MAX_BOUND, feasible_ind) \n\treturn feasible_ind\n", 
" \tif (value in (u'1', u'0')): \n\t \treturn bool(int(value)) \n\traise ValueError((u'%r \tis \tnot \t0 \tor \t1' % value))\n", 
" \tif isinstance(obj, str): \n\t \tobj = obj.strip().lower() \n\t \tif (obj in ('true', 'yes', 'on', 'y', 't', '1')): \n\t \t \treturn True \n\t \tif (obj in ('false', 'no', 'off', 'n', 'f', '0')): \n\t \t \treturn False \n\t \traise ValueError(('Unable \tto \tinterpret \tvalue \t\"%s\" \tas \tboolean' % obj)) \n\treturn bool(obj)\n", 
" \tglobal _PARSING_CACHE \n\tif (string in _PARSING_CACHE): \n\t \tstack = _PARSING_CACHE[string] \n\telse: \n\t \tif (not _RE_STARTTOKEN.search(string)): \n\t \t \treturn string \n\t \tstack = ParseStack() \n\t \tncallable = 0 \n\t \tfor match in _RE_TOKEN.finditer(string): \n\t \t \tgdict = match.groupdict() \n\t \t \tif gdict['singlequote']: \n\t \t \t \tstack.append(gdict['singlequote']) \n\t \t \telif gdict['doublequote']: \n\t \t \t \tstack.append(gdict['doublequote']) \n\t \t \telif gdict['end']: \n\t \t \t \tif (ncallable <= 0): \n\t \t \t \t \tstack.append(')') \n\t \t \t \t \tcontinue \n\t \t \t \targs = [] \n\t \t \t \twhile stack: \n\t \t \t \t \toperation = stack.pop() \n\t \t \t \t \tif callable(operation): \n\t \t \t \t \t \tif (not strip): \n\t \t \t \t \t \t \tstack.append((operation, [arg for arg in reversed(args)])) \n\t \t \t \t \t \tncallable -= 1 \n\t \t \t \t \t \tbreak \n\t \t \t \t \telse: \n\t \t \t \t \t \targs.append(operation) \n\t \t \telif gdict['start']: \n\t \t \t \tfuncname = _RE_STARTTOKEN.match(gdict['start']).group(1) \n\t \t \t \ttry: \n\t \t \t \t \tstack.append(_INLINE_FUNCS[funcname]) \n\t \t \t \texcept KeyError: \n\t \t \t \t \tstack.append(_INLINE_FUNCS['nomatch']) \n\t \t \t \t \tstack.append(funcname) \n\t \t \t \tncallable += 1 \n\t \t \telif gdict['escaped']: \n\t \t \t \ttoken = gdict['escaped'].lstrip('\\\\') \n\t \t \t \tstack.append(token) \n\t \t \telif gdict['comma']: \n\t \t \t \tif (ncallable > 0): \n\t \t \t \t \tstack.append(None) \n\t \t \t \telse: \n\t \t \t \t \tstack.append(',') \n\t \t \telse: \n\t \t \t \tstack.append(gdict['rest']) \n\t \tif (ncallable > 0): \n\t \t \treturn string \n\t \tif ((_STACK_MAXSIZE > 0) and (_STACK_MAXSIZE < len(stack))): \n\t \t \treturn (string + gdict['stackfull'](*args, **kwargs)) \n\t \telse: \n\t \t \t_PARSING_CACHE[string] = stack \n\tdef _run_stack(item, depth=0): \n\t \tretval = item \n\t \tif isinstance(item, tuple): \n\t \t \tif strip: \n\t \t \t \treturn '' \n\t \t \telse: \n\t \t \t \t(func, arglist) = item \n\t \t \t \targs = [''] \n\t \t \t \tfor arg in arglist: \n\t \t \t \t \tif (arg is None): \n\t \t \t \t \t \targs.append('') \n\t \t \t \t \telse: \n\t \t \t \t \t \targs[(-1)] += _run_stack(arg, depth=(depth + 1)) \n\t \t \t \tkwargs['inlinefunc_stack_depth'] = depth \n\t \t \t \tretval = ('' if strip else func(*args, **kwargs)) \n\t \treturn utils.to_str(retval, force_string=True) \n\treturn ''.join((_run_stack(item) for item in _PARSING_CACHE[string]))\n", 
" \tif (type(s) is str): \n\t \treturn s \n\telse: \n\t \treturn s3_unicode(s).encode('utf-8', 'strict')\n", 
" \tdef wrapper(func): \n\t \t@functools.wraps(func) \n\t \tdef inner(*args, **kwds): \n\t \t \tlock.acquire() \n\t \t \ttry: \n\t \t \t \treturn func(*args, **kwds) \n\t \t \tfinally: \n\t \t \t \tlock.release() \n\t \treturn inner \n\treturn wrapper\n", 
" \treturn auth_is_loggedin_user()\n", 
" \tif ((r.representation == 'html') and (r.name == 'shelter') and r.id and (not r.component)): \n\t \tT = current.T \n\t \tmsg = current.msg \n\t \ts3db = current.s3db \n\t \trecord = r.record \n\t \tctable = s3db.pr_contact \n\t \tstable = s3db.cr_shelter \n\t \tmessage = '' \n\t \ttext = '' \n\t \ts_id = record.id \n\t \ts_name = record.name \n\t \ts_phone = record.phone \n\t \ts_email = record.email \n\t \ts_status = record.status \n\t \tif (s_phone in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_email in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_status in ('', None)): \n\t \t \ts_status = T('Not \tDefined') \n\t \telif (s_status == 1): \n\t \t \ts_status = 'Open' \n\t \telif (s_status == 2): \n\t \t \ts_status = 'Close' \n\t \telse: \n\t \t \ts_status = 'Unassigned \tShelter \tStatus' \n\t \ttext += '************************************************' \n\t \ttext += ('\\n%s \t' % T('Automatic \tMessage')) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Shelter \tID'), s_id)) \n\t \ttext += (' \t%s: \t%s' % (T('Shelter \tname'), s_name)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Email'), s_email)) \n\t \ttext += (' \t%s: \t%s' % (T('Phone'), s_phone)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Working \tStatus'), s_status)) \n\t \ttext += '\\n************************************************\\n' \n\t \turl = URL(c='cr', f='shelter', args=r.id) \n\t \topts = dict(type='SMS', subject=T('Deployment \tRequest'), message=(message + text), url=url) \n\t \toutput = msg.compose(**opts) \n\t \tif attr.get('rheader'): \n\t \t \trheader = attr['rheader'](r) \n\t \t \tif rheader: \n\t \t \t \toutput['rheader'] = rheader \n\t \toutput['title'] = T('Send \tNotification') \n\t \tcurrent.response.view = 'msg/compose.html' \n\t \treturn output \n\telse: \n\t \traise HTTP(501, current.messages.BADMETHOD)\n", 
" \tcan_users_receive_email = email_manager.can_users_receive_thread_email(recipient_list, exploration_id, has_suggestion) \n\tfor (index, recipient_id) in enumerate(recipient_list): \n\t \tif can_users_receive_email[index]: \n\t \t \ttransaction_services.run_in_transaction(_enqueue_feedback_thread_status_change_email_task, recipient_id, feedback_message_reference, old_status, new_status)\n", 
" \tglobal _notifier \n\tif (_notifier is None): \n\t \thost = (CONF.default_publisher_id or socket.gethostname()) \n\t \ttry: \n\t \t \ttransport = oslo_messaging.get_notification_transport(CONF) \n\t \t \t_notifier = oslo_messaging.Notifier(transport, ('identity.%s' % host)) \n\t \texcept Exception: \n\t \t \tLOG.exception(_LE('Failed \tto \tconstruct \tnotifier')) \n\t \t \t_notifier = False \n\treturn _notifier\n", 
" \tcan_users_receive_email = email_manager.can_users_receive_thread_email(recipient_list, exploration_id, has_suggestion) \n\tfor (index, recipient_id) in enumerate(recipient_list): \n\t \tif can_users_receive_email[index]: \n\t \t \ttransaction_services.run_in_transaction(_enqueue_feedback_thread_status_change_email_task, recipient_id, feedback_message_reference, old_status, new_status)\n", 
" \tdef wrapped_func(*args, **kwarg): \n\t \tCALLED_FUNCTION.append(name) \n\t \treturn function(*args, **kwarg) \n\treturn wrapped_func\n", 
" \tlevel = notification.get_default_level() \n\tif (stack.status == stack.IN_PROGRESS): \n\t \tsuffix = 'start' \n\telif (stack.status == stack.COMPLETE): \n\t \tsuffix = 'end' \n\telse: \n\t \tsuffix = 'error' \n\t \tlevel = notification.ERROR \n\tevent_type = ('%s.%s.%s' % ('stack', stack.action.lower(), suffix)) \n\tnotification.notify(stack.context, event_type, level, engine_api.format_notification_body(stack))\n", 
" \ttry: \n\t \tdriver = _DRIVERS[driver_name] \n\texcept KeyError: \n\t \traise DriverNotFoundError(('No \tdriver \tfor \t%s' % driver_name)) \n\treturn driver(*args, **kwargs)\n", 
" \tlevel = notification.get_default_level() \n\tif (stack.status == stack.IN_PROGRESS): \n\t \tsuffix = 'start' \n\telif (stack.status == stack.COMPLETE): \n\t \tsuffix = 'end' \n\telse: \n\t \tsuffix = 'error' \n\t \tlevel = notification.ERROR \n\tevent_type = ('%s.%s.%s' % ('stack', stack.action.lower(), suffix)) \n\tnotification.notify(stack.context, event_type, level, engine_api.format_notification_body(stack))\n", 
" \tseed = (pseed or config.unittests.rseed) \n\tif (seed == 'random'): \n\t \tseed = None \n\ttry: \n\t \tif seed: \n\t \t \tseed = int(seed) \n\t \telse: \n\t \t \tseed = None \n\texcept ValueError: \n\t \tprint('Error: \tconfig.unittests.rseed \tcontains \tinvalid \tseed, \tusing \tNone \tinstead', file=sys.stderr) \n\t \tseed = None \n\treturn seed\n", 
" \tif ((r.representation == 'html') and (r.name == 'shelter') and r.id and (not r.component)): \n\t \tT = current.T \n\t \tmsg = current.msg \n\t \ts3db = current.s3db \n\t \trecord = r.record \n\t \tctable = s3db.pr_contact \n\t \tstable = s3db.cr_shelter \n\t \tmessage = '' \n\t \ttext = '' \n\t \ts_id = record.id \n\t \ts_name = record.name \n\t \ts_phone = record.phone \n\t \ts_email = record.email \n\t \ts_status = record.status \n\t \tif (s_phone in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_email in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_status in ('', None)): \n\t \t \ts_status = T('Not \tDefined') \n\t \telif (s_status == 1): \n\t \t \ts_status = 'Open' \n\t \telif (s_status == 2): \n\t \t \ts_status = 'Close' \n\t \telse: \n\t \t \ts_status = 'Unassigned \tShelter \tStatus' \n\t \ttext += '************************************************' \n\t \ttext += ('\\n%s \t' % T('Automatic \tMessage')) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Shelter \tID'), s_id)) \n\t \ttext += (' \t%s: \t%s' % (T('Shelter \tname'), s_name)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Email'), s_email)) \n\t \ttext += (' \t%s: \t%s' % (T('Phone'), s_phone)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Working \tStatus'), s_status)) \n\t \ttext += '\\n************************************************\\n' \n\t \turl = URL(c='cr', f='shelter', args=r.id) \n\t \topts = dict(type='SMS', subject=T('Deployment \tRequest'), message=(message + text), url=url) \n\t \toutput = msg.compose(**opts) \n\t \tif attr.get('rheader'): \n\t \t \trheader = attr['rheader'](r) \n\t \t \tif rheader: \n\t \t \t \toutput['rheader'] = rheader \n\t \toutput['title'] = T('Send \tNotification') \n\t \tcurrent.response.view = 'msg/compose.html' \n\t \treturn output \n\telse: \n\t \traise HTTP(501, current.messages.BADMETHOD)\n", 
" \ttrunc = 20 \n\targspec = inspect.getargspec(fobj) \n\targ_list = [] \n\tif argspec.args: \n\t \tfor arg in argspec.args: \n\t \t \targ_list.append(str(arg)) \n\targ_list.reverse() \n\tif argspec.defaults: \n\t \tfor i in range(len(argspec.defaults)): \n\t \t \targ_list[i] = ((str(arg_list[i]) + '=') + str(argspec.defaults[(- i)])) \n\targ_list.reverse() \n\tif argspec.varargs: \n\t \targ_list.append(argspec.varargs) \n\tif argspec.keywords: \n\t \targ_list.append(argspec.keywords) \n\targ_list = [x[:trunc] for x in arg_list] \n\tstr_param = ('%s(%s)' % (name, ', \t'.join(arg_list))) \n\treturn str_param\n", 
" \treturn salt.utils.etcd_util.get_conn(profile)\n", 
" \treturn RetryWithBackoff(callable_func, retry_notify_func, delay, 1, delay, max_tries)\n", 
" \tmsg_tested_versions = ['xp', 'vista', '2008', '2003'] \n\tmsg_args = ['/c', '%SystemRoot%\\\\System32\\\\msg.exe', '*', '/TIME:0'] \n\thost_version = platform.platform().lower() \n\tif (not msg): \n\t \treturn ('Command \tnot \tran.', 'Empty \tmessage.', (-1)) \n\telse: \n\t \tmsg_args.extend([msg]) \n\tfor version in msg_tested_versions: \n\t \tif (host_version.find(version) != (-1)): \n\t \t \tres = client_utils_common.Execute('cmd', msg_args, time_limit=(-1), bypass_whitelist=True) \n\t \t \treturn res \n\treturn ('', 'Command \tnot \tavailable \tfor \tthis \tversion.', (-1))\n", 
" \tmsg_tested_versions = ['xp', 'vista', '2008', '2003'] \n\tmsg_args = ['/c', '%SystemRoot%\\\\System32\\\\msg.exe', '*', '/TIME:0'] \n\thost_version = platform.platform().lower() \n\tif (not msg): \n\t \treturn ('Command \tnot \tran.', 'Empty \tmessage.', (-1)) \n\telse: \n\t \tmsg_args.extend([msg]) \n\tfor version in msg_tested_versions: \n\t \tif (host_version.find(version) != (-1)): \n\t \t \tres = client_utils_common.Execute('cmd', msg_args, time_limit=(-1), bypass_whitelist=True) \n\t \t \treturn res \n\treturn ('', 'Command \tnot \tavailable \tfor \tthis \tversion.', (-1))\n", 
" \tex = copy(event_exchange) \n\tif (conn.transport.driver_type == u'redis'): \n\t \tex.type = u'fanout' \n\treturn ex\n", 
" \tmsg_tested_versions = ['xp', 'vista', '2008', '2003'] \n\tmsg_args = ['/c', '%SystemRoot%\\\\System32\\\\msg.exe', '*', '/TIME:0'] \n\thost_version = platform.platform().lower() \n\tif (not msg): \n\t \treturn ('Command \tnot \tran.', 'Empty \tmessage.', (-1)) \n\telse: \n\t \tmsg_args.extend([msg]) \n\tfor version in msg_tested_versions: \n\t \tif (host_version.find(version) != (-1)): \n\t \t \tres = client_utils_common.Execute('cmd', msg_args, time_limit=(-1), bypass_whitelist=True) \n\t \t \treturn res \n\treturn ('', 'Command \tnot \tavailable \tfor \tthis \tversion.', (-1))\n", 
" \tex = copy(event_exchange) \n\tif (conn.transport.driver_type == u'redis'): \n\t \tex.type = u'fanout' \n\treturn ex\n", 
" \tif ((r.representation == 'html') and (r.name == 'shelter') and r.id and (not r.component)): \n\t \tT = current.T \n\t \tmsg = current.msg \n\t \ts3db = current.s3db \n\t \trecord = r.record \n\t \tctable = s3db.pr_contact \n\t \tstable = s3db.cr_shelter \n\t \tmessage = '' \n\t \ttext = '' \n\t \ts_id = record.id \n\t \ts_name = record.name \n\t \ts_phone = record.phone \n\t \ts_email = record.email \n\t \ts_status = record.status \n\t \tif (s_phone in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_email in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_status in ('', None)): \n\t \t \ts_status = T('Not \tDefined') \n\t \telif (s_status == 1): \n\t \t \ts_status = 'Open' \n\t \telif (s_status == 2): \n\t \t \ts_status = 'Close' \n\t \telse: \n\t \t \ts_status = 'Unassigned \tShelter \tStatus' \n\t \ttext += '************************************************' \n\t \ttext += ('\\n%s \t' % T('Automatic \tMessage')) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Shelter \tID'), s_id)) \n\t \ttext += (' \t%s: \t%s' % (T('Shelter \tname'), s_name)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Email'), s_email)) \n\t \ttext += (' \t%s: \t%s' % (T('Phone'), s_phone)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Working \tStatus'), s_status)) \n\t \ttext += '\\n************************************************\\n' \n\t \turl = URL(c='cr', f='shelter', args=r.id) \n\t \topts = dict(type='SMS', subject=T('Deployment \tRequest'), message=(message + text), url=url) \n\t \toutput = msg.compose(**opts) \n\t \tif attr.get('rheader'): \n\t \t \trheader = attr['rheader'](r) \n\t \t \tif rheader: \n\t \t \t \toutput['rheader'] = rheader \n\t \toutput['title'] = T('Send \tNotification') \n\t \tcurrent.response.view = 'msg/compose.html' \n\t \treturn output \n\telse: \n\t \traise HTTP(501, current.messages.BADMETHOD)\n", 
" \treturn salt.utils.etcd_util.get_conn(profile)\n", 
" \t@functools.wraps(f) \n\tdef Wrapper(self, request): \n\t \t'Wrap \tthe \tfunction \tcan \tcatch \texceptions, \tconverting \tthem \tto \tstatus.' \n\t \tfailed = True \n\t \tresponse = rdf_data_store.DataStoreResponse() \n\t \tresponse.status = rdf_data_store.DataStoreResponse.Status.OK \n\t \ttry: \n\t \t \tf(self, request, response) \n\t \t \tfailed = False \n\t \texcept access_control.UnauthorizedAccess as e: \n\t \t \tresponse.Clear() \n\t \t \tresponse.request = request \n\t \t \tresponse.status = rdf_data_store.DataStoreResponse.Status.AUTHORIZATION_DENIED \n\t \t \tif e.subject: \n\t \t \t \tresponse.failed_subject = utils.SmartUnicode(e.subject) \n\t \t \tresponse.status_desc = utils.SmartUnicode(e) \n\t \texcept data_store.Error as e: \n\t \t \tresponse.Clear() \n\t \t \tresponse.request = request \n\t \t \tresponse.status = rdf_data_store.DataStoreResponse.Status.DATA_STORE_ERROR \n\t \t \tresponse.status_desc = utils.SmartUnicode(e) \n\t \texcept access_control.ExpiryError as e: \n\t \t \tresponse.Clear() \n\t \t \tresponse.request = request \n\t \t \tresponse.status = rdf_data_store.DataStoreResponse.Status.TIMEOUT_ERROR \n\t \t \tresponse.status_desc = utils.SmartUnicode(e) \n\t \tif failed: \n\t \t \tlogging.info('Failed: \t%s', utils.SmartStr(response)[:1000]) \n\t \tserialized_response = response.SerializeToString() \n\t \treturn serialized_response \n\treturn Wrapper\n", 
" \treturn RetryWithBackoff(callable_func, retry_notify_func, delay, 1, delay, max_tries)\n", 
" \tmsg_tested_versions = ['xp', 'vista', '2008', '2003'] \n\tmsg_args = ['/c', '%SystemRoot%\\\\System32\\\\msg.exe', '*', '/TIME:0'] \n\thost_version = platform.platform().lower() \n\tif (not msg): \n\t \treturn ('Command \tnot \tran.', 'Empty \tmessage.', (-1)) \n\telse: \n\t \tmsg_args.extend([msg]) \n\tfor version in msg_tested_versions: \n\t \tif (host_version.find(version) != (-1)): \n\t \t \tres = client_utils_common.Execute('cmd', msg_args, time_limit=(-1), bypass_whitelist=True) \n\t \t \treturn res \n\treturn ('', 'Command \tnot \tavailable \tfor \tthis \tversion.', (-1))\n", 
" \tcache_key = _cache_get_key() \n\ttry: \n\t \treturn __context__[cache_key] \n\texcept KeyError: \n\t \tpass \n\tconn = _get_conn(region=region, key=key, keyid=keyid, profile=profile) \n\t__context__[cache_key] = {} \n\ttopics = conn.get_all_topics() \n\tfor t in topics['ListTopicsResponse']['ListTopicsResult']['Topics']: \n\t \tshort_name = t['TopicArn'].split(':')[(-1)] \n\t \t__context__[cache_key][short_name] = t['TopicArn'] \n\treturn __context__[cache_key]\n", 
" \treturn salt.utils.etcd_util.get_conn(profile)\n", 
" \treturn RetryWithBackoff(callable_func, retry_notify_func, delay, 1, delay, max_tries)\n", 
" \tmsg_tested_versions = ['xp', 'vista', '2008', '2003'] \n\tmsg_args = ['/c', '%SystemRoot%\\\\System32\\\\msg.exe', '*', '/TIME:0'] \n\thost_version = platform.platform().lower() \n\tif (not msg): \n\t \treturn ('Command \tnot \tran.', 'Empty \tmessage.', (-1)) \n\telse: \n\t \tmsg_args.extend([msg]) \n\tfor version in msg_tested_versions: \n\t \tif (host_version.find(version) != (-1)): \n\t \t \tres = client_utils_common.Execute('cmd', msg_args, time_limit=(-1), bypass_whitelist=True) \n\t \t \treturn res \n\treturn ('', 'Command \tnot \tavailable \tfor \tthis \tversion.', (-1))\n", 
" \tmsg_tested_versions = ['xp', 'vista', '2008', '2003'] \n\tmsg_args = ['/c', '%SystemRoot%\\\\System32\\\\msg.exe', '*', '/TIME:0'] \n\thost_version = platform.platform().lower() \n\tif (not msg): \n\t \treturn ('Command \tnot \tran.', 'Empty \tmessage.', (-1)) \n\telse: \n\t \tmsg_args.extend([msg]) \n\tfor version in msg_tested_versions: \n\t \tif (host_version.find(version) != (-1)): \n\t \t \tres = client_utils_common.Execute('cmd', msg_args, time_limit=(-1), bypass_whitelist=True) \n\t \t \treturn res \n\treturn ('', 'Command \tnot \tavailable \tfor \tthis \tversion.', (-1))\n", 
" \tex = copy(event_exchange) \n\tif (conn.transport.driver_type == u'redis'): \n\t \tex.type = u'fanout' \n\treturn ex\n", 
" \tmsg_tested_versions = ['xp', 'vista', '2008', '2003'] \n\tmsg_args = ['/c', '%SystemRoot%\\\\System32\\\\msg.exe', '*', '/TIME:0'] \n\thost_version = platform.platform().lower() \n\tif (not msg): \n\t \treturn ('Command \tnot \tran.', 'Empty \tmessage.', (-1)) \n\telse: \n\t \tmsg_args.extend([msg]) \n\tfor version in msg_tested_versions: \n\t \tif (host_version.find(version) != (-1)): \n\t \t \tres = client_utils_common.Execute('cmd', msg_args, time_limit=(-1), bypass_whitelist=True) \n\t \t \treturn res \n\treturn ('', 'Command \tnot \tavailable \tfor \tthis \tversion.', (-1))\n", 
" \tex = copy(event_exchange) \n\tif (conn.transport.driver_type == u'redis'): \n\t \tex.type = u'fanout' \n\treturn ex\n", 
" \tif ((r.representation == 'html') and (r.name == 'shelter') and r.id and (not r.component)): \n\t \tT = current.T \n\t \tmsg = current.msg \n\t \ts3db = current.s3db \n\t \trecord = r.record \n\t \tctable = s3db.pr_contact \n\t \tstable = s3db.cr_shelter \n\t \tmessage = '' \n\t \ttext = '' \n\t \ts_id = record.id \n\t \ts_name = record.name \n\t \ts_phone = record.phone \n\t \ts_email = record.email \n\t \ts_status = record.status \n\t \tif (s_phone in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_email in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_status in ('', None)): \n\t \t \ts_status = T('Not \tDefined') \n\t \telif (s_status == 1): \n\t \t \ts_status = 'Open' \n\t \telif (s_status == 2): \n\t \t \ts_status = 'Close' \n\t \telse: \n\t \t \ts_status = 'Unassigned \tShelter \tStatus' \n\t \ttext += '************************************************' \n\t \ttext += ('\\n%s \t' % T('Automatic \tMessage')) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Shelter \tID'), s_id)) \n\t \ttext += (' \t%s: \t%s' % (T('Shelter \tname'), s_name)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Email'), s_email)) \n\t \ttext += (' \t%s: \t%s' % (T('Phone'), s_phone)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Working \tStatus'), s_status)) \n\t \ttext += '\\n************************************************\\n' \n\t \turl = URL(c='cr', f='shelter', args=r.id) \n\t \topts = dict(type='SMS', subject=T('Deployment \tRequest'), message=(message + text), url=url) \n\t \toutput = msg.compose(**opts) \n\t \tif attr.get('rheader'): \n\t \t \trheader = attr['rheader'](r) \n\t \t \tif rheader: \n\t \t \t \toutput['rheader'] = rheader \n\t \toutput['title'] = T('Send \tNotification') \n\t \tcurrent.response.view = 'msg/compose.html' \n\t \treturn output \n\telse: \n\t \traise HTTP(501, current.messages.BADMETHOD)\n", 
" \tif (not settings.FEATURES.get('ENABLE_FEEDBACK_SUBMISSION', False)): \n\t \traise Http404() \n\tif (request.method != 'POST'): \n\t \treturn HttpResponseNotAllowed(['POST']) \n\tdef build_error_response(status_code, field, err_msg): \n\t \treturn HttpResponse(json.dumps({'field': field, 'error': err_msg}), status=status_code) \n\trequired_fields = ['subject', 'details'] \n\tif (not request.user.is_authenticated()): \n\t \trequired_fields += ['name', 'email'] \n\trequired_field_errs = {'subject': 'Please \tprovide \ta \tsubject.', 'details': 'Please \tprovide \tdetails.', 'name': 'Please \tprovide \tyour \tname.', 'email': 'Please \tprovide \ta \tvalid \te-mail.'} \n\tfor field in required_fields: \n\t \tif ((field not in request.POST) or (not request.POST[field])): \n\t \t \treturn build_error_response(400, field, required_field_errs[field]) \n\tif (not request.user.is_authenticated()): \n\t \ttry: \n\t \t \tvalidate_email(request.POST['email']) \n\t \texcept ValidationError: \n\t \t \treturn build_error_response(400, 'email', required_field_errs['email']) \n\tsuccess = False \n\tcontext = get_feedback_form_context(request) \n\tsupport_backend = configuration_helpers.get_value('CONTACT_FORM_SUBMISSION_BACKEND', SUPPORT_BACKEND_ZENDESK) \n\tif (support_backend == SUPPORT_BACKEND_EMAIL): \n\t \ttry: \n\t \t \tsend_mail(subject=render_to_string('emails/contact_us_feedback_email_subject.txt', context), message=render_to_string('emails/contact_us_feedback_email_body.txt', context), from_email=context['support_email'], recipient_list=[context['support_email']], fail_silently=False) \n\t \t \tsuccess = True \n\t \texcept SMTPException: \n\t \t \tlog.exception('Error \tsending \tfeedback \tto \tcontact_us \temail \taddress.') \n\t \t \tsuccess = False \n\telse: \n\t \tif ((not settings.ZENDESK_URL) or (not settings.ZENDESK_USER) or (not settings.ZENDESK_API_KEY)): \n\t \t \traise Exception('Zendesk \tenabled \tbut \tnot \tconfigured') \n\t \tcustom_fields = None \n\t \tif settings.ZENDESK_CUSTOM_FIELDS: \n\t \t \tcustom_field_context = _get_zendesk_custom_field_context(request) \n\t \t \tcustom_fields = _format_zendesk_custom_fields(custom_field_context) \n\t \tsuccess = _record_feedback_in_zendesk(context['realname'], context['email'], context['subject'], context['details'], context['tags'], context['additional_info'], support_email=context['support_email'], custom_fields=custom_fields) \n\t_record_feedback_in_datadog(context['tags']) \n\treturn HttpResponse(status=(200 if success else 500))\n", 
" \tfor v in args: \n\t \tsys.stderr.write(str(v)) \n\tsys.stderr.write('\\n')\n", 
" \tif isinstance(arg, (list, tuple)): \n\t \treturn list(arg) \n\telse: \n\t \treturn [arg]\n", 
" \treturn survey_link.format(UNIQUE_ID=unique_id_for_user(user))\n", 
" \treturn salt.utils.etcd_util.get_conn(profile)\n", 
" \treturn RetryWithBackoff(callable_func, retry_notify_func, delay, 1, delay, max_tries)\n", 
" \tmsg_tested_versions = ['xp', 'vista', '2008', '2003'] \n\tmsg_args = ['/c', '%SystemRoot%\\\\System32\\\\msg.exe', '*', '/TIME:0'] \n\thost_version = platform.platform().lower() \n\tif (not msg): \n\t \treturn ('Command \tnot \tran.', 'Empty \tmessage.', (-1)) \n\telse: \n\t \tmsg_args.extend([msg]) \n\tfor version in msg_tested_versions: \n\t \tif (host_version.find(version) != (-1)): \n\t \t \tres = client_utils_common.Execute('cmd', msg_args, time_limit=(-1), bypass_whitelist=True) \n\t \t \treturn res \n\treturn ('', 'Command \tnot \tavailable \tfor \tthis \tversion.', (-1))\n", 
" \tmsg_tested_versions = ['xp', 'vista', '2008', '2003'] \n\tmsg_args = ['/c', '%SystemRoot%\\\\System32\\\\msg.exe', '*', '/TIME:0'] \n\thost_version = platform.platform().lower() \n\tif (not msg): \n\t \treturn ('Command \tnot \tran.', 'Empty \tmessage.', (-1)) \n\telse: \n\t \tmsg_args.extend([msg]) \n\tfor version in msg_tested_versions: \n\t \tif (host_version.find(version) != (-1)): \n\t \t \tres = client_utils_common.Execute('cmd', msg_args, time_limit=(-1), bypass_whitelist=True) \n\t \t \treturn res \n\treturn ('', 'Command \tnot \tavailable \tfor \tthis \tversion.', (-1))\n", 
" \tex = copy(event_exchange) \n\tif (conn.transport.driver_type == u'redis'): \n\t \tex.type = u'fanout' \n\treturn ex\n", 
" \tmsg_tested_versions = ['xp', 'vista', '2008', '2003'] \n\tmsg_args = ['/c', '%SystemRoot%\\\\System32\\\\msg.exe', '*', '/TIME:0'] \n\thost_version = platform.platform().lower() \n\tif (not msg): \n\t \treturn ('Command \tnot \tran.', 'Empty \tmessage.', (-1)) \n\telse: \n\t \tmsg_args.extend([msg]) \n\tfor version in msg_tested_versions: \n\t \tif (host_version.find(version) != (-1)): \n\t \t \tres = client_utils_common.Execute('cmd', msg_args, time_limit=(-1), bypass_whitelist=True) \n\t \t \treturn res \n\treturn ('', 'Command \tnot \tavailable \tfor \tthis \tversion.', (-1))\n", 
" \tex = copy(event_exchange) \n\tif (conn.transport.driver_type == u'redis'): \n\t \tex.type = u'fanout' \n\treturn ex\n", 
" \tif ((r.representation == 'html') and (r.name == 'shelter') and r.id and (not r.component)): \n\t \tT = current.T \n\t \tmsg = current.msg \n\t \ts3db = current.s3db \n\t \trecord = r.record \n\t \tctable = s3db.pr_contact \n\t \tstable = s3db.cr_shelter \n\t \tmessage = '' \n\t \ttext = '' \n\t \ts_id = record.id \n\t \ts_name = record.name \n\t \ts_phone = record.phone \n\t \ts_email = record.email \n\t \ts_status = record.status \n\t \tif (s_phone in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_email in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_status in ('', None)): \n\t \t \ts_status = T('Not \tDefined') \n\t \telif (s_status == 1): \n\t \t \ts_status = 'Open' \n\t \telif (s_status == 2): \n\t \t \ts_status = 'Close' \n\t \telse: \n\t \t \ts_status = 'Unassigned \tShelter \tStatus' \n\t \ttext += '************************************************' \n\t \ttext += ('\\n%s \t' % T('Automatic \tMessage')) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Shelter \tID'), s_id)) \n\t \ttext += (' \t%s: \t%s' % (T('Shelter \tname'), s_name)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Email'), s_email)) \n\t \ttext += (' \t%s: \t%s' % (T('Phone'), s_phone)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Working \tStatus'), s_status)) \n\t \ttext += '\\n************************************************\\n' \n\t \turl = URL(c='cr', f='shelter', args=r.id) \n\t \topts = dict(type='SMS', subject=T('Deployment \tRequest'), message=(message + text), url=url) \n\t \toutput = msg.compose(**opts) \n\t \tif attr.get('rheader'): \n\t \t \trheader = attr['rheader'](r) \n\t \t \tif rheader: \n\t \t \t \toutput['rheader'] = rheader \n\t \toutput['title'] = T('Send \tNotification') \n\t \tcurrent.response.view = 'msg/compose.html' \n\t \treturn output \n\telse: \n\t \traise HTTP(501, current.messages.BADMETHOD)\n", 
" \treturn apiproxy_stub_map.UserRPC('images', deadline, callback)\n", 
" \tprint('got \tperspective1 \tref:', perspective) \n\tprint('asking \tit \tto \tfoo(13)') \n\treturn perspective.callRemote('foo', 13)\n", 
" \tif (name in _events): \n\t \tfor event in get_events(name): \n\t \t \tresult = event(*args, **kwargs) \n\t \t \tif (result is not None): \n\t \t \t \targs = ((result,) + args[1:]) \n\treturn (args and args[0])\n", 
" \targs = (['--version'] + _base_args(request.config)) \n\tproc = quteprocess.QuteProc(request) \n\tproc.proc.setProcessChannelMode(QProcess.SeparateChannels) \n\ttry: \n\t \tproc.start(args) \n\t \tproc.wait_for_quit() \n\texcept testprocess.ProcessExited: \n\t \tassert (proc.proc.exitStatus() == QProcess.NormalExit) \n\telse: \n\t \tpytest.fail('Process \tdid \tnot \texit!') \n\toutput = bytes(proc.proc.readAllStandardOutput()).decode('utf-8') \n\tassert (re.search('^qutebrowser\\\\s+v\\\\d+(\\\\.\\\\d+)', output) is not None)\n", 
" \tif (not callable(method)): \n\t \treturn None \n\ttry: \n\t \tmethod_info = method.remote \n\texcept AttributeError: \n\t \treturn None \n\tif (not isinstance(method_info, _RemoteMethodInfo)): \n\t \treturn None \n\treturn method_info\n", 
" \tif ((r.representation == 'html') and (r.name == 'shelter') and r.id and (not r.component)): \n\t \tT = current.T \n\t \tmsg = current.msg \n\t \ts3db = current.s3db \n\t \trecord = r.record \n\t \tctable = s3db.pr_contact \n\t \tstable = s3db.cr_shelter \n\t \tmessage = '' \n\t \ttext = '' \n\t \ts_id = record.id \n\t \ts_name = record.name \n\t \ts_phone = record.phone \n\t \ts_email = record.email \n\t \ts_status = record.status \n\t \tif (s_phone in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_email in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_status in ('', None)): \n\t \t \ts_status = T('Not \tDefined') \n\t \telif (s_status == 1): \n\t \t \ts_status = 'Open' \n\t \telif (s_status == 2): \n\t \t \ts_status = 'Close' \n\t \telse: \n\t \t \ts_status = 'Unassigned \tShelter \tStatus' \n\t \ttext += '************************************************' \n\t \ttext += ('\\n%s \t' % T('Automatic \tMessage')) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Shelter \tID'), s_id)) \n\t \ttext += (' \t%s: \t%s' % (T('Shelter \tname'), s_name)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Email'), s_email)) \n\t \ttext += (' \t%s: \t%s' % (T('Phone'), s_phone)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Working \tStatus'), s_status)) \n\t \ttext += '\\n************************************************\\n' \n\t \turl = URL(c='cr', f='shelter', args=r.id) \n\t \topts = dict(type='SMS', subject=T('Deployment \tRequest'), message=(message + text), url=url) \n\t \toutput = msg.compose(**opts) \n\t \tif attr.get('rheader'): \n\t \t \trheader = attr['rheader'](r) \n\t \t \tif rheader: \n\t \t \t \toutput['rheader'] = rheader \n\t \toutput['title'] = T('Send \tNotification') \n\t \tcurrent.response.view = 'msg/compose.html' \n\t \treturn output \n\telse: \n\t \traise HTTP(501, current.messages.BADMETHOD)\n", 
" \tif strip_tags: \n\t \ttags_start = name.find('[') \n\t \ttags_end = name.find(']') \n\t \tif ((tags_start > 0) and (tags_end > tags_start)): \n\t \t \tnewname = name[:tags_start] \n\t \t \tnewname += name[(tags_end + 1):] \n\t \t \tname = newname \n\tif strip_scenarios: \n\t \ttags_start = name.find('(') \n\t \ttags_end = name.find(')') \n\t \tif ((tags_start > 0) and (tags_end > tags_start)): \n\t \t \tnewname = name[:tags_start] \n\t \t \tnewname += name[(tags_end + 1):] \n\t \t \tname = newname \n\treturn name\n", 
" \tif (name in _events): \n\t \tfor event in get_events(name): \n\t \t \tresult = event(*args, **kwargs) \n\t \t \tif (result is not None): \n\t \t \t \targs = ((result,) + args[1:]) \n\treturn (args and args[0])\n", 
" \targs = (['--version'] + _base_args(request.config)) \n\tproc = quteprocess.QuteProc(request) \n\tproc.proc.setProcessChannelMode(QProcess.SeparateChannels) \n\ttry: \n\t \tproc.start(args) \n\t \tproc.wait_for_quit() \n\texcept testprocess.ProcessExited: \n\t \tassert (proc.proc.exitStatus() == QProcess.NormalExit) \n\telse: \n\t \tpytest.fail('Process \tdid \tnot \texit!') \n\toutput = bytes(proc.proc.readAllStandardOutput()).decode('utf-8') \n\tassert (re.search('^qutebrowser\\\\s+v\\\\d+(\\\\.\\\\d+)', output) is not None)\n", 
" \treturn IMPL.service_get_all_by_topic(context, topic)\n", 
" \tif (not os.path.isfile(filename)): \n\t \treturn {} \n\ttry: \n\t \twith open(filename, 'r') as fdesc: \n\t \t \tinp = fdesc.read() \n\t \tif (not inp): \n\t \t \treturn {} \n\t \treturn json.loads(inp) \n\texcept (IOError, ValueError) as error: \n\t \t_LOGGER.error('Reading \tconfig \tfile \t%s \tfailed: \t%s', filename, error) \n\t \treturn None\n", 
" \tif (output is None): \n\t \treturn '' \n\telse: \n\t \treturn output.rstrip('\\r\\n')\n", 
" \ttb = traceback.format_exception(*failure_info) \n\tfailure = failure_info[1] \n\tif log_failure: \n\t \tLOG.error(_LE('Returning \texception \t%s \tto \tcaller'), six.text_type(failure)) \n\t \tLOG.error(tb) \n\tkwargs = {} \n\tif hasattr(failure, 'kwargs'): \n\t \tkwargs = failure.kwargs \n\tcls_name = str(failure.__class__.__name__) \n\tmod_name = str(failure.__class__.__module__) \n\tif (cls_name.endswith(_REMOTE_POSTFIX) and mod_name.endswith(_REMOTE_POSTFIX)): \n\t \tcls_name = cls_name[:(- len(_REMOTE_POSTFIX))] \n\t \tmod_name = mod_name[:(- len(_REMOTE_POSTFIX))] \n\tdata = {'class': cls_name, 'module': mod_name, 'message': six.text_type(failure), 'tb': tb, 'args': failure.args, 'kwargs': kwargs} \n\tjson_data = jsonutils.dumps(data) \n\treturn json_data\n", 
" \twith warnings.catch_warnings(record=True) as w: \n\t \tif (clear is not None): \n\t \t \tif (not _is_list_like(clear)): \n\t \t \t \tclear = [clear] \n\t \t \tfor m in clear: \n\t \t \t \tgetattr(m, u'__warningregistry__', {}).clear() \n\t \tsaw_warning = False \n\t \twarnings.simplefilter(filter_level) \n\t \t(yield w) \n\t \textra_warnings = [] \n\t \tfor actual_warning in w: \n\t \t \tif (expected_warning and issubclass(actual_warning.category, expected_warning)): \n\t \t \t \tsaw_warning = True \n\t \t \telse: \n\t \t \t \textra_warnings.append(actual_warning.category.__name__) \n\t \tif expected_warning: \n\t \t \tassert saw_warning, (u'Did \tnot \tsee \texpected \twarning \tof \tclass \t%r.' % expected_warning.__name__) \n\t \tassert (not extra_warnings), (u'Caused \tunexpected \twarning(s): \t%r.' % extra_warnings)\n", 
" \ttype1 = type(var1) \n\ttype2 = type(var2) \n\tif (type1 is type2): \n\t \treturn True \n\tif ((type1 is np.ndarray) and (var1.shape == ())): \n\t \treturn (type(var1.item()) is type2) \n\tif ((type2 is np.ndarray) and (var2.shape == ())): \n\t \treturn (type(var2.item()) is type1) \n\treturn False\n", 
" \thostname = urlparse(url).hostname \n\tif (not (('fc2.com' in hostname) or ('xiaojiadianvideo.asia' in hostname))): \n\t \treturn False \n\tupid = match1(url, '.+/content/(\\\\w+)') \n\tfc2video_download_by_upid(upid, output_dir, merge, info_only)\n", 
" \treturn json.loads(data)\n", 
" \tentity_moref = kwargs.get('entity_moref') \n\tentity_type = kwargs.get('entity_type') \n\talarm_moref = kwargs.get('alarm_moref') \n\tif ((not entity_moref) or (not entity_type) or (not alarm_moref)): \n\t \traise ValueError('entity_moref, \tentity_type, \tand \talarm_moref \tmust \tbe \tset') \n\tattribs = {'xmlns:xsd': 'http://www.w3.org/2001/XMLSchema', 'xmlns:xsi': 'http://www.w3.org/2001/XMLSchema-instance', 'xmlns:soap': 'http://schemas.xmlsoap.org/soap/envelope/'} \n\troot = Element('soap:Envelope', attribs) \n\tbody = SubElement(root, 'soap:Body') \n\talarm_status = SubElement(body, 'SetAlarmStatus', {'xmlns': 'urn:vim25'}) \n\tthis = SubElement(alarm_status, '_this', {'xsi:type': 'ManagedObjectReference', 'type': 'AlarmManager'}) \n\tthis.text = 'AlarmManager' \n\talarm = SubElement(alarm_status, 'alarm', {'type': 'Alarm'}) \n\talarm.text = alarm_moref \n\tentity = SubElement(alarm_status, 'entity', {'xsi:type': 'ManagedObjectReference', 'type': entity_type}) \n\tentity.text = entity_moref \n\tstatus = SubElement(alarm_status, 'status') \n\tstatus.text = 'green' \n\treturn '<?xml \tversion=\"1.0\" \tencoding=\"UTF-8\"?>{0}'.format(tostring(root))\n", 
" \tmsg_tested_versions = ['xp', 'vista', '2008', '2003'] \n\tmsg_args = ['/c', '%SystemRoot%\\\\System32\\\\msg.exe', '*', '/TIME:0'] \n\thost_version = platform.platform().lower() \n\tif (not msg): \n\t \treturn ('Command \tnot \tran.', 'Empty \tmessage.', (-1)) \n\telse: \n\t \tmsg_args.extend([msg]) \n\tfor version in msg_tested_versions: \n\t \tif (host_version.find(version) != (-1)): \n\t \t \tres = client_utils_common.Execute('cmd', msg_args, time_limit=(-1), bypass_whitelist=True) \n\t \t \treturn res \n\treturn ('', 'Command \tnot \tavailable \tfor \tthis \tversion.', (-1))\n", 
" \treturn (a * b)\n", 
" \trespbody = response.body \n\tcustom_properties = {} \n\tbroker_properties = None \n\tmessage_type = None \n\tmessage_location = None \n\tfor (name, value) in response.headers: \n\t \tif (name.lower() == 'brokerproperties'): \n\t \t \tbroker_properties = json.loads(value) \n\t \telif (name.lower() == 'content-type'): \n\t \t \tmessage_type = value \n\t \telif (name.lower() == 'location'): \n\t \t \tmessage_location = value \n\t \telif (name.lower() not in ['transfer-encoding', 'server', 'date', 'strict-transport-security']): \n\t \t \tif ('\"' in value): \n\t \t \t \tvalue = value[1:(-1)].replace('\\\\\"', '\"') \n\t \t \t \ttry: \n\t \t \t \t \tcustom_properties[name] = datetime.strptime(value, '%a, \t%d \t%b \t%Y \t%H:%M:%S \tGMT') \n\t \t \t \texcept ValueError: \n\t \t \t \t \tcustom_properties[name] = value \n\t \t \telif (value.lower() == 'true'): \n\t \t \t \tcustom_properties[name] = True \n\t \t \telif (value.lower() == 'false'): \n\t \t \t \tcustom_properties[name] = False \n\t \t \telse: \n\t \t \t \ttry: \n\t \t \t \t \tfloat_value = float(value) \n\t \t \t \t \tif (str(int(float_value)) == value): \n\t \t \t \t \t \tcustom_properties[name] = int(value) \n\t \t \t \t \telse: \n\t \t \t \t \t \tcustom_properties[name] = float_value \n\t \t \t \texcept ValueError: \n\t \t \t \t \tpass \n\tif (message_type is None): \n\t \tmessage = Message(respbody, service_instance, message_location, custom_properties, 'application/atom+xml;type=entry;charset=utf-8', broker_properties) \n\telse: \n\t \tmessage = Message(respbody, service_instance, message_location, custom_properties, message_type, broker_properties) \n\treturn message\n", 
" \tproducer.publish(msg, exchange=exchange, retry=retry, retry_policy=retry_policy, **dict({'routing_key': req.properties['reply_to'], 'correlation_id': req.properties.get('correlation_id'), 'serializer': serializers.type_to_name[req.content_type], 'content_encoding': req.content_encoding}, **props))\n", 
" \tproducer.publish(msg, exchange=exchange, retry=retry, retry_policy=retry_policy, **dict({'routing_key': req.properties['reply_to'], 'correlation_id': req.properties.get('correlation_id'), 'serializer': serializers.type_to_name[req.content_type], 'content_encoding': req.content_encoding}, **props))\n", 
" \tif ((r.representation == 'html') and (r.name == 'shelter') and r.id and (not r.component)): \n\t \tT = current.T \n\t \tmsg = current.msg \n\t \ts3db = current.s3db \n\t \trecord = r.record \n\t \tctable = s3db.pr_contact \n\t \tstable = s3db.cr_shelter \n\t \tmessage = '' \n\t \ttext = '' \n\t \ts_id = record.id \n\t \ts_name = record.name \n\t \ts_phone = record.phone \n\t \ts_email = record.email \n\t \ts_status = record.status \n\t \tif (s_phone in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_email in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_status in ('', None)): \n\t \t \ts_status = T('Not \tDefined') \n\t \telif (s_status == 1): \n\t \t \ts_status = 'Open' \n\t \telif (s_status == 2): \n\t \t \ts_status = 'Close' \n\t \telse: \n\t \t \ts_status = 'Unassigned \tShelter \tStatus' \n\t \ttext += '************************************************' \n\t \ttext += ('\\n%s \t' % T('Automatic \tMessage')) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Shelter \tID'), s_id)) \n\t \ttext += (' \t%s: \t%s' % (T('Shelter \tname'), s_name)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Email'), s_email)) \n\t \ttext += (' \t%s: \t%s' % (T('Phone'), s_phone)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Working \tStatus'), s_status)) \n\t \ttext += '\\n************************************************\\n' \n\t \turl = URL(c='cr', f='shelter', args=r.id) \n\t \topts = dict(type='SMS', subject=T('Deployment \tRequest'), message=(message + text), url=url) \n\t \toutput = msg.compose(**opts) \n\t \tif attr.get('rheader'): \n\t \t \trheader = attr['rheader'](r) \n\t \t \tif rheader: \n\t \t \t \toutput['rheader'] = rheader \n\t \toutput['title'] = T('Send \tNotification') \n\t \tcurrent.response.view = 'msg/compose.html' \n\t \treturn output \n\telse: \n\t \traise HTTP(501, current.messages.BADMETHOD)\n", 
" \tenter_return = None \n\ttry: \n\t \tif isinstance(enter_func, functools.partial): \n\t \t \tenter_func_name = enter_func.func.__name__ \n\t \telse: \n\t \t \tenter_func_name = enter_func.__name__ \n\t \tLOG.debug('Entering \tcontext. \tFunction: \t%(func_name)s, \tuse_enter_return: \t%(use)s.', {'func_name': enter_func_name, 'use': use_enter_return}) \n\t \tenter_return = enter_func() \n\t \t(yield enter_return) \n\tfinally: \n\t \tif isinstance(exit_func, functools.partial): \n\t \t \texit_func_name = exit_func.func.__name__ \n\t \telse: \n\t \t \texit_func_name = exit_func.__name__ \n\t \tLOG.debug('Exiting \tcontext. \tFunction: \t%(func_name)s, \tuse_enter_return: \t%(use)s.', {'func_name': exit_func_name, 'use': use_enter_return}) \n\t \tif (enter_return is not None): \n\t \t \tif use_enter_return: \n\t \t \t \tignore_exception(exit_func, enter_return) \n\t \t \telse: \n\t \t \t \tignore_exception(exit_func)\n", 
" \tglobal _path_created \n\tif (not isinstance(name, StringTypes)): \n\t \traise DistutilsInternalError, (\"mkpath: \t'name' \tmust \tbe \ta \tstring \t(got \t%r)\" % (name,)) \n\tname = os.path.normpath(name) \n\tcreated_dirs = [] \n\tif (os.path.isdir(name) or (name == '')): \n\t \treturn created_dirs \n\tif _path_created.get(os.path.abspath(name)): \n\t \treturn created_dirs \n\t(head, tail) = os.path.split(name) \n\ttails = [tail] \n\twhile (head and tail and (not os.path.isdir(head))): \n\t \t(head, tail) = os.path.split(head) \n\t \ttails.insert(0, tail) \n\tfor d in tails: \n\t \thead = os.path.join(head, d) \n\t \tabs_head = os.path.abspath(head) \n\t \tif _path_created.get(abs_head): \n\t \t \tcontinue \n\t \tif (verbose >= 1): \n\t \t \tlog.info('creating \t%s', head) \n\t \tif (not dry_run): \n\t \t \ttry: \n\t \t \t \tos.mkdir(head) \n\t \t \t \tcreated_dirs.append(head) \n\t \t \texcept OSError as exc: \n\t \t \t \traise DistutilsFileError, (\"could \tnot \tcreate \t'%s': \t%s\" % (head, exc[(-1)])) \n\t \t_path_created[abs_head] = 1 \n\treturn created_dirs\n", 
" \ttry: \n\t \t(module, attr) = name.rsplit('.', 1) \n\texcept ValueError as error: \n\t \traise ImproperlyConfigured(('Error \timporting \tclass \t%s \tin \t%s: \t\"%s\"' % (name, setting, error))) \n\ttry: \n\t \tmod = import_module(module) \n\texcept ImportError as error: \n\t \traise ImproperlyConfigured(('Error \timporting \tmodule \t%s \tin \t%s: \t\"%s\"' % (module, setting, error))) \n\ttry: \n\t \tcls = getattr(mod, attr) \n\texcept AttributeError: \n\t \traise ImproperlyConfigured(('Module \t\"%s\" \tdoes \tnot \tdefine \ta \t\"%s\" \tclass \tin \t%s' % (module, attr, setting))) \n\treturn cls\n", 
" \tos.environ['LANG'] = 'C' \n\tcli = _DRIVERS.get('HORCM') \n\treturn importutils.import_object('.'.join([_DRIVER_DIR, cli[driver_info['proto']]]), conf, driver_info, db)\n", 
" \tos.environ['LANG'] = 'C' \n\tcli = _DRIVERS.get('HORCM') \n\treturn importutils.import_object('.'.join([_DRIVER_DIR, cli[driver_info['proto']]]), conf, driver_info, db)\n", 
" \tif name.startswith('.'): \n\t \tif (not package): \n\t \t \traise TypeError(\"relative \timports \trequire \tthe \t'package' \targument\") \n\t \tlevel = 0 \n\t \tfor character in name: \n\t \t \tif (character != '.'): \n\t \t \t \tbreak \n\t \t \tlevel += 1 \n\t \tname = _resolve_name(name[level:], package, level) \n\t__import__(name) \n\treturn sys.modules[name]\n", 
" \ttry: \n\t \treturn namedModule(name) \n\texcept ImportError: \n\t \treturn default\n", 
" \tif (not at): \n\t \tat = utcnow() \n\tst = at.strftime((_ISO8601_TIME_FORMAT if (not subsecond) else _ISO8601_TIME_FORMAT_SUBSECOND)) \n\ttz = (at.tzinfo.tzname(None) if at.tzinfo else 'UTC') \n\tst += ('Z' if (tz == 'UTC') else tz) \n\treturn st\n", 
" \ttry: \n\t \treturn iso8601.parse_date(timestr) \n\texcept iso8601.ParseError as e: \n\t \traise ValueError(encodeutils.exception_to_unicode(e)) \n\texcept TypeError as e: \n\t \traise ValueError(encodeutils.exception_to_unicode(e))\n", 
" \tnow = datetime.utcnow().replace(tzinfo=iso8601.iso8601.UTC) \n\tif iso: \n\t \treturn datetime_tuple_to_iso(now) \n\telse: \n\t \treturn now\n", 
" \treturn formatdate(timegm(dt.utctimetuple()))\n", 
" \toffset = timestamp.utcoffset() \n\tif (offset is None): \n\t \treturn timestamp \n\treturn (timestamp.replace(tzinfo=None) - offset)\n", 
" \tif os.path.isfile(db_file_name): \n\t \tfrom datetime import timedelta \n\t \tfile_datetime = datetime.fromtimestamp(os.stat(db_file_name).st_ctime) \n\t \tif ((datetime.today() - file_datetime) >= timedelta(hours=older_than)): \n\t \t \tif verbose: \n\t \t \t \tprint u'File \tis \told' \n\t \t \treturn True \n\t \telse: \n\t \t \tif verbose: \n\t \t \t \tprint u'File \tis \trecent' \n\t \t \treturn False \n\telse: \n\t \tif verbose: \n\t \t \tprint u'File \tdoes \tnot \texist' \n\t \treturn True\n", 
" \tif (not os.path.exists(source)): \n\t \traise DistutilsFileError((\"file \t'%s' \tdoes \tnot \texist\" % os.path.abspath(source))) \n\tif (not os.path.exists(target)): \n\t \treturn True \n\treturn (os.stat(source)[ST_MTIME] > os.stat(target)[ST_MTIME])\n", 
" \tif utcnow.override_time: \n\t \ttry: \n\t \t \treturn utcnow.override_time.pop(0) \n\t \texcept AttributeError: \n\t \t \treturn utcnow.override_time \n\tif with_timezone: \n\t \treturn datetime.datetime.now(tz=iso8601.iso8601.UTC) \n\treturn datetime.datetime.utcnow()\n", 
" \tif utcnow.override_time: \n\t \ttry: \n\t \t \treturn utcnow.override_time.pop(0) \n\t \texcept AttributeError: \n\t \t \treturn utcnow.override_time \n\tif with_timezone: \n\t \treturn datetime.datetime.now(tz=iso8601.iso8601.UTC) \n\treturn datetime.datetime.utcnow()\n", 
" \treturn isotime(datetime.datetime.utcfromtimestamp(timestamp), microsecond)\n", 
" \tif utcnow.override_time: \n\t \ttry: \n\t \t \treturn utcnow.override_time.pop(0) \n\t \texcept AttributeError: \n\t \t \treturn utcnow.override_time \n\tif with_timezone: \n\t \treturn datetime.datetime.now(tz=iso8601.iso8601.UTC) \n\treturn datetime.datetime.utcnow()\n", 
" \tmatch = standard_duration_re.match(value) \n\tif (not match): \n\t \tmatch = iso8601_duration_re.match(value) \n\tif match: \n\t \tkw = match.groupdict() \n\t \tsign = ((-1) if (kw.pop('sign', '+') == '-') else 1) \n\t \tif kw.get('microseconds'): \n\t \t \tkw['microseconds'] = kw['microseconds'].ljust(6, '0') \n\t \tif (kw.get('seconds') and kw.get('microseconds') and kw['seconds'].startswith('-')): \n\t \t \tkw['microseconds'] = ('-' + kw['microseconds']) \n\t \tkw = {k: float(v) for (k, v) in kw.items() if (v is not None)} \n\t \treturn (sign * datetime.timedelta(**kw))\n", 
" \treturn call_talib_with_ohlc(barDs, count, talib.CDLADVANCEBLOCK)\n", 
" \ttry: \n\t \tparent = next(klass.local_attr_ancestors(name)) \n\texcept (StopIteration, KeyError): \n\t \treturn None \n\ttry: \n\t \tmeth_node = parent[name] \n\texcept KeyError: \n\t \treturn None \n\tif isinstance(meth_node, astroid.Function): \n\t \treturn meth_node \n\treturn None\n", 
" \tif value.tzinfo: \n\t \tvalue = value.astimezone(UTC) \n\treturn (long((calendar.timegm(value.timetuple()) * 1000000L)) + value.microsecond)\n", 
" \t(p, u) = getparser(use_datetime=use_datetime, use_builtin_types=use_builtin_types) \n\tp.feed(data) \n\tp.close() \n\treturn (u.close(), u.getmethodname())\n", 
" \tlater = (mktime(date1.timetuple()) + (date1.microsecond / 1000000.0)) \n\tearlier = (mktime(date2.timetuple()) + (date2.microsecond / 1000000.0)) \n\treturn (later - earlier)\n", 
" \tgenerate_completions(event)\n", 
" \tif ((not os.path.exists(path)) and (CONF.libvirt.images_type != 'rbd')): \n\t \traise exception.DiskNotFound(location=path) \n\ttry: \n\t \tif (os.path.isdir(path) and os.path.exists(os.path.join(path, 'DiskDescriptor.xml'))): \n\t \t \tpath = os.path.join(path, 'root.hds') \n\t \tcmd = ('env', 'LC_ALL=C', 'LANG=C', 'qemu-img', 'info', path) \n\t \tif (format is not None): \n\t \t \tcmd = (cmd + ('-f', format)) \n\t \t(out, err) = utils.execute(prlimit=QEMU_IMG_LIMITS, *cmd) \n\texcept processutils.ProcessExecutionError as exp: \n\t \tif (exp.exit_code == (-9)): \n\t \t \tmsg = (_('qemu-img \taborted \tby \tprlimits \twhen \tinspecting \t%(path)s \t: \t%(exp)s') % {'path': path, 'exp': exp}) \n\t \telse: \n\t \t \tmsg = (_('qemu-img \tfailed \tto \texecute \ton \t%(path)s \t: \t%(exp)s') % {'path': path, 'exp': exp}) \n\t \traise exception.InvalidDiskInfo(reason=msg) \n\tif (not out): \n\t \tmsg = (_('Failed \tto \trun \tqemu-img \tinfo \ton \t%(path)s \t: \t%(error)s') % {'path': path, 'error': err}) \n\t \traise exception.InvalidDiskInfo(reason=msg) \n\treturn imageutils.QemuImgInfo(out)\n", 
" \tif (in_format is None): \n\t \traise RuntimeError('convert_image \twithout \tinput \tformat \tis \ta \tsecurity \trisk') \n\t_convert_image(source, dest, in_format, out_format, run_as_root)\n", 
" \turl = urllib.parse.urlparse(image_href) \n\tnetloc = url.netloc \n\timage_id = url.path.split('/')[(-1)] \n\tuse_ssl = (url.scheme == 'https') \n\treturn (image_id, netloc, use_ssl)\n", 
" \tparams = {} \n\tparams['identity_headers'] = generate_identity_headers(context) \n\tif endpoint.startswith('https://'): \n\t \tparams['insecure'] = CONF.glance.api_insecure \n\t \tparams['ssl_compression'] = False \n\t \tsslutils.is_enabled(CONF) \n\t \tif CONF.ssl.cert_file: \n\t \t \tparams['cert_file'] = CONF.ssl.cert_file \n\t \tif CONF.ssl.key_file: \n\t \t \tparams['key_file'] = CONF.ssl.key_file \n\t \tif CONF.ssl.ca_file: \n\t \t \tparams['cacert'] = CONF.ssl.ca_file \n\treturn glanceclient.Client(str(version), endpoint, **params)\n", 
" \tif (stop is None): \n\t \tstop = len(x) \n\tfor i in reversed(xrange((start + 1), stop)): \n\t \tj = random.randint(start, i) \n\t \t(x[i], x[j]) = (x[j], x[i])\n", 
" \tfor attr in ['created_at', 'updated_at', 'deleted_at']: \n\t \tif image_meta.get(attr): \n\t \t \timage_meta[attr] = timeutils.parse_isotime(image_meta[attr]) \n\treturn image_meta\n", 
" \t(exc_type, exc_value, exc_trace) = sys.exc_info() \n\tnew_exc = _translate_image_exception(image_id, exc_value) \n\tsix.reraise(type(new_exc), new_exc, exc_trace)\n", 
" \t(exc_type, exc_value, exc_trace) = sys.exc_info() \n\tnew_exc = _translate_plain_exception(exc_value) \n\tsix.reraise(type(new_exc), new_exc, exc_trace)\n", 
" \tif ('/' not in str(image_href)): \n\t \timage_service = get_default_image_service() \n\t \treturn (image_service, image_href) \n\ttry: \n\t \t(image_id, endpoint) = _endpoint_from_image_ref(image_href) \n\t \tglance_client = GlanceClientWrapper(context=context, endpoint=endpoint) \n\texcept ValueError: \n\t \traise exception.InvalidImageRef(image_href=image_href) \n\timage_service = GlanceImageServiceV2(client=glance_client) \n\treturn (image_service, image_id)\n", 
" \tif (not name): \n\t \traise SaltInvocationError('Required \tparameter \t`name` \tis \tmissing.') \n\tif job_exists(name): \n\t \traise SaltInvocationError('Job \t`{0}` \talready \texists.'.format(name)) \n\tif (not config_xml): \n\t \tconfig_xml = jenkins.EMPTY_CONFIG_XML \n\telse: \n\t \tconfig_xml_file = __salt__['cp.cache_file'](config_xml, saltenv) \n\t \twith salt.utils.fopen(config_xml_file) as _fp: \n\t \t \tconfig_xml = _fp.read() \n\tserver = _connect() \n\ttry: \n\t \tserver.create_job(name, config_xml) \n\texcept jenkins.JenkinsException as err: \n\t \traise SaltInvocationError('Something \twent \twrong \t{0}.'.format(err)) \n\treturn config_xml\n", 
" \tif at_time: \n\t \tcmd = \"echo \t'{0}' \t| \tat \t{1}\".format(cmd, _cmd_quote(at_time)) \n\treturn (not bool(__salt__['cmd.retcode'](cmd, python_shell=True)))\n", 
" \terrback = (errback or _ensure_errback) \n\twith pool.acquire(block=True) as conn: \n\t \tconn.ensure_connection(errback=errback) \n\t \tchannel = conn.default_channel \n\t \trevive = partial(revive_connection, conn, on_revive=on_revive) \n\t \tinsured = conn.autoretry(fun, channel, errback=errback, on_revive=revive, **opts) \n\t \t(retval, _) = insured(*args, **dict(kwargs, connection=conn)) \n\t \treturn retval\n", 
" \tif (not unit): \n\t \tunit = CONF.instance_usage_audit_period \n\toffset = 0 \n\tif ('@' in unit): \n\t \t(unit, offset) = unit.split('@', 1) \n\t \toffset = int(offset) \n\tif (before is not None): \n\t \trightnow = before \n\telse: \n\t \trightnow = timeutils.utcnow() \n\tif (unit not in ('month', 'day', 'year', 'hour')): \n\t \traise ValueError('Time \tperiod \tmust \tbe \thour, \tday, \tmonth \tor \tyear') \n\tif (unit == 'month'): \n\t \tif (offset == 0): \n\t \t \toffset = 1 \n\t \tend = datetime.datetime(day=offset, month=rightnow.month, year=rightnow.year) \n\t \tif (end >= rightnow): \n\t \t \tyear = rightnow.year \n\t \t \tif (1 >= rightnow.month): \n\t \t \t \tyear -= 1 \n\t \t \t \tmonth = (12 + (rightnow.month - 1)) \n\t \t \telse: \n\t \t \t \tmonth = (rightnow.month - 1) \n\t \t \tend = datetime.datetime(day=offset, month=month, year=year) \n\t \tyear = end.year \n\t \tif (1 >= end.month): \n\t \t \tyear -= 1 \n\t \t \tmonth = (12 + (end.month - 1)) \n\t \telse: \n\t \t \tmonth = (end.month - 1) \n\t \tbegin = datetime.datetime(day=offset, month=month, year=year) \n\telif (unit == 'year'): \n\t \tif (offset == 0): \n\t \t \toffset = 1 \n\t \tend = datetime.datetime(day=1, month=offset, year=rightnow.year) \n\t \tif (end >= rightnow): \n\t \t \tend = datetime.datetime(day=1, month=offset, year=(rightnow.year - 1)) \n\t \t \tbegin = datetime.datetime(day=1, month=offset, year=(rightnow.year - 2)) \n\t \telse: \n\t \t \tbegin = datetime.datetime(day=1, month=offset, year=(rightnow.year - 1)) \n\telif (unit == 'day'): \n\t \tend = datetime.datetime(hour=offset, day=rightnow.day, month=rightnow.month, year=rightnow.year) \n\t \tif (end >= rightnow): \n\t \t \tend = (end - datetime.timedelta(days=1)) \n\t \tbegin = (end - datetime.timedelta(days=1)) \n\telif (unit == 'hour'): \n\t \tend = rightnow.replace(minute=offset, second=0, microsecond=0) \n\t \tif (end >= rightnow): \n\t \t \tend = (end - datetime.timedelta(hours=1)) \n\t \tbegin = (end - datetime.timedelta(hours=1)) \n\treturn (begin, end)\n", 
" \tif (length is None): \n\t \tlength = CONF.password_length \n\tr = random.SystemRandom() \n\tpassword = [r.choice(s) for s in symbolgroups] \n\tr.shuffle(password) \n\tpassword = password[:length] \n\tlength -= len(password) \n\tsymbols = ''.join(symbolgroups) \n\tpassword.extend([r.choice(symbols) for _i in range(length)]) \n\tr.shuffle(password) \n\treturn ''.join(password)\n", 
" \tglobal CSSAttrCache \n\tCSSAttrCache = {} \n\tif xhtml: \n\t \tparser = html5lib.XHTMLParser(tree=treebuilders.getTreeBuilder(u'dom')) \n\telse: \n\t \tparser = html5lib.HTMLParser(tree=treebuilders.getTreeBuilder(u'dom')) \n\tif isinstance(src, six.text_type): \n\t \tif (not encoding): \n\t \t \tencoding = u'utf-8' \n\t \tsrc = src.encode(encoding) \n\t \tsrc = pisaTempFile(src, capacity=context.capacity) \n\tdocument = parser.parse(src) \n\tif xml_output: \n\t \tif encoding: \n\t \t \txml_output.write(document.toprettyxml(encoding=encoding)) \n\t \telse: \n\t \t \txml_output.write(document.toprettyxml(encoding=u'utf8')) \n\tif default_css: \n\t \tcontext.addDefaultCSS(default_css) \n\tpisaPreLoop(document, context) \n\tcontext.parseCSS() \n\tpisaLoop(document, context) \n\treturn context\n", 
" \treturn _XHTML_ESCAPE_RE.sub((lambda match: _XHTML_ESCAPE_DICT[match.group(0)]), to_basestring(value))\n", 
" \tif ((value is None) or isinstance(value, six.binary_type)): \n\t \treturn value \n\tif (not isinstance(value, six.text_type)): \n\t \tvalue = six.text_type(value) \n\treturn value.encode('utf-8')\n", 
" \ttry: \n\t \tos.remove(fname) \n\texcept OSError: \n\t \tpass\n", 
" \treturn [api for api in apis if (api['name'] == name)]\n", 
" \titems = [] \n\tfor (k, v) in d.items(): \n\t \tnew_key = (((parent_key + '.') + k) if parent_key else k) \n\t \tif isinstance(v, collections.MutableMapping): \n\t \t \titems.extend(list(flatten_dict(v, new_key).items())) \n\t \telse: \n\t \t \titems.append((new_key, v)) \n\treturn dict(items)\n", 
" \tdata_copy = deepcopy(data) \n\tfield_names = {} \n\tfor (key, value) in data.iteritems(): \n\t \tif (key in DEFAULT_FIELD_NAMES): \n\t \t \tfield_names[key] = data_copy.pop(key) \n\treturn (field_names, data_copy)\n", 
" \treturn dict(((k.upper(), v) for (k, v) in dictionary.items()))\n", 
" \treturn dict([(k, v) for (k, v) in six.iteritems(master_dict) if (k in keys)])\n", 
" \tif isinstance(obj, cls): \n\t \treturn obj \n\traise Exception((_('Expected \tobject \tof \ttype: \t%s') % str(cls)))\n", 
" \treturn STRING_BOOLS[string.strip().lower()]\n", 
" \ttry: \n\t \tparts = urlparse.urlparse(url) \n\t \tscheme = parts[0] \n\t \tnetloc = parts[1] \n\t \tif (scheme and netloc): \n\t \t \treturn True \n\t \telse: \n\t \t \treturn False \n\texcept: \n\t \treturn False\n", 
" \tbits = (4294967295 ^ ((1 << (32 - mask)) - 1)) \n\treturn socket.inet_ntoa(struct.pack('>I', bits))\n", 
" \tif (not CONF.monkey_patch): \n\t \treturn \n\tif six.PY2: \n\t \tis_method = inspect.ismethod \n\telse: \n\t \tdef is_method(obj): \n\t \t \treturn (inspect.ismethod(obj) or inspect.isfunction(obj)) \n\tfor module_and_decorator in CONF.monkey_patch_modules: \n\t \t(module, decorator_name) = module_and_decorator.split(':') \n\t \tdecorator = importutils.import_class(decorator_name) \n\t \t__import__(module) \n\t \tmodule_data = pyclbr.readmodule_ex(module) \n\t \tfor (key, value) in module_data.items(): \n\t \t \tif isinstance(value, pyclbr.Class): \n\t \t \t \tclz = importutils.import_class(('%s.%s' % (module, key))) \n\t \t \t \tfor (method, func) in inspect.getmembers(clz, is_method): \n\t \t \t \t \tsetattr(clz, method, decorator(('%s.%s.%s' % (module, key, method)), func)) \n\t \t \tif isinstance(value, pyclbr.Function): \n\t \t \t \tfunc = importutils.import_class(('%s.%s' % (module, key))) \n\t \t \t \tsetattr(sys.modules[module], key, decorator(('%s.%s' % (module, key)), func))\n", 
" \tfor item in list_: \n\t \tif (item.get(search_field) == value): \n\t \t \treturn item.get(output_field, value) \n\treturn value\n", 
" \tend = clock() \n\ttotal = (end - START) \n\tprint('Completion \ttime: \t{0} \tseconds.'.format(total))\n", 
" \tkwargs = {} \n\tv_list = ['public', 'private'] \n\tcf_list = ['ami', 'ari', 'aki', 'bare', 'ovf'] \n\tdf_list = ['ami', 'ari', 'aki', 'vhd', 'vmdk', 'raw', 'qcow2', 'vdi', 'iso'] \n\tkwargs['copy_from'] = location \n\tif (visibility is not None): \n\t \tif (visibility not in v_list): \n\t \t \traise SaltInvocationError(('\"visibility\" \tneeds \tto \tbe \tone \t' + 'of \tthe \tfollowing: \t{0}'.format(', \t'.join(v_list)))) \n\t \telif (visibility == 'public'): \n\t \t \tkwargs['is_public'] = True \n\t \telse: \n\t \t \tkwargs['is_public'] = False \n\telse: \n\t \tkwargs['is_public'] = True \n\tif (container_format not in cf_list): \n\t \traise SaltInvocationError(('\"container_format\" \tneeds \tto \tbe \t' + 'one \tof \tthe \tfollowing: \t{0}'.format(', \t'.join(cf_list)))) \n\telse: \n\t \tkwargs['container_format'] = container_format \n\tif (disk_format not in df_list): \n\t \traise SaltInvocationError(('\"disk_format\" \tneeds \tto \tbe \tone \t' + 'of \tthe \tfollowing: \t{0}'.format(', \t'.join(df_list)))) \n\telse: \n\t \tkwargs['disk_format'] = disk_format \n\tif (protected is not None): \n\t \tkwargs['protected'] = protected \n\tg_client = _auth(profile, api_version=1) \n\timage = g_client.images.create(name=name, **kwargs) \n\treturn image_show(image.id, profile=profile)\n", 
" \ttry: \n\t \tif lib_cls: \n\t \t \treturn lib_cls(lib) \n\t \telse: \n\t \t \treturn ctypes.CDLL(lib) \n\texcept Exception: \n\t \tif name: \n\t \t \tlib_msg = ('%s \t(%s)' % (name, lib)) \n\t \telse: \n\t \t \tlib_msg = lib \n\t \tlib_msg += ' \tcould \tnot \tbe \tloaded' \n\t \tif (sys.platform == 'cygwin'): \n\t \t \tlib_msg += ' \tin \tcygwin' \n\t \t_LOGGER.error(lib_msg, exc_info=True) \n\t \treturn None\n", 
" \treturn (_request_ctx_stack.top is not None)\n", 
" \tpath = os.path.join(base, dev) \n\tif partition: \n\t \tpath += str(partition) \n\treturn path\n", 
" \tif hasattr(td, 'total_seconds'): \n\t \treturn td.total_seconds() \n\tms = td.microseconds \n\tsecs = (td.seconds + ((td.days * 24) * 3600)) \n\treturn ((ms + (secs * (10 ** 6))) / (10 ** 6))\n", 
" \tif six.PY3: \n\t \thostname = hostname.encode('latin-1', 'ignore') \n\t \thostname = hostname.decode('latin-1') \n\telif isinstance(hostname, six.text_type): \n\t \thostname = hostname.encode('latin-1', 'ignore') \n\thostname = re.sub('[ \t_]', '-', hostname) \n\thostname = re.sub('[^\\\\w.-]+', '', hostname) \n\thostname = hostname.lower() \n\thostname = hostname.strip('.-') \n\treturn hostname\n", 
" \tglobal _FILE_CACHE \n\tif force_reload: \n\t \tdelete_cached_file(filename) \n\treloaded = False \n\tmtime = os.path.getmtime(filename) \n\tcache_info = _FILE_CACHE.setdefault(filename, {}) \n\tif ((not cache_info) or (mtime > cache_info.get('mtime', 0))): \n\t \tLOG.debug('Reloading \tcached \tfile \t%s', filename) \n\t \twith open(filename) as fap: \n\t \t \tcache_info['data'] = fap.read() \n\t \tcache_info['mtime'] = mtime \n\t \treloaded = True \n\treturn (reloaded, cache_info['data'])\n", 
" \treturn open(*args, **kwargs)\n", 
" \treturn open(fn, 'r').read()\n", 
" \tdef is_dict_like(thing): \n\t \treturn (hasattr(thing, 'has_key') or isinstance(thing, dict)) \n\tdef get(thing, attr, default): \n\t \tif is_dict_like(thing): \n\t \t \treturn thing.get(attr, default) \n\t \telse: \n\t \t \treturn getattr(thing, attr, default) \n\tdef set_value(thing, attr, val): \n\t \tif is_dict_like(thing): \n\t \t \tthing[attr] = val \n\t \telse: \n\t \t \tsetattr(thing, attr, val) \n\tdef delete(thing, attr): \n\t \tif is_dict_like(thing): \n\t \t \tdel thing[attr] \n\t \telse: \n\t \t \tdelattr(thing, attr) \n\tNOT_PRESENT = object() \n\told_values = {} \n\tfor (attr, new_value) in kwargs.items(): \n\t \told_values[attr] = get(obj, attr, NOT_PRESENT) \n\t \tset_value(obj, attr, new_value) \n\ttry: \n\t \t(yield) \n\tfinally: \n\t \tfor (attr, old_value) in old_values.items(): \n\t \t \tif (old_value is NOT_PRESENT): \n\t \t \t \tdelete(obj, attr) \n\t \t \telse: \n\t \t \t \tset_value(obj, attr, old_value)\n", 
" \tservice = _service_get(s_name, **connection_args) \n\treturn ((service is not None) and (service.get_svrstate() == 'UP'))\n", 
" \tmac = [250, 22, 62, random.randint(0, 255), random.randint(0, 255), random.randint(0, 255)] \n\treturn ':'.join(map((lambda x: ('%02x' % x)), mac))\n", 
" \ttry: \n\t \t(out, _err) = execute('cat', file_path, run_as_root=True) \n\t \treturn out \n\texcept processutils.ProcessExecutionError: \n\t \traise exception.FileNotFound(file_path=file_path)\n", 
" \tif (owner_uid is None): \n\t \towner_uid = os.getuid() \n\torig_uid = os.stat(path).st_uid \n\tif (orig_uid != owner_uid): \n\t \texecute('chown', owner_uid, path, run_as_root=True) \n\ttry: \n\t \t(yield) \n\tfinally: \n\t \tif (orig_uid != owner_uid): \n\t \t \texecute('chown', orig_uid, path, run_as_root=True)\n", 
" \tif (len(s1) != len(s2)): \n\t \treturn False \n\tresult = 0 \n\tfor (a, b) in zip(s1, s2): \n\t \tresult |= (ord(a) ^ ord(b)) \n\treturn (result == 0)\n", 
" \tif (not encountered): \n\t \tencountered = [] \n\tfor subclass in clazz.__subclasses__(): \n\t \tif (subclass not in encountered): \n\t \t \tencountered.append(subclass) \n\t \t \tfor subsubclass in walk_class_hierarchy(subclass, encountered): \n\t \t \t \t(yield subsubclass) \n\t \t \t(yield subclass)\n", 
" \tglobal _path_created \n\tif (not isinstance(name, StringTypes)): \n\t \traise DistutilsInternalError, (\"mkpath: \t'name' \tmust \tbe \ta \tstring \t(got \t%r)\" % (name,)) \n\tname = os.path.normpath(name) \n\tcreated_dirs = [] \n\tif (os.path.isdir(name) or (name == '')): \n\t \treturn created_dirs \n\tif _path_created.get(os.path.abspath(name)): \n\t \treturn created_dirs \n\t(head, tail) = os.path.split(name) \n\ttails = [tail] \n\twhile (head and tail and (not os.path.isdir(head))): \n\t \t(head, tail) = os.path.split(head) \n\t \ttails.insert(0, tail) \n\tfor d in tails: \n\t \thead = os.path.join(head, d) \n\t \tabs_head = os.path.abspath(head) \n\t \tif _path_created.get(abs_head): \n\t \t \tcontinue \n\t \tif (verbose >= 1): \n\t \t \tlog.info('creating \t%s', head) \n\t \tif (not dry_run): \n\t \t \ttry: \n\t \t \t \tos.mkdir(head) \n\t \t \t \tcreated_dirs.append(head) \n\t \t \texcept OSError as exc: \n\t \t \t \traise DistutilsFileError, (\"could \tnot \tcreate \t'%s': \t%s\" % (head, exc[(-1)])) \n\t \t_path_created[abs_head] = 1 \n\treturn created_dirs\n", 
" \tif (not ((type(number) is types.LongType) or (type(number) is types.IntType))): \n\t \traise TypeError('You \tmust \tpass \ta \tlong \tor \tan \tint') \n\tstring = '' \n\twhile (number > 0): \n\t \tstring = ('%s%s' % (byte((number & 255)), string)) \n\t \tnumber /= 256 \n\treturn string\n", 
" \tif context.is_admin: \n\t \treturn True \n\t(rule, target, credentials) = _prepare_check(context, action, target, pluralized) \n\ttry: \n\t \tresult = _ENFORCER.enforce(rule, target, credentials, action=action, do_raise=True) \n\texcept policy.PolicyNotAuthorized: \n\t \twith excutils.save_and_reraise_exception(): \n\t \t \tlog_rule_list(rule) \n\t \t \tLOG.debug(\"Failed \tpolicy \tcheck \tfor \t'%s'\", action) \n\treturn result\n", 
" \tinit() \n\tcredentials = context.to_policy_values() \n\ttarget = credentials \n\treturn _ENFORCER.authorize('context_is_admin', target, credentials)\n", 
" \tdef decorator(fx): \n\t \tfx._event_id = id \n\t \tfx._event_name = event \n\t \treturn fx \n\treturn decorator\n", 
" \tif ('id' not in sort_keys): \n\t \tLOG.warning(_LW('Id \tnot \tin \tsort_keys; \tis \tsort_keys \tunique?')) \n\tassert (not (sort_dir and sort_dirs)) \n\tif ((sort_dirs is None) and (sort_dir is None)): \n\t \tsort_dir = 'asc' \n\tif (sort_dirs is None): \n\t \tsort_dirs = [sort_dir for _sort_key in sort_keys] \n\tassert (len(sort_dirs) == len(sort_keys)) \n\tfor (current_sort_key, current_sort_dir) in zip(sort_keys, sort_dirs): \n\t \tsort_dir_func = {'asc': sqlalchemy.asc, 'desc': sqlalchemy.desc}[current_sort_dir] \n\t \ttry: \n\t \t \tsort_key_attr = getattr(model, current_sort_key) \n\t \texcept AttributeError: \n\t \t \traise exception.InvalidInput(reason='Invalid \tsort \tkey') \n\t \tif (not api.is_orm_value(sort_key_attr)): \n\t \t \traise exception.InvalidInput(reason='Invalid \tsort \tkey') \n\t \tquery = query.order_by(sort_dir_func(sort_key_attr)) \n\tif (marker is not None): \n\t \tmarker_values = [] \n\t \tfor sort_key in sort_keys: \n\t \t \tv = getattr(marker, sort_key) \n\t \t \tmarker_values.append(v) \n\t \tcriteria_list = [] \n\t \tfor i in range(0, len(sort_keys)): \n\t \t \tcrit_attrs = [] \n\t \t \tfor j in range(0, i): \n\t \t \t \tmodel_attr = getattr(model, sort_keys[j]) \n\t \t \t \tcrit_attrs.append((model_attr == marker_values[j])) \n\t \t \tmodel_attr = getattr(model, sort_keys[i]) \n\t \t \tif (sort_dirs[i] == 'desc'): \n\t \t \t \tcrit_attrs.append((model_attr < marker_values[i])) \n\t \t \telif (sort_dirs[i] == 'asc'): \n\t \t \t \tcrit_attrs.append((model_attr > marker_values[i])) \n\t \t \telse: \n\t \t \t \traise ValueError(_(\"Unknown \tsort \tdirection, \tmust \tbe \t'desc' \tor \t'asc'\")) \n\t \t \tcriteria = sqlalchemy.sql.and_(*crit_attrs) \n\t \t \tcriteria_list.append(criteria) \n\t \tf = sqlalchemy.sql.or_(*criteria_list) \n\t \tquery = query.filter(f) \n\tif (limit is not None): \n\t \tquery = query.limit(limit) \n\tif offset: \n\t \tquery = query.offset(offset) \n\treturn query\n", 
" \taddresses = [] \n\tfor interface in netifaces.interfaces(): \n\t \ttry: \n\t \t \tiface_data = netifaces.ifaddresses(interface) \n\t \t \tfor family in iface_data: \n\t \t \t \tif (family not in (netifaces.AF_INET, netifaces.AF_INET6)): \n\t \t \t \t \tcontinue \n\t \t \t \tfor address in iface_data[family]: \n\t \t \t \t \taddr = address['addr'] \n\t \t \t \t \tif (family == netifaces.AF_INET6): \n\t \t \t \t \t \taddr = addr.split('%')[0] \n\t \t \t \t \taddresses.append(addr) \n\t \texcept ValueError: \n\t \t \tpass \n\treturn addresses\n", 
" \tdef decorated(func): \n\t \t'\\n \t \t \t \t \t \t \t \tDecorator \tfor \tthe \tcreation \tfunction.\\n \t \t \t \t \t \t \t \t' \n\t \t_WRITE_MODEL[model] = func \n\t \treturn func \n\treturn decorated\n", 
" \treturn IMPL.volume_type_get(context, id, inactive, expected_fields)\n", 
" \tresponse = _client_wrapper('remove_volume', name) \n\t_clear_context() \n\treturn response\n", 
" \tint_id = ec2_id_to_id(ec2_id) \n\treturn get_instance_uuid_from_int_id(context, int_id)\n", 
" \tint_id = ec2_id_to_id(ec2_id) \n\treturn get_instance_uuid_from_int_id(context, int_id)\n", 
" \tsession = Session.object_session(series) \n\treleases = session.query(Episode).join(Episode.releases, Episode.series).filter((Series.id == series.id)) \n\tif downloaded: \n\t \treleases = releases.filter((Release.downloaded == True)) \n\tif (season is not None): \n\t \treleases = releases.filter((Episode.season == season)) \n\tif (series.identified_by and (series.identified_by != u'auto')): \n\t \treleases = releases.filter((Episode.identified_by == series.identified_by)) \n\tif (series.identified_by in [u'ep', u'sequence']): \n\t \tlatest_release = releases.order_by(desc(Episode.season), desc(Episode.number)).first() \n\telif (series.identified_by == u'date'): \n\t \tlatest_release = releases.order_by(desc(Episode.identifier)).first() \n\telse: \n\t \tlatest_release = releases.order_by(desc(Episode.first_seen.label(u'ep_first_seen'))).first() \n\tif (not latest_release): \n\t \tlog.debug(u'get_latest_release \treturning \tNone, \tno \tdownloaded \tepisodes \tfound \tfor: \t%s', series.name) \n\t \treturn \n\treturn latest_release\n", 
" \tqueue_dir = __opts__['sqlite_queue_dir'] \n\tdb = os.path.join(queue_dir, '{0}.db'.format(queue)) \n\tlog.debug('Connecting \tto: \t \t{0}'.format(db)) \n\tcon = lite.connect(db) \n\ttables = _list_tables(con) \n\tif (queue not in tables): \n\t \t_create_table(con, queue) \n\treturn con\n", 
" \tdefaults = {'host': 'salt', 'user': 'salt', 'pass': 'salt', 'db': 'salt', 'port': 3306, 'ssl_ca': None, 'ssl_cert': None, 'ssl_key': None} \n\tattrs = {'host': 'host', 'user': 'user', 'pass': 'pass', 'db': 'db', 'port': 'port', 'ssl_ca': 'ssl_ca', 'ssl_cert': 'ssl_cert', 'ssl_key': 'ssl_key'} \n\t_options = salt.returners.get_returner_options(__virtualname__, ret, attrs, __salt__=__salt__, __opts__=__opts__, defaults=defaults) \n\tfor (k, v) in _options.iteritems(): \n\t \tif (isinstance(v, string_types) and (v.lower() == 'none')): \n\t \t \t_options[k] = None \n\t \tif (k == 'port'): \n\t \t \t_options[k] = int(v) \n\treturn _options\n", 
" \treturn (file_path in _db_content.get('files'))\n", 
" \tif hasattr(obj, 'bind'): \n\t \tconn = obj.bind \n\telse: \n\t \ttry: \n\t \t \tconn = object_session(obj).bind \n\t \texcept UnmappedInstanceError: \n\t \t \tconn = obj \n\tif (not hasattr(conn, 'execute')): \n\t \traise TypeError('This \tmethod \taccepts \tonly \tSession, \tEngine, \tConnection \tand \tdeclarative \tmodel \tobjects.') \n\treturn conn\n", 
" \tif hasattr(obj, 'bind'): \n\t \tconn = obj.bind \n\telse: \n\t \ttry: \n\t \t \tconn = object_session(obj).bind \n\t \texcept UnmappedInstanceError: \n\t \t \tconn = obj \n\tif (not hasattr(conn, 'execute')): \n\t \traise TypeError('This \tmethod \taccepts \tonly \tSession, \tEngine, \tConnection \tand \tdeclarative \tmodel \tobjects.') \n\treturn conn\n", 
" \tglobal _REPOSITORY \n\trel_path = 'migrate_repo' \n\tif (database == 'api'): \n\t \trel_path = os.path.join('api_migrations', 'migrate_repo') \n\tpath = os.path.join(os.path.abspath(os.path.dirname(__file__)), rel_path) \n\tassert os.path.exists(path) \n\tif (_REPOSITORY.get(database) is None): \n\t \t_REPOSITORY[database] = Repository(path) \n\treturn _REPOSITORY[database]\n", 
" \tif (not context): \n\t \tLOG.warning(_LW('Use \tof \tempty \trequest \tcontext \tis \tdeprecated'), DeprecationWarning) \n\t \traise Exception('die') \n\treturn context.is_admin\n", 
" \tif (not context): \n\t \treturn False \n\tif context.is_admin: \n\t \treturn False \n\tif ((not context.user_id) or (not context.project_id)): \n\t \treturn False \n\treturn True\n", 
" \tif is_user_context(context): \n\t \tif (not context.project_id): \n\t \t \traise exception.Forbidden() \n\t \telif (context.project_id != project_id): \n\t \t \traise exception.Forbidden()\n", 
" \tif is_user_context(context): \n\t \tif (not context.user_id): \n\t \t \traise exception.Forbidden() \n\t \telif (context.user_id != user_id): \n\t \t \traise exception.Forbidden()\n", 
" \tif is_user_context(context): \n\t \tif (not context.quota_class): \n\t \t \traise exception.Forbidden() \n\t \telif (context.quota_class != class_name): \n\t \t \traise exception.Forbidden()\n", 
" \tdef wrapper(*args, **kwargs): \n\t \tif (not is_admin_context(args[0])): \n\t \t \traise exception.AdminRequired() \n\t \treturn f(*args, **kwargs) \n\treturn wrapper\n", 
" \t@functools.wraps(f) \n\tdef wrapper(*args, **kwargs): \n\t \tnova.context.require_context(args[0]) \n\t \treturn f(*args, **kwargs) \n\treturn wrapper\n", 
" \t@functools.wraps(f) \n\tdef wrapper(context, volume_id, *args, **kwargs): \n\t \tif (not resource_exists(context, models.Volume, volume_id)): \n\t \t \traise exception.VolumeNotFound(volume_id=volume_id) \n\t \treturn f(context, volume_id, *args, **kwargs) \n\treturn wrapper\n", 
" \t@functools.wraps(f) \n\tdef wrapper(context, snapshot_id, *args, **kwargs): \n\t \tif (not resource_exists(context, models.Snapshot, snapshot_id)): \n\t \t \traise exception.SnapshotNotFound(snapshot_id=snapshot_id) \n\t \treturn f(context, snapshot_id, *args, **kwargs) \n\treturn wrapper\n", 
" \tif (read_deleted is None): \n\t \tread_deleted = context.read_deleted \n\tquery_kwargs = {} \n\tif ('no' == read_deleted): \n\t \tquery_kwargs['deleted'] = False \n\telif ('only' == read_deleted): \n\t \tquery_kwargs['deleted'] = True \n\telif ('yes' == read_deleted): \n\t \tpass \n\telse: \n\t \traise ValueError((_(\"Unrecognized \tread_deleted \tvalue \t'%s'\") % read_deleted)) \n\tquery = sqlalchemyutils.model_query(model, context.session, args, **query_kwargs) \n\tif (nova.context.is_user_context(context) and project_only): \n\t \tif (project_only == 'allow_none'): \n\t \t \tquery = query.filter(or_((model.project_id == context.project_id), (model.project_id == null()))) \n\t \telse: \n\t \t \tquery = query.filter_by(project_id=context.project_id) \n\treturn query\n", 
" \tfilter_dict = {} \n\tif (filters is None): \n\t \tfilters = {} \n\tfor (key, value) in six.iteritems(filters): \n\t \tif isinstance(value, (list, tuple, set, frozenset)): \n\t \t \tcolumn_attr = getattr(model, key) \n\t \t \tquery = query.filter(column_attr.in_(value)) \n\t \telse: \n\t \t \tfilter_dict[key] = value \n\tif filter_dict: \n\t \tquery = query.filter_by(**filter_dict) \n\treturn query\n", 
" \tinst_type_dict = dict(inst_type_query) \n\textra_specs = {x['key']: x['value'] for x in inst_type_query['extra_specs']} \n\tinst_type_dict['extra_specs'] = extra_specs \n\treturn inst_type_dict\n", 
" \treturn model_query(context, models.Reservation, read_deleted='no').filter(models.Reservation.uuid.in_(reservations)).with_lockmode('update')\n", 
" \tspecs = values.get('extra_specs') \n\tspecs_refs = [] \n\tif specs: \n\t \tfor (k, v) in specs.items(): \n\t \t \tspecs_ref = models.InstanceTypeExtraSpecs() \n\t \t \tspecs_ref['key'] = k \n\t \t \tspecs_ref['value'] = v \n\t \t \tspecs_refs.append(specs_ref) \n\tvalues['extra_specs'] = specs_refs \n\tinstance_type_ref = models.InstanceTypes() \n\tinstance_type_ref.update(values) \n\tif (projects is None): \n\t \tprojects = [] \n\ttry: \n\t \tinstance_type_ref.save(context.session) \n\texcept db_exc.DBDuplicateEntry as e: \n\t \tif ('flavorid' in e.columns): \n\t \t \traise exception.FlavorIdExists(flavor_id=values['flavorid']) \n\t \traise exception.FlavorExists(name=values['name']) \n\texcept Exception as e: \n\t \traise db_exc.DBError(e) \n\tfor project in set(projects): \n\t \taccess_ref = models.InstanceTypeProjects() \n\t \taccess_ref.update({'instance_type_id': instance_type_ref.id, 'project_id': project}) \n\t \taccess_ref.save(context.session) \n\treturn _dict_with_extra_specs(instance_type_ref)\n", 
" \tsession = get_session() \n\twith session.begin(): \n\t \tfilters = (filters or {}) \n\t \tfilters['context'] = context \n\t \tquery = _generate_paginate_query(context, session, marker, limit, sort_keys, sort_dirs, filters, offset, models.VolumeTypes) \n\t \tif (query is None): \n\t \t \tif list_result: \n\t \t \t \treturn [] \n\t \t \treturn {} \n\t \trows = query.all() \n\t \tif list_result: \n\t \t \tresult = [_dict_with_extra_specs_if_authorized(context, row) for row in rows] \n\t \t \treturn result \n\t \tresult = {row['name']: _dict_with_extra_specs_if_authorized(context, row) for row in rows} \n\t \treturn result\n", 
" \treq_volume_types = [] \n\tfor vol_t in volume_type_list: \n\t \tif (not uuidutils.is_uuid_like(vol_t)): \n\t \t \tvol_type = _volume_type_get_by_name(context, vol_t) \n\t \telse: \n\t \t \tvol_type = _volume_type_get(context, vol_t) \n\t \treq_volume_types.append(vol_type) \n\treturn req_volume_types\n", 
" \treq_volume_types = [] \n\tfor vol_t in volume_type_list: \n\t \tif (not uuidutils.is_uuid_like(vol_t)): \n\t \t \tvol_type = _volume_type_get_by_name(context, vol_t) \n\t \telse: \n\t \t \tvol_type = _volume_type_get(context, vol_t) \n\t \treq_volume_types.append(vol_type) \n\treturn req_volume_types\n", 
" \tquery = model_query(context, models.Volume, read_deleted='yes') \n\tquery = query.filter(or_((models.Volume.deleted_at == None), (models.Volume.deleted_at > begin))) \n\tif end: \n\t \tquery = query.filter((models.Volume.created_at < end)) \n\tif project_id: \n\t \tquery = query.filter_by(project_id=project_id) \n\tquery = query.options(joinedload('volume_metadata')).options(joinedload('volume_type')).options(joinedload('volume_attachment')).options(joinedload('consistencygroup')).options(joinedload('group')) \n\tif is_admin_context(context): \n\t \tquery = query.options(joinedload('volume_admin_metadata')) \n\treturn query.all()\n", 
" \treturn _volume_glance_metadata_get(context, volume_id)\n", 
" \treturn IMPL.volume_snapshot_glance_metadata_get(context, snapshot_id)\n", 
" \tsession = get_session() \n\twith session.begin(): \n\t \tfor (key, value) in metadata.items(): \n\t \t \trows = session.query(models.VolumeGlanceMetadata).filter_by(volume_id=volume_id).filter_by(key=key).filter_by(deleted=False).all() \n\t \t \tif (len(rows) > 0): \n\t \t \t \traise exception.GlanceMetadataExists(key=key, volume_id=volume_id) \n\t \t \tvol_glance_metadata = models.VolumeGlanceMetadata() \n\t \t \tvol_glance_metadata.volume_id = volume_id \n\t \t \tvol_glance_metadata.key = key \n\t \t \tvol_glance_metadata.value = six.text_type(value) \n\t \t \tsession.add(vol_glance_metadata)\n", 
" \treturn IMPL.volume_glance_metadata_copy_to_snapshot(context, snapshot_id, volume_id)\n", 
" \tsession = get_session() \n\twith session.begin(): \n\t \tfor (key, value) in metadata.items(): \n\t \t \trows = session.query(models.VolumeGlanceMetadata).filter_by(volume_id=volume_id).filter_by(key=key).filter_by(deleted=False).all() \n\t \t \tif (len(rows) > 0): \n\t \t \t \traise exception.GlanceMetadataExists(key=key, volume_id=volume_id) \n\t \t \tvol_glance_metadata = models.VolumeGlanceMetadata() \n\t \t \tvol_glance_metadata.volume_id = volume_id \n\t \t \tvol_glance_metadata.key = key \n\t \t \tvol_glance_metadata.value = six.text_type(value) \n\t \t \tsession.add(vol_glance_metadata)\n", 
" \treturn IMPL.volume_glance_metadata_copy_to_volume(context, volume_id, snapshot_id)\n", 
" \treturn IMPL.db_sync(version=version, database=database, context=context)\n", 
" \treturn IMPL.db_version(database=database, context=context)\n", 
" \treturn IMPL.service_destroy(context, service_id)\n", 
" \treturn IMPL.service_get(context, service_id)\n", 
" \treturn IMPL.service_get_by_host_and_topic(context, host, topic)\n", 
" \treturn IMPL.service_get_all(context, disabled)\n", 
" \treturn IMPL.service_get_all_by_topic(context, topic)\n", 
" \treturn IMPL.service_get_all_by_host(context, host)\n", 
" \treturn IMPL.volume_get_all(context, marker, limit, sort_keys=sort_keys, sort_dirs=sort_dirs, filters=filters, offset=offset)\n", 
" \tservice_instance = salt.utils.vmware.get_service_instance(host=host, username=username, password=password, protocol=protocol, port=port) \n\tvalid_services = ['DCUI', 'TSM', 'SSH', 'ssh', 'lbtd', 'lsassd', 'lwiod', 'netlogond', 'ntpd', 'sfcbd-watchdog', 'snmpd', 'vprobed', 'vpxa', 'xorg'] \n\thost_names = _check_hosts(service_instance, host, host_names) \n\tret = {} \n\tfor host_name in host_names: \n\t \tif (service_name not in valid_services): \n\t \t \tret.update({host_name: {'Error': '{0} \tis \tnot \ta \tvalid \tservice \tname.'.format(service_name)}}) \n\t \t \treturn ret \n\t \thost_ref = _get_host_ref(service_instance, host, host_name=host_name) \n\t \tservices = host_ref.configManager.serviceSystem.serviceInfo.service \n\t \tif ((service_name == 'SSH') or (service_name == 'ssh')): \n\t \t \ttemp_service_name = 'TSM-SSH' \n\t \telse: \n\t \t \ttemp_service_name = service_name \n\t \tfor service in services: \n\t \t \tif (service.key == temp_service_name): \n\t \t \t \tret.update({host_name: {service_name: service.running}}) \n\t \t \t \tbreak \n\t \t \telse: \n\t \t \t \tmsg = \"Could \tnot \tfind \tservice \t'{0}' \tfor \thost \t'{1}'.\".format(service_name, host_name) \n\t \t \t \tret.update({host_name: {'Error': msg}}) \n\t \tif (ret.get(host_name) is None): \n\t \t \tmsg = \"'vsphere.get_service_running' \tfailed \tfor \thost \t{0}.\".format(host_name) \n\t \t \tlog.debug(msg) \n\t \t \tret.update({host_name: {'Error': msg}}) \n\treturn ret\n", 
" \treturn IMPL.service_create(context, values)\n", 
" \treturn IMPL.service_update(context, service_id, values)\n", 
" \treturn IMPL.migration_update(context, id, values)\n", 
" \treturn IMPL.migration_create(context, values)\n", 
" \treturn IMPL.migration_get(context, migration_id)\n", 
" \treturn IMPL.migration_get_by_instance_and_status(context, instance_uuid, status)\n", 
" \treturn IMPL.migration_get_unconfirmed_by_dest_compute(context, confirm_window, dest_compute)\n", 
" \tif salt.utils.is_freebsd(): \n\t \treturn _freebsd_geom() \n\telif salt.utils.is_linux(): \n\t \treturn _linux_disks() \n\telse: \n\t \tlog.trace('Disk \tgrain \tdoes \tnot \tsupport \tOS')\n", 
" \treturn IMPL.instance_create(context, values)\n", 
" \treturn IMPL.floating_ip_allocate_address(context, project_id, pool, auto_assigned)\n", 
" \treturn IMPL.volume_attached(context, volume_id, instance_id, host_name, mountpoint, attach_mode)\n", 
" \treturn IMPL.volume_create(context, values)\n", 
" \treturn IMPL.volume_data_get_for_project(context, project_id)\n", 
" \treturn IMPL.volume_data_get_for_project(context, project_id)\n", 
" \treturn IMPL.volume_destroy(context, volume_id)\n", 
" \treturn IMPL.volume_detached(context, volume_id, attachment_id)\n", 
" \treturn IMPL.volume_get(context, volume_id)\n", 
" \treturn IMPL.volume_get_all(context, marker, limit, sort_keys=sort_keys, sort_dirs=sort_dirs, filters=filters, offset=offset)\n", 
" \treturn IMPL.volume_get_all_by_host(context, host, filters=filters)\n", 
" \treturn IMPL.volume_get_all_by_host(context, host, filters=filters)\n", 
" \treturn IMPL.volume_get_all_by_project(context, project_id, marker, limit, sort_keys=sort_keys, sort_dirs=sort_dirs, filters=filters, offset=offset)\n", 
" \tif volume.volume_type: \n\t \tmetadata = {} \n\t \ttype_id = volume.volume_type_id \n\t \tif (type_id is not None): \n\t \t \tmetadata = volume_types.get_volume_type_extra_specs(type_id) \n\t \tif metadata.get('service_label'): \n\t \t \tif (metadata['service_label'] in config['services'].keys()): \n\t \t \t \treturn metadata['service_label'] \n\treturn 'default'\n", 
" \treturn IMPL.volume_update(context, volume_id, values)\n", 
" \treturn IMPL.snapshot_create(context, values)\n", 
" \treturn IMPL.snapshot_destroy(context, snapshot_id)\n", 
" \treturn IMPL.snapshot_get(context, snapshot_id)\n", 
" \treturn IMPL.snapshot_get_all(context, filters, marker, limit, sort_keys, sort_dirs, offset)\n", 
" \treturn IMPL.snapshot_get_all_by_project(context, project_id, filters, marker, limit, sort_keys, sort_dirs, offset)\n", 
" \treturn IMPL.snapshot_get_all_for_volume(context, volume_id)\n", 
" \treturn IMPL.snapshot_update(context, snapshot_id, values)\n", 
" \treturn IMPL.snapshot_data_get_for_project(context, project_id, volume_type_id)\n", 
" \treturn IMPL.snapshot_metadata_get(context, snapshot_id)\n", 
" \tIMPL.instance_metadata_delete(context, instance_uuid, key)\n", 
" \tIMPL.instance_system_metadata_update(context, instance_uuid, metadata, delete)\n", 
" \treturn IMPL.volume_metadata_get(context, volume_id)\n", 
" \tIMPL.instance_metadata_delete(context, instance_uuid, key)\n", 
" \tIMPL.instance_system_metadata_update(context, instance_uuid, metadata, delete)\n", 
" \treturn IMPL.volume_type_create(context, values, projects)\n", 
" \treturn IMPL.volume_type_get_all(context, inactive, filters, marker=marker, limit=limit, sort_keys=sort_keys, sort_dirs=sort_dirs, offset=offset, list_result=list_result)\n", 
" \treturn IMPL.volume_type_get(context, id, inactive, expected_fields)\n", 
" \treturn IMPL.volume_type_get_by_name(context, name)\n", 
" \treturn IMPL.volume_type_destroy(context, id)\n", 
" \treturn IMPL.volume_get_all_active_by_window(context, begin, end, project_id)\n", 
" \treturn IMPL.volume_type_extra_specs_get(context, volume_type_id)\n", 
" \tIMPL.flavor_extra_specs_delete(context, flavor_id, key)\n", 
" \treturn IMPL.volume_type_extra_specs_update_or_create(context, volume_type_id, extra_specs)\n", 
" \treturn IMPL.volume_glance_metadata_create(context, volume_id, key, value)\n", 
" \treturn IMPL.volume_glance_metadata_get(context, volume_id)\n", 
" \treturn IMPL.volume_snapshot_glance_metadata_get(context, snapshot_id)\n", 
" \treturn IMPL.volume_glance_metadata_copy_to_snapshot(context, snapshot_id, volume_id)\n", 
" \treturn IMPL.volume_glance_metadata_copy_to_volume(context, volume_id, snapshot_id)\n", 
" \treturn IMPL.volume_glance_metadata_delete_by_volume(context, volume_id)\n", 
" \treturn IMPL.volume_glance_metadata_delete_by_snapshot(context, snapshot_id)\n", 
" \tsession = get_session() \n\twith session.begin(): \n\t \tfor (key, value) in metadata.items(): \n\t \t \trows = session.query(models.VolumeGlanceMetadata).filter_by(volume_id=volume_id).filter_by(key=key).filter_by(deleted=False).all() \n\t \t \tif (len(rows) > 0): \n\t \t \t \traise exception.GlanceMetadataExists(key=key, volume_id=volume_id) \n\t \t \tvol_glance_metadata = models.VolumeGlanceMetadata() \n\t \t \tvol_glance_metadata.volume_id = volume_id \n\t \t \tvol_glance_metadata.key = key \n\t \t \tvol_glance_metadata.value = six.text_type(value) \n\t \t \tsession.add(vol_glance_metadata)\n", 
" \textra = getattr(config, 'BOT_EXTRA_BACKEND_DIR', []) \n\treturn SpecificPluginManager(config, 'backends', ErrBot, CORE_BACKENDS, extra_search_dirs=extra)\n", 
" \ttry: \n\t \tconn = _get_conn(region=region, key=key, keyid=keyid, profile=profile) \n\t \tconn.put_bucket_request_payment(Bucket=Bucket, RequestPaymentConfiguration={'Payer': Payer}) \n\t \treturn {'updated': True, 'name': Bucket} \n\texcept ClientError as e: \n\t \treturn {'updated': False, 'error': __utils__['boto3.get_error'](e)}\n", 
" \tret = {} \n\tret['message'] = {} \n\tret['message']['sid'] = None \n\tclient = _get_twilio(profile) \n\ttry: \n\t \tmessage = client.sms.messages.create(body=body, to=to, from_=from_) \n\texcept TwilioRestException as exc: \n\t \tret['_error'] = {} \n\t \tret['_error']['code'] = exc.code \n\t \tret['_error']['msg'] = exc.msg \n\t \tret['_error']['status'] = exc.status \n\t \tlog.debug('Could \tnot \tsend \tsms. \tError: \t{0}'.format(ret)) \n\t \treturn ret \n\tret['message'] = {} \n\tret['message']['sid'] = message.sid \n\tret['message']['price'] = message.price \n\tret['message']['price_unit'] = message.price_unit \n\tret['message']['status'] = message.status \n\tret['message']['num_segments'] = message.num_segments \n\tret['message']['body'] = message.body \n\tret['message']['date_sent'] = str(message.date_sent) \n\tret['message']['date_created'] = str(message.date_created) \n\tlog.info(ret) \n\treturn ret\n", 
" \tconfig_stanzas = CONF.list_all_sections() \n\tif (backend_name not in config_stanzas): \n\t \tmsg = _('Could \tnot \tfind \tbackend \tstanza \t%(backend_name)s \tin \tconfiguration. \tAvailable \tstanzas \tare \t%(stanzas)s') \n\t \tparams = {'stanzas': config_stanzas, 'backend_name': backend_name} \n\t \traise exception.ConfigNotFound(message=(msg % params)) \n\tconfig = configuration.Configuration(driver.volume_opts, config_group=backend_name) \n\tconfig.append_config_values(na_opts.netapp_proxy_opts) \n\tconfig.append_config_values(na_opts.netapp_connection_opts) \n\tconfig.append_config_values(na_opts.netapp_transport_opts) \n\tconfig.append_config_values(na_opts.netapp_basicauth_opts) \n\tconfig.append_config_values(na_opts.netapp_provisioning_opts) \n\tconfig.append_config_values(na_opts.netapp_cluster_opts) \n\tconfig.append_config_values(na_opts.netapp_san_opts) \n\tconfig.append_config_values(na_opts.netapp_replication_opts) \n\treturn config\n", 
" \tconfig_stanzas = CONF.list_all_sections() \n\tif (backend_name not in config_stanzas): \n\t \tmsg = _('Could \tnot \tfind \tbackend \tstanza \t%(backend_name)s \tin \tconfiguration. \tAvailable \tstanzas \tare \t%(stanzas)s') \n\t \tparams = {'stanzas': config_stanzas, 'backend_name': backend_name} \n\t \traise exception.ConfigNotFound(message=(msg % params)) \n\tconfig = configuration.Configuration(driver.volume_opts, config_group=backend_name) \n\tconfig.append_config_values(na_opts.netapp_proxy_opts) \n\tconfig.append_config_values(na_opts.netapp_connection_opts) \n\tconfig.append_config_values(na_opts.netapp_transport_opts) \n\tconfig.append_config_values(na_opts.netapp_basicauth_opts) \n\tconfig.append_config_values(na_opts.netapp_provisioning_opts) \n\tconfig.append_config_values(na_opts.netapp_cluster_opts) \n\tconfig.append_config_values(na_opts.netapp_san_opts) \n\tconfig.append_config_values(na_opts.netapp_replication_opts) \n\treturn config\n", 
" \textra = getattr(config, 'BOT_EXTRA_BACKEND_DIR', []) \n\treturn SpecificPluginManager(config, 'backends', ErrBot, CORE_BACKENDS, extra_search_dirs=extra)\n", 
" \tf = cs.flavors.create(args.name, args.ram, args.vcpus, args.disk, args.id, args.ephemeral, args.swap, args.rxtx_factor, args.is_public) \n\t_print_flavor_list([f])\n", 
" \tsender_name = (frappe.db.get_single_value(u'SMS \tSettings', u'sms_sender_name') or u'ERPNXT') \n\tif ((len(sender_name) > 6) and (frappe.db.get_default(u'country') == u'India')): \n\t \tthrow(u'As \tper \tTRAI \trule, \tsender \tname \tmust \tbe \texactly \t6 \tcharacters.\\n DCTB  DCTB  DCTB Kindly \tchange \tsender \tname \tin \tSetup \t--> \tGlobal \tDefaults.\\n DCTB  DCTB  DCTB Note: \tHyphen, \tspace, \tnumeric \tdigit, \tspecial \tcharacters \tare \tnot \tallowed.') \n\treturn sender_name\n", 
" \tflavorid = _find_flavor(cs, args.flavor) \n\tcs.flavors.delete(flavorid) \n\t_print_flavor_list([flavorid])\n", 
" \tflavorid = _find_flavor(cs, args.flavor) \n\tcs.flavors.delete(flavorid) \n\t_print_flavor_list([flavorid])\n", 
" \tfilters = (filters or {}) \n\tquery = Flavor._flavor_get_query_from_db(context) \n\tif ('min_memory_mb' in filters): \n\t \tquery = query.filter((api_models.Flavors.memory_mb >= filters['min_memory_mb'])) \n\tif ('min_root_gb' in filters): \n\t \tquery = query.filter((api_models.Flavors.root_gb >= filters['min_root_gb'])) \n\tif ('disabled' in filters): \n\t \tquery = query.filter((api_models.Flavors.disabled == filters['disabled'])) \n\tif (('is_public' in filters) and (filters['is_public'] is not None)): \n\t \tthe_filter = [(api_models.Flavors.is_public == filters['is_public'])] \n\t \tif (filters['is_public'] and (context.project_id is not None)): \n\t \t \tthe_filter.extend([api_models.Flavors.projects.any(project_id=context.project_id)]) \n\t \tif (len(the_filter) > 1): \n\t \t \tquery = query.filter(or_(*the_filter)) \n\t \telse: \n\t \t \tquery = query.filter(the_filter[0]) \n\tmarker_row = None \n\tif (marker is not None): \n\t \tmarker_row = Flavor._flavor_get_query_from_db(context).filter_by(flavorid=marker).first() \n\t \tif (not marker_row): \n\t \t \traise exception.MarkerNotFound(marker=marker) \n\tquery = sqlalchemyutils.paginate_query(query, api_models.Flavors, limit, [sort_key, 'id'], marker=marker_row, sort_dir=sort_dir) \n\treturn [_dict_with_extra_specs(i) for i in query.all()]\n", 
" \treturn IMPL.cell_create(context, values)\n", 
" \treturn IMPL.cell_update(context, cell_name, values)\n", 
" \tconn = _get_driver(profile=profile) \n\tzone = conn.get_zone(zone_id=zone_id) \n\treturn conn.delete_zone(zone)\n", 
" \treturn IMPL.cell_get(context, cell_name)\n", 
" \tzones = {} \n\theader = 'zoneid:zonename:state:zonepath:uuid:brand:ip-type'.split(':') \n\tzone_data = __salt__['cmd.run_all']('zoneadm \tlist \t-p \t-c') \n\tif (zone_data['retcode'] == 0): \n\t \tfor zone in zone_data['stdout'].splitlines(): \n\t \t \tzone = zone.split(':') \n\t \t \tzone_t = {} \n\t \t \tfor i in range(0, len(header)): \n\t \t \t \tzone_t[header[i]] = zone[i] \n\t \t \tif (hide_global and (zone_t['zonename'] == 'global')): \n\t \t \t \tcontinue \n\t \t \tif ((not installed) and (zone_t['state'] == 'installed')): \n\t \t \t \tcontinue \n\t \t \tif ((not configured) and (zone_t['state'] == 'configured')): \n\t \t \t \tcontinue \n\t \t \tzones[zone_t['zonename']] = zone_t \n\t \t \tdel zones[zone_t['zonename']]['zonename'] \n\treturn (zones if verbose else sorted(zones.keys()))\n", 
" \treturn IMPL.quota_create(context, project_id, resource, limit, user_id=user_id)\n", 
" \treturn IMPL.quota_get(context, project_id, resource, user_id=user_id)\n", 
" \treturn IMPL.quota_get_all_by_project(context, project_id)\n", 
" \treturn IMPL.quota_update(context, project_id, resource, limit, user_id=user_id)\n", 
" \treturn IMPL.quota_destroy(context, project_id, resource)\n", 
" \treturn IMPL.quota_class_create(context, class_name, resource, limit)\n", 
" \treturn IMPL.quota_class_get(context, class_name, resource)\n", 
" \treturn IMPL.quota_class_get_all_by_name(context, class_name)\n", 
" \treturn IMPL.quota_class_update(context, class_name, resource, limit)\n", 
" \treturn IMPL.quota_class_destroy(context, class_name, resource)\n", 
" \treturn IMPL.quota_class_destroy_all_by_name(context, class_name)\n", 
" \treturn IMPL.quota_create(context, project_id, resource, limit, user_id=user_id)\n", 
" \treturn IMPL.quota_usage_get(context, project_id, resource, user_id=user_id)\n", 
" \treturn IMPL.quota_usage_get_all_by_project(context, project_id)\n", 
" \treturn IMPL.quota_create(context, project_id, resource, limit, user_id=user_id)\n", 
" \treturn IMPL.quota_get(context, project_id, resource, user_id=user_id)\n", 
" \treturn IMPL.quota_get_all_by_project(context, project_id)\n", 
" \treturn IMPL.instance_destroy(context, instance_uuid, constraint)\n", 
" \treturn IMPL.quota_reserve(context, resources, quotas, user_quotas, deltas, expire, until_refresh, max_age, project_id=project_id, user_id=user_id)\n", 
" \treturn IMPL.reservation_commit(context, reservations, project_id=project_id, user_id=user_id)\n", 
" \treturn IMPL.reservation_rollback(context, reservations, project_id=project_id, user_id=user_id)\n", 
" \treturn IMPL.quota_destroy_all_by_project(context, project_id)\n", 
" \treturn IMPL.reservation_expire(context)\n", 
" \treturn IMPL.backup_get(context, backup_id, read_deleted, project_only)\n", 
" \treturn IMPL.backup_get_all(context, filters=filters, marker=marker, limit=limit, offset=offset, sort_keys=sort_keys, sort_dirs=sort_dirs)\n", 
" \treturn IMPL.backup_get_all_by_host(context, host)\n", 
" \treturn IMPL.backup_create(context, values)\n", 
" \treturn IMPL.backup_get_all_by_project(context, project_id, filters=filters, marker=marker, limit=limit, offset=offset, sort_keys=sort_keys, sort_dirs=sort_dirs)\n", 
" \treturn IMPL.backup_update(context, backup_id, values)\n", 
" \treturn IMPL.backup_destroy(context, backup_id)\n", 
" \tgroup.update({'host': host, 'updated_at': timeutils.utcnow(), 'cluster_name': cluster_name}) \n\tgroup.save() \n\treturn group\n", 
" \tdef decorator(func): \n\t \treturn unittest.skipIf((platform.system() not in ['Darwin', 'Linux']), reason)(func) \n\treturn decorator\n", 
" \tclass HTTPConnectionDecorator(object, ): \n\t \t'Decorator \tto \tmock \tthe \tHTTPConecction \tclass.\\n\\n \t \t \t \t \t \t \t \tWraps \tthe \treal \tHTTPConnection \tclass \tso \tthat \twhen \tyou \tinstantiate\\n \t \t \t \t \t \t \t \tthe \tclass \tyou \tmight \tinstead \tget \ta \tfake \tinstance.\\n \t \t \t \t \t \t \t \t' \n\t \tdef __init__(self, wrapped): \n\t \t \tself.wrapped = wrapped \n\t \tdef __call__(self, connection_host, *args, **kwargs): \n\t \t \tif (connection_host == host): \n\t \t \t \treturn FakeHttplibConnection(app, host) \n\t \t \telse: \n\t \t \t \treturn self.wrapped(connection_host, *args, **kwargs) \n\toldHTTPConnection = http_client.HTTPConnection \n\tnew_http_connection = HTTPConnectionDecorator(http_client.HTTPConnection) \n\thttp_client.HTTPConnection = new_http_connection \n\treturn oldHTTPConnection\n", 
" \tclass HTTPConnectionDecorator(object, ): \n\t \t'Decorator \tto \tmock \tthe \tHTTPConecction \tclass.\\n\\n \t \t \t \t \t \t \t \tWraps \tthe \treal \tHTTPConnection \tclass \tso \tthat \twhen \tyou \tinstantiate\\n \t \t \t \t \t \t \t \tthe \tclass \tyou \tmight \tinstead \tget \ta \tfake \tinstance.\\n \t \t \t \t \t \t \t \t' \n\t \tdef __init__(self, wrapped): \n\t \t \tself.wrapped = wrapped \n\t \tdef __call__(self, connection_host, *args, **kwargs): \n\t \t \tif (connection_host == host): \n\t \t \t \treturn FakeHttplibConnection(app, host) \n\t \t \telse: \n\t \t \t \treturn self.wrapped(connection_host, *args, **kwargs) \n\toldHTTPConnection = http_client.HTTPConnection \n\tnew_http_connection = HTTPConnectionDecorator(http_client.HTTPConnection) \n\thttp_client.HTTPConnection = new_http_connection \n\treturn oldHTTPConnection\n", 
" \treturn compare_tree_to_dict(actual, expected, ('rel', 'href', 'type'))\n", 
" \treturn compare_tree_to_dict(actual, expected, ('base', 'type'))\n", 
" \tfor (elem, data) in zip(actual, expected): \n\t \tfor key in keys: \n\t \t \tif (elem.get(key) != data.get(key)): \n\t \t \t \treturn False \n\treturn True\n", 
" \tdef wrapped_func(*args, **kwarg): \n\t \tCALLED_FUNCTION.append(name) \n\t \treturn function(*args, **kwarg) \n\treturn wrapped_func\n", 
" \tif hasattr(td, 'total_seconds'): \n\t \treturn td.total_seconds() \n\tms = td.microseconds \n\tsecs = (td.seconds + ((td.days * 24) * 3600)) \n\treturn ((ms + (secs * (10 ** 6))) / (10 ** 6))\n", 
" \tglobal _fake_execute_repliers \n\t_fake_execute_repliers = repliers\n", 
" \treturn ('', '')\n", 
" \tglobal _fake_execute_repliers \n\tprocess_input = kwargs.get('process_input', None) \n\tcheck_exit_code = kwargs.get('check_exit_code', 0) \n\tdelay_on_retry = kwargs.get('delay_on_retry', True) \n\tattempts = kwargs.get('attempts', 1) \n\trun_as_root = kwargs.get('run_as_root', False) \n\tcmd_str = ' \t'.join((str(part) for part in cmd_parts)) \n\tLOG.debug('Faking \texecution \tof \tcmd \t(subprocess): \t%s', cmd_str) \n\t_fake_execute_log.append(cmd_str) \n\treply_handler = fake_execute_default_reply_handler \n\tfor fake_replier in _fake_execute_repliers: \n\t \tif re.match(fake_replier[0], cmd_str): \n\t \t \treply_handler = fake_replier[1] \n\t \t \tLOG.debug('Faked \tcommand \tmatched \t%s', fake_replier[0]) \n\t \t \tbreak \n\tif isinstance(reply_handler, six.string_types): \n\t \treply = (reply_handler, '') \n\telse: \n\t \ttry: \n\t \t \treply = reply_handler(cmd_parts, process_input=process_input, delay_on_retry=delay_on_retry, attempts=attempts, run_as_root=run_as_root, check_exit_code=check_exit_code) \n\t \texcept processutils.ProcessExecutionError as e: \n\t \t \tLOG.debug('Faked \tcommand \traised \tan \texception \t%s', e) \n\t \t \traise \n\tLOG.debug(\"Reply \tto \tfaked \tcommand \tis \tstdout='%(stdout)s' \tstderr='%(stderr)s'\", {'stdout': reply[0], 'stderr': reply[1]}) \n\tgreenthread.sleep(0) \n\treturn reply\n", 
" \tfor (module, func) in funcs.items(): \n\t \ttest.stub_out(module, func)\n", 
" \tif (parsed_url.scheme == 'https'): \n\t \tconn = httplib.HTTPSConnection(parsed_url.netloc) \n\telse: \n\t \tconn = httplib.HTTPConnection(parsed_url.netloc) \n\treturn conn\n", 
" \treturn ''.join((random.choice((string.ascii_uppercase + string.digits)) for _x in range(length)))\n", 
" \treturn ''.join((random.choice(string.digits) for _x in range(length)))\n", 
" \twhile True: \n\t \tif numeric: \n\t \t \tcandidate = (prefix + generate_random_numeric(8)) \n\t \telse: \n\t \t \tcandidate = (prefix + generate_random_alphanumeric(8)) \n\t \tif (candidate not in items): \n\t \t \treturn candidate \n\t \tLOG.debug(('Random \tcollision \ton \t%s' % candidate))\n", 
" \tif (key in data): \n\t \tdata[key].add(value) \n\telse: \n\t \tdata[key] = set([value])\n", 
" \treturn Table(name, MetaData(bind=session.bind), autoload=True)\n", 
" \textra_specs = (extra_specs or {}) \n\tprojects = (projects or []) \n\televated = (context if context.is_admin else context.elevated()) \n\ttry: \n\t \ttype_ref = db.volume_type_create(elevated, dict(name=name, extra_specs=extra_specs, is_public=is_public, description=description), projects=projects) \n\texcept db_exc.DBError: \n\t \tLOG.exception(_LE('DB \terror:')) \n\t \traise exception.VolumeTypeCreateFailed(name=name, extra_specs=extra_specs) \n\treturn type_ref\n", 
" \tif (id is None): \n\t \tmsg = _('id \tcannot \tbe \tNone') \n\t \traise exception.InvalidVolumeType(reason=msg) \n\televated = (context if context.is_admin else context.elevated()) \n\treturn db.volume_type_destroy(elevated, id)\n", 
" \tvol_types = db.volume_type_get_all(context, inactive, filters=filters, marker=marker, limit=limit, sort_keys=sort_keys, sort_dirs=sort_dirs, offset=offset, list_result=list_result) \n\treturn vol_types\n", 
" \tif (id is None): \n\t \tmsg = _('id \tcannot \tbe \tNone') \n\t \traise exception.InvalidVolumeType(reason=msg) \n\tif (ctxt is None): \n\t \tctxt = context.get_admin_context() \n\treturn db.volume_type_get(ctxt, id, expected_fields=expected_fields)\n", 
" \tif (name is None): \n\t \tmsg = _('name \tcannot \tbe \tNone') \n\t \traise exception.InvalidVolumeType(reason=msg) \n\treturn db.volume_type_get_by_name(context, name)\n", 
" \tname = CONF.default_volume_type \n\tvol_type = {} \n\tif (name is not None): \n\t \tctxt = context.get_admin_context() \n\t \ttry: \n\t \t \tvol_type = get_volume_type_by_name(ctxt, name) \n\t \texcept exception.VolumeTypeNotFoundByName: \n\t \t \tLOG.exception(_LE('Default \tvolume \ttype \tis \tnot \tfound. \tPlease \tcheck \tdefault_volume_type \tconfig:')) \n\treturn vol_type\n", 
" \t(audit_start, audit_end) = notifications.audit_period_bounds(current_period) \n\tbw = notifications.bandwidth_usage(instance_ref, audit_start, ignore_missing_network_data) \n\tif (system_metadata is None): \n\t \tsystem_metadata = utils.instance_sys_meta(instance_ref) \n\timage_meta = notifications.image_meta(system_metadata) \n\textra_info = dict(audit_period_beginning=str(audit_start), audit_period_ending=str(audit_end), bandwidth=bw, image_meta=image_meta) \n\tif extra_usage_info: \n\t \textra_info.update(extra_usage_info) \n\tnotify_about_instance_usage(notifier, context, instance_ref, 'exists', system_metadata=system_metadata, extra_usage_info=extra_info)\n", 
" \t@functools.wraps(func) \n\tdef wrapped(self, context, target_obj, *args, **kwargs): \n\t \tcheck_policy(context, func.__name__, target_obj) \n\t \treturn func(self, context, target_obj, *args, **kwargs) \n\treturn wrapped\n", 
" \tfrom boto.support.layer1 import SupportConnection \n\treturn SupportConnection(aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key, **kwargs)\n", 
" \tbasename = os.path.basename(filename) \n\tfor pattern in options.exclude: \n\t \tif fnmatch(basename, pattern): \n\t \t \treturn True\n", 
" \tdirname = dirname.rstrip('/') \n\tif excluded(dirname): \n\t \treturn \n\tfor (root, dirs, files) in os.walk(dirname): \n\t \tif options.verbose: \n\t \t \tmessage(('directory \t' + root)) \n\t \toptions.counters['directories'] = (options.counters.get('directories', 0) + 1) \n\t \tdirs.sort() \n\t \tfor subdir in dirs: \n\t \t \tif excluded(subdir): \n\t \t \t \tdirs.remove(subdir) \n\t \tfiles.sort() \n\t \tfor filename in files: \n\t \t \tinput_file(os.path.join(root, filename))\n", 
" \timport cPickle \n\tdata = [1, 1.0, 2j, 2L, System.Int64(1), System.UInt64(1), System.UInt32(1), System.Int16(1), System.UInt16(1), System.Byte(1), System.SByte(1), System.Decimal(1), System.Char.MaxValue, System.DBNull.Value, System.Single(1.0), System.DateTime.Now, None, {}, (), [], {'a': 2}, (42,), [42], System.StringSplitOptions.RemoveEmptyEntries] \n\tdata.append(list(data)) \n\tdata.append(tuple(data)) \n\tclass X: \n\t \tdef __init__(self): \n\t \t \tself.abc = 3 \n\tclass Y(object, ): \n\t \tdef __init__(self): \n\t \t \tself.abc = 3 \n\tdata.append(X().__dict__) \n\tdata.append(Y().__dict__) \n\tl = [] \n\tl.append(l) \n\tdata.append(l) \n\td = {} \n\tcnt = 100 \n\tfor x in data: \n\t \td[cnt] = x \n\t \tcnt += 1 \n\tdata.append(d) \n\td1 = {} \n\td2 = {} \n\td1['abc'] = d2 \n\td1['foo'] = 'baz' \n\td2['abc'] = d1 \n\tdata.append(d1) \n\tdata.append(d2) \n\tfor value in data: \n\t \tfor newVal in (cPickle.loads(cPickle.dumps(value)), clr.Deserialize(*clr.Serialize(value))): \n\t \t \tAreEqual(type(newVal), type(value)) \n\t \t \ttry: \n\t \t \t \tAreEqual(newVal, value) \n\t \t \texcept RuntimeError as e: \n\t \t \t \tAreEqual(e.message, 'maximum \trecursion \tdepth \texceeded \tin \tcmp') \n\t \t \t \tAssert(((type(newVal) is list) or (type(newVal) is dict))) \n\tAssertError(ValueError, clr.Deserialize, 'unknown', 'foo') \n\tal = System.Collections.ArrayList() \n\tal.Add(2) \n\tgl = System.Collections.Generic.List[int]() \n\tgl.Add(2) \n\tfor value in (al, gl): \n\t \tfor newX in (cPickle.loads(cPickle.dumps(value)), clr.Deserialize(*clr.Serialize(value))): \n\t \t \tAreEqual(value.Count, newX.Count) \n\t \t \tfor i in xrange(value.Count): \n\t \t \t \tAreEqual(value[i], newX[i]) \n\tht = System.Collections.Hashtable() \n\tht['foo'] = 'bar' \n\tgd = System.Collections.Generic.Dictionary[(str, str)]() \n\tgd['foo'] = 'bar' \n\tfor value in (ht, gd): \n\t \tfor newX in (cPickle.loads(cPickle.dumps(value)), clr.Deserialize(*clr.Serialize(value))): \n\t \t \tAreEqual(value.Count, newX.Count) \n\t \t \tfor key in value.Keys: \n\t \t \t \tAreEqual(value[key], newX[key]) \n\tfor tempX in [System.Exception('some \tmessage')]: \n\t \tfor newX in (cPickle.loads(cPickle.dumps(tempX)), clr.Deserialize(*clr.Serialize(tempX))): \n\t \t \tAreEqual(newX.Message, tempX.Message) \n\ttry: \n\t \texec ' \tprint \t1' \n\texcept Exception as tempX: \n\t \tpass \n\tnewX = cPickle.loads(cPickle.dumps(tempX)) \n\tfor attr in ['args', 'filename', 'text', 'lineno', 'msg', 'offset', 'print_file_and_line', 'message']: \n\t \tAreEqual(eval(('newX.%s' % attr)), eval(('tempX.%s' % attr))) \n\tclass K(System.Exception, ): \n\t \tother = 'something \telse' \n\ttempX = K() \n\ttempX = System.Exception\n", 
" \ttry: \n\t \tfunc(*args) \n\texcept exc: \n\t \treturn True \n\telse: \n\t \treturn False\n", 
" \ttry: \n\t \tfn(*args, **kwargs) \n\texcept Exception as e: \n\t \tassert (e.__class__ == cls), ('got \t%s, \texpected \t%s' % (e.__class__.__name__, cls.__name__)) \n\telse: \n\t \traise AssertionError(('%s \tnot \traised' % cls))\n", 
" \treturn s3_rest_controller()\n", 
" \tresult = [] \n\tchecks = get_qualitychecks() \n\tfor (check, cat) in checks.items(): \n\t \tresult.append({'code': check, 'is_critical': (cat == Category.CRITICAL), 'title': (u'%s' % check_names.get(check, check)), 'url': path_obj.get_translate_url(check=check)}) \n\tdef alphabetical_critical_first(item): \n\t \tcritical_first = (0 if item['is_critical'] else 1) \n\t \treturn (critical_first, item['title'].lower()) \n\tresult = sorted(result, key=alphabetical_critical_first) \n\treturn result\n", 
" \tif (('nova/virt' in filename) and (not filename.endswith('fake.py'))): \n\t \tif logical_line.startswith('from \tnova \timport \tdb'): \n\t \t \t(yield (0, 'N307: \tnova.db \timport \tnot \tallowed \tin \tnova/virt/*'))\n", 
" \tdata = get_instance(name, provider) \n\tif (data is None): \n\t \treturn False \n\treturn True\n", 
" \tcount_failed = count_all = 0 \n\treport = BaseReport(options) \n\tcounters = report.counters \n\tchecks = (options.physical_checks + options.logical_checks) \n\tfor (name, check, argument_names) in checks: \n\t \tfor line in check.__doc__.splitlines(): \n\t \t \tline = line.lstrip() \n\t \t \tmatch = SELFTEST_REGEX.match(line) \n\t \t \tif (match is None): \n\t \t \t \tcontinue \n\t \t \t(code, source) = match.groups() \n\t \t \tlines = [(part.replace('\\\\t', ' DCTB ') + '\\n') for part in source.split('\\\\n')] \n\t \t \tchecker = Checker(lines=lines, options=options, report=report) \n\t \t \tchecker.check_all() \n\t \t \terror = None \n\t \t \tif (code == 'Okay'): \n\t \t \t \tif (len(counters) > len(options.benchmark_keys)): \n\t \t \t \t \tcodes = [key for key in counters if (key not in options.benchmark_keys)] \n\t \t \t \t \terror = ('incorrectly \tfound \t%s' % ', \t'.join(codes)) \n\t \t \telif (not counters.get(code)): \n\t \t \t \terror = ('failed \tto \tfind \t%s' % code) \n\t \t \tfor key in (set(counters) - set(options.benchmark_keys)): \n\t \t \t \tdel counters[key] \n\t \t \tcount_all += 1 \n\t \t \tif (not error): \n\t \t \t \tif options.verbose: \n\t \t \t \t \tprint ('%s: \t%s' % (code, source)) \n\t \t \telse: \n\t \t \t \tcount_failed += 1 \n\t \t \t \tprint ('pycodestyle.py: \t%s:' % error) \n\t \t \t \tfor line in checker.lines: \n\t \t \t \t \tprint line.rstrip() \n\treturn (count_failed, count_all)\n", 
" \tif (sys.platform == 'win32'): \n\t \traise SkipTest('Skipping \tline \tendings \tcheck \ton \tWindows') \n\treport = list() \n\tgood_exts = ('.py', '.dat', '.sel', '.lout', '.css', '.js', '.lay', '.txt', '.elc', '.csd', '.sfp', '.json', '.hpts', '.vmrk', '.vhdr', '.head', '.eve', '.ave', '.cov', '.label') \n\tfor (dirpath, dirnames, filenames) in os.walk(dir_): \n\t \tfor fname in filenames: \n\t \t \tif ((op.splitext(fname)[1] not in good_exts) or (fname in skip_files)): \n\t \t \t \tcontinue \n\t \t \tfilename = op.join(dirpath, fname) \n\t \t \trelfilename = op.relpath(filename, dir_) \n\t \t \ttry: \n\t \t \t \twith open(filename, 'rb') as fid: \n\t \t \t \t \ttext = fid.read().decode('utf-8') \n\t \t \texcept UnicodeDecodeError: \n\t \t \t \treport.append(('In \t%s \tfound \tnon-decodable \tbytes' % relfilename)) \n\t \t \telse: \n\t \t \t \tcrcount = text.count('\\r') \n\t \t \t \tif crcount: \n\t \t \t \t \treport.append(('In \t%s \tfound \t%i/%i \tCR/LF' % (relfilename, crcount, text.count('\\n')))) \n\tif (len(report) > 0): \n\t \traise AssertionError(('Found \t%s \tfiles \twith \tincorrect \tendings:\\n%s' % (len(report), '\\n'.join(report))))\n", 
" \tglobal fr, st \n\tfr = inspect.currentframe() \n\tst = inspect.stack() \n\tp = x \n\tq = (y / 0)\n", 
" \treturn docstring.split(u'\\n')[0]\n", 
" \treturn (string.find(s, '\\n') != (-1))\n", 
" \turl = api_url(host, port, '/Users/AuthenticateByName') \n\tr = requests.post(url, headers=headers, data=auth_data) \n\treturn r.json().get('AccessToken')\n", 
" \treturn (not re.match('[a-z]+://', file_location))\n", 
" \treturn (not re.match('[a-z]+://', file_location))\n", 
" \treturn (not re.match('[a-z]+://', file_location))\n", 
" \tif (not _state): \n\t \traise RuntimeError('no \tactive \tinput()') \n\treturn _state.filename()\n", 
" \tfrom spyder.utils.programs import is_module_installed \n\tif is_module_installed('jedi', '=0.9.0'): \n\t \timport jedi \n\telse: \n\t \traise ImportError((\"jedi \t%s \tcan't \tbe \tpatched\" % jedi.__version__)) \n\tfrom spyder.utils.introspection import docstrings \n\tjedi.evaluate.representation.docstrings = docstrings \n\tfrom jedi.evaluate.compiled import CompiledObject, builtin, _create_from_name, debug \n\tclass CompiledObject(CompiledObject, ): \n\t \tdef _execute_function(self, evaluator, params): \n\t \t \tif (self.type != 'funcdef'): \n\t \t \t \treturn \n\t \t \tfrom spyder.utils.introspection import docstrings \n\t \t \ttypes = docstrings.find_return_types(evaluator, self) \n\t \t \tif types: \n\t \t \t \tfor result in types: \n\t \t \t \t \tdebug.dbg('docstrings \ttype \treturn: \t%s \tin \t%s', result, self) \n\t \t \t \t \t(yield result) \n\t \t \tfor name in self._parse_function_doc()[1].split(): \n\t \t \t \ttry: \n\t \t \t \t \tbltn_obj = _create_from_name(builtin, builtin, name) \n\t \t \t \texcept AttributeError: \n\t \t \t \t \tcontinue \n\t \t \t \telse: \n\t \t \t \t \tif (isinstance(bltn_obj, CompiledObject) and (bltn_obj.obj is None)): \n\t \t \t \t \t \tcontinue \n\t \t \t \t \tfor result in evaluator.execute(bltn_obj, params): \n\t \t \t \t \t \t(yield result) \n\t \t@property \n\t \tdef raw_doc(self): \n\t \t \ttry: \n\t \t \t \tdoc = unicode(self.doc) \n\t \t \texcept NameError: \n\t \t \t \tdoc = self.doc \n\t \t \treturn doc \n\tjedi.evaluate.compiled.CompiledObject = CompiledObject \n\tfrom jedi.evaluate.precedence import tree, calculate \n\tdef calculate_children(evaluator, children): \n\t \t'\\n \t \t \t \t \t \t \t \tCalculate \ta \tlist \tof \tchildren \twith \toperators.\\n \t \t \t \t \t \t \t \t' \n\t \titerator = iter(children) \n\t \ttypes = evaluator.eval_element(next(iterator)) \n\t \tfor operator in iterator: \n\t \t \ttry: \n\t \t \t \tright = next(iterator) \n\t \t \t \tif tree.is_node(operator, 'comp_op'): \n\t \t \t \t \toperator = ' \t'.join((str(c.value) for c in operator.children)) \n\t \t \t \tif (operator in ('and', 'or')): \n\t \t \t \t \tleft_bools = set([left.py__bool__() for left in types]) \n\t \t \t \t \tif (left_bools == set([True])): \n\t \t \t \t \t \tif (operator == 'and'): \n\t \t \t \t \t \t \ttypes = evaluator.eval_element(right) \n\t \t \t \t \telif (left_bools == set([False])): \n\t \t \t \t \t \tif (operator != 'and'): \n\t \t \t \t \t \t \ttypes = evaluator.eval_element(right) \n\t \t \t \telse: \n\t \t \t \t \ttypes = calculate(evaluator, types, operator, evaluator.eval_element(right)) \n\t \t \texcept StopIteration: \n\t \t \t \tdebug.warning('calculate_children \tStopIteration \t%s', types) \n\t \tdebug.dbg('calculate_children \ttypes \t%s', types) \n\t \treturn types \n\tjedi.evaluate.precedence.calculate_children = calculate_children \n\tfrom jedi.evaluate.representation import tree, InstanceName, Instance, compiled, FunctionExecution, InstanceElement \n\tdef get_instance_el(evaluator, instance, var, is_class_var=False): \n\t \t'\\n \t \t \t \t \t \t \t \tReturns \tan \tInstanceElement \tif \tit \tmakes \tsense, \totherwise \tleaves \tthe \tobject\\n \t \t \t \t \t \t \t \tuntouched.\\n\\n \t \t \t \t \t \t \t \tBasically \thaving \tan \tInstanceElement \tis \tcontext \tinformation. \tThat \tis \tneeded\\n \t \t \t \t \t \t \t \tin \tquite \ta \tlot \tof \tcases, \twhich \tincludes \tNodes \tlike \t``power``, \tthat \tneed \tto\\n \t \t \t \t \t \t \t \tknow \twhere \ta \tself \tname \tcomes \tfrom \tfor \texample.\\n \t \t \t \t \t \t \t \t' \n\t \tif isinstance(var, tree.Name): \n\t \t \tparent = get_instance_el(evaluator, instance, var.parent, is_class_var) \n\t \t \treturn InstanceName(var, parent) \n\t \telif (var is None): \n\t \t \treturn var \n\t \telif ((var.type != 'funcdef') and isinstance(var, (Instance, compiled.CompiledObject, tree.Leaf, tree.Module, FunctionExecution))): \n\t \t \treturn var \n\t \tvar = evaluator.wrap(var) \n\t \treturn InstanceElement(evaluator, instance, var, is_class_var) \n\tjedi.evaluate.representation.get_instance_el = get_instance_el \n\treturn jedi\n", 
" \treturn _shell_command(['/usr/bin/git', 'rev-parse', '--verify', '--short', 'HEAD']).strip()\n", 
" \tarr.reverse() \n\treturn int(''.join(arr), 16)\n", 
" \tcmd = ['arp', '-n', ip_address] \n\tarp = subprocess.Popen(cmd, stdout=subprocess.PIPE) \n\t(out, _) = arp.communicate() \n\tmatch = re.search('(([0-9A-Fa-f]{1,2}\\\\:){5}[0-9A-Fa-f]{1,2})', str(out)) \n\tif match: \n\t \treturn match.group(0) \n\t_LOGGER.info('No \tMAC \taddress \tfound \tfor \t%s', ip_address) \n\treturn None\n", 
" \tif (adapter_type == 'vmxnet'): \n\t \treturn vim.vm.device.VirtualVmxnet() \n\telif (adapter_type == 'vmxnet2'): \n\t \treturn vim.vm.device.VirtualVmxnet2() \n\telif (adapter_type == 'vmxnet3'): \n\t \treturn vim.vm.device.VirtualVmxnet3() \n\telif (adapter_type == 'e1000'): \n\t \treturn vim.vm.device.VirtualE1000() \n\telif (adapter_type == 'e1000e'): \n\t \treturn vim.vm.device.VirtualE1000e()\n", 
" \tif (adapter_type == 'vmxnet'): \n\t \treturn vim.vm.device.VirtualVmxnet() \n\telif (adapter_type == 'vmxnet2'): \n\t \treturn vim.vm.device.VirtualVmxnet2() \n\telif (adapter_type == 'vmxnet3'): \n\t \treturn vim.vm.device.VirtualVmxnet3() \n\telif (adapter_type == 'e1000'): \n\t \treturn vim.vm.device.VirtualE1000() \n\telif (adapter_type == 'e1000e'): \n\t \treturn vim.vm.device.VirtualE1000e()\n", 
" \tcmd = ['arp', '-n', ip_address] \n\tarp = subprocess.Popen(cmd, stdout=subprocess.PIPE) \n\t(out, _) = arp.communicate() \n\tmatch = re.search('(([0-9A-Fa-f]{1,2}\\\\:){5}[0-9A-Fa-f]{1,2})', str(out)) \n\tif match: \n\t \treturn match.group(0) \n\t_LOGGER.info('No \tMAC \taddress \tfound \tfor \t%s', ip_address) \n\treturn None\n", 
" \tif (adapter_type == 'vmxnet'): \n\t \treturn vim.vm.device.VirtualVmxnet() \n\telif (adapter_type == 'vmxnet2'): \n\t \treturn vim.vm.device.VirtualVmxnet2() \n\telif (adapter_type == 'vmxnet3'): \n\t \treturn vim.vm.device.VirtualVmxnet3() \n\telif (adapter_type == 'e1000'): \n\t \treturn vim.vm.device.VirtualE1000() \n\telif (adapter_type == 'e1000e'): \n\t \treturn vim.vm.device.VirtualE1000e()\n", 
" \tif (adapter_type == 'vmxnet'): \n\t \treturn vim.vm.device.VirtualVmxnet() \n\telif (adapter_type == 'vmxnet2'): \n\t \treturn vim.vm.device.VirtualVmxnet2() \n\telif (adapter_type == 'vmxnet3'): \n\t \treturn vim.vm.device.VirtualVmxnet3() \n\telif (adapter_type == 'e1000'): \n\t \treturn vim.vm.device.VirtualE1000() \n\telif (adapter_type == 'e1000e'): \n\t \treturn vim.vm.device.VirtualE1000e()\n", 
" \tcmd = ' \t'.join([prefix, os.path.join(cli_dir, cmd), flags, action, params]) \n\tcmd = cmd.strip() \n\tLOG.info(\"running: \t'%s'\", cmd) \n\tif six.PY2: \n\t \tcmd = cmd.encode('utf-8') \n\tcmd = shlex.split(cmd) \n\tresult = '' \n\tresult_err = '' \n\tstdout = subprocess.PIPE \n\tstderr = (subprocess.STDOUT if merge_stderr else subprocess.PIPE) \n\tproc = subprocess.Popen(cmd, stdout=stdout, stderr=stderr) \n\t(result, result_err) = proc.communicate() \n\tif ((not fail_ok) and (proc.returncode != 0)): \n\t \traise exceptions.CommandFailed(proc.returncode, cmd, result, result_err) \n\tif six.PY2: \n\t \treturn result \n\telse: \n\t \treturn os.fsdecode(result)\n", 
" \ttry: \n\t \tip = data.public_ips[0] \n\texcept Exception: \n\t \tip = data.private_ips[0] \n\treturn ip\n", 
" \tif (__grains__['os_family'] == 'RedHat'): \n\t \treturn __virtualname__ \n\treturn (False, 'The \trh_ip \texecution \tmodule \tcannot \tbe \tloaded: \tthis \tmodule \tis \tonly \tavailable \ton \tRHEL/Fedora \tbased \tdistributions.')\n", 
" \tif (netaddr.IPNetwork(network).version == 6): \n\t \treturn 'IPv6' \n\telif (netaddr.IPNetwork(network).version == 4): \n\t \treturn 'IPv4'\n", 
" \ttry: \n\t \tip = data.public_ips[0] \n\texcept Exception: \n\t \tip = data.private_ips[0] \n\treturn ip\n", 
" \treturn xenapi._session.call_xenapi(method, *args)\n", 
" \tctxt = context.get_admin_context(read_deleted='only') \n\torphaned_instances = [] \n\tfor (vm_ref, vm_rec) in _get_applicable_vm_recs(xenapi): \n\t \ttry: \n\t \t \tuuid = vm_rec['other_config']['nova_uuid'] \n\t \t \tinstance = db.instance_get_by_uuid(ctxt, uuid) \n\t \texcept (KeyError, exception.InstanceNotFound): \n\t \t \tprint_xen_object('INFO: \tIgnoring \tVM', vm_rec, indent_level=0) \n\t \t \tcontinue \n\t \tis_active_and_deleting = ((instance.vm_state == 'active') and (instance.task_state == 'deleting')) \n\t \tis_zombie_vm = ((instance.vm_state != 'active') and timeutils.is_older_than(instance.updated_at, CONF.zombie_instance_updated_at_window)) \n\t \tif (is_active_and_deleting or is_zombie_vm): \n\t \t \torphaned_instances.append((vm_ref, vm_rec, instance)) \n\treturn orphaned_instances\n", 
" \txenapi._vmops._destroy(instance, vm_ref)\n", 
" \tfor vm_ref in call_xenapi(xenapi, 'VM.get_all'): \n\t \ttry: \n\t \t \tvm_rec = call_xenapi(xenapi, 'VM.get_record', vm_ref) \n\t \texcept XenAPI.Failure as e: \n\t \t \tif (e.details[0] != 'HANDLE_INVALID'): \n\t \t \t \traise \n\t \t \tcontinue \n\t \tif (vm_rec['is_a_template'] or vm_rec['is_control_domain']): \n\t \t \tcontinue \n\t \t(yield (vm_ref, vm_rec))\n", 
" \tif (not CONF.verbose): \n\t \treturn \n\tuuid = obj['uuid'] \n\ttry: \n\t \tname_label = obj['name_label'] \n\texcept KeyError: \n\t \tname_label = '' \n\tmsg = (\"%s \t(%s) \t'%s'\" % (obj_type, uuid, name_label)) \n\tindent = ((' \t' * spaces_per_indent) * indent_level) \n\tprint ''.join([indent, msg])\n", 
" \tdef _is_null_ref(ref): \n\t \treturn (ref == 'OpaqueRef:NULL') \n\tdef _add_vdi_and_parents_to_connected(vdi_rec, indent_level): \n\t \tindent_level += 1 \n\t \tvdi_and_parent_uuids = [] \n\t \tcur_vdi_rec = vdi_rec \n\t \twhile True: \n\t \t \tcur_vdi_uuid = cur_vdi_rec['uuid'] \n\t \t \tprint_xen_object('VDI', vdi_rec, indent_level=indent_level) \n\t \t \tconnected_vdi_uuids.add(cur_vdi_uuid) \n\t \t \tvdi_and_parent_uuids.append(cur_vdi_uuid) \n\t \t \ttry: \n\t \t \t \tparent_vdi_uuid = vdi_rec['sm_config']['vhd-parent'] \n\t \t \texcept KeyError: \n\t \t \t \tparent_vdi_uuid = None \n\t \t \tif (parent_vdi_uuid and (parent_vdi_uuid != cur_vdi_uuid)): \n\t \t \t \tindent_level += 1 \n\t \t \t \tcur_vdi_ref = call_xenapi(xenapi, 'VDI.get_by_uuid', parent_vdi_uuid) \n\t \t \t \ttry: \n\t \t \t \t \tcur_vdi_rec = call_xenapi(xenapi, 'VDI.get_record', cur_vdi_ref) \n\t \t \t \texcept XenAPI.Failure as e: \n\t \t \t \t \tif (e.details[0] != 'HANDLE_INVALID'): \n\t \t \t \t \t \traise \n\t \t \t \t \tbreak \n\t \t \telse: \n\t \t \t \tbreak \n\tfor (vm_ref, vm_rec) in _get_applicable_vm_recs(xenapi): \n\t \tindent_level = 0 \n\t \tprint_xen_object('VM', vm_rec, indent_level=indent_level) \n\t \tvbd_refs = vm_rec['VBDs'] \n\t \tfor vbd_ref in vbd_refs: \n\t \t \ttry: \n\t \t \t \tvbd_rec = call_xenapi(xenapi, 'VBD.get_record', vbd_ref) \n\t \t \texcept XenAPI.Failure as e: \n\t \t \t \tif (e.details[0] != 'HANDLE_INVALID'): \n\t \t \t \t \traise \n\t \t \t \tcontinue \n\t \t \tindent_level = 1 \n\t \t \tprint_xen_object('VBD', vbd_rec, indent_level=indent_level) \n\t \t \tvbd_vdi_ref = vbd_rec['VDI'] \n\t \t \tif _is_null_ref(vbd_vdi_ref): \n\t \t \t \tcontinue \n\t \t \ttry: \n\t \t \t \tvdi_rec = call_xenapi(xenapi, 'VDI.get_record', vbd_vdi_ref) \n\t \t \texcept XenAPI.Failure as e: \n\t \t \t \tif (e.details[0] != 'HANDLE_INVALID'): \n\t \t \t \t \traise \n\t \t \t \tcontinue \n\t \t \t_add_vdi_and_parents_to_connected(vdi_rec, indent_level)\n", 
" \tdef _system_owned(vdi_rec): \n\t \tvdi_name = vdi_rec['name_label'] \n\t \treturn (vdi_name.startswith('USB') or vdi_name.endswith('.iso') or (vdi_rec['type'] == 'system')) \n\tfor vdi_ref in call_xenapi(xenapi, 'VDI.get_all'): \n\t \ttry: \n\t \t \tvdi_rec = call_xenapi(xenapi, 'VDI.get_record', vdi_ref) \n\t \texcept XenAPI.Failure as e: \n\t \t \tif (e.details[0] != 'HANDLE_INVALID'): \n\t \t \t \traise \n\t \t \tcontinue \n\t \tvdi_uuid = vdi_rec['uuid'] \n\t \tall_vdi_uuids.add(vdi_uuid) \n\t \tif _system_owned(vdi_rec): \n\t \t \tprint_xen_object('SYSTEM \tVDI', vdi_rec, indent_level=0) \n\t \t \tconnected_vdi_uuids.add(vdi_uuid) \n\t \telif (not vdi_rec['managed']): \n\t \t \tprint_xen_object('UNMANAGED \tVDI', vdi_rec, indent_level=0) \n\t \t \tconnected_vdi_uuids.add(vdi_uuid)\n", 
" \tconnected_vdi_uuids = set() \n\t_find_vdis_connected_to_vm(xenapi, connected_vdi_uuids) \n\tall_vdi_uuids = set() \n\t_find_all_vdis_and_system_vdis(xenapi, all_vdi_uuids, connected_vdi_uuids) \n\torphaned_vdi_uuids = (all_vdi_uuids - connected_vdi_uuids) \n\treturn orphaned_vdi_uuids\n", 
" \tfor vdi_uuid in vdi_uuids: \n\t \tif CONF.verbose: \n\t \t \tprint ('ORPHANED \tVDI \t(%s)' % vdi_uuid) \n\t \telse: \n\t \t \tprint vdi_uuid\n", 
" \tfor vdi_uuid in vdi_uuids: \n\t \tif CONF.verbose: \n\t \t \tprint ('CLEANING \tVDI \t(%s)' % vdi_uuid) \n\t \tvdi_ref = call_xenapi(xenapi, 'VDI.get_by_uuid', vdi_uuid) \n\t \ttry: \n\t \t \tcall_xenapi(xenapi, 'VDI.destroy', vdi_ref) \n\t \texcept XenAPI.Failure as exc: \n\t \t \tsys.stderr.write(('Skipping \t%s: \t%s' % (vdi_uuid, exc)))\n", 
" \tfor (vm_ref, vm_rec, orphaned_instance) in orphaned_instances: \n\t \tif CONF.verbose: \n\t \t \tprint ('ORPHANED \tINSTANCE \t(%s)' % orphaned_instance.name) \n\t \telse: \n\t \t \tprint orphaned_instance.name\n", 
" \tfor (vm_ref, vm_rec, instance) in orphaned_instances: \n\t \tif CONF.verbose: \n\t \t \tprint ('CLEANING \tINSTANCE \t(%s)' % instance.name) \n\t \tcleanup_instance(xenapi, instance, vm_ref, vm_rec)\n", 
" \tstates = enum(u'INIT', u'DISABLED', u'RISING', u'FALLING', u'UP', u'DOWN') \n\tdef exabgp(target): \n\t \tu'Communicate \tnew \tstate \tto \tExaBGP' \n\t \tif (target not in (states.UP, states.DOWN, states.DISABLED)): \n\t \t \treturn \n\t \tlogger.info(u'send \tannounces \tfor \t%s \tstate \tto \tExaBGP', target) \n\t \tmetric = vars(options).get(u'{0}_metric'.format(str(target).lower())) \n\t \tfor ip in options.ips: \n\t \t \tif options.withdraw_on_down: \n\t \t \t \tcommand = (u'announce' if (target is states.UP) else u'withdraw') \n\t \t \telse: \n\t \t \t \tcommand = u'announce' \n\t \t \tannounce = u'route \t{0} \tnext-hop \t{1}'.format(str(ip), (options.next_hop or u'self')) \n\t \t \tif (command == u'announce'): \n\t \t \t \tannounce = u'{0} \tmed \t{1}'.format(announce, metric) \n\t \t \t \tif options.community: \n\t \t \t \t \tannounce = u'{0} \tcommunity \t[ \t{1} \t]'.format(announce, options.community) \n\t \t \t \tif options.extended_community: \n\t \t \t \t \tannounce = u'{0} \textended-community \t[ \t{1} \t]'.format(announce, options.extended_community) \n\t \t \t \tif options.large_community: \n\t \t \t \t \tannounce = u'{0} \tlarge-community \t[ \t{1} \t]'.format(announce, options.large_community) \n\t \t \t \tif options.as_path: \n\t \t \t \t \tannounce = u'{0} \tas-path \t[ \t{1} \t]'.format(announce, options.as_path) \n\t \t \tlogger.debug(u'exabgp: \t%s \t%s', command, announce) \n\t \t \tprint(u'{0} \t{1}'.format(command, announce)) \n\t \t \tsys.stdout.flush() \n\t \t \tsys.stdin.readline() \n\t \t \tmetric += options.increase \n\tdef trigger(target): \n\t \tu'Trigger \ta \tstate \tchange \tand \texecute \tthe \tappropriate \tcommands' \n\t \tif ((target == states.RISING) and (options.rise <= 1)): \n\t \t \ttarget = states.UP \n\t \telif ((target == states.FALLING) and (options.fall <= 1)): \n\t \t \ttarget = states.DOWN \n\t \tlogger.debug(u'Transition \tto \t%s', str(target)) \n\t \tcmds = [] \n\t \tcmds.extend((vars(options).get(u'{0}_execute'.format(str(target).lower()), []) or [])) \n\t \tcmds.extend((vars(options).get(u'execute', []) or [])) \n\t \tfor cmd in cmds: \n\t \t \tlogger.debug(u'Transition \tto \t%s, \texecute \t`%s`', str(target), cmd) \n\t \t \tenv = os.environ.copy() \n\t \t \tenv.update({u'STATE': str(target)}) \n\t \t \twith open(os.devnull, u'w') as fnull: \n\t \t \t \tsubprocess.call(cmd, shell=True, stdout=fnull, stderr=fnull, env=env) \n\t \treturn target \n\tdef one(checks, state): \n\t \tu'Execute \tone \tloop \titeration.' \n\t \tdisabled = ((options.disable is not None) and os.path.exists(options.disable)) \n\t \tsuccessful = (disabled or check(options.command, options.timeout)) \n\t \tif ((state != states.DISABLED) and disabled): \n\t \t \tstate = trigger(states.DISABLED) \n\t \telif (state == states.INIT): \n\t \t \tif (successful and (options.rise <= 1)): \n\t \t \t \tstate = trigger(states.UP) \n\t \t \telif successful: \n\t \t \t \tstate = trigger(states.RISING) \n\t \t \t \tchecks = 1 \n\t \t \telse: \n\t \t \t \tstate = trigger(states.FALLING) \n\t \t \t \tchecks = 1 \n\t \telif (state == states.DISABLED): \n\t \t \tif (not disabled): \n\t \t \t \tstate = trigger(states.INIT) \n\t \telif (state == states.RISING): \n\t \t \tif successful: \n\t \t \t \tchecks += 1 \n\t \t \t \tif (checks >= options.rise): \n\t \t \t \t \tstate = trigger(states.UP) \n\t \t \telse: \n\t \t \t \tstate = trigger(states.FALLING) \n\t \t \t \tchecks = 1 \n\t \telif (state == states.FALLING): \n\t \t \tif (not successful): \n\t \t \t \tchecks += 1 \n\t \t \t \tif (checks >= options.fall): \n\t \t \t \t \tstate = trigger(states.DOWN) \n\t \t \telse: \n\t \t \t \tstate = trigger(states.RISING) \n\t \t \t \tchecks = 1 \n\t \telif (state == states.UP): \n\t \t \tif (not successful): \n\t \t \t \tstate = trigger(states.FALLING) \n\t \t \t \tchecks = 1 \n\t \telif (state == states.DOWN): \n\t \t \tif successful: \n\t \t \t \tstate = trigger(states.RISING) \n\t \t \t \tchecks = 1 \n\t \telse: \n\t \t \traise ValueError(u'Unhandled \tstate: \t{0}'.format(str(state))) \n\t \texabgp(state) \n\t \treturn (checks, state) \n\tchecks = 0 \n\tstate = states.INIT \n\twhile True: \n\t \t(checks, state) = one(checks, state) \n\t \tif (state in (states.FALLING, states.RISING)): \n\t \t \ttime.sleep(options.fast) \n\t \telse: \n\t \t \ttime.sleep(options.interval)\n", 
" \treturn getattr(settings, 'CMS_CACHE_DURATIONS', {'menus': (60 * 60), 'content': 60, 'permissions': (60 * 60)})\n", 
" \tdevnull = open(os.devnull, 'w') \n\tcommand = map(str, command) \n\tproc = subprocess.Popen(command, close_fds=True, stdout=subprocess.PIPE, stderr=devnull) \n\tdevnull.close() \n\tstdout = proc.communicate()[0] \n\treturn stdout.strip()\n", 
" \tdevnull = open(os.devnull, 'w') \n\tcommand = map(str, command) \n\tsubprocess.call(command, close_fds=True, stdout=devnull, stderr=devnull) \n\tdevnull.close()\n", 
" \ttext = (format % args) \n\tif (_Level > 0): \n\t \tindent = (' DCTB ' * _Level) \n\t \tlines = text.split('\\n') \n\t \tfor i in range(len(lines)): \n\t \t \tif (lines[i] and (lines[i][0] != '#')): \n\t \t \t \tlines[i] = (indent + lines[i]) \n\t \ttext = '\\n'.join(lines) \n\t_File.write((text + '\\n'))\n", 
" \tif (not isinstance(value, collections.Mapping)): \n\t \traise TypeError(('%s \tmust \tbe \tan \tinstance \tof \tdict, \tbson.son.SON, \tor \tother \ttype \tthat \tinherits \tfrom \tcollections.Mapping' % (option,)))\n", 
" \tmethod_lists = [rtorrent.methods, rtorrent.file.methods, rtorrent.tracker.methods, rtorrent.peer.methods, rtorrent.torrent.methods] \n\tfor l in method_lists: \n\t \tfor m in l: \n\t \t \tif (m.rpc_call.lower() == rpc_call.lower()): \n\t \t \t \treturn m \n\treturn (-1)\n", 
" \tsafe = isinstance(value, SafeData) \n\tvalue = value.replace(arg, u'') \n\tif (safe and (arg != ';')): \n\t \treturn mark_safe(value) \n\treturn value\n", 
" \tvbd_rec = {} \n\tvbd_rec['VM'] = vm_ref \n\tif (vdi_ref is None): \n\t \tvdi_ref = 'OpaqueRef:NULL' \n\tvbd_rec['VDI'] = vdi_ref \n\tvbd_rec['userdevice'] = str(userdevice) \n\tvbd_rec['bootable'] = bootable \n\tvbd_rec['mode'] = ((read_only and 'RO') or 'RW') \n\tvbd_rec['type'] = vbd_type \n\tvbd_rec['unpluggable'] = unpluggable \n\tvbd_rec['empty'] = empty \n\tvbd_rec['other_config'] = {} \n\tvbd_rec['qos_algorithm_type'] = '' \n\tvbd_rec['qos_algorithm_params'] = {} \n\tvbd_rec['qos_supported_algorithms'] = [] \n\tLOG.debug('Creating \t%(vbd_type)s-type \tVBD \tfor \tVM \t%(vm_ref)s, \tVDI \t%(vdi_ref)s \t... \t', {'vbd_type': vbd_type, 'vm_ref': vm_ref, 'vdi_ref': vdi_ref}) \n\tvbd_ref = session.call_xenapi('VBD.create', vbd_rec) \n\tLOG.debug('Created \tVBD \t%(vbd_ref)s \tfor \tVM \t%(vm_ref)s, \tVDI \t%(vdi_ref)s.', {'vbd_ref': vbd_ref, 'vm_ref': vm_ref, 'vdi_ref': vdi_ref}) \n\tif osvol: \n\t \tsession.call_xenapi('VBD.add_to_other_config', vbd_ref, 'osvol', 'True') \n\treturn vbd_ref\n", 
" \tdef decorator(request, *args, **kwargs): \n\t \tif (get_path('', site=site) is None): \n\t \t \traise ImproperlyConfigured(_('Error \tfinding \tUpload-Folder \t(site.storage.location \t+ \tsite.directory). \tMaybe \tit \tdoes \tnot \texist?')) \n\t \tif (get_path(request.GET.get('dir', ''), site=site) is None): \n\t \t \tmsg = _('The \trequested \tFolder \tdoes \tnot \texist.') \n\t \t \tmessages.add_message(request, messages.ERROR, msg) \n\t \t \tredirect_url = (reverse('filebrowser:fb_browse', current_app=site.name) + query_helper(request.GET, u'', 'dir')) \n\t \t \treturn HttpResponseRedirect(redirect_url) \n\t \treturn function(request, *args, **kwargs) \n\treturn decorator\n", 
" \tfrom sabnzbd.encoding import unicoder \n\tif (path == ''): \n\t \tif NT: \n\t \t \tentries = [{'name': (letter + ':\\\\'), 'path': (letter + ':\\\\')} for letter in get_win_drives()] \n\t \t \tentries.insert(0, {'current_path': 'Root'}) \n\t \t \treturn entries \n\t \telse: \n\t \t \tpath = '/' \n\tpath = sabnzbd.misc.real_path(sabnzbd.DIR_HOME, path) \n\twhile (path and (not os.path.isdir(path))): \n\t \tif (path == os.path.dirname(path)): \n\t \t \treturn folders_at_path('', include_parent) \n\t \telse: \n\t \t \tpath = os.path.dirname(path) \n\tpath = os.path.abspath(os.path.normpath(path)) \n\tparent_path = os.path.dirname(path) \n\tif ((path == parent_path) and (os.name == 'nt')): \n\t \tparent_path = '' \n\tfile_list = [] \n\ttry: \n\t \tfor filename in os.listdir(path): \n\t \t \tfpath = os.path.join(path, filename) \n\t \t \ttry: \n\t \t \t \tif NT: \n\t \t \t \t \tdoit = (((win32api.GetFileAttributes(fpath) & MASK) == TMASK) and (filename != 'PerfLogs')) \n\t \t \t \telif (not show_hidden): \n\t \t \t \t \tdoit = (not filename.startswith('.')) \n\t \t \t \telse: \n\t \t \t \t \tdoit = True \n\t \t \texcept: \n\t \t \t \tdoit = False \n\t \t \tif doit: \n\t \t \t \tfile_list.append({'name': unicoder(filename), 'path': unicoder(fpath)}) \n\t \tfile_list = filter((lambda entry: os.path.isdir(entry['path'])), file_list) \n\t \tfile_list = filter((lambda entry: (entry['name'].lower() not in _JUNKFOLDERS)), file_list) \n\t \tfile_list = sorted(file_list, (lambda x, y: cmp(os.path.basename(x['name']).lower(), os.path.basename(y['path']).lower()))) \n\texcept: \n\t \tpass \n\tfile_list.insert(0, {'current_path': path}) \n\tif (include_parent and (parent_path != path)): \n\t \tfile_list.insert(1, {'name': '..', 'path': parent_path}) \n\treturn file_list\n", 
" \tf = open(filename, 'w') \n\ttry: \n\t \tf.write(data) \n\tfinally: \n\t \tf.close() \n\tif report: \n\t \tgdb_report(filename) \n\treturn filename\n", 
" \tpaths = [] \n\tis_module = (lambda path: path.endswith('.py')) \n\tfor (dir_path, dir_names, file_names) in os.walk(root_dir): \n\t \tnew_paths = [os.path.join(dir_path, file_name) for file_name in file_names] \n\t \tnew_paths = filter(is_module, new_paths) \n\t \tnew_paths = filter(should_include, new_paths) \n\t \tpaths.extend(new_paths) \n\treturn paths\n", 
" \thdr = gethdr(open(file, 'r')) \n\t(data_size, encoding, sample_rate, channels, info) = hdr \n\twhile (info[(-1):] == '\\x00'): \n\t \tinfo = info[:(-1)] \n\tprint 'File \tname: \t \t', file \n\tprint 'Data \tsize: \t \t', data_size \n\tprint 'Encoding: \t \t \t', encoding \n\tprint 'Sample \trate:', sample_rate \n\tprint 'Channels: \t \t \t', channels \n\tprint 'Info: \t \t \t \t \t \t \t', repr(info)\n", 
" \tassert (script['id'] == 'afp-ls') \n\tresult = {'total': {'files': 0, 'bytes': 0}, 'volumes': []} \n\tstate = 0 \n\tcur_vol = None \n\tfor line in script['output'].splitlines(): \n\t \tif (state == 0): \n\t \t \tif line.startswith(' \t \t \t \tPERMISSION'): \n\t \t \t \tpass \n\t \t \telif line.startswith(' \t \t \t \t'): \n\t \t \t \tif (cur_vol is None): \n\t \t \t \t \tsys.stderr.write(('WARNING: \tskip \tfile \tentry \toutside \ta \tvolume \t[%r]\\n' % line[4:])) \n\t \t \t \telse: \n\t \t \t \t \t(permission, uid, gid, size, date, time, fname) = line[4:].split(None, 6) \n\t \t \t \t \tif size.isdigit(): \n\t \t \t \t \t \tsize = int(size) \n\t \t \t \t \t \tresult['total']['bytes'] += size \n\t \t \t \t \tcur_vol['files'].append({'permission': permission, 'uid': uid, 'gid': gid, 'size': size, 'filename': fname, 'time': ('%s \t%s' % (date, time))}) \n\t \t \t \t \tresult['total']['files'] += 1 \n\t \t \telif line.startswith(' \t \tERROR: \t'): \n\t \t \t \tpass \n\t \t \telif (line == ' \t \t'): \n\t \t \t \tstate = 1 \n\t \t \telif line.startswith(' \t \t'): \n\t \t \t \tresult['volumes'].append(cur_vol) \n\t \t \t \tcur_vol = {'volume': line[2:], 'files': []} \n\t \telif (state == 1): \n\t \t \tif line.startswith(' \t \t'): \n\t \t \t \tresult.setdefault('info', []).append((line[3].lower() + line[4:])) \n\t \t \telse: \n\t \t \t \tsys.stderr.write(('WARNING: \tskip \tnot \tunderstood \tline \t[%r]\\n' % line)) \n\treturn (result if result['volumes'] else None)\n", 
" \treturn (len(unimplemented_abstract_methods(node)) > 0)\n", 
" \ttry: \n\t \tp = sp.Popen(cmd, stdout=sp.PIPE, stderr=sp.PIPE) \n\t \t(stdout, stderr) = p.communicate() \n\texcept OSError as e: \n\t \tif DEBUG: \n\t \t \traise \n\t \tif (e.errno == errno.ENOENT): \n\t \t \tmsg = 'Command \tnot \tfound: \t`{0}`'.format(' \t'.join(cmd)) \n\t \t \traise _CommandNotFound(msg, cmd) \n\t \telse: \n\t \t \traise _AHBoostrapSystemExit('An \tunexpected \terror \toccurred \twhen \trunning \tthe \t`{0}` \tcommand:\\n{1}'.format(' \t'.join(cmd), str(e))) \n\ttry: \n\t \tstdio_encoding = (locale.getdefaultlocale()[1] or 'latin1') \n\texcept ValueError: \n\t \tstdio_encoding = 'latin1' \n\tif (not isinstance(stdout, _text_type)): \n\t \tstdout = stdout.decode(stdio_encoding, 'replace') \n\tif (not isinstance(stderr, _text_type)): \n\t \tstderr = stderr.decode(stdio_encoding, 'replace') \n\treturn (p.returncode, stdout, stderr)\n", 
" \ttry: \n\t \ttry: \n\t \t \tconfig = _prepareconfig(args, plugins) \n\t \texcept ConftestImportFailure as e: \n\t \t \ttw = py.io.TerminalWriter(sys.stderr) \n\t \t \tfor line in traceback.format_exception(*e.excinfo): \n\t \t \t \ttw.line(line.rstrip(), red=True) \n\t \t \ttw.line(('ERROR: \tcould \tnot \tload \t%s\\n' % e.path), red=True) \n\t \t \treturn 4 \n\t \telse: \n\t \t \ttry: \n\t \t \t \tconfig.pluginmanager.check_pending() \n\t \t \t \treturn config.hook.pytest_cmdline_main(config=config) \n\t \t \tfinally: \n\t \t \t \tconfig._ensure_unconfigure() \n\texcept UsageError as e: \n\t \tfor msg in e.args: \n\t \t \tsys.stderr.write(('ERROR: \t%s\\n' % (msg,))) \n\t \treturn 4\n", 
" \tif (space_compress is None): \n\t \tspace_compress = (not g.template_debug) \n\tif is_api(): \n\t \tres = (res or u'') \n\t \tif ((not c.allowed_callback) and request.environ.get('WANT_RAW_JSON')): \n\t \t \tres = scriptsafe_dumps(res) \n\t \telse: \n\t \t \tres = websafe_json(simplejson.dumps(res)) \n\t \tif c.allowed_callback: \n\t \t \tres = ('/**/%s(%s)' % (websafe_json(c.allowed_callback), res)) \n\telif space_compress: \n\t \tres = spaceCompress(res) \n\treturn res\n", 
" \tif os.path.exists(dirpath): \n\t \tshutil.rmtree(dirpath, ignore_errors=True)\n", 
" \tsrc = os.path.expanduser(src) \n\tdst = os.path.expanduser(dst) \n\tif (not os.path.isabs(src)): \n\t \traise SaltInvocationError('File \tpath \tmust \tbe \tabsolute.') \n\ttry: \n\t \tos.rename(src, dst) \n\t \treturn True \n\texcept OSError: \n\t \traise CommandExecutionError(\"Could \tnot \trename \t'{0}' \tto \t'{1}'\".format(src, dst)) \n\treturn False\n", 
" \treturn os.path.split(path)[1].startswith('.')\n", 
" \tinstalled_names = set((d.project_name.lower() for d in installed_dists)) \n\tmissing_requirements = set() \n\tfor requirement in dist.requires(): \n\t \tif (requirement.project_name.lower() not in installed_names): \n\t \t \tmissing_requirements.add(requirement) \n\t \t \t(yield requirement)\n", 
" \tparent_refs = pointer.node__parent \n\tassert (len(parent_refs) == 1), 'Pointer \tmust \thave \texactly \tone \tparent.' \n\treturn parent_refs[0]\n", 
" \tclass TestTable(tables.Table, ): \n\t \ta = tables.Column() \n\t \tb = tables.Column() \n\t \tc = tables.Column() \n\tassert ([u'a', u'b', u'c'] == TestTable([]).columns.names()) \n\tassert ([u'b', u'a', u'c'] == TestTable([], sequence=(u'b', u'a', u'c')).columns.names()) \n\tclass TestTable2(TestTable, ): \n\t \tclass Meta: \n\t \t \tsequence = (u'b', u'a', u'c') \n\tassert ([u'b', u'a', u'c'] == TestTable2([]).columns.names()) \n\tassert ([u'a', u'b', u'c'] == TestTable2([], sequence=(u'a', u'b', u'c')).columns.names()) \n\tclass TestTable3(TestTable, ): \n\t \tclass Meta: \n\t \t \tsequence = (u'c',) \n\tassert ([u'c', u'a', u'b'] == TestTable3([]).columns.names()) \n\tassert ([u'c', u'a', u'b'] == TestTable([], sequence=(u'c',)).columns.names()) \n\tclass TestTable4(TestTable, ): \n\t \tclass Meta: \n\t \t \tsequence = (u'...',) \n\tassert ([u'a', u'b', u'c'] == TestTable4([]).columns.names()) \n\tassert ([u'a', u'b', u'c'] == TestTable([], sequence=(u'...',)).columns.names()) \n\tclass TestTable5(TestTable, ): \n\t \tclass Meta: \n\t \t \tsequence = (u'b', u'...') \n\tassert ([u'b', u'a', u'c'] == TestTable5([]).columns.names()) \n\tassert ([u'b', u'a', u'c'] == TestTable([], sequence=(u'b', u'...')).columns.names()) \n\tclass TestTable6(TestTable, ): \n\t \tclass Meta: \n\t \t \tsequence = (u'...', u'b') \n\tassert ([u'a', u'c', u'b'] == TestTable6([]).columns.names()) \n\tassert ([u'a', u'c', u'b'] == TestTable([], sequence=(u'...', u'b')).columns.names()) \n\tclass TestTable7(TestTable, ): \n\t \tclass Meta: \n\t \t \tsequence = (u'b', u'...', u'a') \n\tassert ([u'b', u'c', u'a'] == TestTable7([]).columns.names()) \n\tassert ([u'b', u'c', u'a'] == TestTable([], sequence=(u'b', u'...', u'a')).columns.names()) \n\tclass TestTable8(TestTable, ): \n\t \td = tables.Column() \n\t \te = tables.Column() \n\t \tf = tables.Column() \n\t \tclass Meta: \n\t \t \tsequence = (u'd', u'...') \n\tclass TestTable9(TestTable, ): \n\t \td = tables.Column() \n\t \te = tables.Column() \n\t \tf = tables.Column() \n\tassert ([u'd', u'a', u'b', u'c', u'e', u'f'] == TestTable8([]).columns.names()) \n\tassert ([u'd', u'a', u'b', u'c', u'e', u'f'] == TestTable9([], sequence=(u'd', u'...')).columns.names())\n", 
" \tLOG.debug('Asking \txapi \tto \tfetch \tvhd \timage \t%s', image_id, instance=instance) \n\thandler = _choose_download_handler(context, instance) \n\ttry: \n\t \tvdis = handler.download_image(context, session, instance, image_id) \n\texcept Exception: \n\t \tdefault_handler = _default_download_handler() \n\t \tif (type(handler) == type(default_handler)): \n\t \t \traise \n\t \tLOG.exception(_LE(\"Download \thandler \t'%(handler)s' \traised \tan \texception, \tfalling \tback \tto \tdefault \thandler \t'%(default_handler)s'\"), {'handler': handler, 'default_handler': default_handler}) \n\t \tvdis = default_handler.download_image(context, session, instance, image_id) \n\tscan_default_sr(session) \n\tvdi_uuid = vdis['root']['uuid'] \n\ttry: \n\t \t_check_vdi_size(context, session, instance, vdi_uuid) \n\texcept Exception: \n\t \twith excutils.save_and_reraise_exception(): \n\t \t \tmsg = 'Error \twhile \tchecking \tvdi \tsize' \n\t \t \tLOG.debug(msg, instance=instance, exc_info=True) \n\t \t \tfor vdi in vdis.values(): \n\t \t \t \tvdi_uuid = vdi['uuid'] \n\t \t \t \tvdi_ref = session.call_xenapi('VDI.get_by_uuid', vdi_uuid) \n\t \t \t \tsafe_destroy_vdis(session, [vdi_ref]) \n\treturn vdis\n", 
" \tdest_dir = os.path.abspath(os.path.realpath(dest_dir)) \n\tclone_data = container.clone_data(dest_dir) \n\tcls = type(container) \n\tif (cls is Container): \n\t \treturn cls(None, None, container.log, clone_data=clone_data) \n\treturn cls(None, container.log, clone_data=clone_data)\n", 
" \tversion = get_sympy_version() \n\tdoctypename = defaultdict(str, {'html': 'zip', 'pdf': 'pdf'}) \n\twinos = defaultdict(str, {'win32': 'win32', 'win32-orig': 'linux-i686'}) \n\tif (file in {'source-orig', 'source'}): \n\t \tname = 'sympy-{version}.tar.gz' \n\telif (file == 'source-orig-notar'): \n\t \tname = 'sympy-{version}' \n\telif (file in {'win32', 'win32-orig'}): \n\t \tname = 'sympy-{version}.{wintype}.exe' \n\telif (file in {'html', 'pdf', 'html-nozip'}): \n\t \tname = 'sympy-docs-{type}-{version}' \n\t \tif (file == 'html-nozip'): \n\t \t \tfile = 'html' \n\t \telse: \n\t \t \tname += '.{extension}' \n\telif (file == 'pdf-orig'): \n\t \tname = 'sympy-{version}.pdf' \n\telse: \n\t \traise ValueError((file + ' \tis \tnot \ta \trecognized \targument')) \n\tret = name.format(version=version, type=file, extension=doctypename[file], wintype=winos[file]) \n\treturn ret\n", 
" \textracted = cat_file_to_cmd(tarball, 'tar \txvf \t- \t2>/dev/null', return_output=True).splitlines() \n\tdir = None \n\tfor line in extracted: \n\t \tline = re.sub('^./', '', line) \n\t \tif ((not line) or (line == '.')): \n\t \t \tcontinue \n\t \ttopdir = line.split('/')[0] \n\t \tif os.path.isdir(topdir): \n\t \t \tif dir: \n\t \t \t \tassert (dir == topdir) \n\t \t \telse: \n\t \t \t \tdir = topdir \n\tif dir: \n\t \treturn dir \n\telse: \n\t \traise NameError('extracting \ttarball \tproduced \tno \tdir')\n", 
" \ttry: \n\t \tjson_data = json.loads(data) \n\texcept ValueError as err: \n\t \tsnippet = repr(data) \n\t \tif (len(snippet) > 35): \n\t \t \tsnippet = (snippet[:35] + ' \t...') \n\t \telse: \n\t \t \tsnippet = data \n\t \traise exception('Unable \tto \tparse \t{0}: \t{1} \t({2})'.format(name, err, snippet)) \n\tif schema: \n\t \tjson_data = schema.validate(json_data, name=name, exception=exception) \n\treturn json_data\n", 
" \tdef decorator(func): \n\t \tif (not hasattr(func, 'wsgi_serializers')): \n\t \t \tfunc.wsgi_serializers = {} \n\t \tfunc.wsgi_serializers.update(serializers) \n\t \treturn func \n\treturn decorator\n", 
" \tdef decorator(func): \n\t \tif (not hasattr(func, 'wsgi_deserializers')): \n\t \t \tfunc.wsgi_deserializers = {} \n\t \tfunc.wsgi_deserializers.update(deserializers) \n\t \treturn func \n\treturn decorator\n", 
" \tdef decorator(func): \n\t \tfunc.wsgi_code = code \n\t \treturn func \n\treturn decorator\n", 
" \ttry: \n\t \tdecoded = jsonutils.loads(body) \n\texcept ValueError: \n\t \tmsg = _('cannot \tunderstand \tJSON') \n\t \traise exception.MalformedRequestBody(reason=msg) \n\tif (len(decoded) != 1): \n\t \tmsg = _('too \tmany \tbody \tkeys') \n\t \traise exception.MalformedRequestBody(reason=msg) \n\treturn list(decoded.keys())[0]\n", 
" \ttry: \n\t \tdecoded = jsonutils.loads(body) \n\texcept ValueError: \n\t \tmsg = _('cannot \tunderstand \tJSON') \n\t \traise exception.MalformedRequestBody(reason=msg) \n\tif (len(decoded) != 1): \n\t \tmsg = _('too \tmany \tbody \tkeys') \n\t \traise exception.MalformedRequestBody(reason=msg) \n\treturn list(decoded.keys())[0]\n", 
" \tdef decorator(func): \n\t \tfunc.wsgi_action = name \n\t \treturn func \n\treturn decorator\n", 
" \tdef decorator(func): \n\t \tfunc.wsgi_extends = (func.__name__, kwargs.get('action')) \n\t \treturn func \n\tif args: \n\t \treturn decorator(*args) \n\treturn decorator\n", 
" \tif (not isinstance(default, int)): \n\t \tmsg = (\"'%s' \tobject \tcannot \tbe \tinterpreted \tas \tan \tinteger\" % type(default).__name__) \n\t \traise TypeError(msg) \n\ttry: \n\t \treturn len(obj) \n\texcept TypeError: \n\t \tpass \n\ttry: \n\t \thint = type(obj).__length_hint__ \n\texcept AttributeError: \n\t \treturn default \n\ttry: \n\t \tval = hint(obj) \n\texcept TypeError: \n\t \treturn default \n\tif (val is NotImplemented): \n\t \treturn default \n\tif (not isinstance(val, int)): \n\t \tmsg = ('__length_hint__ \tmust \tbe \tinteger, \tnot \t%s' % type(val).__name__) \n\t \traise TypeError(msg) \n\tif (val < 0): \n\t \tmsg = '__length_hint__() \tshould \treturn \t>= \t0' \n\t \traise ValueError(msg) \n\treturn val\n", 
" \treturn itertools.chain(element.iterfind((_OLD_NAMESPACE_PREFIX + tag)), element.iterfind((_NEW_NAMESPACE_PREFIX + tag)))\n", 
" \tfor (key, value) in sub_dict.items(): \n\t \tif isinstance(value, list): \n\t \t \tfor repeated_element in value: \n\t \t \t \tsub_element = ET.SubElement(parent, key) \n\t \t \t \t_add_element_attrs(sub_element, repeated_element.get('attrs', {})) \n\t \t \t \tchildren = repeated_element.get('children', None) \n\t \t \t \tif isinstance(children, dict): \n\t \t \t \t \t_add_sub_elements_from_dict(sub_element, children) \n\t \t \t \telif isinstance(children, str): \n\t \t \t \t \tsub_element.text = children \n\t \telse: \n\t \t \tsub_element = ET.SubElement(parent, key) \n\t \t \t_add_element_attrs(sub_element, value.get('attrs', {})) \n\t \t \tchildren = value.get('children', None) \n\t \t \tif isinstance(children, dict): \n\t \t \t \t_add_sub_elements_from_dict(sub_element, children) \n\t \t \telif isinstance(children, str): \n\t \t \t \tsub_element.text = children\n", 
" \treturn ''.join([t for t in educate_tokens(tokenize(text), attr, language)])\n", 
" \tglobal CSSAttrCache \n\tCSSAttrCache = {} \n\tif xhtml: \n\t \tparser = html5lib.XHTMLParser(tree=treebuilders.getTreeBuilder(u'dom')) \n\telse: \n\t \tparser = html5lib.HTMLParser(tree=treebuilders.getTreeBuilder(u'dom')) \n\tif isinstance(src, six.text_type): \n\t \tif (not encoding): \n\t \t \tencoding = u'utf-8' \n\t \tsrc = src.encode(encoding) \n\t \tsrc = pisaTempFile(src, capacity=context.capacity) \n\tdocument = parser.parse(src) \n\tif xml_output: \n\t \tif encoding: \n\t \t \txml_output.write(document.toprettyxml(encoding=encoding)) \n\t \telse: \n\t \t \txml_output.write(document.toprettyxml(encoding=u'utf8')) \n\tif default_css: \n\t \tcontext.addDefaultCSS(default_css) \n\tpisaPreLoop(document, context) \n\tcontext.parseCSS() \n\tpisaLoop(document, context) \n\treturn context\n", 
" \tret = {'name': name, 'result': False, 'changes': {}, 'comment': ''} \n\told_user = __salt__['nxos.cmd']('get_user', username=name) \n\tif (not old_user): \n\t \tret['result'] = True \n\t \tret['comment'] = 'User \tdoes \tnot \texist' \n\t \treturn ret \n\tif ((__opts__['test'] is True) and old_user): \n\t \tret['result'] = None \n\t \tret['comment'] = 'User \twill \tbe \tremoved' \n\t \tret['changes']['old'] = old_user \n\t \tret['changes']['new'] = '' \n\t \treturn ret \n\t__salt__['nxos.cmd']('remove_user', username=name) \n\tif __salt__['nxos.cmd']('get_user', username=name): \n\t \tret['comment'] = 'Failed \tto \tremove \tuser' \n\telse: \n\t \tret['result'] = True \n\t \tret['comment'] = 'User \tremoved' \n\t \tret['changes']['old'] = old_user \n\t \tret['changes']['new'] = '' \n\treturn ret\n", 
" \tour_dir = path[0] \n\tfor (dirpath, dirnames, filenames) in os.walk(our_dir): \n\t \trelpath = os.path.relpath(dirpath, our_dir) \n\t \tif (relpath == '.'): \n\t \t \trelpkg = '' \n\t \telse: \n\t \t \trelpkg = ('.%s' % '.'.join(relpath.split(os.sep))) \n\t \tfor fname in filenames: \n\t \t \t(root, ext) = os.path.splitext(fname) \n\t \t \tif ((ext not in ('.py', '.pyc')) or (root == '__init__') or (fname in FILES_TO_SKIP)): \n\t \t \t \tcontinue \n\t \t \tif ((ext == '.pyc') and ((root + '.py') in filenames)): \n\t \t \t \tcontinue \n\t \t \tclassname = ('%s%s' % (root[0].upper(), root[1:])) \n\t \t \tclasspath = ('%s%s.%s.%s' % (package, relpkg, root, classname)) \n\t \t \tif ((ext_list is not None) and (classname not in ext_list)): \n\t \t \t \tlogger.debug(('Skipping \textension: \t%s' % classpath)) \n\t \t \t \tcontinue \n\t \t \ttry: \n\t \t \t \text_mgr.load_extension(classpath) \n\t \t \texcept Exception as exc: \n\t \t \t \tlogger.warning(_LW('Failed \tto \tload \textension \t%(classpath)s: \t%(exc)s'), {'classpath': classpath, 'exc': exc}) \n\t \tsubdirs = [] \n\t \tfor dname in dirnames: \n\t \t \tif (not os.path.exists(os.path.join(dirpath, dname, '__init__.py'))): \n\t \t \t \tcontinue \n\t \t \text_name = ('%s%s.%s.extension' % (package, relpkg, dname)) \n\t \t \ttry: \n\t \t \t \text = importutils.import_class(ext_name) \n\t \t \texcept ImportError: \n\t \t \t \tsubdirs.append(dname) \n\t \t \telse: \n\t \t \t \ttry: \n\t \t \t \t \text(ext_mgr) \n\t \t \t \texcept Exception as exc: \n\t \t \t \t \tlogger.warning(_LW('Failed \tto \tload \textension \t%(ext_name)s: \t%(exc)s'), {'ext_name': ext_name, 'exc': exc}) \n\t \tdirnames[:] = subdirs\n", 
" \ttask_map = _STATE_MAP.get(vm_state, dict(default='UNKNOWN')) \n\tstatus = task_map.get(task_state, task_map['default']) \n\tif (status == 'UNKNOWN'): \n\t \tLOG.error(_LE('status \tis \tUNKNOWN \tfrom \tvm_state=%(vm_state)s \ttask_state=%(task_state)s. \tBad \tupgrade \tor \tdb \tcorrupted?'), {'vm_state': vm_state, 'task_state': task_state}) \n\treturn status\n", 
" \tvm_states = set() \n\ttask_states = set() \n\tlower_statuses = [status.lower() for status in statuses] \n\tfor (state, task_map) in _STATE_MAP.items(): \n\t \tfor (task_state, mapped_state) in task_map.items(): \n\t \t \tstatus_string = mapped_state \n\t \t \tif (status_string.lower() in lower_statuses): \n\t \t \t \tvm_states.add(state) \n\t \t \t \ttask_states.add(task_state) \n\treturn (sorted(vm_states), sorted(task_states))\n", 
" \tmax_limit = _get_pagination_max_limit() \n\tlimit = _get_limit_param(request) \n\tif (max_limit > 0): \n\t \tlimit = (min(max_limit, limit) or max_limit) \n\tif (not limit): \n\t \treturn (None, None) \n\tmarker = request.GET.get('marker', None) \n\treturn (limit, marker)\n", 
" \tlimit = request.GET.get('limit', 0) \n\ttry: \n\t \tlimit = int(limit) \n\t \tif (limit >= 0): \n\t \t \treturn limit \n\texcept ValueError: \n\t \tpass \n\tmsg = (_(\"Limit \tmust \tbe \tan \tinteger \t0 \tor \tgreater \tand \tnot \t'%s'\") % limit) \n\traise exceptions.BadRequest(resource='limit', msg=msg)\n", 
" \treturn request.GET['marker']\n", 
" \tparams = get_pagination_params(request) \n\toffset = params.get('offset', 0) \n\tlimit = CONF.api.max_limit \n\tlimit = min(limit, (params.get('limit') or limit)) \n\treturn items[offset:(offset + limit)]\n", 
" \tparams = get_pagination_params(request) \n\tlimit = CONF.api.max_limit \n\tlimit = min(limit, params.get('limit', limit)) \n\tmarker = params.get('marker', None) \n\treturn (limit, marker)\n", 
" \tmax_limit = (max_limit or CONF.osapi_max_limit) \n\t(marker, limit, __) = get_pagination_params(request.GET.copy(), max_limit) \n\tstart_index = 0 \n\tif marker: \n\t \tstart_index = (-1) \n\t \tfor (i, item) in enumerate(items): \n\t \t \tif ('flavorid' in item): \n\t \t \t \tif (item['flavorid'] == marker): \n\t \t \t \t \tstart_index = (i + 1) \n\t \t \t \t \tbreak \n\t \t \telif ((item['id'] == marker) or (item.get('uuid') == marker)): \n\t \t \t \tstart_index = (i + 1) \n\t \t \t \tbreak \n\t \tif (start_index < 0): \n\t \t \tmsg = (_('marker \t[%s] \tnot \tfound') % marker) \n\t \t \traise webob.exc.HTTPBadRequest(explanation=msg) \n\trange_end = (start_index + limit) \n\treturn items[start_index:range_end]\n", 
" \treturn urlparse.urlsplit(('%s' % href)).path.split('/')[(-1)]\n", 
" \tparsed_url = urllib.parse.urlsplit(href) \n\turl_parts = parsed_url.path.split('/', 2) \n\texpression = re.compile('^v([0-9]+|[0-9]+\\\\.[0-9]+)(/.*|$)') \n\tfor x in range(len(url_parts)): \n\t \tif expression.match(url_parts[x]): \n\t \t \tdel url_parts[x] \n\t \t \tbreak \n\tnew_path = '/'.join(url_parts) \n\tif (new_path == parsed_url.path): \n\t \tmsg = ('href \t%s \tdoes \tnot \tcontain \tversion' % href) \n\t \tLOG.debug(msg) \n\t \traise ValueError(msg) \n\tparsed_url = list(parsed_url) \n\tparsed_url[2] = new_path \n\treturn urllib.parse.urlunsplit(parsed_url)\n", 
" \tnw_info = compute_utils.get_nw_info_for_instance(instance) \n\treturn get_networks_for_instance_from_nw_info(nw_info)\n", 
" \tattr = exc.kwargs.get('attr') \n\tstate = exc.kwargs.get('state') \n\tif ((attr is not None) and (state is not None)): \n\t \tmsg = (_(\"Cannot \t'%(action)s' \tinstance \t%(server_id)s \twhile \tit \tis \tin \t%(attr)s \t%(state)s\") % {'action': action, 'attr': attr, 'state': state, 'server_id': server_id}) \n\telse: \n\t \tmsg = (_(\"Instance \t%(server_id)s \tis \tin \tan \tinvalid \tstate \tfor \t'%(action)s'\") % {'action': action, 'server_id': server_id}) \n\traise webob.exc.HTTPConflict(explanation=msg)\n", 
" \tpool = cons['pool'] \n\tinfo = {'id': cons['id'], 'console_type': pool['console_type']} \n\treturn dict(console=info)\n", 
" \tpool = cons['pool'] \n\tinfo = {'id': cons['id'], 'console_type': pool['console_type']} \n\treturn dict(console=info)\n", 
" \tif context.is_admin: \n\t \tfor key in ('sort_key', 'sort_dir', 'limit', 'marker'): \n\t \t \tsearch_options.pop(key, None) \n\t \treturn \n\tunknown_options = [opt for opt in search_options if (opt not in allowed_search_options)] \n\tif unknown_options: \n\t \tLOG.debug(\"Removing \toptions \t'%s' \tfrom \tquery\", ', \t'.join(unknown_options)) \n\t \tfor opt in unknown_options: \n\t \t \tsearch_options.pop(opt, None)\n", 
" \treturn urllib.parse.unquote(domain).replace('%2E', '.')\n", 
" \td = _translate_volume_summary_view(context, vol) \n\treturn d\n", 
" \td = {} \n\td['id'] = vol.id \n\td['status'] = vol.status \n\td['size'] = vol.size \n\td['availability_zone'] = vol.availability_zone \n\td['created_at'] = vol.created_at \n\td['attach_time'] = '' \n\td['mountpoint'] = '' \n\td['multiattach'] = getattr(vol, 'multiattach', False) \n\tif vol.attachments: \n\t \td['attachments'] = collections.OrderedDict() \n\t \tfor attachment in vol.attachments: \n\t \t \ta = {attachment['server_id']: {'attachment_id': attachment.get('attachment_id'), 'mountpoint': attachment.get('device')}} \n\t \t \td['attachments'].update(a.items()) \n\t \td['attach_status'] = 'attached' \n\telse: \n\t \td['attach_status'] = 'detached' \n\td['display_name'] = vol.name \n\td['display_description'] = vol.description \n\td['volume_type_id'] = vol.volume_type \n\td['snapshot_id'] = vol.snapshot_id \n\td['bootable'] = strutils.bool_from_string(vol.bootable) \n\td['volume_metadata'] = {} \n\tfor (key, value) in vol.metadata.items(): \n\t \td['volume_metadata'][key] = value \n\tif hasattr(vol, 'volume_image_metadata'): \n\t \td['volume_image_metadata'] = copy.deepcopy(vol.volume_image_metadata) \n\treturn d\n", 
" \td = _translate_attachment_summary_view(volume_id, instance_uuid, mountpoint) \n\treturn d\n", 
" \td = {} \n\td['id'] = volume_id \n\td['volumeId'] = volume_id \n\td['serverId'] = instance_uuid \n\tif mountpoint: \n\t \td['device'] = mountpoint \n\treturn d\n", 
" \td = _translate_snapshot_summary_view(context, vol) \n\treturn d\n", 
" \td = {} \n\td['id'] = snapshot.id \n\td['status'] = snapshot.status \n\td['progress'] = snapshot.progress \n\td['size'] = snapshot.size \n\td['created_at'] = snapshot.created_at \n\td['display_name'] = snapshot.name \n\td['display_description'] = snapshot.description \n\td['volume_id'] = snapshot.volume_id \n\td['project_id'] = snapshot.project_id \n\td['volume_size'] = snapshot.size \n\treturn d\n", 
" \tdef wrapped(self, req, id, service=None, *args, **kwargs): \n\t \tlisted_hosts = _list_hosts(req, service) \n\t \thosts = [h['host_name'] for h in listed_hosts] \n\t \tif (id in hosts): \n\t \t \treturn fn(self, req, id, *args, **kwargs) \n\t \traise exception.HostNotFound(host=id) \n\treturn wrapped\n", 
" \treturn {k: v for (k, v) in item.items() if (k in keys)}\n", 
" \treturn {'net_id': port_info['network_id'], 'port_id': port_info['id'], 'mac_addr': port_info['mac_address'], 'port_state': port_info['status'], 'fixed_ips': port_info.get('fixed_ips', None)}\n", 
" \td = {} \n\td['id'] = vif.uuid \n\td['mac_address'] = vif.address \n\tif api_version_request.is_supported(req, min_version='2.12'): \n\t \td['net_id'] = vif.net_uuid \n\tif req.is_legacy_v2(): \n\t \td['OS-EXT-VIF-NET:net_id'] = vif.net_uuid \n\treturn d\n", 
" \tif (value and (value[0] == value[(-1)] == '\"')): \n\t \tvalue = value[1:(-1)] \n\treturn value\n", 
" \tresult = [] \n\tfor item in _parse_list_header(value): \n\t \tif (item[:1] == item[(-1):] == '\"'): \n\t \t \titem = unquote_header_value(item[1:(-1)]) \n\t \tresult.append(item) \n\treturn result\n", 
" \tif (not value): \n\t \treturn ('', {}) \n\tresult = [] \n\tvalue = (',' + value.replace('\\n', ',')) \n\twhile value: \n\t \tmatch = _option_header_start_mime_type.match(value) \n\t \tif (not match): \n\t \t \tbreak \n\t \tresult.append(match.group(1)) \n\t \toptions = {} \n\t \trest = match.group(2) \n\t \twhile rest: \n\t \t \toptmatch = _option_header_piece_re.match(rest) \n\t \t \tif (not optmatch): \n\t \t \t \tbreak \n\t \t \t(option, encoding, _, option_value) = optmatch.groups() \n\t \t \toption = unquote_header_value(option) \n\t \t \tif (option_value is not None): \n\t \t \t \toption_value = unquote_header_value(option_value, (option == 'filename')) \n\t \t \t \tif (encoding is not None): \n\t \t \t \t \toption_value = _unquote(option_value).decode(encoding) \n\t \t \toptions[option] = option_value \n\t \t \trest = rest[optmatch.end():] \n\t \tresult.append(options) \n\t \tif (multiple is False): \n\t \t \treturn tuple(result) \n\t \tvalue = rest \n\treturn (tuple(result) if result else ('', {}))\n", 
" \ttry: \n\t \tfunction(*args, **kwargs) \n\texcept exceptions as e: \n\t \terror_message = str(e) \n\t \tif (message not in error_message): \n\t \t \traise AssertionError(('Error \tmessage \tdoes \tnot \tinclude \tthe \texpected \tstring: \t%r. \tObserved \terror \tmessage: \t%r' % (message, error_message))) \n\telse: \n\t \tif isinstance(exceptions, tuple): \n\t \t \tnames = ' \tor \t'.join((e.__name__ for e in exceptions)) \n\t \telse: \n\t \t \tnames = exceptions.__name__ \n\t \traise AssertionError(('%s \tnot \traised \tby \t%s' % (names, function.__name__)))\n", 
" \tif (image_type == 'kernel'): \n\t \treturn 'aki' \n\tif (image_type == 'ramdisk'): \n\t \treturn 'ari' \n\tif (image_type not in ['aki', 'ari']): \n\t \treturn 'ami' \n\treturn image_type\n", 
" \treturn objects.S3ImageMapping.get_by_id(context, image_id).uuid\n", 
" \treturn objects.S3ImageMapping.get_by_id(context, image_id).uuid\n", 
" \ttry: \n\t \treturn int(ec2_id.split('-')[(-1)], 16) \n\texcept ValueError: \n\t \traise exception.InvalidEc2Id(ec2_id=ec2_id)\n", 
" \ttemplate = (image_type + '-%08x') \n\treturn id_to_ec2_id(image_id, template=template)\n", 
" \tif isinstance(instance, obj_base.NovaObject): \n\t \tnw_info = instance.info_cache.network_info \n\telse: \n\t \tinfo_cache = (instance.info_cache or {}) \n\t \tnw_info = info_cache.get('network_info') \n\tif (not nw_info): \n\t \tnw_info = [] \n\treturn get_ip_info_for_instance_from_nw_info(nw_info)\n", 
" \ttry: \n\t \treturn int(ec2_id.split('-')[(-1)], 16) \n\texcept ValueError: \n\t \traise exception.InvalidEc2Id(ec2_id=ec2_id)\n", 
" \tif (instance_id is None): \n\t \treturn None \n\telif uuidutils.is_uuid_like(instance_id): \n\t \tctxt = context.get_admin_context() \n\t \tint_id = get_int_id_from_instance_uuid(ctxt, instance_id) \n\t \treturn id_to_ec2_id(int_id) \n\telse: \n\t \treturn id_to_ec2_id(instance_id)\n", 
" \tint_id = ec2_id_to_id(ec2_id) \n\treturn get_instance_uuid_from_int_id(context, int_id)\n", 
" \tif uuidutils.is_uuid_like(volume_id): \n\t \tctxt = context.get_admin_context() \n\t \tint_id = get_int_id_from_volume_uuid(ctxt, volume_id) \n\t \treturn id_to_ec2_id(int_id, 'vol-%08x') \n\telse: \n\t \treturn id_to_ec2_id(volume_id, 'vol-%08x')\n", 
" \tif uuidutils.is_uuid_like(volume_id): \n\t \tctxt = context.get_admin_context() \n\t \tint_id = get_int_id_from_volume_uuid(ctxt, volume_id) \n\t \treturn id_to_ec2_id(int_id, 'vol-%08x') \n\telse: \n\t \treturn id_to_ec2_id(volume_id, 'vol-%08x')\n", 
" \tctxt = context.get_admin_context() \n\tint_id = ec2_id_to_id(ec2_id) \n\treturn get_snapshot_uuid_from_int_id(ctxt, int_id)\n", 
" \ttimestamp = request.get('Timestamp') \n\texpiry_time = request.get('Expires') \n\tdef parse_strtime(strtime): \n\t \tif _ms_time_regex.match(strtime): \n\t \t \ttime_format = '%Y-%m-%dT%H:%M:%S.%fZ' \n\t \telse: \n\t \t \ttime_format = '%Y-%m-%dT%H:%M:%SZ' \n\t \treturn timeutils.parse_strtime(strtime, time_format) \n\ttry: \n\t \tif (timestamp and expiry_time): \n\t \t \tmsg = _('Request \tmust \tinclude \teither \tTimestamp \tor \tExpires, \tbut \tcannot \tcontain \tboth') \n\t \t \tLOG.error(msg) \n\t \t \traise exception.InvalidRequest(msg) \n\t \telif expiry_time: \n\t \t \tquery_time = parse_strtime(expiry_time) \n\t \t \treturn timeutils.is_older_than(query_time, (-1)) \n\t \telif timestamp: \n\t \t \tquery_time = parse_strtime(timestamp) \n\t \t \tif (query_time and expires): \n\t \t \t \treturn (timeutils.is_older_than(query_time, expires) or timeutils.is_newer_than(query_time, expires)) \n\t \treturn False \n\texcept ValueError: \n\t \tLOG.info(_LI('Timestamp \tis \tinvalid.')) \n\t \treturn True\n", 
" \tctxt = context.get_admin_context() \n\tint_id = ec2_id_to_id(ec2_id) \n\treturn get_snapshot_uuid_from_int_id(ctxt, int_id)\n", 
" \tdef _negative_zero(value): \n\t \tepsilon = 1e-07 \n\t \treturn (0 if (abs(value) < epsilon) else value) \n\tif (len(value) == 0): \n\t \treturn '' \n\tif (value == 'None'): \n\t \treturn None \n\tlowered_value = value.lower() \n\tif (lowered_value == 'true'): \n\t \treturn True \n\tif (lowered_value == 'false'): \n\t \treturn False \n\tfor (prefix, base) in [('0x', 16), ('0b', 2), ('0', 8), ('', 10)]: \n\t \ttry: \n\t \t \tif lowered_value.startswith((prefix, ('-' + prefix))): \n\t \t \t \treturn int(lowered_value, base) \n\t \texcept ValueError: \n\t \t \tpass \n\ttry: \n\t \treturn _negative_zero(float(value)) \n\texcept ValueError: \n\t \treturn value\n", 
" \targs = {} \n\tfor (key, value) in items: \n\t \tparts = key.split('.') \n\t \tkey = str(camelcase_to_underscore(parts[0])) \n\t \tif isinstance(value, six.string_types): \n\t \t \tvalue = _try_convert(value) \n\t \t \tif (len(parts) > 1): \n\t \t \t \td = args.get(key, {}) \n\t \t \t \targs[key] = d \n\t \t \t \tfor k in parts[1:(-1)]: \n\t \t \t \t \tk = camelcase_to_underscore(k) \n\t \t \t \t \tv = d.get(k, {}) \n\t \t \t \t \td[k] = v \n\t \t \t \t \td = v \n\t \t \t \td[camelcase_to_underscore(parts[(-1)])] = value \n\t \t \telse: \n\t \t \t \targs[key] = value \n\treturn args\n", 
" \tvm_states = set() \n\ttask_states = set() \n\tlower_statuses = [status.lower() for status in statuses] \n\tfor (state, task_map) in _STATE_MAP.items(): \n\t \tfor (task_state, mapped_state) in task_map.items(): \n\t \t \tstatus_string = mapped_state \n\t \t \tif (status_string.lower() in lower_statuses): \n\t \t \t \tvm_states.add(state) \n\t \t \t \ttask_states.add(task_state) \n\treturn (sorted(vm_states), sorted(task_states))\n", 
" \tret = OrderedDict() \n\tfor (key, value) in choices.items(): \n\t \tif isinstance(value, dict): \n\t \t \tfor (sub_key, sub_value) in value.items(): \n\t \t \t \tret[sub_key] = sub_value \n\t \telse: \n\t \t \tret[key] = value \n\treturn ret\n", 
" \tbase_string = utils.escape(http_method.upper()) \n\tbase_string += u'&' \n\tbase_string += utils.escape(base_string_uri) \n\tbase_string += u'&' \n\tbase_string += utils.escape(normalized_encoded_request_parameters) \n\treturn base_string\n", 
" \treturn (a * b)\n", 
" \treturn datetime.strptime(timestamp, '%Y-%m-%d \t%H:%M:%S').isoformat()\n", 
" \tpassword = (password or '') \n\tif (six.PY3 and isinstance(password, bytes)): \n\t \tpassword = password.decode('utf-8') \n\tmeta = {} \n\tfor i in range(CHUNKS): \n\t \tmeta[('password_%d' % i)] = password[:CHUNK_LENGTH] \n\t \tpassword = password[CHUNK_LENGTH:] \n\treturn meta\n", 
" \treturn _load_pipeline(loader, local_conf[CONF.api.auth_strategy].split())\n", 
" \tif (not progs): \n\t \treturn True \n\tfor prog in progs: \n\t \tif prog.search(six.text_type(value)): \n\t \t \treturn True \n\treturn False\n", 
" \t(ret, id_matched) = (False, None) \n\tcid = _cron_id(cron) \n\tif cid: \n\t \tif (not identifier): \n\t \t \tidentifier = SALT_CRON_NO_IDENTIFIER \n\t \teidentifier = _encode(identifier) \n\t \tif ((cron.get('cmd', None) != cmd) and (cid == SALT_CRON_NO_IDENTIFIER) and (eidentifier == SALT_CRON_NO_IDENTIFIER)): \n\t \t \tid_matched = False \n\t \telse: \n\t \t \tif ((cron.get('cmd', None) == cmd) and (cid == SALT_CRON_NO_IDENTIFIER) and identifier): \n\t \t \t \tcid = eidentifier \n\t \t \tid_matched = (eidentifier == cid) \n\tif (((id_matched is None) and (cmd == cron.get('cmd', None))) or id_matched): \n\t \tret = True \n\treturn ret\n", 
" \t(valid, _value) = (False, value) \n\tif dtype: \n\t \ttry: \n\t \t \t_value = dtype(value) \n\t \t \tvalid = (_value in within) \n\t \texcept ValueError: \n\t \t \tpass \n\telse: \n\t \tvalid = (_value in within) \n\tif (errmsg is None): \n\t \tif dtype: \n\t \t \ttypename = getattr(dtype, '__name__', (hasattr(dtype, '__class__') and getattr(dtype.__class__, 'name', dtype))) \n\t \t \terrmsg = \"{0} \twithin \t'{1}'\".format(typename, within) \n\t \telse: \n\t \t \terrmsg = \"within \t'{0}'\".format(within) \n\treturn (valid, _value, errmsg)\n", 
" \tsys.stdout.flush() \n\tsys.stderr.flush() \n\tclose_fds = (sys.platform != 'win32') \n\tshell = isinstance(cmd, str) \n\texecutable = None \n\tif (shell and (os.name == 'posix') and ('SHELL' in os.environ)): \n\t \texecutable = os.environ['SHELL'] \n\tp = subprocess.Popen(cmd, shell=shell, executable=executable, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=stderr, close_fds=close_fds) \n\ttry: \n\t \tout = callback(p) \n\texcept KeyboardInterrupt: \n\t \tprint '^C' \n\t \tsys.stdout.flush() \n\t \tsys.stderr.flush() \n\t \tout = None \n\tfinally: \n\t \tif (p.returncode is None): \n\t \t \ttry: \n\t \t \t \tp.terminate() \n\t \t \t \tp.poll() \n\t \t \texcept OSError: \n\t \t \t \tpass \n\t \tif (p.returncode is None): \n\t \t \ttry: \n\t \t \t \tp.kill() \n\t \t \texcept OSError: \n\t \t \t \tpass \n\treturn out\n", 
" \ttry: \n\t \tuuid.UUID(value) \n\t \treturn value \n\texcept (ValueError, AttributeError): \n\t \treturn int(value)\n", 
" \t(orig_exc_type, orig_exc_value, orig_exc_traceback) = sys.exc_info() \n\tif isinstance(new_exc, six.string_types): \n\t \tnew_exc = orig_exc_type(new_exc) \n\tif hasattr(new_exc, 'args'): \n\t \tif (len(new_exc.args) > 0): \n\t \t \tnew_message = ', \t'.join((str(arg) for arg in new_exc.args)) \n\t \telse: \n\t \t \tnew_message = '' \n\t \tnew_message += ('\\n\\nOriginal \texception:\\n DCTB ' + orig_exc_type.__name__) \n\t \tif (hasattr(orig_exc_value, 'args') and (len(orig_exc_value.args) > 0)): \n\t \t \tif getattr(orig_exc_value, 'reraised', False): \n\t \t \t \tnew_message += (': \t' + str(orig_exc_value.args[0])) \n\t \t \telse: \n\t \t \t \tnew_message += (': \t' + ', \t'.join((str(arg) for arg in orig_exc_value.args))) \n\t \tnew_exc.args = ((new_message,) + new_exc.args[1:]) \n\tnew_exc.__cause__ = orig_exc_value \n\tnew_exc.reraised = True \n\tsix.reraise(type(new_exc), new_exc, orig_exc_traceback)\n", 
" \tret = _ConvertToList(arg) \n\tfor element in ret: \n\t \tif (not isinstance(element, element_type)): \n\t \t \traise TypeError(('%s \tshould \tbe \tsingle \telement \tor \tlist \tof \ttype \t%s' % (arg_name, element_type))) \n\treturn ret\n", 
" \tlogger = logging.getLogger() \n\tloglevel = get_loglevel((loglevel or u'ERROR')) \n\tlogfile = (logfile if logfile else sys.__stderr__) \n\tif (not logger.handlers): \n\t \tif hasattr(logfile, u'write'): \n\t \t \thandler = logging.StreamHandler(logfile) \n\t \telse: \n\t \t \thandler = WatchedFileHandler(logfile) \n\t \tlogger.addHandler(handler) \n\t \tlogger.setLevel(loglevel) \n\treturn logger\n", 
" \treturn ((module in sys.modules) and isinstance(obj, getattr(import_module(module), class_name)))\n", 
" \tvalidate_config_version(config_details.config_files) \n\tprocessed_files = [process_config_file(config_file, config_details.environment) for config_file in config_details.config_files] \n\tconfig_details = config_details._replace(config_files=processed_files) \n\tmain_file = config_details.config_files[0] \n\tvolumes = load_mapping(config_details.config_files, u'get_volumes', u'Volume') \n\tnetworks = load_mapping(config_details.config_files, u'get_networks', u'Network') \n\tservice_dicts = load_services(config_details, main_file) \n\tif (main_file.version != V1): \n\t \tfor service_dict in service_dicts: \n\t \t \tmatch_named_volumes(service_dict, volumes) \n\tservices_using_deploy = [s for s in service_dicts if s.get(u'deploy')] \n\tif services_using_deploy: \n\t \tlog.warn(u\"Some \tservices \t({}) \tuse \tthe \t'deploy' \tkey, \twhich \twill \tbe \tignored. \tCompose \tdoes \tnot \tsupport \tdeploy \tconfiguration \t- \tuse \t`docker \tstack \tdeploy` \tto \tdeploy \tto \ta \tswarm.\".format(u', \t'.join(sorted((s[u'name'] for s in services_using_deploy))))) \n\treturn Config(main_file.version, service_dicts, volumes, networks)\n", 
" \ttry: \n\t \tvalue = args_list.pop(0) \n\texcept IndexError: \n\t \traise BadCommandUsage(msg) \n\tif ((expected_size_after is not None) and (len(args_list) > expected_size_after)): \n\t \traise BadCommandUsage('too \tmany \targuments') \n\treturn value\n", 
" \treturn datetime.datetime.strptime(d['isostr'], _DATETIME_FORMAT)\n", 
" \treturn _aliases.get(alias.lower(), alias)\n", 
" \twith open(CHANGELOG) as f: \n\t \tcur_index = 0 \n\t \tfor line in f: \n\t \t \tmatch = re.search('^\\\\d+\\\\.\\\\d+\\\\.\\\\d+', line) \n\t \t \tif match: \n\t \t \t \tif (cur_index == index): \n\t \t \t \t \treturn match.group(0) \n\t \t \t \telse: \n\t \t \t \t \tcur_index += 1\n", 
" \tgitstr = _git_str() \n\tif (gitstr is None): \n\t \tgitstr = '' \n\tpath = os.path.join(BASEDIR, 'qutebrowser', 'git-commit-id') \n\twith _open(path, 'w', encoding='ascii') as f: \n\t \tf.write(gitstr)\n", 
" \tif (len(sys.argv) < 2): \n\t \treturn True \n\tinfo_commands = ['--help-commands', '--name', '--version', '-V', '--fullname', '--author', '--author-email', '--maintainer', '--maintainer-email', '--contact', '--contact-email', '--url', '--license', '--description', '--long-description', '--platforms', '--classifiers', '--keywords', '--provides', '--requires', '--obsoletes'] \n\tinfo_commands.extend(['egg_info', 'install_egg_info', 'rotate']) \n\tfor command in info_commands: \n\t \tif (command in sys.argv[1:]): \n\t \t \treturn False \n\treturn True\n", 
" \trefs = list_refs(refnames=[refname], repo_dir=repo_dir, limit_to_heads=True) \n\tl = tuple(islice(refs, 2)) \n\tif l: \n\t \tassert (len(l) == 1) \n\t \treturn l[0][1] \n\telse: \n\t \treturn None\n", 
" \tif isinstance(fileIshObject, TextIOBase): \n\t \treturn unicode \n\tif isinstance(fileIshObject, IOBase): \n\t \treturn bytes \n\tencoding = getattr(fileIshObject, 'encoding', None) \n\timport codecs \n\tif isinstance(fileIshObject, (codecs.StreamReader, codecs.StreamWriter)): \n\t \tif encoding: \n\t \t \treturn unicode \n\t \telse: \n\t \t \treturn bytes \n\tif (not _PY3): \n\t \tif isinstance(fileIshObject, file): \n\t \t \tif (encoding is not None): \n\t \t \t \treturn basestring \n\t \t \telse: \n\t \t \t \treturn bytes \n\t \tfrom cStringIO import InputType, OutputType \n\t \tfrom StringIO import StringIO \n\t \tif isinstance(fileIshObject, (StringIO, InputType, OutputType)): \n\t \t \treturn bytes \n\treturn default\n", 
" \tcmd = (_pkg(jail, chroot, root) + ['--version']) \n\treturn __salt__['cmd.run'](cmd).strip()\n", 
" \tcmd = (_pkg(jail, chroot, root) + ['--version']) \n\treturn __salt__['cmd.run'](cmd).strip()\n", 
" \thost = node \n\tport = 27017 \n\tidx = node.rfind(':') \n\tif (idx != (-1)): \n\t \t(host, port) = (node[:idx], int(node[(idx + 1):])) \n\tif host.startswith('['): \n\t \thost = host[1:(-1)] \n\treturn (host, port)\n", 
" \tconn = _get_conn(region=region, key=key, keyid=keyid, profile=profile) \n\tpolicy_arn = _get_policy_arn(policy_name, region, key, keyid, profile) \n\ttry: \n\t \tconn.set_default_policy_version(policy_arn, version_id) \n\t \tlog.info('Set \t{0} \tpolicy \tto \tversion \t{1}.'.format(policy_name, version_id)) \n\texcept boto.exception.BotoServerError as e: \n\t \taws = __utils__['boto.get_error'](e) \n\t \tlog.debug(aws) \n\t \tmsg = 'Failed \tto \tset \t{0} \tpolicy \tto \tversion \t{1}: \t{2}' \n\t \tlog.error(msg.format(policy_name, version_id, aws.get('message'))) \n\t \treturn False \n\treturn True\n", 
" \tvalidate = attr['validate'] \n\tkey = [k for k in validate.keys() if k.startswith('type:dict')] \n\tif (not key): \n\t \tLOG.warning(_LW('Unable \tto \tfind \tdata \ttype \tdescriptor \tfor \tattribute \t%s'), attr_name) \n\t \treturn \n\tdata = validate[key[0]] \n\tif (not isinstance(data, dict)): \n\t \tLOG.debug('Attribute \ttype \tdescriptor \tis \tnot \ta \tdict. \tUnable \tto \tgenerate \tany \tsub-attr \tpolicy \trule \tfor \t%s.', attr_name) \n\t \treturn \n\tsub_attr_rules = [policy.RuleCheck('rule', ('%s:%s:%s' % (action, attr_name, sub_attr_name))) for sub_attr_name in data if (sub_attr_name in target[attr_name])] \n\treturn policy.AndCheck(sub_attr_rules)\n", 
" \trule_method = ('telemetry:' + policy_name) \n\theaders = request.headers \n\tpolicy_dict = dict() \n\tpolicy_dict['roles'] = headers.get('X-Roles', '').split(',') \n\tpolicy_dict['user_id'] = headers.get('X-User-Id') \n\tpolicy_dict['project_id'] = headers.get('X-Project-Id') \n\tif ((_has_rule('default') or _has_rule(rule_method)) and (not pecan.request.enforcer.enforce(rule_method, {}, policy_dict))): \n\t \tpecan.core.abort(status_code=403, detail='RBAC \tAuthorization \tFailed')\n", 
" \ttry: \n\t \ttree_gen = parse(file, format, **kwargs) \n\t \ttree = next(tree_gen) \n\texcept StopIteration: \n\t \traise ValueError('There \tare \tno \ttrees \tin \tthis \tfile.') \n\ttry: \n\t \tnext(tree_gen) \n\texcept StopIteration: \n\t \treturn tree \n\telse: \n\t \traise ValueError('There \tare \tmultiple \ttrees \tin \tthis \tfile; \tuse \tparse() \tinstead.')\n", 
" \tfrom django.contrib.syndication.feeds import Feed as LegacyFeed \n\timport warnings \n\twarnings.warn('The \tsyndication \tfeed() \tview \tis \tdeprecated. \tPlease \tuse \tthe \tnew \tclass \tbased \tview \tAPI.', category=PendingDeprecationWarning) \n\tif (not feed_dict): \n\t \traise Http404('No \tfeeds \tare \tregistered.') \n\ttry: \n\t \t(slug, param) = url.split('/', 1) \n\texcept ValueError: \n\t \t(slug, param) = (url, '') \n\ttry: \n\t \tf = feed_dict[slug] \n\texcept KeyError: \n\t \traise Http404((\"Slug \t%r \tisn't \tregistered.\" % slug)) \n\tif (not issubclass(f, LegacyFeed)): \n\t \tinstance = f() \n\t \tinstance.feed_url = (getattr(f, 'feed_url', None) or request.path) \n\t \tinstance.title_template = (f.title_template or ('feeds/%s_title.html' % slug)) \n\t \tinstance.description_template = (f.description_template or ('feeds/%s_description.html' % slug)) \n\t \treturn instance(request) \n\ttry: \n\t \tfeedgen = f(slug, request).get_feed(param) \n\texcept FeedDoesNotExist: \n\t \traise Http404(('Invalid \tfeed \tparameters. \tSlug \t%r \tis \tvalid, \tbut \tother \tparameters, \tor \tlack \tthereof, \tare \tnot.' % slug)) \n\tresponse = HttpResponse(mimetype=feedgen.mime_type) \n\tfeedgen.write(response, 'utf-8') \n\treturn response\n", 
" \tret = {} \n\tcmd = __execute_kadmin('list_policies') \n\tif ((cmd['retcode'] != 0) or cmd['stderr']): \n\t \tret['comment'] = cmd['stderr'].splitlines()[(-1)] \n\t \tret['result'] = False \n\t \treturn ret \n\tret = {'policies': []} \n\tfor i in cmd['stdout'].splitlines()[1:]: \n\t \tret['policies'].append(i) \n\treturn ret\n", 
" \tdef call_and_assert(arg, context=None): \n\t \tif (context is None): \n\t \t \tcontext = {} \n\t \tresult = function(arg, context=context) \n\t \tassert (result == arg), 'Should \treturn \tthe \targument \tthat \twas \tpassed \tto \tit, \tunchanged \t({arg})'.format(arg=repr(arg)) \n\t \treturn result \n\treturn call_and_assert\n", 
" \tif (not app_messages): \n\t \tapp_messages = get_messages_for_app(app) \n\tif (not app_messages): \n\t \treturn \n\ttpath = frappe.get_pymodule_path(app, u'translations') \n\tfrappe.create_folder(tpath) \n\twrite_csv_file(os.path.join(tpath, (lang + u'.csv')), app_messages, (full_dict or get_full_dict(lang)))\n", 
" \ttb = treebuilders.getTreeBuilder(treebuilder) \n\tp = HTMLParser(tb, namespaceHTMLElements=namespaceHTMLElements) \n\treturn p.parse(doc, encoding=encoding)\n", 
" \tfrom config import get_os, get_checks_places, get_valid_check_class \n\tosname = get_os() \n\tchecks_places = get_checks_places(osname, agentConfig) \n\tfor check_path_builder in checks_places: \n\t \tcheck_path = check_path_builder(check_name) \n\t \tif (not os.path.exists(check_path)): \n\t \t \tcontinue \n\t \t(check_is_valid, check_class, load_failure) = get_valid_check_class(check_name, check_path) \n\t \tif check_is_valid: \n\t \t \treturn check_class \n\tlog.warning(('Failed \tto \tload \tthe \tcheck \tclass \tfor \t%s.' % check_name)) \n\treturn None\n", 
" \targspec = inspect.getargspec(fn) \n\tnum_defaults = len((argspec.defaults or [])) \n\trequired_args = argspec.args[:(len(argspec.args) - num_defaults)] \n\tif (six.get_method_self(fn) is not None): \n\t \trequired_args.pop(0) \n\tmissing = [arg for arg in required_args if (arg not in kwargs)] \n\tmissing = missing[len(args):] \n\treturn missing\n", 
" \tdef wrapper(func): \n\t \t@functools.wraps(func) \n\t \tdef inner(*args, **kwds): \n\t \t \tlock.acquire() \n\t \t \ttry: \n\t \t \t \treturn func(*args, **kwds) \n\t \t \tfinally: \n\t \t \t \tlock.release() \n\t \treturn inner \n\treturn wrapper\n", 
" \tkey = key.replace('-', '_') \n\tif ((key.upper() == 'ALL') and (val.upper() == 'DEFAULT')): \n\t \tfor ci in config: \n\t \t \tconfig[ci].value = config[ci].default \n\t \tconfig.save() \n\t \tmessage = 'Default \tconfiguration \treinstated' \n\telif (not (key.upper() in config)): \n\t \tmessage = ('Unknown \tconfig \titem: \t%s%s%s' % (c.r, key, c.w)) \n\telif (val.upper() == 'DEFAULT'): \n\t \tatt = config[key.upper()] \n\t \tatt.value = att.default \n\t \tmessage = '%s%s%s \tset \tto \t%s%s%s \t(default)' \n\t \tdispval = (att.display or 'None') \n\t \tmessage = (message % (c.y, key, c.w, c.y, dispval, c.w)) \n\t \tconfig.save() \n\telse: \n\t \tmessage = config[key.upper()].set(val) \n\tshowconfig() \n\tg.message = message\n", 
" \tsession = Session.object_session(series) \n\treleases = session.query(Episode).join(Episode.releases, Episode.series).filter((Series.id == series.id)) \n\tif downloaded: \n\t \treleases = releases.filter((Release.downloaded == True)) \n\tif (season is not None): \n\t \treleases = releases.filter((Episode.season == season)) \n\tif (series.identified_by and (series.identified_by != u'auto')): \n\t \treleases = releases.filter((Episode.identified_by == series.identified_by)) \n\tif (series.identified_by in [u'ep', u'sequence']): \n\t \tlatest_release = releases.order_by(desc(Episode.season), desc(Episode.number)).first() \n\telif (series.identified_by == u'date'): \n\t \tlatest_release = releases.order_by(desc(Episode.identifier)).first() \n\telse: \n\t \tlatest_release = releases.order_by(desc(Episode.first_seen.label(u'ep_first_seen'))).first() \n\tif (not latest_release): \n\t \tlog.debug(u'get_latest_release \treturning \tNone, \tno \tdownloaded \tepisodes \tfound \tfor: \t%s', series.name) \n\t \treturn \n\treturn latest_release\n", 
" \ttry: \n\t \tfunc(*args) \n\texcept exc: \n\t \treturn True \n\telse: \n\t \treturn False\n", 
" \traise exception\n", 
" \tif hasattr(obj, 'bind'): \n\t \tconn = obj.bind \n\telse: \n\t \ttry: \n\t \t \tconn = object_session(obj).bind \n\t \texcept UnmappedInstanceError: \n\t \t \tconn = obj \n\tif (not hasattr(conn, 'execute')): \n\t \traise TypeError('This \tmethod \taccepts \tonly \tSession, \tEngine, \tConnection \tand \tdeclarative \tmodel \tobjects.') \n\treturn conn\n", 
" \tqueue_dir = __opts__['sqlite_queue_dir'] \n\tdb = os.path.join(queue_dir, '{0}.db'.format(queue)) \n\tlog.debug('Connecting \tto: \t \t{0}'.format(db)) \n\tcon = lite.connect(db) \n\ttables = _list_tables(con) \n\tif (queue not in tables): \n\t \t_create_table(con, queue) \n\treturn con\n", 
" \tqueue_dir = __opts__['sqlite_queue_dir'] \n\tdb = os.path.join(queue_dir, '{0}.db'.format(queue)) \n\tlog.debug('Connecting \tto: \t \t{0}'.format(db)) \n\tcon = lite.connect(db) \n\ttables = _list_tables(con) \n\tif (queue not in tables): \n\t \t_create_table(con, queue) \n\treturn con\n", 
" \tmsg = 'N340: \tUse \tnova.utils.%(spawn)s() \trather \tthan \tgreenthread.%(spawn)s() \tand \teventlet.%(spawn)s()' \n\tif (('nova/utils.py' in filename) or ('nova/tests/' in filename)): \n\t \treturn \n\tmatch = re.match(spawn_re, logical_line) \n\tif match: \n\t \t(yield (0, (msg % {'spawn': match.group('spawn_part')})))\n", 
" \tdefaults = {'host': 'salt', 'user': 'salt', 'pass': 'salt', 'db': 'salt', 'port': 3306, 'ssl_ca': None, 'ssl_cert': None, 'ssl_key': None} \n\tattrs = {'host': 'host', 'user': 'user', 'pass': 'pass', 'db': 'db', 'port': 'port', 'ssl_ca': 'ssl_ca', 'ssl_cert': 'ssl_cert', 'ssl_key': 'ssl_key'} \n\t_options = salt.returners.get_returner_options(__virtualname__, ret, attrs, __salt__=__salt__, __opts__=__opts__, defaults=defaults) \n\tfor (k, v) in _options.iteritems(): \n\t \tif (isinstance(v, string_types) and (v.lower() == 'none')): \n\t \t \t_options[k] = None \n\t \tif (k == 'port'): \n\t \t \t_options[k] = int(v) \n\treturn _options\n", 
" \treturn (file_path in _db_content.get('files'))\n", 
" \tstrategy = kwargs.pop('strategy', default_strategy) \n\tstrategy = strategies.strategies[strategy] \n\treturn strategy.create(*args, **kwargs)\n", 
" \tif hasattr(obj, 'bind'): \n\t \tconn = obj.bind \n\telse: \n\t \ttry: \n\t \t \tconn = object_session(obj).bind \n\t \texcept UnmappedInstanceError: \n\t \t \tconn = obj \n\tif (not hasattr(conn, 'execute')): \n\t \traise TypeError('This \tmethod \taccepts \tonly \tSession, \tEngine, \tConnection \tand \tdeclarative \tmodel \tobjects.') \n\treturn conn\n", 
" \treturn ''.join(traceback.format_stack())\n", 
" \tif ('id' not in sort_keys): \n\t \tLOG.warning(_LW('Id \tnot \tin \tsort_keys; \tis \tsort_keys \tunique?')) \n\tassert (not (sort_dir and sort_dirs)) \n\tif ((sort_dirs is None) and (sort_dir is None)): \n\t \tsort_dir = 'asc' \n\tif (sort_dirs is None): \n\t \tsort_dirs = [sort_dir for _sort_key in sort_keys] \n\tassert (len(sort_dirs) == len(sort_keys)) \n\tfor (current_sort_key, current_sort_dir) in zip(sort_keys, sort_dirs): \n\t \tsort_dir_func = {'asc': sqlalchemy.asc, 'desc': sqlalchemy.desc}[current_sort_dir] \n\t \ttry: \n\t \t \tsort_key_attr = getattr(model, current_sort_key) \n\t \texcept AttributeError: \n\t \t \traise exception.InvalidInput(reason='Invalid \tsort \tkey') \n\t \tif (not api.is_orm_value(sort_key_attr)): \n\t \t \traise exception.InvalidInput(reason='Invalid \tsort \tkey') \n\t \tquery = query.order_by(sort_dir_func(sort_key_attr)) \n\tif (marker is not None): \n\t \tmarker_values = [] \n\t \tfor sort_key in sort_keys: \n\t \t \tv = getattr(marker, sort_key) \n\t \t \tmarker_values.append(v) \n\t \tcriteria_list = [] \n\t \tfor i in range(0, len(sort_keys)): \n\t \t \tcrit_attrs = [] \n\t \t \tfor j in range(0, i): \n\t \t \t \tmodel_attr = getattr(model, sort_keys[j]) \n\t \t \t \tcrit_attrs.append((model_attr == marker_values[j])) \n\t \t \tmodel_attr = getattr(model, sort_keys[i]) \n\t \t \tif (sort_dirs[i] == 'desc'): \n\t \t \t \tcrit_attrs.append((model_attr < marker_values[i])) \n\t \t \telif (sort_dirs[i] == 'asc'): \n\t \t \t \tcrit_attrs.append((model_attr > marker_values[i])) \n\t \t \telse: \n\t \t \t \traise ValueError(_(\"Unknown \tsort \tdirection, \tmust \tbe \t'desc' \tor \t'asc'\")) \n\t \t \tcriteria = sqlalchemy.sql.and_(*crit_attrs) \n\t \t \tcriteria_list.append(criteria) \n\t \tf = sqlalchemy.sql.or_(*criteria_list) \n\t \tquery = query.filter(f) \n\tif (limit is not None): \n\t \tquery = query.limit(limit) \n\tif offset: \n\t \tquery = query.offset(offset) \n\treturn query\n", 
" \treturn auth_is_loggedin_user()\n", 
" \tif ((r.representation == 'html') and (r.name == 'shelter') and r.id and (not r.component)): \n\t \tT = current.T \n\t \tmsg = current.msg \n\t \ts3db = current.s3db \n\t \trecord = r.record \n\t \tctable = s3db.pr_contact \n\t \tstable = s3db.cr_shelter \n\t \tmessage = '' \n\t \ttext = '' \n\t \ts_id = record.id \n\t \ts_name = record.name \n\t \ts_phone = record.phone \n\t \ts_email = record.email \n\t \ts_status = record.status \n\t \tif (s_phone in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_email in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_status in ('', None)): \n\t \t \ts_status = T('Not \tDefined') \n\t \telif (s_status == 1): \n\t \t \ts_status = 'Open' \n\t \telif (s_status == 2): \n\t \t \ts_status = 'Close' \n\t \telse: \n\t \t \ts_status = 'Unassigned \tShelter \tStatus' \n\t \ttext += '************************************************' \n\t \ttext += ('\\n%s \t' % T('Automatic \tMessage')) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Shelter \tID'), s_id)) \n\t \ttext += (' \t%s: \t%s' % (T('Shelter \tname'), s_name)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Email'), s_email)) \n\t \ttext += (' \t%s: \t%s' % (T('Phone'), s_phone)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Working \tStatus'), s_status)) \n\t \ttext += '\\n************************************************\\n' \n\t \turl = URL(c='cr', f='shelter', args=r.id) \n\t \topts = dict(type='SMS', subject=T('Deployment \tRequest'), message=(message + text), url=url) \n\t \toutput = msg.compose(**opts) \n\t \tif attr.get('rheader'): \n\t \t \trheader = attr['rheader'](r) \n\t \t \tif rheader: \n\t \t \t \toutput['rheader'] = rheader \n\t \toutput['title'] = T('Send \tNotification') \n\t \tcurrent.response.view = 'msg/compose.html' \n\t \treturn output \n\telse: \n\t \traise HTTP(501, current.messages.BADMETHOD)\n", 
" \tcan_users_receive_email = email_manager.can_users_receive_thread_email(recipient_list, exploration_id, has_suggestion) \n\tfor (index, recipient_id) in enumerate(recipient_list): \n\t \tif can_users_receive_email[index]: \n\t \t \ttransaction_services.run_in_transaction(_enqueue_feedback_thread_status_change_email_task, recipient_id, feedback_message_reference, old_status, new_status)\n", 
" \tglobal _notifier \n\tif (_notifier is None): \n\t \thost = (CONF.default_publisher_id or socket.gethostname()) \n\t \ttry: \n\t \t \ttransport = oslo_messaging.get_notification_transport(CONF) \n\t \t \t_notifier = oslo_messaging.Notifier(transport, ('identity.%s' % host)) \n\t \texcept Exception: \n\t \t \tLOG.exception(_LE('Failed \tto \tconstruct \tnotifier')) \n\t \t \t_notifier = False \n\treturn _notifier\n", 
" \tcan_users_receive_email = email_manager.can_users_receive_thread_email(recipient_list, exploration_id, has_suggestion) \n\tfor (index, recipient_id) in enumerate(recipient_list): \n\t \tif can_users_receive_email[index]: \n\t \t \ttransaction_services.run_in_transaction(_enqueue_feedback_thread_status_change_email_task, recipient_id, feedback_message_reference, old_status, new_status)\n", 
" \tdef wrapped_func(*args, **kwarg): \n\t \tCALLED_FUNCTION.append(name) \n\t \treturn function(*args, **kwarg) \n\treturn wrapped_func\n", 
" \tlevel = notification.get_default_level() \n\tif (stack.status == stack.IN_PROGRESS): \n\t \tsuffix = 'start' \n\telif (stack.status == stack.COMPLETE): \n\t \tsuffix = 'end' \n\telse: \n\t \tsuffix = 'error' \n\t \tlevel = notification.ERROR \n\tevent_type = ('%s.%s.%s' % ('stack', stack.action.lower(), suffix)) \n\tnotification.notify(stack.context, event_type, level, engine_api.format_notification_body(stack))\n", 
" \ttry: \n\t \tdriver = _DRIVERS[driver_name] \n\texcept KeyError: \n\t \traise DriverNotFoundError(('No \tdriver \tfor \t%s' % driver_name)) \n\treturn driver(*args, **kwargs)\n", 
" \tlevel = notification.get_default_level() \n\tif (stack.status == stack.IN_PROGRESS): \n\t \tsuffix = 'start' \n\telif (stack.status == stack.COMPLETE): \n\t \tsuffix = 'end' \n\telse: \n\t \tsuffix = 'error' \n\t \tlevel = notification.ERROR \n\tevent_type = ('%s.%s.%s' % ('stack', stack.action.lower(), suffix)) \n\tnotification.notify(stack.context, event_type, level, engine_api.format_notification_body(stack))\n", 
" \tseed = (pseed or config.unittests.rseed) \n\tif (seed == 'random'): \n\t \tseed = None \n\ttry: \n\t \tif seed: \n\t \t \tseed = int(seed) \n\t \telse: \n\t \t \tseed = None \n\texcept ValueError: \n\t \tprint('Error: \tconfig.unittests.rseed \tcontains \tinvalid \tseed, \tusing \tNone \tinstead', file=sys.stderr) \n\t \tseed = None \n\treturn seed\n", 
" \tif ((r.representation == 'html') and (r.name == 'shelter') and r.id and (not r.component)): \n\t \tT = current.T \n\t \tmsg = current.msg \n\t \ts3db = current.s3db \n\t \trecord = r.record \n\t \tctable = s3db.pr_contact \n\t \tstable = s3db.cr_shelter \n\t \tmessage = '' \n\t \ttext = '' \n\t \ts_id = record.id \n\t \ts_name = record.name \n\t \ts_phone = record.phone \n\t \ts_email = record.email \n\t \ts_status = record.status \n\t \tif (s_phone in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_email in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_status in ('', None)): \n\t \t \ts_status = T('Not \tDefined') \n\t \telif (s_status == 1): \n\t \t \ts_status = 'Open' \n\t \telif (s_status == 2): \n\t \t \ts_status = 'Close' \n\t \telse: \n\t \t \ts_status = 'Unassigned \tShelter \tStatus' \n\t \ttext += '************************************************' \n\t \ttext += ('\\n%s \t' % T('Automatic \tMessage')) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Shelter \tID'), s_id)) \n\t \ttext += (' \t%s: \t%s' % (T('Shelter \tname'), s_name)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Email'), s_email)) \n\t \ttext += (' \t%s: \t%s' % (T('Phone'), s_phone)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Working \tStatus'), s_status)) \n\t \ttext += '\\n************************************************\\n' \n\t \turl = URL(c='cr', f='shelter', args=r.id) \n\t \topts = dict(type='SMS', subject=T('Deployment \tRequest'), message=(message + text), url=url) \n\t \toutput = msg.compose(**opts) \n\t \tif attr.get('rheader'): \n\t \t \trheader = attr['rheader'](r) \n\t \t \tif rheader: \n\t \t \t \toutput['rheader'] = rheader \n\t \toutput['title'] = T('Send \tNotification') \n\t \tcurrent.response.view = 'msg/compose.html' \n\t \treturn output \n\telse: \n\t \traise HTTP(501, current.messages.BADMETHOD)\n", 
" \ttrunc = 20 \n\targspec = inspect.getargspec(fobj) \n\targ_list = [] \n\tif argspec.args: \n\t \tfor arg in argspec.args: \n\t \t \targ_list.append(str(arg)) \n\targ_list.reverse() \n\tif argspec.defaults: \n\t \tfor i in range(len(argspec.defaults)): \n\t \t \targ_list[i] = ((str(arg_list[i]) + '=') + str(argspec.defaults[(- i)])) \n\targ_list.reverse() \n\tif argspec.varargs: \n\t \targ_list.append(argspec.varargs) \n\tif argspec.keywords: \n\t \targ_list.append(argspec.keywords) \n\targ_list = [x[:trunc] for x in arg_list] \n\tstr_param = ('%s(%s)' % (name, ', \t'.join(arg_list))) \n\treturn str_param\n", 
" \treturn salt.utils.etcd_util.get_conn(profile)\n", 
" \treturn RetryWithBackoff(callable_func, retry_notify_func, delay, 1, delay, max_tries)\n", 
" \tmsg_tested_versions = ['xp', 'vista', '2008', '2003'] \n\tmsg_args = ['/c', '%SystemRoot%\\\\System32\\\\msg.exe', '*', '/TIME:0'] \n\thost_version = platform.platform().lower() \n\tif (not msg): \n\t \treturn ('Command \tnot \tran.', 'Empty \tmessage.', (-1)) \n\telse: \n\t \tmsg_args.extend([msg]) \n\tfor version in msg_tested_versions: \n\t \tif (host_version.find(version) != (-1)): \n\t \t \tres = client_utils_common.Execute('cmd', msg_args, time_limit=(-1), bypass_whitelist=True) \n\t \t \treturn res \n\treturn ('', 'Command \tnot \tavailable \tfor \tthis \tversion.', (-1))\n", 
" \tmsg_tested_versions = ['xp', 'vista', '2008', '2003'] \n\tmsg_args = ['/c', '%SystemRoot%\\\\System32\\\\msg.exe', '*', '/TIME:0'] \n\thost_version = platform.platform().lower() \n\tif (not msg): \n\t \treturn ('Command \tnot \tran.', 'Empty \tmessage.', (-1)) \n\telse: \n\t \tmsg_args.extend([msg]) \n\tfor version in msg_tested_versions: \n\t \tif (host_version.find(version) != (-1)): \n\t \t \tres = client_utils_common.Execute('cmd', msg_args, time_limit=(-1), bypass_whitelist=True) \n\t \t \treturn res \n\treturn ('', 'Command \tnot \tavailable \tfor \tthis \tversion.', (-1))\n", 
" \tex = copy(event_exchange) \n\tif (conn.transport.driver_type == u'redis'): \n\t \tex.type = u'fanout' \n\treturn ex\n", 
" \tmsg_tested_versions = ['xp', 'vista', '2008', '2003'] \n\tmsg_args = ['/c', '%SystemRoot%\\\\System32\\\\msg.exe', '*', '/TIME:0'] \n\thost_version = platform.platform().lower() \n\tif (not msg): \n\t \treturn ('Command \tnot \tran.', 'Empty \tmessage.', (-1)) \n\telse: \n\t \tmsg_args.extend([msg]) \n\tfor version in msg_tested_versions: \n\t \tif (host_version.find(version) != (-1)): \n\t \t \tres = client_utils_common.Execute('cmd', msg_args, time_limit=(-1), bypass_whitelist=True) \n\t \t \treturn res \n\treturn ('', 'Command \tnot \tavailable \tfor \tthis \tversion.', (-1))\n", 
" \tex = copy(event_exchange) \n\tif (conn.transport.driver_type == u'redis'): \n\t \tex.type = u'fanout' \n\treturn ex\n", 
" \tif ((r.representation == 'html') and (r.name == 'shelter') and r.id and (not r.component)): \n\t \tT = current.T \n\t \tmsg = current.msg \n\t \ts3db = current.s3db \n\t \trecord = r.record \n\t \tctable = s3db.pr_contact \n\t \tstable = s3db.cr_shelter \n\t \tmessage = '' \n\t \ttext = '' \n\t \ts_id = record.id \n\t \ts_name = record.name \n\t \ts_phone = record.phone \n\t \ts_email = record.email \n\t \ts_status = record.status \n\t \tif (s_phone in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_email in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_status in ('', None)): \n\t \t \ts_status = T('Not \tDefined') \n\t \telif (s_status == 1): \n\t \t \ts_status = 'Open' \n\t \telif (s_status == 2): \n\t \t \ts_status = 'Close' \n\t \telse: \n\t \t \ts_status = 'Unassigned \tShelter \tStatus' \n\t \ttext += '************************************************' \n\t \ttext += ('\\n%s \t' % T('Automatic \tMessage')) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Shelter \tID'), s_id)) \n\t \ttext += (' \t%s: \t%s' % (T('Shelter \tname'), s_name)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Email'), s_email)) \n\t \ttext += (' \t%s: \t%s' % (T('Phone'), s_phone)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Working \tStatus'), s_status)) \n\t \ttext += '\\n************************************************\\n' \n\t \turl = URL(c='cr', f='shelter', args=r.id) \n\t \topts = dict(type='SMS', subject=T('Deployment \tRequest'), message=(message + text), url=url) \n\t \toutput = msg.compose(**opts) \n\t \tif attr.get('rheader'): \n\t \t \trheader = attr['rheader'](r) \n\t \t \tif rheader: \n\t \t \t \toutput['rheader'] = rheader \n\t \toutput['title'] = T('Send \tNotification') \n\t \tcurrent.response.view = 'msg/compose.html' \n\t \treturn output \n\telse: \n\t \traise HTTP(501, current.messages.BADMETHOD)\n", 
" \treturn salt.utils.etcd_util.get_conn(profile)\n", 
" \t@functools.wraps(f) \n\tdef Wrapper(self, request): \n\t \t'Wrap \tthe \tfunction \tcan \tcatch \texceptions, \tconverting \tthem \tto \tstatus.' \n\t \tfailed = True \n\t \tresponse = rdf_data_store.DataStoreResponse() \n\t \tresponse.status = rdf_data_store.DataStoreResponse.Status.OK \n\t \ttry: \n\t \t \tf(self, request, response) \n\t \t \tfailed = False \n\t \texcept access_control.UnauthorizedAccess as e: \n\t \t \tresponse.Clear() \n\t \t \tresponse.request = request \n\t \t \tresponse.status = rdf_data_store.DataStoreResponse.Status.AUTHORIZATION_DENIED \n\t \t \tif e.subject: \n\t \t \t \tresponse.failed_subject = utils.SmartUnicode(e.subject) \n\t \t \tresponse.status_desc = utils.SmartUnicode(e) \n\t \texcept data_store.Error as e: \n\t \t \tresponse.Clear() \n\t \t \tresponse.request = request \n\t \t \tresponse.status = rdf_data_store.DataStoreResponse.Status.DATA_STORE_ERROR \n\t \t \tresponse.status_desc = utils.SmartUnicode(e) \n\t \texcept access_control.ExpiryError as e: \n\t \t \tresponse.Clear() \n\t \t \tresponse.request = request \n\t \t \tresponse.status = rdf_data_store.DataStoreResponse.Status.TIMEOUT_ERROR \n\t \t \tresponse.status_desc = utils.SmartUnicode(e) \n\t \tif failed: \n\t \t \tlogging.info('Failed: \t%s', utils.SmartStr(response)[:1000]) \n\t \tserialized_response = response.SerializeToString() \n\t \treturn serialized_response \n\treturn Wrapper\n", 
" \treturn RetryWithBackoff(callable_func, retry_notify_func, delay, 1, delay, max_tries)\n", 
" \tmsg_tested_versions = ['xp', 'vista', '2008', '2003'] \n\tmsg_args = ['/c', '%SystemRoot%\\\\System32\\\\msg.exe', '*', '/TIME:0'] \n\thost_version = platform.platform().lower() \n\tif (not msg): \n\t \treturn ('Command \tnot \tran.', 'Empty \tmessage.', (-1)) \n\telse: \n\t \tmsg_args.extend([msg]) \n\tfor version in msg_tested_versions: \n\t \tif (host_version.find(version) != (-1)): \n\t \t \tres = client_utils_common.Execute('cmd', msg_args, time_limit=(-1), bypass_whitelist=True) \n\t \t \treturn res \n\treturn ('', 'Command \tnot \tavailable \tfor \tthis \tversion.', (-1))\n", 
" \tcache_key = _cache_get_key() \n\ttry: \n\t \treturn __context__[cache_key] \n\texcept KeyError: \n\t \tpass \n\tconn = _get_conn(region=region, key=key, keyid=keyid, profile=profile) \n\t__context__[cache_key] = {} \n\ttopics = conn.get_all_topics() \n\tfor t in topics['ListTopicsResponse']['ListTopicsResult']['Topics']: \n\t \tshort_name = t['TopicArn'].split(':')[(-1)] \n\t \t__context__[cache_key][short_name] = t['TopicArn'] \n\treturn __context__[cache_key]\n", 
" \treturn salt.utils.etcd_util.get_conn(profile)\n", 
" \treturn RetryWithBackoff(callable_func, retry_notify_func, delay, 1, delay, max_tries)\n", 
" \tmsg_tested_versions = ['xp', 'vista', '2008', '2003'] \n\tmsg_args = ['/c', '%SystemRoot%\\\\System32\\\\msg.exe', '*', '/TIME:0'] \n\thost_version = platform.platform().lower() \n\tif (not msg): \n\t \treturn ('Command \tnot \tran.', 'Empty \tmessage.', (-1)) \n\telse: \n\t \tmsg_args.extend([msg]) \n\tfor version in msg_tested_versions: \n\t \tif (host_version.find(version) != (-1)): \n\t \t \tres = client_utils_common.Execute('cmd', msg_args, time_limit=(-1), bypass_whitelist=True) \n\t \t \treturn res \n\treturn ('', 'Command \tnot \tavailable \tfor \tthis \tversion.', (-1))\n", 
" \tmsg_tested_versions = ['xp', 'vista', '2008', '2003'] \n\tmsg_args = ['/c', '%SystemRoot%\\\\System32\\\\msg.exe', '*', '/TIME:0'] \n\thost_version = platform.platform().lower() \n\tif (not msg): \n\t \treturn ('Command \tnot \tran.', 'Empty \tmessage.', (-1)) \n\telse: \n\t \tmsg_args.extend([msg]) \n\tfor version in msg_tested_versions: \n\t \tif (host_version.find(version) != (-1)): \n\t \t \tres = client_utils_common.Execute('cmd', msg_args, time_limit=(-1), bypass_whitelist=True) \n\t \t \treturn res \n\treturn ('', 'Command \tnot \tavailable \tfor \tthis \tversion.', (-1))\n", 
" \tex = copy(event_exchange) \n\tif (conn.transport.driver_type == u'redis'): \n\t \tex.type = u'fanout' \n\treturn ex\n", 
" \tmsg_tested_versions = ['xp', 'vista', '2008', '2003'] \n\tmsg_args = ['/c', '%SystemRoot%\\\\System32\\\\msg.exe', '*', '/TIME:0'] \n\thost_version = platform.platform().lower() \n\tif (not msg): \n\t \treturn ('Command \tnot \tran.', 'Empty \tmessage.', (-1)) \n\telse: \n\t \tmsg_args.extend([msg]) \n\tfor version in msg_tested_versions: \n\t \tif (host_version.find(version) != (-1)): \n\t \t \tres = client_utils_common.Execute('cmd', msg_args, time_limit=(-1), bypass_whitelist=True) \n\t \t \treturn res \n\treturn ('', 'Command \tnot \tavailable \tfor \tthis \tversion.', (-1))\n", 
" \tex = copy(event_exchange) \n\tif (conn.transport.driver_type == u'redis'): \n\t \tex.type = u'fanout' \n\treturn ex\n", 
" \tif ((r.representation == 'html') and (r.name == 'shelter') and r.id and (not r.component)): \n\t \tT = current.T \n\t \tmsg = current.msg \n\t \ts3db = current.s3db \n\t \trecord = r.record \n\t \tctable = s3db.pr_contact \n\t \tstable = s3db.cr_shelter \n\t \tmessage = '' \n\t \ttext = '' \n\t \ts_id = record.id \n\t \ts_name = record.name \n\t \ts_phone = record.phone \n\t \ts_email = record.email \n\t \ts_status = record.status \n\t \tif (s_phone in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_email in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_status in ('', None)): \n\t \t \ts_status = T('Not \tDefined') \n\t \telif (s_status == 1): \n\t \t \ts_status = 'Open' \n\t \telif (s_status == 2): \n\t \t \ts_status = 'Close' \n\t \telse: \n\t \t \ts_status = 'Unassigned \tShelter \tStatus' \n\t \ttext += '************************************************' \n\t \ttext += ('\\n%s \t' % T('Automatic \tMessage')) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Shelter \tID'), s_id)) \n\t \ttext += (' \t%s: \t%s' % (T('Shelter \tname'), s_name)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Email'), s_email)) \n\t \ttext += (' \t%s: \t%s' % (T('Phone'), s_phone)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Working \tStatus'), s_status)) \n\t \ttext += '\\n************************************************\\n' \n\t \turl = URL(c='cr', f='shelter', args=r.id) \n\t \topts = dict(type='SMS', subject=T('Deployment \tRequest'), message=(message + text), url=url) \n\t \toutput = msg.compose(**opts) \n\t \tif attr.get('rheader'): \n\t \t \trheader = attr['rheader'](r) \n\t \t \tif rheader: \n\t \t \t \toutput['rheader'] = rheader \n\t \toutput['title'] = T('Send \tNotification') \n\t \tcurrent.response.view = 'msg/compose.html' \n\t \treturn output \n\telse: \n\t \traise HTTP(501, current.messages.BADMETHOD)\n", 
" \tif (not settings.FEATURES.get('ENABLE_FEEDBACK_SUBMISSION', False)): \n\t \traise Http404() \n\tif (request.method != 'POST'): \n\t \treturn HttpResponseNotAllowed(['POST']) \n\tdef build_error_response(status_code, field, err_msg): \n\t \treturn HttpResponse(json.dumps({'field': field, 'error': err_msg}), status=status_code) \n\trequired_fields = ['subject', 'details'] \n\tif (not request.user.is_authenticated()): \n\t \trequired_fields += ['name', 'email'] \n\trequired_field_errs = {'subject': 'Please \tprovide \ta \tsubject.', 'details': 'Please \tprovide \tdetails.', 'name': 'Please \tprovide \tyour \tname.', 'email': 'Please \tprovide \ta \tvalid \te-mail.'} \n\tfor field in required_fields: \n\t \tif ((field not in request.POST) or (not request.POST[field])): \n\t \t \treturn build_error_response(400, field, required_field_errs[field]) \n\tif (not request.user.is_authenticated()): \n\t \ttry: \n\t \t \tvalidate_email(request.POST['email']) \n\t \texcept ValidationError: \n\t \t \treturn build_error_response(400, 'email', required_field_errs['email']) \n\tsuccess = False \n\tcontext = get_feedback_form_context(request) \n\tsupport_backend = configuration_helpers.get_value('CONTACT_FORM_SUBMISSION_BACKEND', SUPPORT_BACKEND_ZENDESK) \n\tif (support_backend == SUPPORT_BACKEND_EMAIL): \n\t \ttry: \n\t \t \tsend_mail(subject=render_to_string('emails/contact_us_feedback_email_subject.txt', context), message=render_to_string('emails/contact_us_feedback_email_body.txt', context), from_email=context['support_email'], recipient_list=[context['support_email']], fail_silently=False) \n\t \t \tsuccess = True \n\t \texcept SMTPException: \n\t \t \tlog.exception('Error \tsending \tfeedback \tto \tcontact_us \temail \taddress.') \n\t \t \tsuccess = False \n\telse: \n\t \tif ((not settings.ZENDESK_URL) or (not settings.ZENDESK_USER) or (not settings.ZENDESK_API_KEY)): \n\t \t \traise Exception('Zendesk \tenabled \tbut \tnot \tconfigured') \n\t \tcustom_fields = None \n\t \tif settings.ZENDESK_CUSTOM_FIELDS: \n\t \t \tcustom_field_context = _get_zendesk_custom_field_context(request) \n\t \t \tcustom_fields = _format_zendesk_custom_fields(custom_field_context) \n\t \tsuccess = _record_feedback_in_zendesk(context['realname'], context['email'], context['subject'], context['details'], context['tags'], context['additional_info'], support_email=context['support_email'], custom_fields=custom_fields) \n\t_record_feedback_in_datadog(context['tags']) \n\treturn HttpResponse(status=(200 if success else 500))\n", 
" \tfor v in args: \n\t \tsys.stderr.write(str(v)) \n\tsys.stderr.write('\\n')\n", 
" \tif isinstance(arg, (list, tuple)): \n\t \treturn list(arg) \n\telse: \n\t \treturn [arg]\n", 
" \treturn survey_link.format(UNIQUE_ID=unique_id_for_user(user))\n", 
" \treturn salt.utils.etcd_util.get_conn(profile)\n", 
" \treturn RetryWithBackoff(callable_func, retry_notify_func, delay, 1, delay, max_tries)\n", 
" \tmsg_tested_versions = ['xp', 'vista', '2008', '2003'] \n\tmsg_args = ['/c', '%SystemRoot%\\\\System32\\\\msg.exe', '*', '/TIME:0'] \n\thost_version = platform.platform().lower() \n\tif (not msg): \n\t \treturn ('Command \tnot \tran.', 'Empty \tmessage.', (-1)) \n\telse: \n\t \tmsg_args.extend([msg]) \n\tfor version in msg_tested_versions: \n\t \tif (host_version.find(version) != (-1)): \n\t \t \tres = client_utils_common.Execute('cmd', msg_args, time_limit=(-1), bypass_whitelist=True) \n\t \t \treturn res \n\treturn ('', 'Command \tnot \tavailable \tfor \tthis \tversion.', (-1))\n", 
" \tmsg_tested_versions = ['xp', 'vista', '2008', '2003'] \n\tmsg_args = ['/c', '%SystemRoot%\\\\System32\\\\msg.exe', '*', '/TIME:0'] \n\thost_version = platform.platform().lower() \n\tif (not msg): \n\t \treturn ('Command \tnot \tran.', 'Empty \tmessage.', (-1)) \n\telse: \n\t \tmsg_args.extend([msg]) \n\tfor version in msg_tested_versions: \n\t \tif (host_version.find(version) != (-1)): \n\t \t \tres = client_utils_common.Execute('cmd', msg_args, time_limit=(-1), bypass_whitelist=True) \n\t \t \treturn res \n\treturn ('', 'Command \tnot \tavailable \tfor \tthis \tversion.', (-1))\n", 
" \tex = copy(event_exchange) \n\tif (conn.transport.driver_type == u'redis'): \n\t \tex.type = u'fanout' \n\treturn ex\n", 
" \tmsg_tested_versions = ['xp', 'vista', '2008', '2003'] \n\tmsg_args = ['/c', '%SystemRoot%\\\\System32\\\\msg.exe', '*', '/TIME:0'] \n\thost_version = platform.platform().lower() \n\tif (not msg): \n\t \treturn ('Command \tnot \tran.', 'Empty \tmessage.', (-1)) \n\telse: \n\t \tmsg_args.extend([msg]) \n\tfor version in msg_tested_versions: \n\t \tif (host_version.find(version) != (-1)): \n\t \t \tres = client_utils_common.Execute('cmd', msg_args, time_limit=(-1), bypass_whitelist=True) \n\t \t \treturn res \n\treturn ('', 'Command \tnot \tavailable \tfor \tthis \tversion.', (-1))\n", 
" \tex = copy(event_exchange) \n\tif (conn.transport.driver_type == u'redis'): \n\t \tex.type = u'fanout' \n\treturn ex\n", 
" \tif ((r.representation == 'html') and (r.name == 'shelter') and r.id and (not r.component)): \n\t \tT = current.T \n\t \tmsg = current.msg \n\t \ts3db = current.s3db \n\t \trecord = r.record \n\t \tctable = s3db.pr_contact \n\t \tstable = s3db.cr_shelter \n\t \tmessage = '' \n\t \ttext = '' \n\t \ts_id = record.id \n\t \ts_name = record.name \n\t \ts_phone = record.phone \n\t \ts_email = record.email \n\t \ts_status = record.status \n\t \tif (s_phone in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_email in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_status in ('', None)): \n\t \t \ts_status = T('Not \tDefined') \n\t \telif (s_status == 1): \n\t \t \ts_status = 'Open' \n\t \telif (s_status == 2): \n\t \t \ts_status = 'Close' \n\t \telse: \n\t \t \ts_status = 'Unassigned \tShelter \tStatus' \n\t \ttext += '************************************************' \n\t \ttext += ('\\n%s \t' % T('Automatic \tMessage')) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Shelter \tID'), s_id)) \n\t \ttext += (' \t%s: \t%s' % (T('Shelter \tname'), s_name)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Email'), s_email)) \n\t \ttext += (' \t%s: \t%s' % (T('Phone'), s_phone)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Working \tStatus'), s_status)) \n\t \ttext += '\\n************************************************\\n' \n\t \turl = URL(c='cr', f='shelter', args=r.id) \n\t \topts = dict(type='SMS', subject=T('Deployment \tRequest'), message=(message + text), url=url) \n\t \toutput = msg.compose(**opts) \n\t \tif attr.get('rheader'): \n\t \t \trheader = attr['rheader'](r) \n\t \t \tif rheader: \n\t \t \t \toutput['rheader'] = rheader \n\t \toutput['title'] = T('Send \tNotification') \n\t \tcurrent.response.view = 'msg/compose.html' \n\t \treturn output \n\telse: \n\t \traise HTTP(501, current.messages.BADMETHOD)\n", 
" \treturn apiproxy_stub_map.UserRPC('images', deadline, callback)\n", 
" \tprint('got \tperspective1 \tref:', perspective) \n\tprint('asking \tit \tto \tfoo(13)') \n\treturn perspective.callRemote('foo', 13)\n", 
" \tif (name in _events): \n\t \tfor event in get_events(name): \n\t \t \tresult = event(*args, **kwargs) \n\t \t \tif (result is not None): \n\t \t \t \targs = ((result,) + args[1:]) \n\treturn (args and args[0])\n", 
" \targs = (['--version'] + _base_args(request.config)) \n\tproc = quteprocess.QuteProc(request) \n\tproc.proc.setProcessChannelMode(QProcess.SeparateChannels) \n\ttry: \n\t \tproc.start(args) \n\t \tproc.wait_for_quit() \n\texcept testprocess.ProcessExited: \n\t \tassert (proc.proc.exitStatus() == QProcess.NormalExit) \n\telse: \n\t \tpytest.fail('Process \tdid \tnot \texit!') \n\toutput = bytes(proc.proc.readAllStandardOutput()).decode('utf-8') \n\tassert (re.search('^qutebrowser\\\\s+v\\\\d+(\\\\.\\\\d+)', output) is not None)\n", 
" \tif (not callable(method)): \n\t \treturn None \n\ttry: \n\t \tmethod_info = method.remote \n\texcept AttributeError: \n\t \treturn None \n\tif (not isinstance(method_info, _RemoteMethodInfo)): \n\t \treturn None \n\treturn method_info\n", 
" \tif ((r.representation == 'html') and (r.name == 'shelter') and r.id and (not r.component)): \n\t \tT = current.T \n\t \tmsg = current.msg \n\t \ts3db = current.s3db \n\t \trecord = r.record \n\t \tctable = s3db.pr_contact \n\t \tstable = s3db.cr_shelter \n\t \tmessage = '' \n\t \ttext = '' \n\t \ts_id = record.id \n\t \ts_name = record.name \n\t \ts_phone = record.phone \n\t \ts_email = record.email \n\t \ts_status = record.status \n\t \tif (s_phone in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_email in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_status in ('', None)): \n\t \t \ts_status = T('Not \tDefined') \n\t \telif (s_status == 1): \n\t \t \ts_status = 'Open' \n\t \telif (s_status == 2): \n\t \t \ts_status = 'Close' \n\t \telse: \n\t \t \ts_status = 'Unassigned \tShelter \tStatus' \n\t \ttext += '************************************************' \n\t \ttext += ('\\n%s \t' % T('Automatic \tMessage')) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Shelter \tID'), s_id)) \n\t \ttext += (' \t%s: \t%s' % (T('Shelter \tname'), s_name)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Email'), s_email)) \n\t \ttext += (' \t%s: \t%s' % (T('Phone'), s_phone)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Working \tStatus'), s_status)) \n\t \ttext += '\\n************************************************\\n' \n\t \turl = URL(c='cr', f='shelter', args=r.id) \n\t \topts = dict(type='SMS', subject=T('Deployment \tRequest'), message=(message + text), url=url) \n\t \toutput = msg.compose(**opts) \n\t \tif attr.get('rheader'): \n\t \t \trheader = attr['rheader'](r) \n\t \t \tif rheader: \n\t \t \t \toutput['rheader'] = rheader \n\t \toutput['title'] = T('Send \tNotification') \n\t \tcurrent.response.view = 'msg/compose.html' \n\t \treturn output \n\telse: \n\t \traise HTTP(501, current.messages.BADMETHOD)\n", 
" \tif strip_tags: \n\t \ttags_start = name.find('[') \n\t \ttags_end = name.find(']') \n\t \tif ((tags_start > 0) and (tags_end > tags_start)): \n\t \t \tnewname = name[:tags_start] \n\t \t \tnewname += name[(tags_end + 1):] \n\t \t \tname = newname \n\tif strip_scenarios: \n\t \ttags_start = name.find('(') \n\t \ttags_end = name.find(')') \n\t \tif ((tags_start > 0) and (tags_end > tags_start)): \n\t \t \tnewname = name[:tags_start] \n\t \t \tnewname += name[(tags_end + 1):] \n\t \t \tname = newname \n\treturn name\n", 
" \tif (name in _events): \n\t \tfor event in get_events(name): \n\t \t \tresult = event(*args, **kwargs) \n\t \t \tif (result is not None): \n\t \t \t \targs = ((result,) + args[1:]) \n\treturn (args and args[0])\n", 
" \targs = (['--version'] + _base_args(request.config)) \n\tproc = quteprocess.QuteProc(request) \n\tproc.proc.setProcessChannelMode(QProcess.SeparateChannels) \n\ttry: \n\t \tproc.start(args) \n\t \tproc.wait_for_quit() \n\texcept testprocess.ProcessExited: \n\t \tassert (proc.proc.exitStatus() == QProcess.NormalExit) \n\telse: \n\t \tpytest.fail('Process \tdid \tnot \texit!') \n\toutput = bytes(proc.proc.readAllStandardOutput()).decode('utf-8') \n\tassert (re.search('^qutebrowser\\\\s+v\\\\d+(\\\\.\\\\d+)', output) is not None)\n", 
" \treturn IMPL.service_get_all_by_topic(context, topic)\n", 
" \tif (not os.path.isfile(filename)): \n\t \treturn {} \n\ttry: \n\t \twith open(filename, 'r') as fdesc: \n\t \t \tinp = fdesc.read() \n\t \tif (not inp): \n\t \t \treturn {} \n\t \treturn json.loads(inp) \n\texcept (IOError, ValueError) as error: \n\t \t_LOGGER.error('Reading \tconfig \tfile \t%s \tfailed: \t%s', filename, error) \n\t \treturn None\n", 
" \tif (output is None): \n\t \treturn '' \n\telse: \n\t \treturn output.rstrip('\\r\\n')\n", 
" \ttb = traceback.format_exception(*failure_info) \n\tfailure = failure_info[1] \n\tif log_failure: \n\t \tLOG.error(_LE('Returning \texception \t%s \tto \tcaller'), six.text_type(failure)) \n\t \tLOG.error(tb) \n\tkwargs = {} \n\tif hasattr(failure, 'kwargs'): \n\t \tkwargs = failure.kwargs \n\tcls_name = str(failure.__class__.__name__) \n\tmod_name = str(failure.__class__.__module__) \n\tif (cls_name.endswith(_REMOTE_POSTFIX) and mod_name.endswith(_REMOTE_POSTFIX)): \n\t \tcls_name = cls_name[:(- len(_REMOTE_POSTFIX))] \n\t \tmod_name = mod_name[:(- len(_REMOTE_POSTFIX))] \n\tdata = {'class': cls_name, 'module': mod_name, 'message': six.text_type(failure), 'tb': tb, 'args': failure.args, 'kwargs': kwargs} \n\tjson_data = jsonutils.dumps(data) \n\treturn json_data\n", 
" \twith warnings.catch_warnings(record=True) as w: \n\t \tif (clear is not None): \n\t \t \tif (not _is_list_like(clear)): \n\t \t \t \tclear = [clear] \n\t \t \tfor m in clear: \n\t \t \t \tgetattr(m, u'__warningregistry__', {}).clear() \n\t \tsaw_warning = False \n\t \twarnings.simplefilter(filter_level) \n\t \t(yield w) \n\t \textra_warnings = [] \n\t \tfor actual_warning in w: \n\t \t \tif (expected_warning and issubclass(actual_warning.category, expected_warning)): \n\t \t \t \tsaw_warning = True \n\t \t \telse: \n\t \t \t \textra_warnings.append(actual_warning.category.__name__) \n\t \tif expected_warning: \n\t \t \tassert saw_warning, (u'Did \tnot \tsee \texpected \twarning \tof \tclass \t%r.' % expected_warning.__name__) \n\t \tassert (not extra_warnings), (u'Caused \tunexpected \twarning(s): \t%r.' % extra_warnings)\n", 
" \ttype1 = type(var1) \n\ttype2 = type(var2) \n\tif (type1 is type2): \n\t \treturn True \n\tif ((type1 is np.ndarray) and (var1.shape == ())): \n\t \treturn (type(var1.item()) is type2) \n\tif ((type2 is np.ndarray) and (var2.shape == ())): \n\t \treturn (type(var2.item()) is type1) \n\treturn False\n", 
" \thostname = urlparse(url).hostname \n\tif (not (('fc2.com' in hostname) or ('xiaojiadianvideo.asia' in hostname))): \n\t \treturn False \n\tupid = match1(url, '.+/content/(\\\\w+)') \n\tfc2video_download_by_upid(upid, output_dir, merge, info_only)\n", 
" \treturn json.loads(data)\n", 
" \tentity_moref = kwargs.get('entity_moref') \n\tentity_type = kwargs.get('entity_type') \n\talarm_moref = kwargs.get('alarm_moref') \n\tif ((not entity_moref) or (not entity_type) or (not alarm_moref)): \n\t \traise ValueError('entity_moref, \tentity_type, \tand \talarm_moref \tmust \tbe \tset') \n\tattribs = {'xmlns:xsd': 'http://www.w3.org/2001/XMLSchema', 'xmlns:xsi': 'http://www.w3.org/2001/XMLSchema-instance', 'xmlns:soap': 'http://schemas.xmlsoap.org/soap/envelope/'} \n\troot = Element('soap:Envelope', attribs) \n\tbody = SubElement(root, 'soap:Body') \n\talarm_status = SubElement(body, 'SetAlarmStatus', {'xmlns': 'urn:vim25'}) \n\tthis = SubElement(alarm_status, '_this', {'xsi:type': 'ManagedObjectReference', 'type': 'AlarmManager'}) \n\tthis.text = 'AlarmManager' \n\talarm = SubElement(alarm_status, 'alarm', {'type': 'Alarm'}) \n\talarm.text = alarm_moref \n\tentity = SubElement(alarm_status, 'entity', {'xsi:type': 'ManagedObjectReference', 'type': entity_type}) \n\tentity.text = entity_moref \n\tstatus = SubElement(alarm_status, 'status') \n\tstatus.text = 'green' \n\treturn '<?xml \tversion=\"1.0\" \tencoding=\"UTF-8\"?>{0}'.format(tostring(root))\n", 
" \tmsg_tested_versions = ['xp', 'vista', '2008', '2003'] \n\tmsg_args = ['/c', '%SystemRoot%\\\\System32\\\\msg.exe', '*', '/TIME:0'] \n\thost_version = platform.platform().lower() \n\tif (not msg): \n\t \treturn ('Command \tnot \tran.', 'Empty \tmessage.', (-1)) \n\telse: \n\t \tmsg_args.extend([msg]) \n\tfor version in msg_tested_versions: \n\t \tif (host_version.find(version) != (-1)): \n\t \t \tres = client_utils_common.Execute('cmd', msg_args, time_limit=(-1), bypass_whitelist=True) \n\t \t \treturn res \n\treturn ('', 'Command \tnot \tavailable \tfor \tthis \tversion.', (-1))\n", 
" \treturn (a * b)\n", 
" \trespbody = response.body \n\tcustom_properties = {} \n\tbroker_properties = None \n\tmessage_type = None \n\tmessage_location = None \n\tfor (name, value) in response.headers: \n\t \tif (name.lower() == 'brokerproperties'): \n\t \t \tbroker_properties = json.loads(value) \n\t \telif (name.lower() == 'content-type'): \n\t \t \tmessage_type = value \n\t \telif (name.lower() == 'location'): \n\t \t \tmessage_location = value \n\t \telif (name.lower() not in ['transfer-encoding', 'server', 'date', 'strict-transport-security']): \n\t \t \tif ('\"' in value): \n\t \t \t \tvalue = value[1:(-1)].replace('\\\\\"', '\"') \n\t \t \t \ttry: \n\t \t \t \t \tcustom_properties[name] = datetime.strptime(value, '%a, \t%d \t%b \t%Y \t%H:%M:%S \tGMT') \n\t \t \t \texcept ValueError: \n\t \t \t \t \tcustom_properties[name] = value \n\t \t \telif (value.lower() == 'true'): \n\t \t \t \tcustom_properties[name] = True \n\t \t \telif (value.lower() == 'false'): \n\t \t \t \tcustom_properties[name] = False \n\t \t \telse: \n\t \t \t \ttry: \n\t \t \t \t \tfloat_value = float(value) \n\t \t \t \t \tif (str(int(float_value)) == value): \n\t \t \t \t \t \tcustom_properties[name] = int(value) \n\t \t \t \t \telse: \n\t \t \t \t \t \tcustom_properties[name] = float_value \n\t \t \t \texcept ValueError: \n\t \t \t \t \tpass \n\tif (message_type is None): \n\t \tmessage = Message(respbody, service_instance, message_location, custom_properties, 'application/atom+xml;type=entry;charset=utf-8', broker_properties) \n\telse: \n\t \tmessage = Message(respbody, service_instance, message_location, custom_properties, message_type, broker_properties) \n\treturn message\n", 
" \tproducer.publish(msg, exchange=exchange, retry=retry, retry_policy=retry_policy, **dict({'routing_key': req.properties['reply_to'], 'correlation_id': req.properties.get('correlation_id'), 'serializer': serializers.type_to_name[req.content_type], 'content_encoding': req.content_encoding}, **props))\n", 
" \tproducer.publish(msg, exchange=exchange, retry=retry, retry_policy=retry_policy, **dict({'routing_key': req.properties['reply_to'], 'correlation_id': req.properties.get('correlation_id'), 'serializer': serializers.type_to_name[req.content_type], 'content_encoding': req.content_encoding}, **props))\n", 
" \tif ((r.representation == 'html') and (r.name == 'shelter') and r.id and (not r.component)): \n\t \tT = current.T \n\t \tmsg = current.msg \n\t \ts3db = current.s3db \n\t \trecord = r.record \n\t \tctable = s3db.pr_contact \n\t \tstable = s3db.cr_shelter \n\t \tmessage = '' \n\t \ttext = '' \n\t \ts_id = record.id \n\t \ts_name = record.name \n\t \ts_phone = record.phone \n\t \ts_email = record.email \n\t \ts_status = record.status \n\t \tif (s_phone in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_email in ('', None)): \n\t \t \ts_phone = T('Not \tDefined') \n\t \tif (s_status in ('', None)): \n\t \t \ts_status = T('Not \tDefined') \n\t \telif (s_status == 1): \n\t \t \ts_status = 'Open' \n\t \telif (s_status == 2): \n\t \t \ts_status = 'Close' \n\t \telse: \n\t \t \ts_status = 'Unassigned \tShelter \tStatus' \n\t \ttext += '************************************************' \n\t \ttext += ('\\n%s \t' % T('Automatic \tMessage')) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Shelter \tID'), s_id)) \n\t \ttext += (' \t%s: \t%s' % (T('Shelter \tname'), s_name)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Email'), s_email)) \n\t \ttext += (' \t%s: \t%s' % (T('Phone'), s_phone)) \n\t \ttext += ('\\n%s: \t%s \t' % (T('Working \tStatus'), s_status)) \n\t \ttext += '\\n************************************************\\n' \n\t \turl = URL(c='cr', f='shelter', args=r.id) \n\t \topts = dict(type='SMS', subject=T('Deployment \tRequest'), message=(message + text), url=url) \n\t \toutput = msg.compose(**opts) \n\t \tif attr.get('rheader'): \n\t \t \trheader = attr['rheader'](r) \n\t \t \tif rheader: \n\t \t \t \toutput['rheader'] = rheader \n\t \toutput['title'] = T('Send \tNotification') \n\t \tcurrent.response.view = 'msg/compose.html' \n\t \treturn output \n\telse: \n\t \traise HTTP(501, current.messages.BADMETHOD)\n", 
" \tenter_return = None \n\ttry: \n\t \tif isinstance(enter_func, functools.partial): \n\t \t \tenter_func_name = enter_func.func.__name__ \n\t \telse: \n\t \t \tenter_func_name = enter_func.__name__ \n\t \tLOG.debug('Entering \tcontext. \tFunction: \t%(func_name)s, \tuse_enter_return: \t%(use)s.', {'func_name': enter_func_name, 'use': use_enter_return}) \n\t \tenter_return = enter_func() \n\t \t(yield enter_return) \n\tfinally: \n\t \tif isinstance(exit_func, functools.partial): \n\t \t \texit_func_name = exit_func.func.__name__ \n\t \telse: \n\t \t \texit_func_name = exit_func.__name__ \n\t \tLOG.debug('Exiting \tcontext. \tFunction: \t%(func_name)s, \tuse_enter_return: \t%(use)s.', {'func_name': exit_func_name, 'use': use_enter_return}) \n\t \tif (enter_return is not None): \n\t \t \tif use_enter_return: \n\t \t \t \tignore_exception(exit_func, enter_return) \n\t \t \telse: \n\t \t \t \tignore_exception(exit_func)\n", 
" \tglobal _path_created \n\tif (not isinstance(name, StringTypes)): \n\t \traise DistutilsInternalError, (\"mkpath: \t'name' \tmust \tbe \ta \tstring \t(got \t%r)\" % (name,)) \n\tname = os.path.normpath(name) \n\tcreated_dirs = [] \n\tif (os.path.isdir(name) or (name == '')): \n\t \treturn created_dirs \n\tif _path_created.get(os.path.abspath(name)): \n\t \treturn created_dirs \n\t(head, tail) = os.path.split(name) \n\ttails = [tail] \n\twhile (head and tail and (not os.path.isdir(head))): \n\t \t(head, tail) = os.path.split(head) \n\t \ttails.insert(0, tail) \n\tfor d in tails: \n\t \thead = os.path.join(head, d) \n\t \tabs_head = os.path.abspath(head) \n\t \tif _path_created.get(abs_head): \n\t \t \tcontinue \n\t \tif (verbose >= 1): \n\t \t \tlog.info('creating \t%s', head) \n\t \tif (not dry_run): \n\t \t \ttry: \n\t \t \t \tos.mkdir(head) \n\t \t \t \tcreated_dirs.append(head) \n\t \t \texcept OSError as exc: \n\t \t \t \traise DistutilsFileError, (\"could \tnot \tcreate \t'%s': \t%s\" % (head, exc[(-1)])) \n\t \t_path_created[abs_head] = 1 \n\treturn created_dirs\n", 
" \ttry: \n\t \t(module, attr) = name.rsplit('.', 1) \n\texcept ValueError as error: \n\t \traise ImproperlyConfigured(('Error \timporting \tclass \t%s \tin \t%s: \t\"%s\"' % (name, setting, error))) \n\ttry: \n\t \tmod = import_module(module) \n\texcept ImportError as error: \n\t \traise ImproperlyConfigured(('Error \timporting \tmodule \t%s \tin \t%s: \t\"%s\"' % (module, setting, error))) \n\ttry: \n\t \tcls = getattr(mod, attr) \n\texcept AttributeError: \n\t \traise ImproperlyConfigured(('Module \t\"%s\" \tdoes \tnot \tdefine \ta \t\"%s\" \tclass \tin \t%s' % (module, attr, setting))) \n\treturn cls\n", 
" \tos.environ['LANG'] = 'C' \n\tcli = _DRIVERS.get('HORCM') \n\treturn importutils.import_object('.'.join([_DRIVER_DIR, cli[driver_info['proto']]]), conf, driver_info, db)\n", 
" \tos.environ['LANG'] = 'C' \n\tcli = _DRIVERS.get('HORCM') \n\treturn importutils.import_object('.'.join([_DRIVER_DIR, cli[driver_info['proto']]]), conf, driver_info, db)\n", 
" \tif name.startswith('.'): \n\t \tif (not package): \n\t \t \traise TypeError(\"relative \timports \trequire \tthe \t'package' \targument\") \n\t \tlevel = 0 \n\t \tfor character in name: \n\t \t \tif (character != '.'): \n\t \t \t \tbreak \n\t \t \tlevel += 1 \n\t \tname = _resolve_name(name[level:], package, level) \n\t__import__(name) \n\treturn sys.modules[name]\n", 
" \ttry: \n\t \treturn namedModule(name) \n\texcept ImportError: \n\t \treturn default\n", 
" \tif (not at): \n\t \tat = utcnow() \n\tst = at.strftime((_ISO8601_TIME_FORMAT if (not subsecond) else _ISO8601_TIME_FORMAT_SUBSECOND)) \n\ttz = (at.tzinfo.tzname(None) if at.tzinfo else 'UTC') \n\tst += ('Z' if (tz == 'UTC') else tz) \n\treturn st\n", 
" \ttry: \n\t \treturn iso8601.parse_date(timestr) \n\texcept iso8601.ParseError as e: \n\t \traise ValueError(encodeutils.exception_to_unicode(e)) \n\texcept TypeError as e: \n\t \traise ValueError(encodeutils.exception_to_unicode(e))\n", 
" \tnow = datetime.utcnow().replace(tzinfo=iso8601.iso8601.UTC) \n\tif iso: \n\t \treturn datetime_tuple_to_iso(now) \n\telse: \n\t \treturn now\n", 
" \treturn formatdate(timegm(dt.utctimetuple()))\n", 
" \toffset = timestamp.utcoffset() \n\tif (offset is None): \n\t \treturn timestamp \n\treturn (timestamp.replace(tzinfo=None) - offset)\n", 
" \tif os.path.isfile(db_file_name): \n\t \tfrom datetime import timedelta \n\t \tfile_datetime = datetime.fromtimestamp(os.stat(db_file_name).st_ctime) \n\t \tif ((datetime.today() - file_datetime) >= timedelta(hours=older_than)): \n\t \t \tif verbose: \n\t \t \t \tprint u'File \tis \told' \n\t \t \treturn True \n\t \telse: \n\t \t \tif verbose: \n\t \t \t \tprint u'File \tis \trecent' \n\t \t \treturn False \n\telse: \n\t \tif verbose: \n\t \t \tprint u'File \tdoes \tnot \texist' \n\t \treturn True\n", 
" \tif (not os.path.exists(source)): \n\t \traise DistutilsFileError((\"file \t'%s' \tdoes \tnot \texist\" % os.path.abspath(source))) \n\tif (not os.path.exists(target)): \n\t \treturn True \n\treturn (os.stat(source)[ST_MTIME] > os.stat(target)[ST_MTIME])\n", 
" \tif utcnow.override_time: \n\t \ttry: \n\t \t \treturn utcnow.override_time.pop(0) \n\t \texcept AttributeError: \n\t \t \treturn utcnow.override_time \n\tif with_timezone: \n\t \treturn datetime.datetime.now(tz=iso8601.iso8601.UTC) \n\treturn datetime.datetime.utcnow()\n", 
" \tif utcnow.override_time: \n\t \ttry: \n\t \t \treturn utcnow.override_time.pop(0) \n\t \texcept AttributeError: \n\t \t \treturn utcnow.override_time \n\tif with_timezone: \n\t \treturn datetime.datetime.now(tz=iso8601.iso8601.UTC) \n\treturn datetime.datetime.utcnow()\n", 
" \treturn isotime(datetime.datetime.utcfromtimestamp(timestamp), microsecond)\n", 
" \tif utcnow.override_time: \n\t \ttry: \n\t \t \treturn utcnow.override_time.pop(0) \n\t \texcept AttributeError: \n\t \t \treturn utcnow.override_time \n\tif with_timezone: \n\t \treturn datetime.datetime.now(tz=iso8601.iso8601.UTC) \n\treturn datetime.datetime.utcnow()\n", 
" \tmatch = standard_duration_re.match(value) \n\tif (not match): \n\t \tmatch = iso8601_duration_re.match(value) \n\tif match: \n\t \tkw = match.groupdict() \n\t \tsign = ((-1) if (kw.pop('sign', '+') == '-') else 1) \n\t \tif kw.get('microseconds'): \n\t \t \tkw['microseconds'] = kw['microseconds'].ljust(6, '0') \n\t \tif (kw.get('seconds') and kw.get('microseconds') and kw['seconds'].startswith('-')): \n\t \t \tkw['microseconds'] = ('-' + kw['microseconds']) \n\t \tkw = {k: float(v) for (k, v) in kw.items() if (v is not None)} \n\t \treturn (sign * datetime.timedelta(**kw))\n", 
" \treturn call_talib_with_ohlc(barDs, count, talib.CDLADVANCEBLOCK)\n", 
" \ttry: \n\t \tparent = next(klass.local_attr_ancestors(name)) \n\texcept (StopIteration, KeyError): \n\t \treturn None \n\ttry: \n\t \tmeth_node = parent[name] \n\texcept KeyError: \n\t \treturn None \n\tif isinstance(meth_node, astroid.Function): \n\t \treturn meth_node \n\treturn None\n", 
" \tif value.tzinfo: \n\t \tvalue = value.astimezone(UTC) \n\treturn (long((calendar.timegm(value.timetuple()) * 1000000L)) + value.microsecond)\n", 
" \t(p, u) = getparser(use_datetime=use_datetime, use_builtin_types=use_builtin_types) \n\tp.feed(data) \n\tp.close() \n\treturn (u.close(), u.getmethodname())\n", 
" \tlater = (mktime(date1.timetuple()) + (date1.microsecond / 1000000.0)) \n\tearlier = (mktime(date2.timetuple()) + (date2.microsecond / 1000000.0)) \n\treturn (later - earlier)\n", 
" \tgenerate_completions(event)\n", 
" \treturn os.path.join('$pybasedir', *args)\n", 
" \treturn os.path.join('$bindir', *args)\n", 
" \treturn os.path.join('$state_path', *args)\n", 
" \treturn os.path.join('$pybasedir', *args)\n", 
" \treturn os.path.join('$bindir', *args)\n", 
" \treturn os.path.join('$state_path', *args)\n", 
" \tif (not CONF.notifications.notify_on_api_faults): \n\t \treturn \n\tpayload = {'url': url, 'exception': six.text_type(exception), 'status': status} \n\trpc.get_notifier('api').error((common_context.get_current() or nova.context.get_admin_context()), 'api.fault', payload)\n", 
" \tif (not CONF.notifications.notify_on_state_change): \n\t \treturn \n\tupdate_with_state_change = False \n\told_vm_state = old_instance['vm_state'] \n\tnew_vm_state = new_instance['vm_state'] \n\told_task_state = old_instance['task_state'] \n\tnew_task_state = new_instance['task_state'] \n\tif (old_vm_state != new_vm_state): \n\t \tupdate_with_state_change = True \n\telif ((CONF.notifications.notify_on_state_change == 'vm_and_task_state') and (old_task_state != new_task_state)): \n\t \tupdate_with_state_change = True \n\tif update_with_state_change: \n\t \tsend_update_with_states(context, new_instance, old_vm_state, new_vm_state, old_task_state, new_task_state, service, host) \n\telse: \n\t \ttry: \n\t \t \told_display_name = None \n\t \t \tif (new_instance['display_name'] != old_instance['display_name']): \n\t \t \t \told_display_name = old_instance['display_name'] \n\t \t \t_send_instance_update_notification(context, new_instance, service=service, host=host, old_display_name=old_display_name) \n\t \texcept exception.InstanceNotFound: \n\t \t \tLOG.debug('Failed \tto \tsend \tinstance \tupdate \tnotification. \tThe \tinstance \tcould \tnot \tbe \tfound \tand \twas \tmost \tlikely \tdeleted.', instance=new_instance) \n\t \texcept Exception: \n\t \t \tLOG.exception(_LE('Failed \tto \tsend \tstate \tupdate \tnotification'), instance=new_instance)\n", 
" \tif (not CONF.notifications.notify_on_state_change): \n\t \treturn \n\tfire_update = True \n\tif verify_states: \n\t \tfire_update = False \n\t \tif (old_vm_state != new_vm_state): \n\t \t \tfire_update = True \n\t \telif ((CONF.notifications.notify_on_state_change == 'vm_and_task_state') and (old_task_state != new_task_state)): \n\t \t \tfire_update = True \n\tif fire_update: \n\t \ttry: \n\t \t \t_send_instance_update_notification(context, instance, old_vm_state=old_vm_state, old_task_state=old_task_state, new_vm_state=new_vm_state, new_task_state=new_task_state, service=service, host=host) \n\t \texcept exception.InstanceNotFound: \n\t \t \tLOG.debug('Failed \tto \tsend \tinstance \tupdate \tnotification. \tThe \tinstance \tcould \tnot \tbe \tfound \tand \twas \tmost \tlikely \tdeleted.', instance=instance) \n\t \texcept Exception: \n\t \t \tLOG.exception(_LE('Failed \tto \tsend \tstate \tupdate \tnotification'), instance=instance)\n", 
" \tpayload = info_from_instance(context, instance, None, None) \n\tpayload.update(_compute_states_payload(instance, old_vm_state, old_task_state, new_vm_state, new_task_state)) \n\t(audit_start, audit_end) = audit_period_bounds(current_period=True) \n\tpayload['audit_period_beginning'] = audit_start \n\tpayload['audit_period_ending'] = audit_end \n\tbw = bandwidth_usage(instance, audit_start) \n\tpayload['bandwidth'] = bw \n\tif old_display_name: \n\t \tpayload['old_display_name'] = old_display_name \n\trpc.get_notifier(service, host).info(context, 'compute.instance.update', payload) \n\t_send_versioned_instance_update(context, instance, payload, host, service)\n", 
" \t(begin, end) = utils.last_completed_audit_period() \n\tif current_period: \n\t \taudit_start = end \n\t \taudit_end = timeutils.utcnow() \n\telse: \n\t \taudit_start = begin \n\t \taudit_end = end \n\treturn (audit_start, audit_end)\n", 
" \tadmin_context = nova.context.get_admin_context(read_deleted='yes') \n\tdef _get_nwinfo_old_skool(): \n\t \t'Support \tfor \tgetting \tnetwork \tinfo \twithout \tobjects.' \n\t \tif (instance_ref.get('info_cache') and (instance_ref['info_cache'].get('network_info') is not None)): \n\t \t \tcached_info = instance_ref['info_cache']['network_info'] \n\t \t \tif isinstance(cached_info, network_model.NetworkInfo): \n\t \t \t \treturn cached_info \n\t \t \treturn network_model.NetworkInfo.hydrate(cached_info) \n\t \ttry: \n\t \t \treturn network.API().get_instance_nw_info(admin_context, instance_ref) \n\t \texcept Exception: \n\t \t \ttry: \n\t \t \t \twith excutils.save_and_reraise_exception(): \n\t \t \t \t \tLOG.exception(_LE('Failed \tto \tget \tnw_info'), instance=instance_ref) \n\t \t \texcept Exception: \n\t \t \t \tif ignore_missing_network_data: \n\t \t \t \t \treturn \n\t \t \t \traise \n\tif isinstance(instance_ref, obj_base.NovaObject): \n\t \tnw_info = instance_ref.info_cache.network_info \n\t \tif (nw_info is None): \n\t \t \tnw_info = network_model.NetworkInfo() \n\telse: \n\t \tnw_info = _get_nwinfo_old_skool() \n\tmacs = [vif['address'] for vif in nw_info] \n\tuuids = [instance_ref['uuid']] \n\tbw_usages = objects.BandwidthUsageList.get_by_uuids(admin_context, uuids, audit_start) \n\tbw = {} \n\tfor b in bw_usages: \n\t \tif (b.mac in macs): \n\t \t \tlabel = ('net-name-not-found-%s' % b.mac) \n\t \t \tfor vif in nw_info: \n\t \t \t \tif (vif['address'] == b.mac): \n\t \t \t \t \tlabel = vif['network']['label'] \n\t \t \t \t \tbreak \n\t \t \tbw[label] = dict(bw_in=b.bw_in, bw_out=b.bw_out) \n\treturn bw\n", 
" \timage_meta = {} \n\tfor (md_key, md_value) in system_metadata.items(): \n\t \tif md_key.startswith('image_'): \n\t \t \timage_meta[md_key[6:]] = md_value \n\treturn image_meta\n", 
" \tdef null_safe_str(s): \n\t \treturn (str(s) if s else '') \n\tdef null_safe_int(s): \n\t \treturn (int(s) if s else '') \n\tdef null_safe_isotime(s): \n\t \tif isinstance(s, datetime.datetime): \n\t \t \treturn utils.strtime(s) \n\t \telse: \n\t \t \treturn (str(s) if s else '') \n\timage_ref_url = glance.generate_image_url(instance.image_ref) \n\tinstance_type = instance.get_flavor() \n\tinstance_type_name = instance_type.get('name', '') \n\tinstance_flavorid = instance_type.get('flavorid', '') \n\tinstance_info = dict(tenant_id=instance.project_id, user_id=instance.user_id, instance_id=instance.uuid, display_name=instance.display_name, reservation_id=instance.reservation_id, hostname=instance.hostname, instance_type=instance_type_name, instance_type_id=instance.instance_type_id, instance_flavor_id=instance_flavorid, architecture=instance.architecture, memory_mb=instance.flavor.memory_mb, disk_gb=(instance.flavor.root_gb + instance.flavor.ephemeral_gb), vcpus=instance.flavor.vcpus, root_gb=instance.flavor.root_gb, ephemeral_gb=instance.flavor.ephemeral_gb, host=instance.host, node=instance.node, availability_zone=instance.availability_zone, cell_name=null_safe_str(instance.cell_name), created_at=str(instance.created_at), terminated_at=null_safe_isotime(instance.get('terminated_at', None)), deleted_at=null_safe_isotime(instance.get('deleted_at', None)), launched_at=null_safe_isotime(instance.get('launched_at', None)), image_ref_url=image_ref_url, os_type=instance.os_type, kernel_id=instance.kernel_id, ramdisk_id=instance.ramdisk_id, state=instance.vm_state, state_description=null_safe_str(instance.task_state), progress=null_safe_int(instance.progress), access_ip_v4=instance.access_ip_v4, access_ip_v6=instance.access_ip_v6) \n\tif (network_info is not None): \n\t \tfixed_ips = [] \n\t \tfor vif in network_info: \n\t \t \tfor ip in vif.fixed_ips(): \n\t \t \t \tip['label'] = vif['network']['label'] \n\t \t \t \tip['vif_mac'] = vif['address'] \n\t \t \t \tfixed_ips.append(ip) \n\t \tinstance_info['fixed_ips'] = fixed_ips \n\timage_meta_props = image_meta(instance.system_metadata) \n\tinstance_info['image_meta'] = image_meta_props \n\tinstance_info['metadata'] = instance.metadata \n\tinstance_info.update(kw) \n\treturn instance_info\n", 
" \tkwargs = {} \n\tv_list = ['public', 'private'] \n\tcf_list = ['ami', 'ari', 'aki', 'bare', 'ovf'] \n\tdf_list = ['ami', 'ari', 'aki', 'vhd', 'vmdk', 'raw', 'qcow2', 'vdi', 'iso'] \n\tkwargs['copy_from'] = location \n\tif (visibility is not None): \n\t \tif (visibility not in v_list): \n\t \t \traise SaltInvocationError(('\"visibility\" \tneeds \tto \tbe \tone \t' + 'of \tthe \tfollowing: \t{0}'.format(', \t'.join(v_list)))) \n\t \telif (visibility == 'public'): \n\t \t \tkwargs['is_public'] = True \n\t \telse: \n\t \t \tkwargs['is_public'] = False \n\telse: \n\t \tkwargs['is_public'] = True \n\tif (container_format not in cf_list): \n\t \traise SaltInvocationError(('\"container_format\" \tneeds \tto \tbe \t' + 'one \tof \tthe \tfollowing: \t{0}'.format(', \t'.join(cf_list)))) \n\telse: \n\t \tkwargs['container_format'] = container_format \n\tif (disk_format not in df_list): \n\t \traise SaltInvocationError(('\"disk_format\" \tneeds \tto \tbe \tone \t' + 'of \tthe \tfollowing: \t{0}'.format(', \t'.join(df_list)))) \n\telse: \n\t \tkwargs['disk_format'] = disk_format \n\tif (protected is not None): \n\t \tkwargs['protected'] = protected \n\tg_client = _auth(profile, api_version=1) \n\timage = g_client.images.create(name=name, **kwargs) \n\treturn image_show(image.id, profile=profile)\n", 
" \treturn ('%s/images/%s' % (generate_glance_url(), image_ref))\n", 
" \turl = urllib.parse.urlparse(image_href) \n\tnetloc = url.netloc \n\timage_id = url.path.split('/')[(-1)] \n\tuse_ssl = (url.scheme == 'https') \n\treturn (image_id, netloc, use_ssl)\n", 
" \tparams = {} \n\tparams['identity_headers'] = generate_identity_headers(context) \n\tif endpoint.startswith('https://'): \n\t \tparams['insecure'] = CONF.glance.api_insecure \n\t \tparams['ssl_compression'] = False \n\t \tsslutils.is_enabled(CONF) \n\t \tif CONF.ssl.cert_file: \n\t \t \tparams['cert_file'] = CONF.ssl.cert_file \n\t \tif CONF.ssl.key_file: \n\t \t \tparams['key_file'] = CONF.ssl.key_file \n\t \tif CONF.ssl.ca_file: \n\t \t \tparams['cacert'] = CONF.ssl.ca_file \n\treturn glanceclient.Client(str(version), endpoint, **params)\n", 
" \tapi_servers = [] \n\tfor api_server in CONF.glance.api_servers: \n\t \tif ('//' not in api_server): \n\t \t \tapi_server = ('http://' + api_server) \n\t \t \tLOG.warning(_LW(\"No \tprotocol \tspecified \tin \tfor \tapi_server \t'%s', \tplease \tupdate \t[glance] \tapi_servers \twith \tfully \tqualified \turl \tincluding \tscheme \t(http \t/ \thttps)\"), api_server) \n\t \tapi_servers.append(api_server) \n\trandom.shuffle(api_servers) \n\treturn itertools.cycle(api_servers)\n", 
" \tfor attr in ['created_at', 'updated_at', 'deleted_at']: \n\t \tif image_meta.get(attr): \n\t \t \timage_meta[attr] = timeutils.parse_isotime(image_meta[attr]) \n\treturn image_meta\n", 
" \t(exc_type, exc_value, exc_trace) = sys.exc_info() \n\tnew_exc = _translate_image_exception(image_id, exc_value) \n\tsix.reraise(type(new_exc), new_exc, exc_trace)\n", 
" \t(exc_type, exc_value, exc_trace) = sys.exc_info() \n\tnew_exc = _translate_plain_exception(exc_value) \n\tsix.reraise(type(new_exc), new_exc, exc_trace)\n", 
" \tif ('/' not in str(image_href)): \n\t \timage_service = get_default_image_service() \n\t \treturn (image_service, image_href) \n\ttry: \n\t \t(image_id, endpoint) = _endpoint_from_image_ref(image_href) \n\t \tglance_client = GlanceClientWrapper(context=context, endpoint=endpoint) \n\texcept ValueError: \n\t \traise exception.InvalidImageRef(image_href=image_href) \n\timage_service = GlanceImageServiceV2(client=glance_client) \n\treturn (image_service, image_id)\n", 
" \treturn {k: v for (k, v) in original.items() if ('_pass' not in k)}\n", 
" \tdef inner(f): \n\t \tdef wrapped(self, context, *args, **kw): \n\t \t \ttry: \n\t \t \t \treturn f(self, context, *args, **kw) \n\t \t \texcept Exception as e: \n\t \t \t \twith excutils.save_and_reraise_exception(): \n\t \t \t \t \tif (notifier or get_notifier): \n\t \t \t \t \t \tcall_dict = _get_call_dict(f, self, context, *args, **kw) \n\t \t \t \t \t \tfunction_name = f.__name__ \n\t \t \t \t \t \t_emit_exception_notification((notifier or get_notifier()), context, e, function_name, call_dict, binary) \n\t \treturn functools.wraps(f)(wrapped) \n\treturn inner\n", 
" \tif (not context): \n\t \treturn False \n\tif context.is_admin: \n\t \treturn False \n\tif ((not context.user_id) or (not context.project_id)): \n\t \treturn False \n\treturn True\n", 
" \tif ((not ctxt.is_admin) and (not is_user_context(ctxt))): \n\t \traise exception.Forbidden()\n", 
" \tif ((not ctxt.is_admin) and (not is_user_context(ctxt))): \n\t \traise exception.Forbidden()\n", 
" \tif is_user_context(context): \n\t \tif (not context.project_id): \n\t \t \traise exception.Forbidden() \n\t \telif (context.project_id != project_id): \n\t \t \traise exception.Forbidden()\n", 
" \tif is_user_context(context): \n\t \tif (not context.user_id): \n\t \t \traise exception.Forbidden() \n\t \telif (context.user_id != user_id): \n\t \t \traise exception.Forbidden()\n", 
" \tif is_user_context(context): \n\t \tif (not context.quota_class): \n\t \t \traise exception.Forbidden() \n\t \telif (context.quota_class != class_name): \n\t \t \traise exception.Forbidden()\n", 
" \tdef outer(f): \n\t \tf.__hook_name__ = name \n\t \t@functools.wraps(f) \n\t \tdef inner(*args, **kwargs): \n\t \t \tmanager = _HOOKS.setdefault(name, HookManager(name)) \n\t \t \tfunction = None \n\t \t \tif pass_function: \n\t \t \t \tfunction = f \n\t \t \tmanager.run_pre(name, args, kwargs, f=function) \n\t \t \trv = f(*args, **kwargs) \n\t \t \tmanager.run_post(name, rv, args, kwargs, f=function) \n\t \t \treturn rv \n\t \treturn inner \n\treturn outer\n", 
" \t_HOOKS.clear()\n", 
" \taddresses = [] \n\tfor interface in netifaces.interfaces(): \n\t \ttry: \n\t \t \tiface_data = netifaces.ifaddresses(interface) \n\t \t \tfor family in iface_data: \n\t \t \t \tif (family not in (netifaces.AF_INET, netifaces.AF_INET6)): \n\t \t \t \t \tcontinue \n\t \t \t \tfor address in iface_data[family]: \n\t \t \t \t \taddr = address['addr'] \n\t \t \t \t \tif (family == netifaces.AF_INET6): \n\t \t \t \t \t \taddr = addr.split('%')[0] \n\t \t \t \t \taddresses.append(addr) \n\t \texcept ValueError: \n\t \t \tpass \n\treturn addresses\n", 
" \tif ((not os.path.exists(path)) and (CONF.libvirt.images_type != 'rbd')): \n\t \traise exception.DiskNotFound(location=path) \n\ttry: \n\t \tif (os.path.isdir(path) and os.path.exists(os.path.join(path, 'DiskDescriptor.xml'))): \n\t \t \tpath = os.path.join(path, 'root.hds') \n\t \tcmd = ('env', 'LC_ALL=C', 'LANG=C', 'qemu-img', 'info', path) \n\t \tif (format is not None): \n\t \t \tcmd = (cmd + ('-f', format)) \n\t \t(out, err) = utils.execute(prlimit=QEMU_IMG_LIMITS, *cmd) \n\texcept processutils.ProcessExecutionError as exp: \n\t \tif (exp.exit_code == (-9)): \n\t \t \tmsg = (_('qemu-img \taborted \tby \tprlimits \twhen \tinspecting \t%(path)s \t: \t%(exp)s') % {'path': path, 'exp': exp}) \n\t \telse: \n\t \t \tmsg = (_('qemu-img \tfailed \tto \texecute \ton \t%(path)s \t: \t%(exp)s') % {'path': path, 'exp': exp}) \n\t \traise exception.InvalidDiskInfo(reason=msg) \n\tif (not out): \n\t \tmsg = (_('Failed \tto \trun \tqemu-img \tinfo \ton \t%(path)s \t: \t%(error)s') % {'path': path, 'error': err}) \n\t \traise exception.InvalidDiskInfo(reason=msg) \n\treturn imageutils.QemuImgInfo(out)\n", 
" \tif (in_format is None): \n\t \traise RuntimeError('convert_image \twithout \tinput \tformat \tis \ta \tsecurity \trisk') \n\t_convert_image(source, dest, in_format, out_format, run_as_root)\n", 
" \tif (use_ipv6 is None): \n\t \tuse_ipv6 = CONF.use_ipv6 \n\tif (not template): \n\t \ttemplate = CONF.injected_network_template \n\tif (not (network_info and template)): \n\t \treturn \n\tnets = [] \n\tifc_num = (-1) \n\tipv6_is_available = False \n\tfor vif in network_info: \n\t \tif ((not vif['network']) or (not vif['network']['subnets'])): \n\t \t \tcontinue \n\t \tnetwork = vif['network'] \n\t \tsubnet_v4 = _get_first_network(network, 4) \n\t \tsubnet_v6 = _get_first_network(network, 6) \n\t \tifc_num += 1 \n\t \tif (not network.get_meta('injected')): \n\t \t \tcontinue \n\t \thwaddress = vif.get('address') \n\t \taddress = None \n\t \tnetmask = None \n\t \tgateway = '' \n\t \tbroadcast = None \n\t \tdns = None \n\t \troutes = [] \n\t \tif subnet_v4: \n\t \t \tif (subnet_v4.get_meta('dhcp_server') is not None): \n\t \t \t \tcontinue \n\t \t \tif subnet_v4['ips']: \n\t \t \t \tip = subnet_v4['ips'][0] \n\t \t \t \taddress = ip['address'] \n\t \t \t \tnetmask = model.get_netmask(ip, subnet_v4) \n\t \t \t \tif subnet_v4['gateway']: \n\t \t \t \t \tgateway = subnet_v4['gateway']['address'] \n\t \t \t \tbroadcast = str(subnet_v4.as_netaddr().broadcast) \n\t \t \t \tdns = ' \t'.join([i['address'] for i in subnet_v4['dns']]) \n\t \t \t \tfor route_ref in subnet_v4['routes']: \n\t \t \t \t \t(net, mask) = get_net_and_mask(route_ref['cidr']) \n\t \t \t \t \troute = {'gateway': str(route_ref['gateway']['address']), 'cidr': str(route_ref['cidr']), 'network': net, 'netmask': mask} \n\t \t \t \t \troutes.append(route) \n\t \taddress_v6 = None \n\t \tgateway_v6 = '' \n\t \tnetmask_v6 = None \n\t \tdns_v6 = None \n\t \thave_ipv6 = (use_ipv6 and subnet_v6) \n\t \tif have_ipv6: \n\t \t \tif (subnet_v6.get_meta('dhcp_server') is not None): \n\t \t \t \tcontinue \n\t \t \tif subnet_v6['ips']: \n\t \t \t \tipv6_is_available = True \n\t \t \t \tip_v6 = subnet_v6['ips'][0] \n\t \t \t \taddress_v6 = ip_v6['address'] \n\t \t \t \tnetmask_v6 = model.get_netmask(ip_v6, subnet_v6) \n\t \t \t \tif subnet_v6['gateway']: \n\t \t \t \t \tgateway_v6 = subnet_v6['gateway']['address'] \n\t \t \t \tdns_v6 = ' \t'.join([i['address'] for i in subnet_v6['dns']]) \n\t \tnet_info = {'name': ('eth%d' % ifc_num), 'hwaddress': hwaddress, 'address': address, 'netmask': netmask, 'gateway': gateway, 'broadcast': broadcast, 'dns': dns, 'routes': routes, 'address_v6': address_v6, 'gateway_v6': gateway_v6, 'netmask_v6': netmask_v6, 'dns_v6': dns_v6} \n\t \tnets.append(net_info) \n\tif (not nets): \n\t \treturn \n\t(tmpl_path, tmpl_file) = os.path.split(template) \n\tenv = jinja2.Environment(loader=jinja2.FileSystemLoader(tmpl_path), trim_blocks=True) \n\ttemplate = env.get_template(tmpl_file) \n\treturn template.render({'interfaces': nets, 'use_ipv6': ipv6_is_available, 'libvirt_virt_type': libvirt_virt_type})\n", 
" \tinterface = str(interface) \n\tif (interface not in sys.interfaces): \n\t \treturn None \n\tif (sys.name == 'default'): \n\t \treturn 'default' \n\tmac = sys.get_mac_address(interface) \n\tip = sys.get_ip_address(interface) \n\tif ((mac is not None) and (mac != '')): \n\t \treturn ('01-' + '-'.join(mac.split(':')).lower()) \n\telif ((ip is not None) and (ip != '')): \n\t \treturn get_host_ip(ip) \n\telse: \n\t \treturn sys.name\n", 
" \ttry: \n\t \tos.mkdir(path) \n\texcept OSError: \n\t \tlogging.error(\"OSError \twhile \tcreating \tdir \t'{0}'\".format(path)) \n\t \treturn False \n\treturn True\n", 
" \treturn urljoin(BASE_URL, path)\n", 
" \tinterface = str(interface) \n\tif (interface not in sys.interfaces): \n\t \treturn None \n\tif (sys.name == 'default'): \n\t \treturn 'default' \n\tmac = sys.get_mac_address(interface) \n\tip = sys.get_ip_address(interface) \n\tif ((mac is not None) and (mac != '')): \n\t \treturn ('01-' + '-'.join(mac.split(':')).lower()) \n\telif ((ip is not None) and (ip != '')): \n\t \treturn get_host_ip(ip) \n\telse: \n\t \treturn sys.name\n", 
" \treturn IPDevice(device_name, namespace=namespace).link.address\n", 
" \tip = netaddr.ip.IPAddress(ip) \n\tcidr = netaddr.ip.IPNetwork(ip) \n\tif (len(cidr) == 1): \n\t \treturn pretty_hex(ip) \n\telse: \n\t \tpretty = pretty_hex(cidr[0]) \n\t \tif ((not shorten) or (len(cidr) <= 8)): \n\t \t \treturn pretty \n\t \telse: \n\t \t \tcutoff = ((32 - cidr.prefixlen) / 4) \n\t \t \treturn pretty[0:(- cutoff)]\n", 
" \tsession = Session.object_session(series) \n\treleases = session.query(Episode).join(Episode.releases, Episode.series).filter((Series.id == series.id)) \n\tif downloaded: \n\t \treleases = releases.filter((Release.downloaded == True)) \n\tif (season is not None): \n\t \treleases = releases.filter((Episode.season == season)) \n\tif (series.identified_by and (series.identified_by != u'auto')): \n\t \treleases = releases.filter((Episode.identified_by == series.identified_by)) \n\tif (series.identified_by in [u'ep', u'sequence']): \n\t \tlatest_release = releases.order_by(desc(Episode.season), desc(Episode.number)).first() \n\telif (series.identified_by == u'date'): \n\t \tlatest_release = releases.order_by(desc(Episode.identifier)).first() \n\telse: \n\t \tlatest_release = releases.order_by(desc(Episode.first_seen.label(u'ep_first_seen'))).first() \n\tif (not latest_release): \n\t \tlog.debug(u'get_latest_release \treturning \tNone, \tno \tdownloaded \tepisodes \tfound \tfor: \t%s', series.name) \n\t \treturn \n\treturn latest_release\n", 
" \tif hasattr(obj, 'bind'): \n\t \tconn = obj.bind \n\telse: \n\t \ttry: \n\t \t \tconn = object_session(obj).bind \n\t \texcept UnmappedInstanceError: \n\t \t \tconn = obj \n\tif (not hasattr(conn, 'execute')): \n\t \traise TypeError('This \tmethod \taccepts \tonly \tSession, \tEngine, \tConnection \tand \tdeclarative \tmodel \tobjects.') \n\treturn conn\n", 
" \tglobal _REPOSITORY \n\trel_path = 'migrate_repo' \n\tif (database == 'api'): \n\t \trel_path = os.path.join('api_migrations', 'migrate_repo') \n\tpath = os.path.join(os.path.abspath(os.path.dirname(__file__)), rel_path) \n\tassert os.path.exists(path) \n\tif (_REPOSITORY.get(database) is None): \n\t \t_REPOSITORY[database] = Repository(path) \n\treturn _REPOSITORY[database]\n", 
" \tif (read_deleted is None): \n\t \tread_deleted = context.read_deleted \n\tquery_kwargs = {} \n\tif ('no' == read_deleted): \n\t \tquery_kwargs['deleted'] = False \n\telif ('only' == read_deleted): \n\t \tquery_kwargs['deleted'] = True \n\telif ('yes' == read_deleted): \n\t \tpass \n\telse: \n\t \traise ValueError((_(\"Unrecognized \tread_deleted \tvalue \t'%s'\") % read_deleted)) \n\tquery = sqlalchemyutils.model_query(model, context.session, args, **query_kwargs) \n\tif (nova.context.is_user_context(context) and project_only): \n\t \tif (project_only == 'allow_none'): \n\t \t \tquery = query.filter(or_((model.project_id == context.project_id), (model.project_id == null()))) \n\t \telse: \n\t \t \tquery = query.filter_by(project_id=context.project_id) \n\treturn query\n", 
" \tdef match(node): \n\t \treturn isinstance(node, target_cls) \n\treturn match\n", 
" \treturn IMPL.db_sync(version=version, database=database, context=context)\n", 
" \treturn IMPL.db_version(database=database, context=context)\n", 
" \tdef _parse_address(fields): \n\t \tmacs = [] \n\t \tfor interface in fields.interfaces: \n\t \t \tmacs.append(interface['address']) \n\t \treturn ', \t'.join((('%s' % i) for i in macs)) \n\tformatters = {'MAC \tAddress': _parse_address} \n\t_translate_baremetal_node_keys(nodes) \n\tutils.print_list(nodes, ['ID', 'Host', 'Task \tState', 'CPUs', 'Memory_MB', 'Disk_GB', 'MAC \tAddress', 'PM \tAddress', 'PM \tUsername', 'PM \tPassword', 'Terminal \tPort'], formatters=formatters)\n", 
" \tcmd = 'iscsiadm \t--mode \tnode' \n\toutput = utils.system_output(cmd) \n\tpattern = '(\\\\d+\\\\.\\\\d+\\\\.\\\\d+\\\\.\\\\d+|\\\\W:{2}\\\\d\\\\W):\\\\d+,\\\\d+\\\\s+([\\\\w\\\\.\\\\-:\\\\d]+)' \n\tnodes = [] \n\tif ('No \trecords \tfound' not in output): \n\t \tnodes = re.findall(pattern, output) \n\treturn nodes\n", 
" \tcmd = 'iscsiadm \t--mode \tnode' \n\toutput = utils.system_output(cmd) \n\tpattern = '(\\\\d+\\\\.\\\\d+\\\\.\\\\d+\\\\.\\\\d+|\\\\W:{2}\\\\d\\\\W):\\\\d+,\\\\d+\\\\s+([\\\\w\\\\.\\\\-:\\\\d]+)' \n\tnodes = [] \n\tif ('No \trecords \tfound' not in output): \n\t \tnodes = re.findall(pattern, output) \n\treturn nodes\n", 
" \tif (call != 'function'): \n\t \traise SaltCloudSystemExit('The \trescan_hba \tfunction \tmust \tbe \tcalled \twith \t-f \tor \t--function.') \n\thba = (kwargs.get('hba') if (kwargs and ('hba' in kwargs)) else None) \n\thost_name = (kwargs.get('host') if (kwargs and ('host' in kwargs)) else None) \n\tif (not host_name): \n\t \traise SaltCloudSystemExit('You \tmust \tspecify \tname \tof \tthe \thost \tsystem.') \n\thost_ref = salt.utils.vmware.get_mor_by_property(_get_si(), vim.HostSystem, host_name) \n\ttry: \n\t \tif hba: \n\t \t \tlog.info('Rescanning \tHBA \t{0} \ton \thost \t{1}'.format(hba, host_name)) \n\t \t \thost_ref.configManager.storageSystem.RescanHba(hba) \n\t \t \tret = 'rescanned \tHBA \t{0}'.format(hba) \n\t \telse: \n\t \t \tlog.info('Rescanning \tall \tHBAs \ton \thost \t{0}'.format(host_name)) \n\t \t \thost_ref.configManager.storageSystem.RescanAllHba() \n\t \t \tret = 'rescanned \tall \tHBAs' \n\texcept Exception as exc: \n\t \tlog.error('Error \twhile \trescaning \tHBA \ton \thost \t{0}: \t{1}'.format(host_name, exc), exc_info_on_loglevel=logging.DEBUG) \n\t \treturn {host_name: 'failed \tto \trescan \tHBA'} \n\treturn {host_name: ret}\n", 
" \tif mountpoint.startswith('/dev/'): \n\t \tmountpoint = mountpoint[5:] \n\tif re.match('^[hs]d[a-p]$', mountpoint): \n\t \treturn (ord(mountpoint[2:3]) - ord('a')) \n\telif re.match('^x?vd[a-p]$', mountpoint): \n\t \treturn (ord(mountpoint[(-1)]) - ord('a')) \n\telif re.match('^[0-9]+$', mountpoint): \n\t \treturn int(mountpoint, 10) \n\telse: \n\t \tLOG.warning(_LW('Mountpoint \tcannot \tbe \ttranslated: \t%s'), mountpoint) \n\t \treturn (-1)\n", 
" \treturn u', \t'.join((repr_readers(h) + repr_writers(h)))\n", 
" \timage_ref = instance.image_ref \n\tLOG.debug('Downloading \timage \tfile \tdata \t%(image_ref)s \tto \tthe \tdata \tstore \t%(data_store_name)s', {'image_ref': image_ref, 'data_store_name': ds_name}, instance=instance) \n\tmetadata = IMAGE_API.get(context, image_ref) \n\tfile_size = int(metadata['size']) \n\tread_iter = IMAGE_API.download(context, image_ref) \n\tread_file_handle = rw_handles.ImageReadHandle(read_iter) \n\twrite_file_handle = rw_handles.FileWriteHandle(host, port, dc_name, ds_name, cookies, file_path, file_size) \n\timage_transfer(read_file_handle, write_file_handle) \n\tLOG.debug('Downloaded \timage \tfile \tdata \t%(image_ref)s \tto \t%(upload_name)s \ton \tthe \tdata \tstore \t%(data_store_name)s', {'image_ref': image_ref, 'upload_name': ('n/a' if (file_path is None) else file_path), 'data_store_name': ('n/a' if (ds_name is None) else ds_name)}, instance=instance)\n", 
" \tLOG.debug('Uploading \timage \t%s', image_id, instance=instance) \n\tmetadata = IMAGE_API.get(context, image_id) \n\tread_handle = rw_handles.VmdkReadHandle(session, session._host, session._port, vm, None, vmdk_size) \n\timage_metadata = {'disk_format': constants.DISK_FORMAT_VMDK, 'is_public': metadata['is_public'], 'name': metadata['name'], 'status': 'active', 'container_format': constants.CONTAINER_FORMAT_BARE, 'size': 0, 'properties': {'vmware_image_version': 1, 'vmware_disktype': 'streamOptimized', 'owner_id': instance.project_id}} \n\tupdater = loopingcall.FixedIntervalLoopingCall(read_handle.update_progress) \n\ttry: \n\t \tupdater.start(interval=NFC_LEASE_UPDATE_PERIOD) \n\t \tIMAGE_API.update(context, image_id, image_metadata, data=read_handle) \n\tfinally: \n\t \tupdater.stop() \n\t \tread_handle.close() \n\tLOG.debug('Uploaded \timage \t%s \tto \tthe \tGlance \timage \tserver', image_id, instance=instance)\n", 
" \tif (adapter_type in [constants.ADAPTER_TYPE_LSILOGICSAS, constants.ADAPTER_TYPE_PARAVIRTUAL]): \n\t \tvmdk_adapter_type = constants.DEFAULT_ADAPTER_TYPE \n\telse: \n\t \tvmdk_adapter_type = adapter_type \n\treturn vmdk_adapter_type\n", 
" \tfor c in _CLASSES: \n\t \tif (c == 'files'): \n\t \t \t_db_content[c] = [] \n\t \telse: \n\t \t \t_db_content[c] = {}\n", 
" \tcleanup() \n\tcreate_network() \n\tcreate_folder() \n\tcreate_host_network_system() \n\tcreate_host_storage_system() \n\tds_ref1 = create_datastore('ds1', 1024, 500) \n\tcreate_host(ds_ref=ds_ref1) \n\tds_ref2 = create_datastore('ds2', 1024, 500) \n\tcreate_host(ds_ref=ds_ref2) \n\tcreate_datacenter('dc1', ds_ref1) \n\tcreate_datacenter('dc2', ds_ref2) \n\tcreate_res_pool() \n\tcreate_cluster('test_cluster', ds_ref1) \n\tcreate_cluster('test_cluster2', ds_ref2)\n", 
" \tfor c in _CLASSES: \n\t \tif (c == 'files'): \n\t \t \t_db_content[c] = [] \n\t \telse: \n\t \t \t_db_content[c] = {}\n", 
" \t_db_content.setdefault(table, {}) \n\t_db_content[table][table_obj.obj] = table_obj\n", 
" \tlst_objs = FakeRetrieveResult() \n\tfor key in _db_content[obj_type]: \n\t \tlst_objs.add_object(_db_content[obj_type][key]) \n\treturn lst_objs\n", 
" \t_db_content['files'].append(file_path)\n", 
" \tif (file_path.find('.vmdk') != (-1)): \n\t \tif (file_path not in _db_content.get('files')): \n\t \t \traise vexc.FileNotFoundException(file_path) \n\t \t_db_content.get('files').remove(file_path) \n\telse: \n\t \tto_delete = set() \n\t \tfor file in _db_content.get('files'): \n\t \t \tif (file.find(file_path) != (-1)): \n\t \t \t \tto_delete.add(file) \n\t \tfor file in to_delete: \n\t \t \t_db_content.get('files').remove(file)\n", 
" \tpass\n", 
" \treturn {'type': 'fake'}\n", 
" \t_db_content['files'].append(file_path)\n", 
" \tpass\n", 
" \tds_file_path = ((('[' + ds_name) + '] \t') + file_path) \n\t_add_file(ds_file_path)\n", 
" \tif (_db_content.get('VirtualMachine', None) is None): \n\t \traise exception.NotFound('There \tis \tno \tVM \tregistered') \n\tif (vm_ref not in _db_content.get('VirtualMachine')): \n\t \traise exception.NotFound(('Virtual \tMachine \twith \tref \t%s \tis \tnot \tthere' % vm_ref)) \n\treturn _db_content.get('VirtualMachine')[vm_ref]\n", 
" \tobj_spec = client_factory.create('ns0:ObjectSpec') \n\tobj_spec.obj = obj \n\tobj_spec.skip = False \n\tif (select_set is not None): \n\t \tobj_spec.selectSet = select_set \n\treturn obj_spec\n", 
" \tobj_spec = client_factory.create('ns0:ObjectSpec') \n\tobj_spec.obj = obj \n\tobj_spec.skip = False \n\tif (select_set is not None): \n\t \tobj_spec.selectSet = select_set \n\treturn obj_spec\n", 
" \tTraversalSpec = vmodl.query.PropertyCollector.TraversalSpec \n\tSelectionSpec = vmodl.query.PropertyCollector.SelectionSpec \n\trpToRp = TraversalSpec(name='rpToRp', type=vim.ResourcePool, path='resourcePool', skip=False) \n\trpToRp.selectSet.extend((SelectionSpec(name='rpToRp'), SelectionSpec(name='rpToVm'))) \n\trpToVm = TraversalSpec(name='rpToVm', type=vim.ResourcePool, path='vm', skip=False) \n\tcrToRp = TraversalSpec(name='crToRp', type=vim.ComputeResource, path='resourcePool', skip=False) \n\tcrToRp.selectSet.extend((SelectionSpec(name='rpToRp'), SelectionSpec(name='rpToVm'))) \n\tcrToH = TraversalSpec(name='crToH', type=vim.ComputeResource, path='host', skip=False) \n\tdcToHf = TraversalSpec(name='dcToHf', type=vim.Datacenter, path='hostFolder', skip=False) \n\tdcToHf.selectSet.extend((SelectionSpec(name='visitFolders'),)) \n\tdcToVmf = TraversalSpec(name='dcToVmf', type=vim.Datacenter, path='vmFolder', skip=False) \n\tdcToVmf.selectSet.extend((SelectionSpec(name='visitFolders'),)) \n\tdcToNet = TraversalSpec(name='dcToNet', type=vim.Datacenter, path='networkFolder', skip=False) \n\tdcToNet.selectSet.extend((SelectionSpec(name='visitFolders'),)) \n\tdcToDs = TraversalSpec(name='dcToDs', type=vim.Datacenter, path='datastore', skip=False) \n\tdcToDs.selectSet.extend((SelectionSpec(name='visitFolders'),)) \n\thToVm = TraversalSpec(name='hToVm', type=vim.HostSystem, path='vm', skip=False) \n\thToVm.selectSet.extend((SelectionSpec(name='visitFolders'),)) \n\tvisitFolders = TraversalSpec(name='visitFolders', type=vim.Folder, path='childEntity', skip=False) \n\tvisitFolders.selectSet.extend((SelectionSpec(name='visitFolders'), SelectionSpec(name='dcToHf'), SelectionSpec(name='dcToVmf'), SelectionSpec(name='dcToNet'), SelectionSpec(name='crToH'), SelectionSpec(name='crToRp'), SelectionSpec(name='dcToDs'), SelectionSpec(name='hToVm'), SelectionSpec(name='rpToVm'))) \n\tfullTraversal = SelectionSpec.Array((visitFolders, dcToHf, dcToVmf, dcToNet, crToH, crToRp, dcToDs, rpToRp, hToVm, rpToVm)) \n\treturn fullTraversal\n", 
" \tprop_spec = client_factory.create('ns0:PropertySpec') \n\tprop_spec.type = spec_type \n\tprop_spec.pathSet = properties \n\treturn prop_spec\n", 
" \tobj_spec = client_factory.create('ns0:ObjectSpec') \n\tobj_spec.obj = obj \n\tobj_spec.skip = False \n\tif (select_set is not None): \n\t \tobj_spec.selectSet = select_set \n\treturn obj_spec\n", 
" \tprop_filter_spec = client_factory.create('ns0:PropertyFilterSpec') \n\tprop_filter_spec.propSet = prop_spec \n\tprop_filter_spec.objectSet = obj_spec \n\treturn prop_filter_spec\n", 
" \tclient_factory = vim.client.factory \n\tif (mobj is None): \n\t \treturn None \n\tusecoll = collector \n\tif (usecoll is None): \n\t \tusecoll = vim.service_content.propertyCollector \n\tproperty_filter_spec = client_factory.create('ns0:PropertyFilterSpec') \n\tproperty_spec = client_factory.create('ns0:PropertySpec') \n\tproperty_spec.all = ((properties is None) or (len(properties) == 0)) \n\tproperty_spec.pathSet = properties \n\tproperty_spec.type = type \n\tobject_spec = client_factory.create('ns0:ObjectSpec') \n\tobject_spec.obj = mobj \n\tobject_spec.skip = False \n\tproperty_filter_spec.propSet = [property_spec] \n\tproperty_filter_spec.objectSet = [object_spec] \n\toptions = client_factory.create('ns0:RetrieveOptions') \n\toptions.maxObjects = CONF.vmware.maximum_objects \n\treturn vim.RetrievePropertiesEx(usecoll, specSet=[property_filter_spec], options=options)\n", 
" \tclient_factory = vim.client.factory \n\tif (mobj is None): \n\t \treturn None \n\tusecoll = collector \n\tif (usecoll is None): \n\t \tusecoll = vim.service_content.propertyCollector \n\tproperty_filter_spec = client_factory.create('ns0:PropertyFilterSpec') \n\tproperty_spec = client_factory.create('ns0:PropertySpec') \n\tproperty_spec.all = ((properties is None) or (len(properties) == 0)) \n\tproperty_spec.pathSet = properties \n\tproperty_spec.type = type \n\tobject_spec = client_factory.create('ns0:ObjectSpec') \n\tobject_spec.obj = mobj \n\tobject_spec.skip = False \n\tproperty_filter_spec.propSet = [property_spec] \n\tproperty_filter_spec.objectSet = [object_spec] \n\toptions = client_factory.create('ns0:RetrieveOptions') \n\toptions.maxObjects = CONF.vmware.maximum_objects \n\treturn vim.RetrievePropertiesEx(usecoll, specSet=[property_filter_spec], options=options)\n", 
" \treturn vutil.get_objects(vim, type, CONF.vmware.maximum_objects, properties_to_collect, all)\n", 
" \tprop_spec = client_factory.create('ns0:PropertySpec') \n\tprop_spec.type = spec_type \n\tprop_spec.pathSet = properties \n\treturn prop_spec\n", 
" \tobj_spec = client_factory.create('ns0:ObjectSpec') \n\tobj_spec.obj = obj \n\tobj_spec.skip = False \n\tif (select_set is not None): \n\t \tobj_spec.selectSet = select_set \n\treturn obj_spec\n", 
" \tprop_filter_spec = client_factory.create('ns0:PropertyFilterSpec') \n\tprop_filter_spec.propSet = prop_spec \n\tprop_filter_spec.objectSet = obj_spec \n\treturn prop_filter_spec\n", 
" \tclient_factory = vim.client.factory \n\tif (len(obj_list) == 0): \n\t \treturn [] \n\tprop_spec = get_prop_spec(client_factory, type, properties) \n\tlst_obj_specs = [] \n\tfor obj in obj_list: \n\t \tlst_obj_specs.append(get_obj_spec(client_factory, obj)) \n\tprop_filter_spec = get_prop_filter_spec(client_factory, lst_obj_specs, [prop_spec]) \n\toptions = client_factory.create('ns0:RetrieveOptions') \n\toptions.maxObjects = CONF.vmware.maximum_objects \n\treturn vim.RetrievePropertiesEx(vim.service_content.propertyCollector, specSet=[prop_filter_spec], options=options)\n", 
" \tvlan_num = vif['network'].get_meta('vlan') \n\tbridge = vif['network']['bridge'] \n\tvlan_interface = CONF.vmware.vlan_interface \n\tnetwork_ref = network_util.get_network_with_the_name(session, bridge, cluster) \n\tif (network_ref and (network_ref['type'] == 'DistributedVirtualPortgroup')): \n\t \treturn network_ref \n\tif (not network_ref): \n\t \tvswitch_associated = _get_associated_vswitch_for_interface(session, vlan_interface, cluster) \n\t \tnetwork_util.create_port_group(session, bridge, vswitch_associated, (vlan_num if create_vlan else 0), cluster) \n\t \tnetwork_ref = network_util.get_network_with_the_name(session, bridge, cluster) \n\telif create_vlan: \n\t \tvswitch_associated = _get_associated_vswitch_for_interface(session, vlan_interface, cluster) \n\t \t_get_pg_info = network_util.get_vlanid_and_vswitch_for_portgroup \n\t \t(pg_vlanid, pg_vswitch) = _get_pg_info(session, bridge, cluster) \n\t \tif (pg_vswitch != vswitch_associated): \n\t \t \traise exception.InvalidVLANPortGroup(bridge=bridge, expected=vswitch_associated, actual=pg_vswitch) \n\t \tif (pg_vlanid != vlan_num): \n\t \t \traise exception.InvalidVLANTag(bridge=bridge, tag=vlan_num, pgroup=pg_vlanid) \n\treturn network_ref\n", 
" \treturn ''.join([quote(environ.get('SCRIPT_NAME', '')), quote(environ.get('PATH_INFO', ''))])\n", 
" \tif (call != 'function'): \n\t \traise SaltCloudSystemExit('The \tlist_datastores \tfunction \tmust \tbe \tcalled \twith \t-f \tor \t--function.') \n\treturn {'Datastores': salt.utils.vmware.list_datastores(_get_si())}\n", 
" \tconfig_spec = client_factory.create('ns0:VirtualMachineConfigSpec') \n\tconfig_spec.name = instance.uuid \n\tconfig_spec.guestId = os_type \n\tconfig_spec.instanceUuid = instance.uuid \n\tif metadata: \n\t \tconfig_spec.annotation = metadata \n\tconfig_spec.version = extra_specs.hw_version \n\tif (os_type in ('vmkernel5Guest', 'vmkernel6Guest', 'vmkernel65Guest', 'windowsHyperVGuest')): \n\t \tconfig_spec.nestedHVEnabled = 'True' \n\tif profile_spec: \n\t \tconfig_spec.vmProfile = [profile_spec] \n\tvm_file_info = client_factory.create('ns0:VirtualMachineFileInfo') \n\tvm_file_info.vmPathName = (('[' + data_store_name) + ']') \n\tconfig_spec.files = vm_file_info \n\ttools_info = client_factory.create('ns0:ToolsConfigInfo') \n\ttools_info.afterPowerOn = True \n\ttools_info.afterResume = True \n\ttools_info.beforeGuestStandby = True \n\ttools_info.beforeGuestShutdown = True \n\ttools_info.beforeGuestReboot = True \n\tconfig_spec.tools = tools_info \n\tconfig_spec.numCPUs = int(instance.vcpus) \n\tif extra_specs.cores_per_socket: \n\t \tconfig_spec.numCoresPerSocket = int(extra_specs.cores_per_socket) \n\tconfig_spec.memoryMB = int(instance.memory_mb) \n\tif extra_specs.cpu_limits.has_limits(): \n\t \tconfig_spec.cpuAllocation = _get_allocation_info(client_factory, extra_specs.cpu_limits, 'ns0:ResourceAllocationInfo') \n\tif extra_specs.memory_limits.has_limits(): \n\t \tconfig_spec.memoryAllocation = _get_allocation_info(client_factory, extra_specs.memory_limits, 'ns0:ResourceAllocationInfo') \n\tdevices = [] \n\tfor vif_info in vif_infos: \n\t \tvif_spec = _create_vif_spec(client_factory, vif_info, extra_specs.vif_limits) \n\t \tdevices.append(vif_spec) \n\tserial_port_spec = create_serial_port_spec(client_factory) \n\tif serial_port_spec: \n\t \tdevices.append(serial_port_spec) \n\tconfig_spec.deviceChange = devices \n\textra_config = [] \n\topt = client_factory.create('ns0:OptionValue') \n\topt.key = 'nvp.vm-uuid' \n\topt.value = instance.uuid \n\textra_config.append(opt) \n\topt = client_factory.create('ns0:OptionValue') \n\topt.key = 'disk.EnableUUID' \n\topt.value = True \n\textra_config.append(opt) \n\tport_index = 0 \n\tfor vif_info in vif_infos: \n\t \tif vif_info['iface_id']: \n\t \t \textra_config.append(_iface_id_option_value(client_factory, vif_info['iface_id'], port_index)) \n\t \t \tport_index += 1 \n\tif (CONF.vmware.console_delay_seconds and (CONF.vmware.console_delay_seconds > 0)): \n\t \topt = client_factory.create('ns0:OptionValue') \n\t \topt.key = 'keyboard.typematicMinDelay' \n\t \topt.value = (CONF.vmware.console_delay_seconds * 1000000) \n\t \textra_config.append(opt) \n\tconfig_spec.extraConfig = extra_config \n\tmanaged_by = client_factory.create('ns0:ManagedByInfo') \n\tmanaged_by.extensionKey = constants.EXTENSION_KEY \n\tmanaged_by.type = constants.EXTENSION_TYPE_INSTANCE \n\tconfig_spec.managedBy = managed_by \n\treturn config_spec\n", 
" \tvirtual_device_config = client_factory.create('ns0:VirtualDeviceConfigSpec') \n\tvirtual_device_config.operation = 'add' \n\tif (adapter_type == constants.ADAPTER_TYPE_BUSLOGIC): \n\t \tvirtual_controller = client_factory.create('ns0:VirtualBusLogicController') \n\telif (adapter_type == constants.ADAPTER_TYPE_LSILOGICSAS): \n\t \tvirtual_controller = client_factory.create('ns0:VirtualLsiLogicSASController') \n\telif (adapter_type == constants.ADAPTER_TYPE_PARAVIRTUAL): \n\t \tvirtual_controller = client_factory.create('ns0:ParaVirtualSCSIController') \n\telse: \n\t \tvirtual_controller = client_factory.create('ns0:VirtualLsiLogicController') \n\tvirtual_controller.key = key \n\tvirtual_controller.busNumber = bus_number \n\tvirtual_controller.sharedBus = 'noSharing' \n\tvirtual_device_config.device = virtual_controller \n\treturn virtual_device_config\n", 
" \tnetwork_spec = client_factory.create('ns0:VirtualDeviceConfigSpec') \n\tnetwork_spec.operation = 'add' \n\tvif_info['vif_model'] = convert_vif_model(vif_info['vif_model']) \n\tvif = ('ns0:' + vif_info['vif_model']) \n\tnet_device = client_factory.create(vif) \n\tnetwork_ref = vif_info['network_ref'] \n\tnetwork_name = vif_info['network_name'] \n\tmac_address = vif_info['mac_address'] \n\tbacking = None \n\tif (network_ref and (network_ref['type'] == 'OpaqueNetwork')): \n\t \tbacking = client_factory.create('ns0:VirtualEthernetCardOpaqueNetworkBackingInfo') \n\t \tbacking.opaqueNetworkId = network_ref['network-id'] \n\t \tbacking.opaqueNetworkType = network_ref['network-type'] \n\t \tif network_ref['use-external-id']: \n\t \t \tif hasattr(net_device, 'externalId'): \n\t \t \t \tnet_device.externalId = vif_info['iface_id'] \n\t \t \telse: \n\t \t \t \tdp = client_factory.create('ns0:DynamicProperty') \n\t \t \t \tdp.name = '__externalId__' \n\t \t \t \tdp.val = vif_info['iface_id'] \n\t \t \t \tnet_device.dynamicProperty = [dp] \n\telif (network_ref and (network_ref['type'] == 'DistributedVirtualPortgroup')): \n\t \tbacking = client_factory.create('ns0:VirtualEthernetCardDistributedVirtualPortBackingInfo') \n\t \tportgroup = client_factory.create('ns0:DistributedVirtualSwitchPortConnection') \n\t \tportgroup.switchUuid = network_ref['dvsw'] \n\t \tportgroup.portgroupKey = network_ref['dvpg'] \n\t \tif ('dvs_port_key' in network_ref): \n\t \t \tportgroup.portKey = network_ref['dvs_port_key'] \n\t \tbacking.port = portgroup \n\telse: \n\t \tbacking = client_factory.create('ns0:VirtualEthernetCardNetworkBackingInfo') \n\t \tbacking.deviceName = network_name \n\tconnectable_spec = client_factory.create('ns0:VirtualDeviceConnectInfo') \n\tconnectable_spec.startConnected = True \n\tconnectable_spec.allowGuestControl = True \n\tconnectable_spec.connected = True \n\tnet_device.connectable = connectable_spec \n\tnet_device.backing = backing \n\tnet_device.key = (-47) \n\tnet_device.addressType = 'manual' \n\tnet_device.macAddress = mac_address \n\tnet_device.wakeOnLanEnabled = True \n\tif (vif_limits and vif_limits.has_limits()): \n\t \tif hasattr(net_device, 'resourceAllocation'): \n\t \t \tnet_device.resourceAllocation = _get_allocation_info(client_factory, vif_limits, 'ns0:VirtualEthernetCardResourceAllocation') \n\t \telse: \n\t \t \tmsg = _('Limits \tonly \tsupported \tfrom \tvCenter \t6.0 \tand \tabove') \n\t \t \traise exception.Invalid(msg) \n\tnetwork_spec.device = net_device \n\treturn network_spec\n", 
" \tconfig_spec = client_factory.create('ns0:VirtualMachineConfigSpec') \n\tdevice_config_spec = [] \n\tvirtual_device_config_spec = _create_virtual_disk_spec(client_factory, controller_key, disk_type, file_path, disk_size, linked_clone, unit_number, device_name, disk_io_limits) \n\tdevice_config_spec.append(virtual_device_config_spec) \n\tconfig_spec.deviceChange = device_config_spec \n\treturn config_spec\n", 
" \tconfig_spec = client_factory.create('ns0:VirtualMachineConfigSpec') \n\tdevice_config_spec = [] \n\tvirtual_device_config_spec = detach_virtual_disk_spec(client_factory, device, destroy_disk) \n\tdevice_config_spec.append(virtual_device_config_spec) \n\tconfig_spec.deviceChange = device_config_spec \n\treturn config_spec\n", 
" \tif (adapter_type in [constants.ADAPTER_TYPE_LSILOGICSAS, constants.ADAPTER_TYPE_PARAVIRTUAL]): \n\t \tvmdk_adapter_type = constants.DEFAULT_ADAPTER_TYPE \n\telse: \n\t \tvmdk_adapter_type = adapter_type \n\treturn vmdk_adapter_type\n", 
" \tif (hardware_devices.__class__.__name__ == 'ArrayOfVirtualDevice'): \n\t \thardware_devices = hardware_devices.VirtualDevice \n\tfor device in hardware_devices: \n\t \tif ((device.__class__.__name__ == 'VirtualDisk') and (device.backing.__class__.__name__ == 'VirtualDiskRawDiskMappingVer1BackingInfo') and (device.backing.lunUuid == uuid)): \n\t \t \treturn device\n", 
" \tcreate_vmdk_spec = client_factory.create('ns0:FileBackedVirtualDiskSpec') \n\tcreate_vmdk_spec.adapterType = get_vmdk_adapter_type(adapter_type) \n\tcreate_vmdk_spec.diskType = disk_type \n\tcreate_vmdk_spec.capacityKb = size_in_kb \n\treturn create_vmdk_spec\n", 
" \tcreate_vmdk_spec = client_factory.create('ns0:FileBackedVirtualDiskSpec') \n\tcreate_vmdk_spec.adapterType = get_vmdk_adapter_type(adapter_type) \n\tcreate_vmdk_spec.diskType = disk_type \n\tcreate_vmdk_spec.capacityKb = size_in_kb \n\treturn create_vmdk_spec\n", 
" \tcreate_vmdk_spec = client_factory.create('ns0:FileBackedVirtualDiskSpec') \n\tcreate_vmdk_spec.adapterType = get_vmdk_adapter_type(adapter_type) \n\tcreate_vmdk_spec.diskType = disk_type \n\tcreate_vmdk_spec.capacityKb = size_in_kb \n\treturn create_vmdk_spec\n", 
" \tvirtual_device_config = client_factory.create('ns0:VirtualDeviceConfigSpec') \n\tvirtual_device_config.operation = 'add' \n\tif ((file_path is None) or linked_clone): \n\t \tvirtual_device_config.fileOperation = 'create' \n\tvirtual_disk = client_factory.create('ns0:VirtualDisk') \n\tif ((disk_type == 'rdm') or (disk_type == 'rdmp')): \n\t \tdisk_file_backing = client_factory.create('ns0:VirtualDiskRawDiskMappingVer1BackingInfo') \n\t \tdisk_file_backing.compatibilityMode = ('virtualMode' if (disk_type == 'rdm') else 'physicalMode') \n\t \tdisk_file_backing.diskMode = 'independent_persistent' \n\t \tdisk_file_backing.deviceName = (device_name or '') \n\telse: \n\t \tdisk_file_backing = client_factory.create('ns0:VirtualDiskFlatVer2BackingInfo') \n\t \tdisk_file_backing.diskMode = 'persistent' \n\t \tif (disk_type == constants.DISK_TYPE_THIN): \n\t \t \tdisk_file_backing.thinProvisioned = True \n\t \telif (disk_type == constants.DISK_TYPE_EAGER_ZEROED_THICK): \n\t \t \tdisk_file_backing.eagerlyScrub = True \n\tdisk_file_backing.fileName = (file_path or '') \n\tconnectable_spec = client_factory.create('ns0:VirtualDeviceConnectInfo') \n\tconnectable_spec.startConnected = True \n\tconnectable_spec.allowGuestControl = False \n\tconnectable_spec.connected = True \n\tif (not linked_clone): \n\t \tvirtual_disk.backing = disk_file_backing \n\telse: \n\t \tvirtual_disk.backing = copy.copy(disk_file_backing) \n\t \tvirtual_disk.backing.fileName = '' \n\t \tvirtual_disk.backing.parent = disk_file_backing \n\tvirtual_disk.connectable = connectable_spec \n\tvirtual_disk.key = (-100) \n\tvirtual_disk.controllerKey = controller_key \n\tvirtual_disk.unitNumber = (unit_number or 0) \n\tvirtual_disk.capacityInKB = (disk_size or 0) \n\tif (disk_io_limits and disk_io_limits.has_limits()): \n\t \tvirtual_disk.storageIOAllocation = _get_allocation_info(client_factory, disk_io_limits, 'ns0:StorageIOAllocationInfo') \n\tvirtual_device_config.device = virtual_disk \n\treturn virtual_device_config\n", 
" \tvirtual_device_config = client_factory.create('ns0:VirtualDeviceConfigSpec') \n\tvirtual_device_config.operation = 'remove' \n\tif destroy_disk: \n\t \tvirtual_device_config.fileOperation = 'destroy' \n\tvirtual_device_config.device = device \n\treturn virtual_device_config\n", 
" \tclone_spec = client_factory.create('ns0:VirtualMachineCloneSpec') \n\tclone_spec.location = location \n\tclone_spec.powerOn = power_on \n\tif snapshot: \n\t \tclone_spec.snapshot = snapshot \n\tif (config is not None): \n\t \tclone_spec.config = config \n\tclone_spec.template = template \n\treturn clone_spec\n", 
" \trel_spec = client_factory.create('ns0:VirtualMachineRelocateSpec') \n\trel_spec.datastore = datastore \n\trel_spec.diskMoveType = disk_move_type \n\tif host: \n\t \trel_spec.host = host \n\treturn rel_spec\n", 
" \tconfig_spec = client_factory.create('ns0:VirtualMachineConfigSpec') \n\tconfig_spec.name = instance.uuid \n\tconfig_spec.guestId = os_type \n\tconfig_spec.instanceUuid = instance.uuid \n\tif metadata: \n\t \tconfig_spec.annotation = metadata \n\tconfig_spec.version = extra_specs.hw_version \n\tif (os_type in ('vmkernel5Guest', 'vmkernel6Guest', 'vmkernel65Guest', 'windowsHyperVGuest')): \n\t \tconfig_spec.nestedHVEnabled = 'True' \n\tif profile_spec: \n\t \tconfig_spec.vmProfile = [profile_spec] \n\tvm_file_info = client_factory.create('ns0:VirtualMachineFileInfo') \n\tvm_file_info.vmPathName = (('[' + data_store_name) + ']') \n\tconfig_spec.files = vm_file_info \n\ttools_info = client_factory.create('ns0:ToolsConfigInfo') \n\ttools_info.afterPowerOn = True \n\ttools_info.afterResume = True \n\ttools_info.beforeGuestStandby = True \n\ttools_info.beforeGuestShutdown = True \n\ttools_info.beforeGuestReboot = True \n\tconfig_spec.tools = tools_info \n\tconfig_spec.numCPUs = int(instance.vcpus) \n\tif extra_specs.cores_per_socket: \n\t \tconfig_spec.numCoresPerSocket = int(extra_specs.cores_per_socket) \n\tconfig_spec.memoryMB = int(instance.memory_mb) \n\tif extra_specs.cpu_limits.has_limits(): \n\t \tconfig_spec.cpuAllocation = _get_allocation_info(client_factory, extra_specs.cpu_limits, 'ns0:ResourceAllocationInfo') \n\tif extra_specs.memory_limits.has_limits(): \n\t \tconfig_spec.memoryAllocation = _get_allocation_info(client_factory, extra_specs.memory_limits, 'ns0:ResourceAllocationInfo') \n\tdevices = [] \n\tfor vif_info in vif_infos: \n\t \tvif_spec = _create_vif_spec(client_factory, vif_info, extra_specs.vif_limits) \n\t \tdevices.append(vif_spec) \n\tserial_port_spec = create_serial_port_spec(client_factory) \n\tif serial_port_spec: \n\t \tdevices.append(serial_port_spec) \n\tconfig_spec.deviceChange = devices \n\textra_config = [] \n\topt = client_factory.create('ns0:OptionValue') \n\topt.key = 'nvp.vm-uuid' \n\topt.value = instance.uuid \n\textra_config.append(opt) \n\topt = client_factory.create('ns0:OptionValue') \n\topt.key = 'disk.EnableUUID' \n\topt.value = True \n\textra_config.append(opt) \n\tport_index = 0 \n\tfor vif_info in vif_infos: \n\t \tif vif_info['iface_id']: \n\t \t \textra_config.append(_iface_id_option_value(client_factory, vif_info['iface_id'], port_index)) \n\t \t \tport_index += 1 \n\tif (CONF.vmware.console_delay_seconds and (CONF.vmware.console_delay_seconds > 0)): \n\t \topt = client_factory.create('ns0:OptionValue') \n\t \topt.key = 'keyboard.typematicMinDelay' \n\t \topt.value = (CONF.vmware.console_delay_seconds * 1000000) \n\t \textra_config.append(opt) \n\tconfig_spec.extraConfig = extra_config \n\tmanaged_by = client_factory.create('ns0:ManagedByInfo') \n\tmanaged_by.extensionKey = constants.EXTENSION_KEY \n\tmanaged_by.type = constants.EXTENSION_TYPE_INSTANCE \n\tconfig_spec.managedBy = managed_by \n\treturn config_spec\n", 
" \tvirtual_machine_config_spec = client_factory.create('ns0:VirtualMachineConfigSpec') \n\topt = client_factory.create('ns0:OptionValue') \n\topt.key = 'machine.id' \n\topt.value = machine_id_str \n\tvirtual_machine_config_spec.extraConfig = [opt] \n\treturn virtual_machine_config_spec\n", 
" \tvswitch_port_group_spec = client_factory.create('ns0:HostPortGroupSpec') \n\tvswitch_port_group_spec.name = port_group_name \n\tvswitch_port_group_spec.vswitchName = vswitch_name \n\tvswitch_port_group_spec.vlanId = int(vlan_id) \n\tpolicy = client_factory.create('ns0:HostNetworkPolicy') \n\tnicteaming = client_factory.create('ns0:HostNicTeamingPolicy') \n\tnicteaming.notifySwitches = True \n\tpolicy.nicTeaming = nicteaming \n\tvswitch_port_group_spec.policy = policy \n\treturn vswitch_port_group_spec\n", 
" \tvirtual_machine_config_spec = client_factory.create('ns0:VirtualMachineConfigSpec') \n\topt_enabled = client_factory.create('ns0:OptionValue') \n\topt_enabled.key = 'RemoteDisplay.vnc.enabled' \n\topt_enabled.value = 'true' \n\topt_port = client_factory.create('ns0:OptionValue') \n\topt_port.key = 'RemoteDisplay.vnc.port' \n\topt_port.value = port \n\topt_keymap = client_factory.create('ns0:OptionValue') \n\topt_keymap.key = 'RemoteDisplay.vnc.keyMap' \n\topt_keymap.value = CONF.vnc.keymap \n\textras = [opt_enabled, opt_port, opt_keymap] \n\tvirtual_machine_config_spec.extraConfig = extras \n\treturn virtual_machine_config_spec\n", 
" \tsearch_spec = client_factory.create('ns0:HostDatastoreBrowserSearchSpec') \n\tsearch_spec.matchPattern = [file_name] \n\tsearch_spec.details = client_factory.create('ns0:FileQueryFlags') \n\tsearch_spec.details.fileOwner = False \n\tsearch_spec.details.fileSize = True \n\tsearch_spec.details.fileType = False \n\tsearch_spec.details.modification = False \n\treturn search_spec\n", 
" \tvms = session._call_method(vim_util, 'get_objects', 'VirtualMachine', ['name']) \n\treturn _get_object_from_results(session, vms, vm_name, _get_object_for_value)\n", 
" \tall_clusters = get_all_cluster_mors(session) \n\tfor cluster in all_clusters: \n\t \tif (hasattr(cluster, 'propSet') and (cluster.propSet[0].val == cluster_name)): \n\t \t \treturn cluster.obj\n", 
" \tif (cluster is None): \n\t \tresults = session._call_method(vim_util, 'get_objects', 'HostSystem') \n\t \tsession._call_method(vutil, 'cancel_retrieval', results) \n\t \thost_mor = results.objects[0].obj \n\telse: \n\t \thost_ret = session._call_method(vutil, 'get_object_property', cluster, 'host') \n\t \tif ((not host_ret) or (not host_ret.ManagedObjectReference)): \n\t \t \tmsg = _('No \thost \tavailable \ton \tcluster') \n\t \t \traise exception.NoValidHost(reason=msg) \n\t \thost_mor = host_ret.ManagedObjectReference[0] \n\treturn host_mor\n", 
" \tds = session._call_method(vutil, 'get_object_property', cluster, 'datastore') \n\tif (not ds): \n\t \treturn [] \n\tdata_store_mors = ds.ManagedObjectReference \n\tdata_stores = session._call_method(vim_util, 'get_properties_for_a_collection_of_objects', 'Datastore', data_store_mors, ['summary.type', 'summary.name', 'summary.accessible', 'summary.maintenanceMode']) \n\tallowed = [] \n\twhile data_stores: \n\t \tallowed.extend(_get_allowed_datastores(data_stores, datastore_regex)) \n\t \tdata_stores = session._call_method(vutil, 'continue_retrieval', data_stores) \n\treturn allowed\n", 
" \tvm_networks = session._call_method(vim_util, 'get_object_properties', None, cluster, 'ClusterComputeResource', ['network']) \n\twhile vm_networks: \n\t \tif vm_networks.objects: \n\t \t \tnetwork_obj = _get_network_obj(session, vm_networks.objects, network_name) \n\t \t \tif network_obj: \n\t \t \t \tsession._call_method(vutil, 'cancel_retrieval', vm_networks) \n\t \t \t \treturn network_obj \n\t \tvm_networks = session._call_method(vutil, 'continue_retrieval', vm_networks) \n\tLOG.debug('Network \t%s \tnot \tfound \ton \tcluster!', network_name)\n", 
" \thost_mor = vm_util.get_host_ref(session, cluster) \n\tvswitches_ret = session._call_method(vutil, 'get_object_property', host_mor, 'config.network.vswitch') \n\tif (not vswitches_ret): \n\t \treturn \n\tvswitches = vswitches_ret.HostVirtualSwitch \n\tfor elem in vswitches: \n\t \ttry: \n\t \t \tfor nic_elem in elem.pnic: \n\t \t \t \tif (str(nic_elem).split('-')[(-1)].find(vlan_interface) != (-1)): \n\t \t \t \t \treturn elem.name \n\t \texcept AttributeError: \n\t \t \tpass\n", 
" \thost_mor = vm_util.get_host_ref(session, cluster) \n\tphysical_nics_ret = session._call_method(vutil, 'get_object_property', host_mor, 'config.network.pnic') \n\tif (not physical_nics_ret): \n\t \treturn False \n\tphysical_nics = physical_nics_ret.PhysicalNic \n\tfor pnic in physical_nics: \n\t \tif (vlan_interface == pnic.device): \n\t \t \treturn True \n\treturn False\n", 
" \thost_mor = vm_util.get_host_ref(session, cluster) \n\tport_grps_on_host_ret = session._call_method(vutil, 'get_object_property', host_mor, 'config.network.portgroup') \n\tif (not port_grps_on_host_ret): \n\t \tmsg = _('ESX \tSOAP \tserver \treturned \tan \tempty \tport \tgroup \tfor \tthe \thost \tsystem \tin \tits \tresponse') \n\t \tLOG.error(msg) \n\t \traise exception.NovaException(msg) \n\tport_grps_on_host = port_grps_on_host_ret.HostPortGroup \n\tfor p_gp in port_grps_on_host: \n\t \tif (p_gp.spec.name == pg_name): \n\t \t \tp_grp_vswitch_name = p_gp.spec.vswitchName \n\t \t \treturn (p_gp.spec.vlanId, p_grp_vswitch_name) \n\treturn (None, None)\n", 
" \tclient_factory = session.vim.client.factory \n\tadd_prt_grp_spec = vm_util.get_add_vswitch_port_group_spec(client_factory, vswitch_name, pg_name, vlan_id) \n\thost_mor = vm_util.get_host_ref(session, cluster) \n\tnetwork_system_mor = session._call_method(vutil, 'get_object_property', host_mor, 'configManager.networkSystem') \n\tLOG.debug('Creating \tPort \tGroup \twith \tname \t%s \ton \tthe \tESX \thost', pg_name) \n\ttry: \n\t \tsession._call_method(session.vim, 'AddPortGroup', network_system_mor, portgrp=add_prt_grp_spec) \n\texcept vexc.AlreadyExistsException: \n\t \tLOG.debug('Port \tGroup \t%s \talready \texists.', pg_name) \n\tLOG.debug('Created \tPort \tGroup \twith \tname \t%s \ton \tthe \tESX \thost', pg_name)\n", 
" \tglobal _FAKE_NODES \n\t_FAKE_NODES = nodes\n", 
" \tglobal _FAKE_NODES \n\t_FAKE_NODES = [CONF.host]\n", 
" \tLOCK_PATH = os.path.join(CONF.instances_path, 'locks') \n\t@utils.synchronized('storage-registry-lock', external=True, lock_path=LOCK_PATH) \n\tdef do_register_storage_use(storage_path, hostname): \n\t \td = {} \n\t \tid_path = os.path.join(storage_path, 'compute_nodes') \n\t \tif os.path.exists(id_path): \n\t \t \twith open(id_path) as f: \n\t \t \t \ttry: \n\t \t \t \t \td = jsonutils.loads(f.read()) \n\t \t \t \texcept ValueError: \n\t \t \t \t \tLOG.warning(_LW('Cannot \tdecode \tJSON \tfrom \t%(id_path)s'), {'id_path': id_path}) \n\t \td[hostname] = time.time() \n\t \twith open(id_path, 'w') as f: \n\t \t \tf.write(jsonutils.dumps(d)) \n\treturn do_register_storage_use(storage_path, hostname)\n", 
" \tLOCK_PATH = os.path.join(CONF.instances_path, 'locks') \n\t@utils.synchronized('storage-registry-lock', external=True, lock_path=LOCK_PATH) \n\tdef do_get_storage_users(storage_path): \n\t \td = {} \n\t \tid_path = os.path.join(storage_path, 'compute_nodes') \n\t \tif os.path.exists(id_path): \n\t \t \twith open(id_path) as f: \n\t \t \t \ttry: \n\t \t \t \t \td = jsonutils.loads(f.read()) \n\t \t \t \texcept ValueError: \n\t \t \t \t \tLOG.warning(_LW('Cannot \tdecode \tJSON \tfrom \t%(id_path)s'), {'id_path': id_path}) \n\t \trecent_users = [] \n\t \tfor node in d: \n\t \t \tif ((time.time() - d[node]) < TWENTY_FOUR_HOURS): \n\t \t \t \trecent_users.append(node) \n\t \treturn recent_users \n\treturn do_get_storage_users(storage_path)\n", 
" \tflavor = instance.get_flavor() \n\tmem = str((int(flavor.memory_mb) * units.Mi)) \n\tvcpus = str(flavor.vcpus) \n\tvcpu_weight = flavor.vcpu_weight \n\tvcpu_params = {} \n\tif (vcpu_weight is not None): \n\t \tvcpu_params = {'weight': str(vcpu_weight), 'cap': '0'} \n\tcpu_mask_list = hardware.get_vcpu_pin_set() \n\tif cpu_mask_list: \n\t \tcpu_mask = hardware.format_cpu_spec(cpu_mask_list, allow_ranges=False) \n\t \tvcpu_params['mask'] = cpu_mask \n\tviridian = ('true' if (instance['os_type'] == 'windows') else 'false') \n\trec = {'actions_after_crash': 'destroy', 'actions_after_reboot': 'restart', 'actions_after_shutdown': 'destroy', 'affinity': '', 'blocked_operations': {}, 'ha_always_run': False, 'ha_restart_priority': '', 'HVM_boot_params': {}, 'HVM_boot_policy': '', 'is_a_template': False, 'memory_dynamic_min': mem, 'memory_dynamic_max': mem, 'memory_static_min': '0', 'memory_static_max': mem, 'memory_target': mem, 'name_description': '', 'name_label': name_label, 'other_config': {'nova_uuid': str(instance['uuid'])}, 'PCI_bus': '', 'platform': {'acpi': 'true', 'apic': 'true', 'pae': 'true', 'viridian': viridian, 'timeoffset': '0'}, 'PV_args': '', 'PV_bootloader': '', 'PV_bootloader_args': '', 'PV_kernel': '', 'PV_legacy_args': '', 'PV_ramdisk': '', 'recommendations': '', 'tags': [], 'user_version': '0', 'VCPUs_at_startup': vcpus, 'VCPUs_max': vcpus, 'VCPUs_params': vcpu_params, 'xenstore_data': {'vm-data/allowvssprovider': 'false'}} \n\tif use_pv_kernel: \n\t \trec['platform']['nx'] = 'false' \n\t \tif instance['kernel_id']: \n\t \t \trec['PV_args'] = 'root=/dev/xvda1' \n\t \t \trec['PV_kernel'] = kernel \n\t \t \trec['PV_ramdisk'] = ramdisk \n\t \telse: \n\t \t \trec['PV_bootloader'] = 'pygrub' \n\telse: \n\t \trec['platform']['nx'] = 'true' \n\t \trec['HVM_boot_params'] = {'order': 'dc'} \n\t \trec['HVM_boot_policy'] = 'BIOS \torder' \n\tif device_id: \n\t \trec['platform']['device_id'] = str(device_id).zfill(4) \n\tvm_ref = session.VM.create(rec) \n\tLOG.debug('Created \tVM', instance=instance) \n\treturn vm_ref\n", 
" \ttry: \n\t \tsession.VM.destroy(vm_ref) \n\texcept session.XenAPI.Failure: \n\t \tLOG.exception(_LE('Destroy \tVM \tfailed')) \n\t \treturn \n\tLOG.debug('VM \tdestroyed', instance=instance)\n", 
" \tvbd_refs = session.VM.get_VBDs(vm_ref) \n\trequested_device = str(dev_number) \n\tif vbd_refs: \n\t \tfor vbd_ref in vbd_refs: \n\t \t \ttry: \n\t \t \t \tuser_device = session.VBD.get_userdevice(vbd_ref) \n\t \t \t \tif (user_device == requested_device): \n\t \t \t \t \treturn vbd_ref \n\t \t \texcept session.XenAPI.Failure: \n\t \t \t \tmsg = ('Error \tlooking \tup \tVBD \t%s \tfor \t%s' % (vbd_ref, vm_ref)) \n\t \t \t \tLOG.debug(msg, exc_info=True)\n", 
" \tdef _is_null_ref(ref): \n\t \treturn (ref == 'OpaqueRef:NULL') \n\tdef _add_vdi_and_parents_to_connected(vdi_rec, indent_level): \n\t \tindent_level += 1 \n\t \tvdi_and_parent_uuids = [] \n\t \tcur_vdi_rec = vdi_rec \n\t \twhile True: \n\t \t \tcur_vdi_uuid = cur_vdi_rec['uuid'] \n\t \t \tprint_xen_object('VDI', vdi_rec, indent_level=indent_level) \n\t \t \tconnected_vdi_uuids.add(cur_vdi_uuid) \n\t \t \tvdi_and_parent_uuids.append(cur_vdi_uuid) \n\t \t \ttry: \n\t \t \t \tparent_vdi_uuid = vdi_rec['sm_config']['vhd-parent'] \n\t \t \texcept KeyError: \n\t \t \t \tparent_vdi_uuid = None \n\t \t \tif (parent_vdi_uuid and (parent_vdi_uuid != cur_vdi_uuid)): \n\t \t \t \tindent_level += 1 \n\t \t \t \tcur_vdi_ref = call_xenapi(xenapi, 'VDI.get_by_uuid', parent_vdi_uuid) \n\t \t \t \ttry: \n\t \t \t \t \tcur_vdi_rec = call_xenapi(xenapi, 'VDI.get_record', cur_vdi_ref) \n\t \t \t \texcept XenAPI.Failure as e: \n\t \t \t \t \tif (e.details[0] != 'HANDLE_INVALID'): \n\t \t \t \t \t \traise \n\t \t \t \t \tbreak \n\t \t \telse: \n\t \t \t \tbreak \n\tfor (vm_ref, vm_rec) in _get_applicable_vm_recs(xenapi): \n\t \tindent_level = 0 \n\t \tprint_xen_object('VM', vm_rec, indent_level=indent_level) \n\t \tvbd_refs = vm_rec['VBDs'] \n\t \tfor vbd_ref in vbd_refs: \n\t \t \ttry: \n\t \t \t \tvbd_rec = call_xenapi(xenapi, 'VBD.get_record', vbd_ref) \n\t \t \texcept XenAPI.Failure as e: \n\t \t \t \tif (e.details[0] != 'HANDLE_INVALID'): \n\t \t \t \t \traise \n\t \t \t \tcontinue \n\t \t \tindent_level = 1 \n\t \t \tprint_xen_object('VBD', vbd_rec, indent_level=indent_level) \n\t \t \tvbd_vdi_ref = vbd_rec['VDI'] \n\t \t \tif _is_null_ref(vbd_vdi_ref): \n\t \t \t \tcontinue \n\t \t \ttry: \n\t \t \t \tvdi_rec = call_xenapi(xenapi, 'VDI.get_record', vbd_vdi_ref) \n\t \t \texcept XenAPI.Failure as e: \n\t \t \t \tif (e.details[0] != 'HANDLE_INVALID'): \n\t \t \t \t \traise \n\t \t \t \tcontinue \n\t \t \t_add_vdi_and_parents_to_connected(vdi_rec, indent_level)\n", 
" \ttry: \n\t \tsession.call_xenapi('VBD.destroy', vbd_ref) \n\texcept session.XenAPI.Failure: \n\t \tLOG.exception(_LE('Unable \tto \tdestroy \tVBD')) \n\t \traise exception.StorageError(reason=(_('Unable \tto \tdestroy \tVBD \t%s') % vbd_ref))\n", 
" \tvbd_rec = {} \n\tvbd_rec['VM'] = vm_ref \n\tif (vdi_ref is None): \n\t \tvdi_ref = 'OpaqueRef:NULL' \n\tvbd_rec['VDI'] = vdi_ref \n\tvbd_rec['userdevice'] = str(userdevice) \n\tvbd_rec['bootable'] = bootable \n\tvbd_rec['mode'] = ((read_only and 'RO') or 'RW') \n\tvbd_rec['type'] = vbd_type \n\tvbd_rec['unpluggable'] = unpluggable \n\tvbd_rec['empty'] = empty \n\tvbd_rec['other_config'] = {} \n\tvbd_rec['qos_algorithm_type'] = '' \n\tvbd_rec['qos_algorithm_params'] = {} \n\tvbd_rec['qos_supported_algorithms'] = [] \n\tLOG.debug('Creating \t%(vbd_type)s-type \tVBD \tfor \tVM \t%(vm_ref)s, \tVDI \t%(vdi_ref)s \t... \t', {'vbd_type': vbd_type, 'vm_ref': vm_ref, 'vdi_ref': vdi_ref}) \n\tvbd_ref = session.call_xenapi('VBD.create', vbd_rec) \n\tLOG.debug('Created \tVBD \t%(vbd_ref)s \tfor \tVM \t%(vm_ref)s, \tVDI \t%(vdi_ref)s.', {'vbd_ref': vbd_ref, 'vm_ref': vm_ref, 'vdi_ref': vdi_ref}) \n\tif osvol: \n\t \tsession.call_xenapi('VBD.add_to_other_config', vbd_ref, 'osvol', 'True') \n\treturn vbd_ref\n", 
" \tfor vdi_ref in vdi_refs: \n\t \ttry: \n\t \t \tdestroy_vdi(session, vdi_ref) \n\t \texcept exception.StorageError: \n\t \t \tLOG.debug('Ignoring \terror \twhile \tdestroying \tVDI: \t%s', vdi_ref)\n", 
" \tvdi_ref = session.call_xenapi('VDI.create', {'name_label': name_label, 'name_description': disk_type, 'SR': sr_ref, 'virtual_size': str(virtual_size), 'type': 'User', 'sharable': False, 'read_only': read_only, 'xenstore_data': {}, 'other_config': _get_vdi_other_config(disk_type, instance=instance), 'sm_config': {}, 'tags': []}) \n\tLOG.debug('Created \tVDI \t%(vdi_ref)s \t(%(name_label)s, \t%(virtual_size)s, \t%(read_only)s) \ton \t%(sr_ref)s.', {'vdi_ref': vdi_ref, 'name_label': name_label, 'virtual_size': virtual_size, 'read_only': read_only, 'sr_ref': sr_ref}) \n\treturn vdi_ref\n", 
" \tname_label = 'dummy' \n\tvm_ref = create_vm(session, instance, name_label, None, None) \n\ttry: \n\t \tvbd_ref = create_vbd(session, vm_ref, vdi_ref, 'autodetect', read_only=True) \n\t \ttry: \n\t \t \t(yield vm_ref) \n\t \tfinally: \n\t \t \ttry: \n\t \t \t \tdestroy_vbd(session, vbd_ref) \n\t \t \texcept exception.StorageError: \n\t \t \t \tpass \n\tfinally: \n\t \tdestroy_vm(session, instance, vm_ref)\n", 
" \twith _dummy_vm(session, instance, vdi_to_copy_ref) as vm_ref: \n\t \tlabel = 'snapshot' \n\t \twith snapshot_attached_here(session, instance, vm_ref, label) as vdi_uuids: \n\t \t \timported_vhds = session.call_plugin_serialized('workarounds.py', 'safe_copy_vdis', sr_path=get_sr_path(session, sr_ref=sr_ref), vdi_uuids=vdi_uuids, uuid_stack=_make_uuid_stack()) \n\troot_uuid = imported_vhds['root']['uuid'] \n\tscan_default_sr(session) \n\tvdi_ref = session.call_xenapi('VDI.get_by_uuid', root_uuid) \n\treturn vdi_ref\n", 
" \tvdi_ref = session.call_xenapi('VDI.clone', vdi_to_clone_ref) \n\tLOG.debug('Cloned \tVDI \t%(vdi_ref)s \tfrom \tVDI \t%(vdi_to_clone_ref)s', {'vdi_ref': vdi_ref, 'vdi_to_clone_ref': vdi_to_clone_ref}) \n\treturn vdi_ref\n", 
" \tvbd_refs = _vm_get_vbd_refs(session, vm_ref) \n\tfor vbd_ref in vbd_refs: \n\t \tvbd_rec = _vbd_get_rec(session, vbd_ref) \n\t \tif (vbd_rec['userdevice'] == userdevice): \n\t \t \tvdi_ref = vbd_rec['VDI'] \n\t \t \tvdi_rec = _vdi_get_rec(session, vdi_ref) \n\t \t \treturn (vdi_ref, vdi_rec) \n\traise exception.NovaException((_('No \tprimary \tVDI \tfound \tfor \t%s') % vm_ref))\n", 
" \tif (sr_ref is None): \n\t \tsr_ref = safe_find_sr(session) \n\tpbd_rec = session.call_xenapi('PBD.get_all_records_where', ('field \t\"host\"=\"%s\" \tand \tfield \t\"SR\"=\"%s\"' % (session.host_ref, sr_ref))) \n\tpbd_ref = list(pbd_rec.keys())[0] \n\tdevice_config = pbd_rec[pbd_ref]['device_config'] \n\tif ('path' in device_config): \n\t \treturn device_config['path'] \n\tsr_rec = session.call_xenapi('SR.get_record', sr_ref) \n\tsr_uuid = sr_rec['uuid'] \n\tif (sr_rec['type'] not in ['ext', 'nfs']): \n\t \traise exception.NovaException((_('Only \tfile-based \tSRs \t(ext/NFS) \tare \tsupported \tby \tthis \tfeature. \t \tSR \t%(uuid)s \tis \tof \ttype \t%(type)s') % {'uuid': sr_uuid, 'type': sr_rec['type']})) \n\treturn os.path.join(CONF.xenserver.sr_base_path, sr_uuid)\n", 
" \tcached_images = _find_cached_images(session, sr_ref) \n\tdestroyed = set() \n\tdef destroy_cached_vdi(vdi_uuid, vdi_ref): \n\t \tLOG.debug(\"Destroying \tcached \tVDI \t'%(vdi_uuid)s'\") \n\t \tif (not dry_run): \n\t \t \tdestroy_vdi(session, vdi_ref) \n\t \tdestroyed.add(vdi_uuid) \n\tfor vdi_ref in cached_images.values(): \n\t \tvdi_uuid = session.call_xenapi('VDI.get_uuid', vdi_ref) \n\t \tif all_cached: \n\t \t \tdestroy_cached_vdi(vdi_uuid, vdi_ref) \n\t \t \tcontinue \n\t \tchain = list(_walk_vdi_chain(session, vdi_uuid)) \n\t \tif (len(chain) > 2): \n\t \t \tcontinue \n\t \telif (len(chain) == 2): \n\t \t \troot_vdi_rec = chain[(-1)] \n\t \t \tchildren = _child_vhds(session, sr_ref, [root_vdi_rec['uuid']]) \n\t \t \tif (len(children) > 1): \n\t \t \t \tcontinue \n\t \tdestroy_cached_vdi(vdi_uuid, vdi_ref) \n\treturn destroyed\n", 
" \tcached_images = {} \n\tfor (vdi_ref, vdi_rec) in _get_all_vdis_in_sr(session, sr_ref): \n\t \ttry: \n\t \t \timage_id = vdi_rec['other_config']['image-id'] \n\t \texcept KeyError: \n\t \t \tcontinue \n\t \tcached_images[image_id] = vdi_ref \n\treturn cached_images\n", 
" \tname_label = _get_image_vdi_label(image_id) \n\trecs = session.call_xenapi('VDI.get_all_records_where', ('field \t\"name__label\"=\"%s\"' % name_label)) \n\tnumber_found = len(recs) \n\tif (number_found > 0): \n\t \tif (number_found > 1): \n\t \t \tLOG.warning(_LW('Multiple \tbase \timages \tfor \timage: \t%s'), image_id) \n\t \treturn list(recs.keys())[0]\n", 
" \tif (new_gb == 0): \n\t \tLOG.debug('Skipping \tauto_config_disk \tas \tdestination \tsize \tis \t0GB') \n\t \treturn \n\twith vdi_attached(session, vdi_ref, read_only=False) as dev: \n\t \tpartitions = _get_partitions(dev) \n\t \tif (len(partitions) != 1): \n\t \t \treason = _('Disk \tmust \thave \tonly \tone \tpartition.') \n\t \t \traise exception.CannotResizeDisk(reason=reason) \n\t \t(num, start, old_sectors, fstype, name, flags) = partitions[0] \n\t \tif (fstype not in ('ext3', 'ext4')): \n\t \t \treason = _('Disk \tcontains \ta \tfilesystem \twe \tare \tunable \tto \tresize: \t%s') \n\t \t \traise exception.CannotResizeDisk(reason=(reason % fstype)) \n\t \tif (num != 1): \n\t \t \treason = _('The \tonly \tpartition \tshould \tbe \tpartition \t1.') \n\t \t \traise exception.CannotResizeDisk(reason=reason) \n\t \tnew_sectors = ((new_gb * units.Gi) / SECTOR_SIZE) \n\t \t_resize_part_and_fs(dev, start, old_sectors, new_sectors, flags)\n", 
" \tsr_ref = safe_find_sr(session) \n\tONE_MEG = units.Mi \n\tvirtual_size = (size_mb * ONE_MEG) \n\tvdi_ref = create_vdi(session, sr_ref, instance, name_label, disk_type, virtual_size) \n\ttry: \n\t \tmkfs_in_dom0 = (fs_type in ('ext3', 'swap')) \n\t \twith vdi_attached(session, vdi_ref, read_only=False, dom0=True) as dev: \n\t \t \tpartition_start = '2048' \n\t \t \tpartition_end = '-' \n\t \t \tsession.call_plugin_serialized('partition_utils.py', 'make_partition', dev, partition_start, partition_end) \n\t \t \tif mkfs_in_dom0: \n\t \t \t \tsession.call_plugin_serialized('partition_utils.py', 'mkfs', dev, '1', fs_type, fs_label) \n\t \tif ((fs_type is not None) and (not mkfs_in_dom0)): \n\t \t \twith vdi_attached(session, vdi_ref, read_only=False) as dev: \n\t \t \t \tpartition_path = utils.make_dev_path(dev, partition=1) \n\t \t \t \tutils.mkfs(fs_type, partition_path, fs_label, run_as_root=True) \n\t \tif vm_ref: \n\t \t \tcreate_vbd(session, vm_ref, vdi_ref, userdevice, bootable=False) \n\texcept Exception: \n\t \twith excutils.save_and_reraise_exception(): \n\t \t \tmsg = ('Error \twhile \tgenerating \tdisk \tnumber: \t%s' % userdevice) \n\t \t \tLOG.debug(msg, instance=instance, exc_info=True) \n\t \t \tsafe_destroy_vdis(session, [vdi_ref]) \n\treturn vdi_ref\n", 
" \tif CONF.xenserver.independent_compute: \n\t \traise exception.NotSupportedWithOption(operation='Non-VHD \timages', option='CONF.xenserver.independent_compute') \n\tfilename = '' \n\tif (CONF.xenserver.cache_images != 'none'): \n\t \targs = {} \n\t \targs['cached-image'] = image_id \n\t \targs['new-image-uuid'] = uuidutils.generate_uuid() \n\t \tfilename = session.call_plugin('kernel.py', 'create_kernel_ramdisk', args) \n\tif (filename == ''): \n\t \treturn _fetch_disk_image(context, session, instance, name_label, image_id, image_type) \n\telse: \n\t \tvdi_type = ImageType.to_string(image_type) \n\t \treturn {vdi_type: dict(uuid=None, file=filename)}\n", 
" \tcache_images = CONF.xenserver.cache_images.lower() \n\tif (image_type == ImageType.DISK_ISO): \n\t \tcache = False \n\telif (cache_images == 'all'): \n\t \tcache = True \n\telif (cache_images == 'some'): \n\t \tsys_meta = utils.instance_sys_meta(instance) \n\t \ttry: \n\t \t \tcache = strutils.bool_from_string(sys_meta['image_cache_in_nova']) \n\t \texcept KeyError: \n\t \t \tcache = False \n\telif (cache_images == 'none'): \n\t \tcache = False \n\telse: \n\t \tLOG.warning(_LW(\"Unrecognized \tcache_images \tvalue \t'%s', \tdefaulting \tto \tTrue\"), CONF.xenserver.cache_images) \n\t \tcache = True \n\tstart_time = timeutils.utcnow() \n\tif cache: \n\t \t(downloaded, vdis) = _create_cached_image(context, session, instance, name_label, image_id, image_type) \n\telse: \n\t \tvdis = _fetch_image(context, session, instance, name_label, image_id, image_type) \n\t \tdownloaded = True \n\tduration = timeutils.delta_seconds(start_time, timeutils.utcnow()) \n\tLOG.info(_LI('Image \tcreation \tdata, \tcacheable: \t%(cache)s, \tdownloaded: \t%(downloaded)s \tduration: \t%(duration).2f \tsecs \tfor \timage \t%(image_id)s'), {'image_id': image_id, 'cache': cache, 'downloaded': downloaded, 'duration': duration}) \n\tfor (vdi_type, vdi) in vdis.items(): \n\t \tvdi_ref = session.call_xenapi('VDI.get_by_uuid', vdi['uuid']) \n\t \t_set_vdi_info(session, vdi_ref, vdi_type, name_label, vdi_type, instance) \n\treturn vdis\n", 
" \tif (image_type == ImageType.DISK_VHD): \n\t \tvdis = _fetch_vhd_image(context, session, instance, image_id) \n\telse: \n\t \tif CONF.xenserver.independent_compute: \n\t \t \traise exception.NotSupportedWithOption(operation='Non-VHD \timages', option='CONF.xenserver.independent_compute') \n\t \tvdis = _fetch_disk_image(context, session, instance, name_label, image_id, image_type) \n\tfor (vdi_type, vdi) in vdis.items(): \n\t \tvdi_uuid = vdi['uuid'] \n\t \tLOG.debug(\"Fetched \tVDIs \tof \ttype \t'%(vdi_type)s' \twith \tUUID \t'%(vdi_uuid)s'\", {'vdi_type': vdi_type, 'vdi_uuid': vdi_uuid}, instance=instance) \n\treturn vdis\n", 
" \tLOG.debug('Asking \txapi \tto \tfetch \tvhd \timage \t%s', image_id, instance=instance) \n\thandler = _choose_download_handler(context, instance) \n\ttry: \n\t \tvdis = handler.download_image(context, session, instance, image_id) \n\texcept Exception: \n\t \tdefault_handler = _default_download_handler() \n\t \tif (type(handler) == type(default_handler)): \n\t \t \traise \n\t \tLOG.exception(_LE(\"Download \thandler \t'%(handler)s' \traised \tan \texception, \tfalling \tback \tto \tdefault \thandler \t'%(default_handler)s'\"), {'handler': handler, 'default_handler': default_handler}) \n\t \tvdis = default_handler.download_image(context, session, instance, image_id) \n\tscan_default_sr(session) \n\tvdi_uuid = vdis['root']['uuid'] \n\ttry: \n\t \t_check_vdi_size(context, session, instance, vdi_uuid) \n\texcept Exception: \n\t \twith excutils.save_and_reraise_exception(): \n\t \t \tmsg = 'Error \twhile \tchecking \tvdi \tsize' \n\t \t \tLOG.debug(msg, instance=instance, exc_info=True) \n\t \t \tfor vdi in vdis.values(): \n\t \t \t \tvdi_uuid = vdi['uuid'] \n\t \t \t \tvdi_ref = session.call_xenapi('VDI.get_by_uuid', vdi_uuid) \n\t \t \t \tsafe_destroy_vdis(session, [vdi_ref]) \n\treturn vdis\n", 
" \tsize_bytes = 0 \n\tfor vdi_rec in _walk_vdi_chain(session, vdi_uuid): \n\t \tcur_vdi_uuid = vdi_rec['uuid'] \n\t \tvdi_size_bytes = int(vdi_rec['physical_utilisation']) \n\t \tLOG.debug('vdi_uuid=%(cur_vdi_uuid)s \tvdi_size_bytes=%(vdi_size_bytes)d', {'cur_vdi_uuid': cur_vdi_uuid, 'vdi_size_bytes': vdi_size_bytes}) \n\t \tsize_bytes += vdi_size_bytes \n\treturn size_bytes\n", 
" \timage_type_str = ImageType.to_string(image_type) \n\tLOG.debug('Fetching \timage \t%(image_id)s, \ttype \t%(image_type_str)s', {'image_id': image_id, 'image_type_str': image_type_str}, instance=instance) \n\tif (image_type == ImageType.DISK_ISO): \n\t \tsr_ref = _safe_find_iso_sr(session) \n\telse: \n\t \tsr_ref = safe_find_sr(session) \n\tglance_image = image_utils.GlanceImage(context, image_id) \n\tif glance_image.is_raw_tgz(): \n\t \timage = image_utils.RawTGZImage(glance_image) \n\telse: \n\t \timage = image_utils.RawImage(glance_image) \n\tvirtual_size = image.get_size() \n\tvdi_size = virtual_size \n\tLOG.debug('Size \tfor \timage \t%(image_id)s: \t%(virtual_size)d', {'image_id': image_id, 'virtual_size': virtual_size}, instance=instance) \n\tif (image_type == ImageType.DISK): \n\t \tvdi_size += MBR_SIZE_BYTES \n\telif ((image_type in (ImageType.KERNEL, ImageType.RAMDISK)) and (vdi_size > CONF.xenserver.max_kernel_ramdisk_size)): \n\t \tmax_size = CONF.xenserver.max_kernel_ramdisk_size \n\t \traise exception.NovaException((_('Kernel/Ramdisk \timage \tis \ttoo \tlarge: \t%(vdi_size)d \tbytes, \tmax \t%(max_size)d \tbytes') % {'vdi_size': vdi_size, 'max_size': max_size})) \n\tvdi_ref = create_vdi(session, sr_ref, instance, name_label, image_type_str, vdi_size) \n\ttry: \n\t \tfilename = None \n\t \tvdi_uuid = session.call_xenapi('VDI.get_uuid', vdi_ref) \n\t \twith vdi_attached(session, vdi_ref, read_only=False) as dev: \n\t \t \t_stream_disk(session, image.stream_to, image_type, virtual_size, dev) \n\t \tif (image_type in (ImageType.KERNEL, ImageType.RAMDISK)): \n\t \t \tLOG.debug('Copying \tVDI \t%s \tto \t/boot/guest \ton \tdom0', vdi_ref, instance=instance) \n\t \t \targs = {} \n\t \t \targs['vdi-ref'] = vdi_ref \n\t \t \targs['image-size'] = str(vdi_size) \n\t \t \tif (CONF.xenserver.cache_images != 'none'): \n\t \t \t \targs['cached-image'] = image_id \n\t \t \tfilename = session.call_plugin('kernel.py', 'copy_vdi', args) \n\t \t \tdestroy_vdi(session, vdi_ref) \n\t \t \tLOG.debug('Kernel/Ramdisk \tVDI \t%s \tdestroyed', vdi_ref, instance=instance) \n\t \t \tvdi_role = ImageType.get_role(image_type) \n\t \t \treturn {vdi_role: dict(uuid=None, file=filename)} \n\t \telse: \n\t \t \tvdi_role = ImageType.get_role(image_type) \n\t \t \treturn {vdi_role: dict(uuid=vdi_uuid, file=None)} \n\texcept (session.XenAPI.Failure, IOError, OSError) as e: \n\t \tLOG.exception(_LE('Failed \tto \tfetch \tglance \timage'), instance=instance) \n\t \te.args = (e.args + ([dict(type=ImageType.to_string(image_type), uuid=vdi_uuid, file=filename)],)) \n\t \traise\n", 
" \tif (not image_meta.obj_attr_is_set('disk_format')): \n\t \treturn None \n\tdisk_format_map = {'ami': ImageType.DISK, 'aki': ImageType.KERNEL, 'ari': ImageType.RAMDISK, 'raw': ImageType.DISK_RAW, 'vhd': ImageType.DISK_VHD, 'iso': ImageType.DISK_ISO} \n\ttry: \n\t \timage_type = disk_format_map[image_meta.disk_format] \n\texcept KeyError: \n\t \traise exception.InvalidDiskFormat(disk_format=image_meta.disk_format) \n\tLOG.debug('Detected \t%(type)s \tformat \tfor \timage \t%(image)s', {'type': ImageType.to_string(image_type), 'image': image_meta}) \n\treturn image_type\n", 
" \treturn (type(n) in (IntType, LongType, FloatType))\n", 
" \tvbd_refs = session.call_xenapi('VM.get_VBDs', vm_ref) \n\tvdi_refs = [] \n\tif vbd_refs: \n\t \tfor vbd_ref in vbd_refs: \n\t \t \ttry: \n\t \t \t \tvdi_ref = session.call_xenapi('VBD.get_VDI', vbd_ref) \n\t \t \t \tvdi_uuid = session.call_xenapi('VDI.get_uuid', vdi_ref) \n\t \t \t \tLOG.debug('VDI \t%s \tis \tstill \tavailable', vdi_uuid) \n\t \t \t \tvbd_other_config = session.call_xenapi('VBD.get_other_config', vbd_ref) \n\t \t \t \tif (not vbd_other_config.get('osvol')): \n\t \t \t \t \tvdi_refs.append(vdi_ref) \n\t \t \texcept session.XenAPI.Failure: \n\t \t \t \tLOG.exception(_LE('\"Look \tfor \tthe \tVDIs \tfailed')) \n\treturn vdi_refs\n", 
" \tif check_rescue: \n\t \tresult = lookup(session, (name_label + '-rescue'), False) \n\t \tif result: \n\t \t \treturn result \n\tvm_refs = session.call_xenapi('VM.get_by_name_label', name_label) \n\tn = len(vm_refs) \n\tif (n == 0): \n\t \treturn None \n\telif (n > 1): \n\t \traise exception.InstanceExists(name=name_label) \n\telse: \n\t \treturn vm_refs[0]\n", 
" \tkey = str(instance['key_data']) \n\tnet = netutils.get_injected_network_template(network_info) \n\tmetadata = instance['metadata'] \n\tmount_required = (key or net or metadata) \n\tif (not mount_required): \n\t \treturn \n\twith vdi_attached(session, vdi_ref, read_only=False) as dev: \n\t \t_mounted_processing(dev, key, net, metadata)\n", 
" \tpower_state = get_power_state(session, vm_ref) \n\tmax_mem = session.call_xenapi('VM.get_memory_static_max', vm_ref) \n\tmem = session.call_xenapi('VM.get_memory_dynamic_max', vm_ref) \n\tnum_cpu = session.call_xenapi('VM.get_VCPUs_max', vm_ref) \n\treturn hardware.InstanceInfo(state=power_state, max_mem_kb=(int(max_mem) >> 10), mem_kb=(int(mem) >> 10), num_cpu=num_cpu)\n", 
" \ttry: \n\t \tkeys = [] \n\t \tdiags = {} \n\t \tvm_uuid = vm_rec['uuid'] \n\t \txml = _get_rrd(_get_rrd_server(), vm_uuid) \n\t \tif xml: \n\t \t \trrd = minidom.parseString(xml) \n\t \t \tfor (i, node) in enumerate(rrd.firstChild.childNodes): \n\t \t \t \tif (node.localName == 'lastupdate'): \n\t \t \t \t \tdiags['last_update'] = node.firstChild.data \n\t \t \t \tif (node.localName == 'ds'): \n\t \t \t \t \tref = node.childNodes \n\t \t \t \t \tif (len(ref) > 6): \n\t \t \t \t \t \tkeys.append(ref[0].firstChild.data) \n\t \t \t \tif (node.localName == 'rra'): \n\t \t \t \t \trows = node.childNodes[4].childNodes \n\t \t \t \t \tlast_row = rows[(rows.length - 1)].childNodes \n\t \t \t \t \tfor (j, value) in enumerate(last_row): \n\t \t \t \t \t \tdiags[keys[j]] = value.firstChild.data \n\t \t \t \t \tbreak \n\t \treturn diags \n\texcept expat.ExpatError as e: \n\t \tLOG.exception(_LE('Unable \tto \tparse \trrd \tof \t%s'), e) \n\t \treturn {'Unable \tto \tretrieve \tdiagnostics': e}\n", 
" \tadmin_context = nova.context.get_admin_context(read_deleted='yes') \n\tdef _get_nwinfo_old_skool(): \n\t \t'Support \tfor \tgetting \tnetwork \tinfo \twithout \tobjects.' \n\t \tif (instance_ref.get('info_cache') and (instance_ref['info_cache'].get('network_info') is not None)): \n\t \t \tcached_info = instance_ref['info_cache']['network_info'] \n\t \t \tif isinstance(cached_info, network_model.NetworkInfo): \n\t \t \t \treturn cached_info \n\t \t \treturn network_model.NetworkInfo.hydrate(cached_info) \n\t \ttry: \n\t \t \treturn network.API().get_instance_nw_info(admin_context, instance_ref) \n\t \texcept Exception: \n\t \t \ttry: \n\t \t \t \twith excutils.save_and_reraise_exception(): \n\t \t \t \t \tLOG.exception(_LE('Failed \tto \tget \tnw_info'), instance=instance_ref) \n\t \t \texcept Exception: \n\t \t \t \tif ignore_missing_network_data: \n\t \t \t \t \treturn \n\t \t \t \traise \n\tif isinstance(instance_ref, obj_base.NovaObject): \n\t \tnw_info = instance_ref.info_cache.network_info \n\t \tif (nw_info is None): \n\t \t \tnw_info = network_model.NetworkInfo() \n\telse: \n\t \tnw_info = _get_nwinfo_old_skool() \n\tmacs = [vif['address'] for vif in nw_info] \n\tuuids = [instance_ref['uuid']] \n\tbw_usages = objects.BandwidthUsageList.get_by_uuids(admin_context, uuids, audit_start) \n\tbw = {} \n\tfor b in bw_usages: \n\t \tif (b.mac in macs): \n\t \t \tlabel = ('net-name-not-found-%s' % b.mac) \n\t \t \tfor vif in nw_info: \n\t \t \t \tif (vif['address'] == b.mac): \n\t \t \t \t \tlabel = vif['network']['label'] \n\t \t \t \t \tbreak \n\t \t \tbw[label] = dict(bw_in=b.bw_in, bw_out=b.bw_out) \n\treturn bw\n", 
" \tsr_ref = safe_find_sr(session) \n\t_scan_sr(session, sr_ref) \n\treturn sr_ref\n", 
" \tsr_ref = safe_find_sr(session) \n\t_scan_sr(session, sr_ref) \n\treturn sr_ref\n", 
" \tsr_ref = _find_sr(session) \n\tif (sr_ref is None): \n\t \traise exception.StorageRepositoryNotFound() \n\treturn sr_ref\n", 
" \thost = session.host_ref \n\ttry: \n\t \ttokens = CONF.xenserver.sr_matching_filter.split(':') \n\t \tfilter_criteria = tokens[0] \n\t \tfilter_pattern = tokens[1] \n\texcept IndexError: \n\t \tLOG.warning(_LW(\"Flag \tsr_matching_filter \t'%s' \tdoes \tnot \trespect \tformatting \tconvention\"), CONF.xenserver.sr_matching_filter) \n\t \treturn None \n\tif (filter_criteria == 'other-config'): \n\t \t(key, value) = filter_pattern.split('=', 1) \n\t \tfor (sr_ref, sr_rec) in session.get_all_refs_and_recs('SR'): \n\t \t \tif (not ((key in sr_rec['other_config']) and (sr_rec['other_config'][key] == value))): \n\t \t \t \tcontinue \n\t \t \tfor pbd_ref in sr_rec['PBDs']: \n\t \t \t \tpbd_rec = session.get_rec('PBD', pbd_ref) \n\t \t \t \tif (pbd_rec and (pbd_rec['host'] == host)): \n\t \t \t \t \treturn sr_ref \n\telif ((filter_criteria == 'default-sr') and (filter_pattern == 'true')): \n\t \tpool_ref = session.call_xenapi('pool.get_all')[0] \n\t \tsr_ref = session.call_xenapi('pool.get_default_SR', pool_ref) \n\t \tif sr_ref: \n\t \t \treturn sr_ref \n\tLOG.error(_LE(\"XenAPI \tis \tunable \tto \tfind \ta \tStorage \tRepository \tto \tinstall \tguest \tinstances \ton. \tPlease \tcheck \tyour \tconfiguration \t(e.g. \tset \ta \tdefault \tSR \tfor \tthe \tpool) \tand/or \tconfigure \tthe \tflag \t'sr_matching_filter'.\")) \n\treturn None\n", 
" \tsr_ref = _find_iso_sr(session) \n\tif (sr_ref is None): \n\t \traise exception.NotFound(_('Cannot \tfind \tSR \tof \tcontent-type \tISO')) \n\treturn sr_ref\n", 
" \thost = session.host_ref \n\tfor (sr_ref, sr_rec) in session.get_all_refs_and_recs('SR'): \n\t \tLOG.debug('ISO: \tlooking \tat \tSR \t%s', sr_rec) \n\t \tif (not (sr_rec['content_type'] == 'iso')): \n\t \t \tLOG.debug('ISO: \tnot \tiso \tcontent') \n\t \t \tcontinue \n\t \tif ('i18n-key' not in sr_rec['other_config']): \n\t \t \tLOG.debug(\"ISO: \tiso \tcontent_type, \tno \t'i18n-key' \tkey\") \n\t \t \tcontinue \n\t \tif (not (sr_rec['other_config']['i18n-key'] == 'local-storage-iso')): \n\t \t \tLOG.debug(\"ISO: \tiso \tcontent_type, \ti18n-key \tvalue \tnot \t'local-storage-iso'\") \n\t \t \tcontinue \n\t \tLOG.debug('ISO: \tSR \tMATCHing \tour \tcriteria') \n\t \tfor pbd_ref in sr_rec['PBDs']: \n\t \t \tLOG.debug('ISO: \tISO, \tlooking \tto \tsee \tif \tit \tis \thost \tlocal') \n\t \t \tpbd_rec = session.get_rec('PBD', pbd_ref) \n\t \t \tif (not pbd_rec): \n\t \t \t \tLOG.debug('ISO: \tPBD \t%s \tdisappeared', pbd_ref) \n\t \t \t \tcontinue \n\t \t \tpbd_rec_host = pbd_rec['host'] \n\t \t \tLOG.debug('ISO: \tPBD \tmatching, \twant \t%(pbd_rec)s, \thave \t%(host)s', {'pbd_rec': pbd_rec, 'host': host}) \n\t \t \tif (pbd_rec_host == host): \n\t \t \t \tLOG.debug('ISO: \tSR \twith \tlocal \tPBD') \n\t \t \t \treturn sr_ref \n\treturn None\n", 
" \txs_url = urlparse.urlparse(CONF.xenserver.connection_url) \n\treturn [xs_url.scheme, xs_url.netloc]\n", 
" \ttry: \n\t \txml = urllib.urlopen(('%s://%s:%s@%s/vm_rrd?uuid=%s' % (server[0], CONF.xenserver.connection_username, CONF.xenserver.connection_password, server[1], vm_uuid))) \n\t \treturn xml.read() \n\texcept IOError: \n\t \tLOG.exception(_LE('Unable \tto \tobtain \tRRD \tXML \tfor \tVM \t%(vm_uuid)s \twith \tserver \tdetails: \t%(server)s.'), {'vm_uuid': vm_uuid, 'server': server}) \n\t \treturn None\n", 
" \ttry: \n\t \txml = urllib.urlopen(('%s://%s:%s@%s/vm_rrd?uuid=%s' % (server[0], CONF.xenserver.connection_username, CONF.xenserver.connection_password, server[1], vm_uuid))) \n\t \treturn xml.read() \n\texcept IOError: \n\t \tLOG.exception(_LE('Unable \tto \tobtain \tRRD \tXML \tfor \tVM \t%(vm_uuid)s \twith \tserver \tdetails: \t%(server)s.'), {'vm_uuid': vm_uuid, 'server': server}) \n\t \treturn None\n", 
" \tfor vbd_ref in session.call_xenapi('VM.get_VBDs', vm_ref): \n\t \ttry: \n\t \t \tvdi_ref = session.call_xenapi('VBD.get_VDI', vbd_ref) \n\t \t \tif (sr_ref == session.call_xenapi('VDI.get_SR', vdi_ref)): \n\t \t \t \t(yield vdi_ref) \n\t \texcept session.XenAPI.Failure: \n\t \t \tcontinue\n", 
" \tscan_default_sr(session) \n\twhile True: \n\t \tvdi_ref = session.call_xenapi('VDI.get_by_uuid', vdi_uuid) \n\t \tvdi_rec = session.call_xenapi('VDI.get_record', vdi_ref) \n\t \t(yield vdi_rec) \n\t \tparent_uuid = _get_vhd_parent_uuid(session, vdi_ref, vdi_rec) \n\t \tif (not parent_uuid): \n\t \t \tbreak \n\t \tvdi_uuid = parent_uuid\n", 
" \tchildren = set() \n\tfor (ref, rec) in _get_all_vdis_in_sr(session, sr_ref): \n\t \trec_uuid = rec['uuid'] \n\t \tif (rec_uuid in vdi_uuid_list): \n\t \t \tcontinue \n\t \tparent_uuid = _get_vhd_parent_uuid(session, ref, rec) \n\t \tif (parent_uuid not in vdi_uuid_list): \n\t \t \tcontinue \n\t \tif (old_snapshots_only and (not _is_vdi_a_snapshot(rec))): \n\t \t \tcontinue \n\t \tchildren.add(rec_uuid) \n\treturn list(children)\n", 
" \tif (len(vdi_uuid_list) == 1): \n\t \tLOG.debug('Old \tchain \tis \tsingle \tVHD, \tcoalesce \tnot \tpossible.', instance=instance) \n\t \treturn \n\tparent_vdi_uuid = vdi_uuid_list[1] \n\tif (_count_children(session, parent_vdi_uuid, sr_ref) > 1): \n\t \tLOG.debug('Parent \thas \tother \tchildren, \tcoalesce \tis \tunlikely.', instance=instance) \n\t \treturn \n\tmax_attempts = CONF.xenserver.vhd_coalesce_max_attempts \n\tgood_parent_uuids = vdi_uuid_list[1:] \n\tfor i in range(max_attempts): \n\t \t_scan_sr(session, sr_ref) \n\t \tparent_uuid = _get_vhd_parent_uuid(session, vdi_ref) \n\t \tif (parent_uuid and (parent_uuid not in good_parent_uuids)): \n\t \t \tLOG.debug('Parent \t%(parent_uuid)s \tnot \tyet \tin \tparent \tlist \t%(good_parent_uuids)s, \twaiting \tfor \tcoalesce...', {'parent_uuid': parent_uuid, 'good_parent_uuids': good_parent_uuids}, instance=instance) \n\t \telse: \n\t \t \tLOG.debug('Coalesce \tdetected, \tbecause \tparent \tis: \t%s', parent_uuid, instance=instance) \n\t \t \treturn \n\t \tgreenthread.sleep(CONF.xenserver.vhd_coalesce_poll_interval) \n\tmsg = (_('VHD \tcoalesce \tattempts \texceeded \t(%d), \tgiving \tup...') % max_attempts) \n\traise exception.NovaException(msg)\n", 
" \tshould_remap = CONF.xenserver.remap_vbd_dev \n\tif (not should_remap): \n\t \treturn dev \n\told_prefix = 'xvd' \n\tnew_prefix = CONF.xenserver.remap_vbd_dev_prefix \n\tremapped_dev = dev.replace(old_prefix, new_prefix) \n\treturn remapped_dev\n", 
" \tdev_path = utils.make_dev_path(dev) \n\tfound_path = None \n\tif dom0: \n\t \tfound_path = session.call_plugin_serialized('partition_utils.py', 'wait_for_dev', dev_path, max_seconds) \n\telse: \n\t \tfor i in range(0, max_seconds): \n\t \t \tif os.path.exists(dev_path): \n\t \t \t \tfound_path = dev_path \n\t \t \t \tbreak \n\t \t \ttime.sleep(1) \n\tif (found_path is None): \n\t \traise exception.StorageError(reason=(_('Timeout \twaiting \tfor \tdevice \t%s \tto \tbe \tcreated') % dev))\n", 
" \tthis_vm_ref = _get_this_vm_ref(session) \n\tvbd_refs = session.call_xenapi('VM.get_VBDs', this_vm_ref) \n\tfor vbd_ref in vbd_refs: \n\t \ttry: \n\t \t \tvdi_ref = session.call_xenapi('VBD.get_VDI', vbd_ref) \n\t \t \tvdi_rec = session.call_xenapi('VDI.get_record', vdi_ref) \n\t \texcept session.XenAPI.Failure as e: \n\t \t \tif (e.details[0] != 'HANDLE_INVALID'): \n\t \t \t \traise \n\t \t \tcontinue \n\t \tif ('nova_instance_uuid' in vdi_rec['other_config']): \n\t \t \tLOG.info(_LI('Disconnecting \tstale \tVDI \t%s \tfrom \tcompute \tdomU'), vdi_rec['uuid']) \n\t \t \tunplug_vbd(session, vbd_ref, this_vm_ref) \n\t \t \tdestroy_vbd(session, vbd_ref)\n", 
" \tdev_path = utils.make_dev_path(dev) \n\t(out, _err) = utils.execute('parted', '--script', '--machine', dev_path, 'unit \ts', 'print', run_as_root=True) \n\tlines = [line for line in out.split('\\n') if line] \n\tpartitions = [] \n\tLOG.debug('Partitions:') \n\tfor line in lines[2:]: \n\t \tline = line.rstrip(';') \n\t \t(num, start, end, size, fstype, name, flags) = line.split(':') \n\t \tnum = int(num) \n\t \tstart = int(start.rstrip('s')) \n\t \tend = int(end.rstrip('s')) \n\t \tsize = int(size.rstrip('s')) \n\t \tLOG.debug(' \t \t%(num)s: \t%(fstype)s \t%(size)d \tsectors', {'num': num, 'fstype': fstype, 'size': size}) \n\t \tpartitions.append((num, start, size, fstype, name, flags)) \n\treturn partitions\n", 
" \tsize = (new_sectors - start) \n\tend = (new_sectors - 1) \n\tdev_path = utils.make_dev_path(dev) \n\tpartition_path = utils.make_dev_path(dev, partition=1) \n\t_repair_filesystem(partition_path) \n\tutils.execute('tune2fs', '-O \t^has_journal', partition_path, run_as_root=True) \n\tif (new_sectors < old_sectors): \n\t \ttry: \n\t \t \tutils.execute('resize2fs', partition_path, ('%ds' % size), run_as_root=True) \n\t \texcept processutils.ProcessExecutionError as exc: \n\t \t \tLOG.error(six.text_type(exc)) \n\t \t \treason = _('Shrinking \tthe \tfilesystem \tdown \twith \tresize2fs \thas \tfailed, \tplease \tcheck \tif \tyou \thave \tenough \tfree \tspace \ton \tyour \tdisk.') \n\t \t \traise exception.ResizeError(reason=reason) \n\tutils.execute('parted', '--script', dev_path, 'rm', '1', run_as_root=True) \n\tutils.execute('parted', '--script', dev_path, 'mkpart', 'primary', ('%ds' % start), ('%ds' % end), run_as_root=True) \n\tif ('boot' in flags.lower()): \n\t \tutils.execute('parted', '--script', dev_path, 'set', '1', 'boot', 'on', run_as_root=True) \n\tif (new_sectors > old_sectors): \n\t \tutils.execute('resize2fs', partition_path, run_as_root=True) \n\tutils.execute('tune2fs', '-j', partition_path, run_as_root=True)\n", 
" \tstart_time = last_log_time = timeutils.utcnow() \n\tEMPTY_BLOCK = ('\\x00' * block_size) \n\tbytes_read = 0 \n\tskipped_bytes = 0 \n\tleft = virtual_size \n\tLOG.debug('Starting \tsparse_copy \tsrc=%(src_path)s \tdst=%(dst_path)s \tvirtual_size=%(virtual_size)d \tblock_size=%(block_size)d', {'src_path': src_path, 'dst_path': dst_path, 'virtual_size': virtual_size, 'block_size': block_size}) \n\twith utils.temporary_chown(src_path): \n\t \twith utils.temporary_chown(dst_path): \n\t \t \twith open(src_path, 'r') as src: \n\t \t \t \twith open(dst_path, 'w') as dst: \n\t \t \t \t \tdata = src.read(min(block_size, left)) \n\t \t \t \t \twhile data: \n\t \t \t \t \t \tif (data == EMPTY_BLOCK): \n\t \t \t \t \t \t \tdst.seek(block_size, os.SEEK_CUR) \n\t \t \t \t \t \t \tleft -= block_size \n\t \t \t \t \t \t \tbytes_read += block_size \n\t \t \t \t \t \t \tskipped_bytes += block_size \n\t \t \t \t \t \telse: \n\t \t \t \t \t \t \tdst.write(data) \n\t \t \t \t \t \t \tdata_len = len(data) \n\t \t \t \t \t \t \tleft -= data_len \n\t \t \t \t \t \t \tbytes_read += data_len \n\t \t \t \t \t \tif (left <= 0): \n\t \t \t \t \t \t \tbreak \n\t \t \t \t \t \tdata = src.read(min(block_size, left)) \n\t \t \t \t \t \tgreenthread.sleep(0) \n\t \t \t \t \t \tlast_log_time = _log_progress_if_required(left, last_log_time, virtual_size) \n\tduration = timeutils.delta_seconds(start_time, timeutils.utcnow()) \n\tcompression_pct = ((float(skipped_bytes) / bytes_read) * 100) \n\tLOG.debug('Finished \tsparse_copy \tin \t%(duration).2f \tsecs, \t%(compression_pct).2f%% \treduction \tin \tsize', {'duration': duration, 'compression_pct': compression_pct})\n", 
" \ttry: \n\t \t(_out, err) = utils.execute('mount', '-t', 'ext2,ext3,ext4,reiserfs', dev_path, dir, run_as_root=True) \n\texcept processutils.ProcessExecutionError as e: \n\t \terr = six.text_type(e) \n\treturn err\n", 
" \tdev_path = utils.make_dev_path(device, partition=1) \n\twith utils.tempdir() as tmpdir: \n\t \terr = _mount_filesystem(dev_path, tmpdir) \n\t \tif (not err): \n\t \t \ttry: \n\t \t \t \tif (not agent.find_guest_agent(tmpdir)): \n\t \t \t \t \tvfs = vfsimpl.VFSLocalFS(imgmodel.LocalFileImage(None, imgmodel.FORMAT_RAW), imgdir=tmpdir) \n\t \t \t \t \tLOG.info(_LI('Manipulating \tinterface \tfiles \tdirectly')) \n\t \t \t \t \tdisk.inject_data_into_fs(vfs, key, net, metadata, None, None) \n\t \t \tfinally: \n\t \t \t \tutils.execute('umount', dev_path, run_as_root=True) \n\t \telse: \n\t \t \tLOG.info(_LI('Failed \tto \tmount \tfilesystem \t(expected \tfor \tnon-linux \tinstances): \t%s'), err)\n", 
" \tDEBUG = True \n\tnet_devices = NetworkDevice.objects.all() \n\tfor a_device in net_devices: \n\t \tif ('ssh' in a_device.device_class): \n\t \t \tif DEBUG: \n\t \t \t \tprint 'Retrieve \tdevice \tconfiguration: \t{} \t{}\\n'.format(a_device.device_name, a_device.device_class) \n\t \t \tssh_connect = SSHConnection(a_device) \n\t \t \tssh_connect.enable_mode() \n\t \t \toutput = ssh_connect.send_command('show \trun\\n') \n\t \t \tfile_name = (a_device.device_name + '.txt') \n\t \t \tfull_path = (CFGS_DIR + file_name) \n\t \t \tif DEBUG: \n\t \t \t \tprint 'Writing \tconfiguration \tfile \tto \tfile \tsystem\\n' \n\t \t \twith open(full_path, 'w') as f: \n\t \t \t \tf.write(output)\n", 
" \tif session.host_checked: \n\t \treturn \n\tthis_vm_uuid = get_this_vm_uuid(session) \n\ttry: \n\t \tsession.call_xenapi('VM.get_by_uuid', this_vm_uuid) \n\t \tsession.host_checked = True \n\texcept session.XenAPI.Failure as exc: \n\t \tif (exc.details[0] != 'UUID_INVALID'): \n\t \t \traise \n\t \traise Exception(_('This \tdomU \tmust \tbe \trunning \ton \tthe \thost \tspecified \tby \tconnection_url'))\n", 
" \timported_vhds = session.call_plugin_serialized('migration.py', 'move_vhds_into_sr', instance_uuid=chain_label, sr_path=get_sr_path(session), uuid_stack=_make_uuid_stack()) \n\tscan_default_sr(session) \n\tvdi_uuid = imported_vhds['root']['uuid'] \n\tvdi_ref = session.call_xenapi('VDI.get_by_uuid', vdi_uuid) \n\t_set_vdi_info(session, vdi_ref, disk_type, vdi_label, disk_type, instance) \n\treturn {'uuid': vdi_uuid, 'ref': vdi_ref}\n", 
" \treturn cmp([int(v) for v in ver1.split('.')], [int(v) for v in ver2.split('.')])\n", 
" \tstep_info = dict(total=total_offset, current=0) \n\tdef bump_progress(): \n\t \tstep_info['current'] += 1 \n\t \tupdate_instance_progress(context, instance, step_info['current'], step_info['total']) \n\tdef step_decorator(f): \n\t \tstep_info['total'] += 1 \n\t \t@functools.wraps(f) \n\t \tdef inner(*args, **kwargs): \n\t \t \trv = f(*args, **kwargs) \n\t \t \tbump_progress() \n\t \t \treturn rv \n\t \treturn inner \n\treturn step_decorator\n", 
" \ttemp_url = urlparse.urlparse(url) \n\treturn url.replace(temp_url.hostname, ('%s' % host_addr))\n", 
" \tvbd_rec['currently_attached'] = False \n\tvbd_rec['device'] = '' \n\tvbd_rec.setdefault('other_config', {}) \n\tvm_ref = vbd_rec['VM'] \n\tvm_rec = _db_content['VM'][vm_ref] \n\tvm_rec['VBDs'].append(vbd_ref) \n\tvm_name_label = _db_content['VM'][vm_ref]['name_label'] \n\tvbd_rec['vm_name_label'] = vm_name_label \n\tvdi_ref = vbd_rec['VDI'] \n\tif (vdi_ref and (vdi_ref != 'OpaqueRef:NULL')): \n\t \tvdi_rec = _db_content['VDI'][vdi_ref] \n\t \tvdi_rec['VBDs'].append(vbd_ref)\n", 
" \tvm_rec.setdefault('domid', '-1') \n\tvm_rec.setdefault('is_control_domain', False) \n\tvm_rec.setdefault('is_a_template', False) \n\tvm_rec.setdefault('memory_static_max', str((8 * units.Gi))) \n\tvm_rec.setdefault('memory_dynamic_max', str((8 * units.Gi))) \n\tvm_rec.setdefault('VCPUs_max', str(4)) \n\tvm_rec.setdefault('VBDs', []) \n\tvm_rec.setdefault('VIFs', []) \n\tvm_rec.setdefault('resident_on', '')\n", 
" \tcommand = ('show \tvlan \tid \t' + vlanid) \n\tbody = execute_show_command(command, module) \n\ttry: \n\t \tvlan_table = body[0]['TABLE_vlanbriefid']['ROW_vlanbriefid'] \n\texcept (TypeError, IndexError): \n\t \treturn {} \n\tkey_map = {'vlanshowbr-vlanid-utf': 'vlan_id', 'vlanshowbr-vlanname': 'name', 'vlanshowbr-vlanstate': 'vlan_state', 'vlanshowbr-shutstate': 'admin_state'} \n\tvlan = apply_key_map(key_map, vlan_table) \n\tvalue_map = {'admin_state': {'shutdown': 'down', 'noshutdown': 'up'}} \n\tvlan = apply_value_map(value_map, vlan) \n\tvlan['mapped_vni'] = get_vni(vlanid, module) \n\treturn vlan\n", 
" \tcreate_sr(name_label='Local \tstorage \tISO', type='iso', other_config={'i18n-original-value-name_label': 'Local \tstorage \tISO', 'i18n-key': 'local-storage-iso'}, physical_size=80000, physical_utilisation=40000, virtual_allocation=80000, host_ref=host_ref) \n\treturn create_sr(name_label='Local \tstorage', type='ext', other_config={'i18n-original-value-name_label': 'Local \tstorage', 'i18n-key': 'local-storage'}, physical_size=40000, physical_utilisation=20000, virtual_allocation=10000, host_ref=host_ref)\n", 
" \treturn ('<value>%s</value>' % saxutils.escape(s))\n", 
" \targ = (args or kwargs) \n\treturn jsonutils.dumps(arg)\n", 
" \tif (addl_args is None): \n\t \taddl_args = {} \n\tif (timeout is None): \n\t \ttimeout = CONF.xenserver.agent_timeout \n\tif (success_codes is None): \n\t \tsuccess_codes = ['0'] \n\tdom_id = session.VM.get_domid(vm_ref) \n\targs = {'id': uuidutils.generate_uuid(), 'dom_id': str(dom_id), 'timeout': str(timeout)} \n\targs.update(addl_args) \n\ttry: \n\t \tret = session.call_plugin('agent.py', method, args) \n\texcept session.XenAPI.Failure as e: \n\t \terr_msg = e.details[(-1)].splitlines()[(-1)] \n\t \tif ('TIMEOUT:' in err_msg): \n\t \t \tLOG.error(_LE('TIMEOUT: \tThe \tcall \tto \t%(method)s \ttimed \tout. \targs=%(args)r'), {'method': method, 'args': args}, instance=instance) \n\t \t \traise exception.AgentTimeout(method=method) \n\t \telif ('REBOOT:' in err_msg): \n\t \t \tLOG.debug('REBOOT: \tThe \tcall \tto \t%(method)s \tdetected \ta \treboot. \targs=%(args)r', {'method': method, 'args': args}, instance=instance) \n\t \t \t_wait_for_new_dom_id(session, vm_ref, dom_id, method) \n\t \t \treturn _call_agent(session, instance, vm_ref, method, addl_args, timeout, success_codes) \n\t \telif ('NOT \tIMPLEMENTED:' in err_msg): \n\t \t \tLOG.error(_LE('NOT \tIMPLEMENTED: \tThe \tcall \tto \t%(method)s \tis \tnot \tsupported \tby \tthe \tagent. \targs=%(args)r'), {'method': method, 'args': args}, instance=instance) \n\t \t \traise exception.AgentNotImplemented(method=method) \n\t \telse: \n\t \t \tLOG.error(_LE('The \tcall \tto \t%(method)s \treturned \tan \terror: \t%(e)s. \targs=%(args)r'), {'method': method, 'args': args, 'e': e}, instance=instance) \n\t \t \traise exception.AgentError(method=method) \n\tif (not isinstance(ret, dict)): \n\t \ttry: \n\t \t \tret = jsonutils.loads(ret) \n\t \texcept TypeError: \n\t \t \tLOG.error(_LE('The \tagent \tcall \tto \t%(method)s \treturned \tan \tinvalid \tresponse: \t%(ret)r. \targs=%(args)r'), {'method': method, 'ret': ret, 'args': args}, instance=instance) \n\t \t \traise exception.AgentError(method=method) \n\tif (ret['returncode'] not in success_codes): \n\t \tLOG.error(_LE('The \tagent \tcall \tto \t%(method)s \treturned \tan \terror: \t%(ret)r. \targs=%(args)r'), {'method': method, 'ret': ret, 'args': args}, instance=instance) \n\t \traise exception.AgentError(method=method) \n\tLOG.debug('The \tagent \tcall \tto \t%(method)s \twas \tsuccessful: \t%(ret)r. \targs=%(args)r', {'method': method, 'ret': ret, 'args': args}, instance=instance) \n\treturn ret['message'].replace('\\\\r\\\\n', '')\n", 
" \tif CONF.xenserver.disable_agent: \n\t \treturn False \n\tagent_rel_path = CONF.xenserver.agent_path \n\tagent_path = os.path.join(base_dir, agent_rel_path) \n\tif os.path.isfile(agent_path): \n\t \tLOG.info(_LI('XenServer \ttools \tinstalled \tin \tthis \timage \tare \tcapable \tof \tnetwork \tinjection. \t \tNetworking \tfiles \twill \tnot \tbemanipulated')) \n\t \treturn True \n\txe_daemon_filename = os.path.join(base_dir, 'usr', 'sbin', 'xe-daemon') \n\tif os.path.isfile(xe_daemon_filename): \n\t \tLOG.info(_LI('XenServer \ttools \tare \tpresent \tin \tthis \timage \tbut \tare \tnot \tcapable \tof \tnetwork \tinjection')) \n\telse: \n\t \tLOG.info(_LI('XenServer \ttools \tare \tnot \tinstalled \tin \tthis \timage')) \n\treturn False\n", 
" \texpr = ('field \t\"name__label\" \t= \t\"%s\" \tor \tfield \t\"bridge\" \t= \t\"%s\"' % (bridge, bridge)) \n\tnetworks = session.network.get_all_records_where(expr) \n\tif (len(networks) == 1): \n\t \treturn list(networks.keys())[0] \n\telif (len(networks) > 1): \n\t \traise exception.NovaException((_('Found \tnon-unique \tnetwork \tfor \tbridge \t%s') % bridge)) \n\telse: \n\t \traise exception.NovaException((_('Found \tno \tnetwork \tfor \tbridge \t%s') % bridge))\n", 
" \tLOG.debug('Forgetting \tSR...') \n\t_unplug_pbds(session, sr_ref) \n\tsession.call_xenapi('SR.forget', sr_ref)\n", 
" \ttry: \n\t \treturn session.call_xenapi('SR.get_by_uuid', sr_uuid) \n\texcept session.XenAPI.Failure as exc: \n\t \tif (exc.details[0] == 'UUID_INVALID'): \n\t \t \treturn None \n\t \traise\n", 
" \ttry: \n\t \tvdi_ref = session.call_xenapi('VBD.get_VDI', vbd_ref) \n\t \tsr_ref = session.call_xenapi('VDI.get_SR', vdi_ref) \n\texcept session.XenAPI.Failure: \n\t \tLOG.exception(_LE('Unable \tto \tfind \tSR \tfrom \tVBD')) \n\t \traise exception.StorageError(reason=(_('Unable \tto \tfind \tSR \tfrom \tVBD \t%s') % vbd_ref)) \n\treturn sr_ref\n", 
" \ttry: \n\t \tvdi_ref = _get_vdi_ref(session, sr_ref, vdi_uuid, target_lun) \n\t \tif (vdi_ref is None): \n\t \t \tgreenthread.sleep(CONF.xenserver.introduce_vdi_retry_wait) \n\t \t \tsession.call_xenapi('SR.scan', sr_ref) \n\t \t \tvdi_ref = _get_vdi_ref(session, sr_ref, vdi_uuid, target_lun) \n\texcept session.XenAPI.Failure: \n\t \tLOG.exception(_LE('Unable \tto \tintroduce \tVDI \ton \tSR')) \n\t \traise exception.StorageError(reason=(_('Unable \tto \tintroduce \tVDI \ton \tSR \t%s') % sr_ref)) \n\tif (not vdi_ref): \n\t \traise exception.StorageError(reason=(_('VDI \tnot \tfound \ton \tSR \t%(sr)s \t(vdi_uuid \t%(vdi_uuid)s, \ttarget_lun \t%(target_lun)s)') % {'sr': sr_ref, 'vdi_uuid': vdi_uuid, 'target_lun': target_lun})) \n\ttry: \n\t \tvdi_rec = session.call_xenapi('VDI.get_record', vdi_ref) \n\t \tLOG.debug(vdi_rec) \n\texcept session.XenAPI.Failure: \n\t \tLOG.exception(_LE('Unable \tto \tget \trecord \tof \tVDI')) \n\t \traise exception.StorageError(reason=(_('Unable \tto \tget \trecord \tof \tVDI \t%s \ton') % vdi_ref)) \n\tif vdi_rec['managed']: \n\t \treturn vdi_ref \n\ttry: \n\t \treturn session.call_xenapi('VDI.introduce', vdi_rec['uuid'], vdi_rec['name_label'], vdi_rec['name_description'], vdi_rec['SR'], vdi_rec['type'], vdi_rec['sharable'], vdi_rec['read_only'], vdi_rec['other_config'], vdi_rec['location'], vdi_rec['xenstore_data'], vdi_rec['sm_config']) \n\texcept session.XenAPI.Failure: \n\t \tLOG.exception(_LE('Unable \tto \tintroduce \tVDI \tfor \tSR')) \n\t \traise exception.StorageError(reason=(_('Unable \tto \tintroduce \tVDI \tfor \tSR \t%s') % sr_ref))\n", 
" \tvolume_id = connection_data['volume_id'] \n\ttarget_portal = connection_data['target_portal'] \n\ttarget_host = _get_target_host(target_portal) \n\ttarget_port = _get_target_port(target_portal) \n\ttarget_iqn = connection_data['target_iqn'] \n\tlog_params = {'vol_id': volume_id, 'host': target_host, 'port': target_port, 'iqn': target_iqn} \n\tLOG.debug('(vol_id,host,port,iqn): \t(%(vol_id)s,%(host)s,%(port)s,%(iqn)s)', log_params) \n\tif ((volume_id is None) or (target_host is None) or (target_iqn is None)): \n\t \traise exception.StorageError(reason=(_('Unable \tto \tobtain \ttarget \tinformation \t%s') % strutils.mask_password(connection_data))) \n\tvolume_info = {} \n\tvolume_info['id'] = volume_id \n\tvolume_info['target'] = target_host \n\tvolume_info['port'] = target_port \n\tvolume_info['targetIQN'] = target_iqn \n\tif (('auth_method' in connection_data) and (connection_data['auth_method'] == 'CHAP')): \n\t \tvolume_info['chapuser'] = connection_data['auth_username'] \n\t \tvolume_info['chappassword'] = connection_data['auth_password'] \n\treturn volume_info\n", 
" \tif mountpoint.startswith('/dev/'): \n\t \tmountpoint = mountpoint[5:] \n\tif re.match('^[hs]d[a-p]$', mountpoint): \n\t \treturn (ord(mountpoint[2:3]) - ord('a')) \n\telif re.match('^x?vd[a-p]$', mountpoint): \n\t \treturn (ord(mountpoint[(-1)]) - ord('a')) \n\telif re.match('^[0-9]+$', mountpoint): \n\t \treturn int(mountpoint, 10) \n\telse: \n\t \tLOG.warning(_LW('Mountpoint \tcannot \tbe \ttranslated: \t%s'), mountpoint) \n\t \treturn (-1)\n", 
" \tsession = get_session() \n\twith session.begin(): \n\t \tquery = _generate_paginate_query(context, session, marker, limit, sort_keys, sort_dirs, filters, offset) \n\t \tif (query is None): \n\t \t \treturn [] \n\t \treturn query.all()\n", 
" \tif iscsi_string: \n\t \thost = iscsi_string.split(':')[0] \n\t \tif (len(host) > 0): \n\t \t \treturn host \n\treturn CONF.xenserver.target_host\n", 
" \tif (iscsi_string and (':' in iscsi_string)): \n\t \treturn iscsi_string.split(':')[1] \n\treturn CONF.xenserver.target_port\n", 
" \ttry: \n\t \tresult = session.call_plugin('xenhost.py', method, args=arg_dict) \n\t \tif (not result): \n\t \t \treturn '' \n\t \treturn jsonutils.loads(result) \n\texcept ValueError: \n\t \tLOG.exception(_LE('Unable \tto \tget \tupdated \tstatus')) \n\t \treturn None \n\texcept session.XenAPI.Failure as e: \n\t \tLOG.error(_LE('The \tcall \tto \t%(method)s \treturned \tan \terror: \t%(e)s.'), {'method': method, 'e': e}) \n\t \treturn e.details[1]\n", 
" \tfor i in objects.InstanceList.get_by_host(context, host): \n\t \tif (i.name == name_label): \n\t \t \treturn i.uuid \n\treturn None\n", 
" \tuuid = session.host.get_uuid(host_ref) \n\tfor (compute_host, host_uuid) in src_aggregate.metadetails.items(): \n\t \tif (host_uuid == uuid): \n\t \t \treturn compute_host \n\traise exception.NoValidHost(reason=('Host \t%(host_uuid)s \tcould \tnot \tbe \tfound \tfrom \taggregate \tmetadata: \t%(metadata)s.' % {'host_uuid': uuid, 'metadata': src_aggregate.metadetails}))\n", 
" \treturn (POOL_FLAG in metadata.keys())\n", 
" \tsize = images.qemu_img_info(path, format).virtual_size \n\treturn int(size)\n", 
" \tif (not isinstance(image, imgmodel.LocalImage)): \n\t \treturn \n\tif (image.format == imgmodel.FORMAT_PLOOP): \n\t \tif (not can_resize_image(image.path, size)): \n\t \t \treturn \n\t \tutils.execute('prl_disk_tool', 'resize', '--size', ('%dM' % (size // units.Mi)), '--resize_partition', '--hdd', image.path, run_as_root=True) \n\t \treturn \n\tif (not can_resize_image(image.path, size)): \n\t \treturn \n\tutils.execute('qemu-img', 'resize', image.path, size) \n\tif ((image.format != imgmodel.FORMAT_RAW) and (not CONF.resize_fs_using_block_device)): \n\t \treturn \n\tif (not is_image_extendable(image)): \n\t \treturn \n\tdef safe_resize2fs(dev, run_as_root=False, finally_call=(lambda : None)): \n\t \ttry: \n\t \t \tresize2fs(dev, run_as_root=run_as_root, check_exit_code=[0]) \n\t \texcept processutils.ProcessExecutionError as exc: \n\t \t \tLOG.debug('Resizing \tthe \tfile \tsystem \twith \tresize2fs \thas \tfailed \twith \terror: \t%s', exc) \n\t \tfinally: \n\t \t \tfinally_call() \n\tif (image.format != imgmodel.FORMAT_RAW): \n\t \tmounter = mount.Mount.instance_for_format(image, None, None) \n\t \tif mounter.get_dev(): \n\t \t \tsafe_resize2fs(mounter.device, run_as_root=True, finally_call=mounter.unget_dev) \n\telse: \n\t \tsafe_resize2fs(image.path)\n", 
" \tLOG.debug('Checking \tif \twe \tcan \tresize \timage \t%(image)s. \tsize=%(size)s', {'image': image, 'size': size}) \n\tvirt_size = get_disk_size(image) \n\tif (virt_size >= size): \n\t \tLOG.debug('Cannot \tresize \timage \t%s \tto \ta \tsmaller \tsize.', image) \n\t \treturn False \n\treturn True\n", 
" \titems = {'image': image, 'key': key, 'net': net, 'metadata': metadata, 'files': files, 'partition': partition} \n\tLOG.debug('Inject \tdata \timage=%(image)s \tkey=%(key)s \tnet=%(net)s \tmetadata=%(metadata)s \tadmin_password=<SANITIZED> \tfiles=%(files)s \tpartition=%(partition)s', items) \n\ttry: \n\t \tfs = vfs.VFS.instance_for_image(image, partition) \n\t \tfs.setup() \n\texcept Exception as e: \n\t \tfor inject in mandatory: \n\t \t \tinject_val = items[inject] \n\t \t \tif inject_val: \n\t \t \t \traise \n\t \tLOG.warning(_LW('Ignoring \terror \tinjecting \tdata \tinto \timage \t%(image)s \t(%(e)s)'), {'image': image, 'e': e}) \n\t \treturn False \n\ttry: \n\t \treturn inject_data_into_fs(fs, key, net, metadata, admin_password, files, mandatory) \n\tfinally: \n\t \tfs.teardown()\n", 
" \timg = _DiskImage(image=image, mount_dir=container_dir) \n\tdev = img.mount() \n\tif (dev is None): \n\t \tLOG.error(_LE(\"Failed \tto \tmount \tcontainer \tfilesystem \t'%(image)s' \ton \t'%(target)s': \t%(errors)s\"), {'image': img, 'target': container_dir, 'errors': img.errors}) \n\t \traise exception.NovaException(img.errors) \n\treturn dev\n", 
" \ttry: \n\t \timg = _DiskImage(image=None, mount_dir=container_dir) \n\t \timg.teardown() \n\t \tif container_root_device: \n\t \t \tif ('loop' in container_root_device): \n\t \t \t \tLOG.debug('Release \tloop \tdevice \t%s', container_root_device) \n\t \t \t \tutils.execute('losetup', '--detach', container_root_device, run_as_root=True, attempts=3) \n\t \t \telif ('nbd' in container_root_device): \n\t \t \t \tLOG.debug('Release \tnbd \tdevice \t%s', container_root_device) \n\t \t \t \tutils.execute('qemu-nbd', '-d', container_root_device, run_as_root=True) \n\t \t \telse: \n\t \t \t \tLOG.debug('No \trelease \tnecessary \tfor \tblock \tdevice \t%s', container_root_device) \n\texcept Exception: \n\t \tLOG.exception(_LE('Failed \tto \tteardown \tcontainer \tfilesystem'))\n", 
" \ttry: \n\t \timg = _DiskImage(image=None, mount_dir=container_dir) \n\t \timg.umount() \n\texcept Exception: \n\t \tLOG.exception(_LE('Failed \tto \tumount \tcontainer \tfilesystem'))\n", 
" \titems = {'key': key, 'net': net, 'metadata': metadata, 'admin_password': admin_password, 'files': files} \n\tfunctions = {'key': _inject_key_into_fs, 'net': _inject_net_into_fs, 'metadata': _inject_metadata_into_fs, 'admin_password': _inject_admin_password_into_fs, 'files': _inject_files_into_fs} \n\tstatus = True \n\tfor (inject, inject_val) in items.items(): \n\t \tif inject_val: \n\t \t \ttry: \n\t \t \t \tinject_func = functions[inject] \n\t \t \t \tinject_func(inject_val, fs) \n\t \t \texcept Exception as e: \n\t \t \t \tif (inject in mandatory): \n\t \t \t \t \traise \n\t \t \t \tLOG.warning(_LW('Ignoring \terror \tinjecting \t%(inject)s \tinto \timage \t(%(e)s)'), {'inject': inject, 'e': e}) \n\t \t \t \tstatus = False \n\treturn status\n", 
" \tif (not fs.has_file(os.path.join('etc', 'selinux'))): \n\t \treturn \n\trclocal = os.path.join('etc', 'rc.local') \n\trc_d = os.path.join('etc', 'rc.d') \n\tif ((not fs.has_file(rclocal)) and fs.has_file(rc_d)): \n\t \trclocal = os.path.join(rc_d, 'rc.local') \n\trestorecon = ['\\n', '# \tAdded \tby \tNova \tto \tensure \tinjected \tssh \tkeys \thave \tthe \tright \tcontext\\n', ('restorecon \t-RF \t%s \t2>/dev/null \t|| \t:\\n' % sshdir)] \n\tif (not fs.has_file(rclocal)): \n\t \trestorecon.insert(0, '#!/bin/sh') \n\t_inject_file_into_fs(fs, rclocal, ''.join(restorecon), append=True) \n\tfs.set_permissions(rclocal, 448)\n", 
" \tLOG.debug('Inject \tkey \tfs=%(fs)s \tkey=%(key)s', {'fs': fs, 'key': key}) \n\tsshdir = os.path.join('root', '.ssh') \n\tfs.make_path(sshdir) \n\tfs.set_ownership(sshdir, 'root', 'root') \n\tfs.set_permissions(sshdir, 448) \n\tkeyfile = os.path.join(sshdir, 'authorized_keys') \n\tkey_data = ''.join(['\\n', '# \tThe \tfollowing \tssh \tkey \twas \tinjected \tby \tNova', '\\n', key.strip(), '\\n']) \n\t_inject_file_into_fs(fs, keyfile, key_data, append=True) \n\tfs.set_permissions(keyfile, 384) \n\t_setup_selinux_for_keys(fs, sshdir)\n", 
" \tLOG.debug('Inject \tkey \tfs=%(fs)s \tnet=%(net)s', {'fs': fs, 'net': net}) \n\tnetdir = os.path.join('etc', 'network') \n\tfs.make_path(netdir) \n\tfs.set_ownership(netdir, 'root', 'root') \n\tfs.set_permissions(netdir, 484) \n\tnetfile = os.path.join('etc', 'network', 'interfaces') \n\t_inject_file_into_fs(fs, netfile, net)\n", 
" \tLOG.debug('Inject \tadmin \tpassword \tfs=%(fs)s \tadmin_passwd=<SANITIZED>', {'fs': fs}) \n\tadmin_user = 'root' \n\tpasswd_path = os.path.join('etc', 'passwd') \n\tshadow_path = os.path.join('etc', 'shadow') \n\tpasswd_data = fs.read_file(passwd_path) \n\tshadow_data = fs.read_file(shadow_path) \n\tnew_shadow_data = _set_passwd(admin_user, admin_passwd, passwd_data, shadow_data) \n\tfs.replace_file(shadow_path, new_shadow_data)\n", 
" \tif (os.name == 'nt'): \n\t \traise exception.NovaException(_('Not \timplemented \ton \tWindows')) \n\talgos = {'SHA-512': '$6$', 'SHA-256': '$5$', 'MD5': '$1$', 'DES': ''} \n\tsalt = _generate_salt() \n\tencrypted_passwd = crypt.crypt(admin_passwd, (algos['MD5'] + salt)) \n\tif (len(encrypted_passwd) == 13): \n\t \tencrypted_passwd = crypt.crypt(admin_passwd, (algos['DES'] + salt)) \n\tp_file = passwd_data.split('\\n') \n\ts_file = shadow_data.split('\\n') \n\tfor entry in p_file: \n\t \tsplit_entry = entry.split(':') \n\t \tif (split_entry[0] == username): \n\t \t \tbreak \n\telse: \n\t \tmsg = _('User \t%(username)s \tnot \tfound \tin \tpassword \tfile.') \n\t \traise exception.NovaException((msg % username)) \n\tnew_shadow = list() \n\tfound = False \n\tfor entry in s_file: \n\t \tsplit_entry = entry.split(':') \n\t \tif (split_entry[0] == username): \n\t \t \tsplit_entry[1] = encrypted_passwd \n\t \t \tfound = True \n\t \tnew_entry = ':'.join(split_entry) \n\t \tnew_shadow.append(new_entry) \n\tif (not found): \n\t \tmsg = _('User \t%(username)s \tnot \tfound \tin \tshadow \tfile.') \n\t \traise exception.NovaException((msg % username)) \n\treturn '\\n'.join(new_shadow)\n", 
" \treturn hashlib.sha1(image_id.encode('utf-8')).hexdigest()\n", 
" \tbase_file = os.path.basename(base_path) \n\treturn (CONF.libvirt.image_info_filename_pattern % {'image': base_file})\n", 
" \tif six.PY2: \n\t \tdigest_size = (hashlib.sha1().digestsize * 2) \n\telse: \n\t \tdigest_size = (hashlib.sha1().digest_size * 2) \n\tregexp = (CONF.libvirt.image_info_filename_pattern % {'image': ('([0-9a-f]{%(digest_size)d}|[0-9a-f]{%(digest_size)d}_sm|[0-9a-f]{%(digest_size)d}_[0-9]+)' % {'digest_size': digest_size})}) \n\tm = re.match(regexp, path) \n\tif m: \n\t \treturn True \n\treturn False\n", 
" \tapp = get_app() \n\tabout = safe_read(apath(('%s/ABOUT' % app), r=request)) \n\tlicense = safe_read(apath(('%s/LICENSE' % app), r=request)) \n\treturn dict(app=app, about=MARKMIN(about), license=MARKMIN(license), progress=report_progress(app))\n", 
" \tcmd = '{0} \tlist \t-t \t{1}'.format(_ipset_cmd(), set) \n\tout = __salt__['cmd.run_all'](cmd, python_shell=False) \n\tif (out['retcode'] > 0): \n\t \treturn False \n\tsetinfo = {} \n\t_tmp = out['stdout'].split('\\n') \n\tfor item in _tmp: \n\t \tif (':' in item): \n\t \t \t(key, value) = item.split(':', 1) \n\t \t \tsetinfo[key] = value[1:] \n\treturn setinfo\n", 
" \timport hashlib \n\tconverted = hashlib.sha1(docstr).hexdigest() \n\treturn converted\n", 
" \twith salt.utils.fopen(filename, 'w') as fp_: \n\t \tfp_.write(data)\n", 
" \tfor disk in mapping: \n\t \tinfo = mapping[disk] \n\t \tif (info['dev'] == disk_dev): \n\t \t \treturn True \n\treturn False\n", 
" \tif CONF.libvirt.disk_prefix: \n\t \treturn CONF.libvirt.disk_prefix \n\tif (disk_bus == 'ide'): \n\t \treturn 'hd' \n\telif (disk_bus == 'virtio'): \n\t \treturn 'vd' \n\telif (disk_bus == 'xen'): \n\t \treturn 'xvd' \n\telif (disk_bus == 'scsi'): \n\t \treturn 'sd' \n\telif (disk_bus == 'usb'): \n\t \treturn 'sd' \n\telif (disk_bus == 'fdc'): \n\t \treturn 'fd' \n\telif (disk_bus == 'uml'): \n\t \treturn 'ubd' \n\telif (disk_bus == 'lxc'): \n\t \treturn None \n\telif (disk_bus == 'sata'): \n\t \treturn 'sd' \n\telse: \n\t \traise exception.InternalError((_('Unable \tto \tdetermine \tdisk \tprefix \tfor \t%s') % disk_bus))\n", 
" \tif (disk_bus == 'ide'): \n\t \treturn 4 \n\telse: \n\t \treturn 26\n", 
" \tdev_prefix = get_dev_prefix_for_disk_bus(bus) \n\tif (dev_prefix is None): \n\t \treturn None \n\tif (assigned_devices is None): \n\t \tassigned_devices = [] \n\tmax_dev = get_dev_count_for_disk_bus(bus) \n\tdevs = range(max_dev) \n\tfor idx in devs: \n\t \tdisk_dev = (dev_prefix + chr((ord('a') + idx))) \n\t \tif (not has_disk_dev(mapping, disk_dev)): \n\t \t \tif (disk_dev not in assigned_devices): \n\t \t \t \treturn disk_dev \n\tmsg = (_(\"No \tfree \tdisk \tdevice \tnames \tfor \tprefix \t'%s'\") % dev_prefix) \n\traise exception.InternalError(msg)\n", 
" \tif (device_type == 'disk'): \n\t \tdisk_bus = osinfo.HardwareProperties(image_meta).disk_model \n\telse: \n\t \tkey = (('hw_' + device_type) + '_bus') \n\t \tdisk_bus = image_meta.properties.get(key) \n\tif (disk_bus is not None): \n\t \tif (not is_disk_bus_valid_for_virt(virt_type, disk_bus)): \n\t \t \traise exception.UnsupportedHardware(model=disk_bus, virt=virt_type) \n\t \treturn disk_bus \n\tif (virt_type == 'uml'): \n\t \tif (device_type == 'disk'): \n\t \t \treturn 'uml' \n\telif (virt_type == 'lxc'): \n\t \treturn 'lxc' \n\telif (virt_type == 'xen'): \n\t \tguest_vm_mode = obj_fields.VMMode.get_from_instance(instance) \n\t \tif (guest_vm_mode == obj_fields.VMMode.HVM): \n\t \t \treturn 'ide' \n\t \telse: \n\t \t \treturn 'xen' \n\telif (virt_type in ('qemu', 'kvm')): \n\t \tif (device_type == 'cdrom'): \n\t \t \tguestarch = libvirt_utils.get_arch(image_meta) \n\t \t \tif (guestarch in (obj_fields.Architecture.PPC, obj_fields.Architecture.PPC64, obj_fields.Architecture.PPCLE, obj_fields.Architecture.PPC64LE, obj_fields.Architecture.S390, obj_fields.Architecture.S390X, obj_fields.Architecture.AARCH64)): \n\t \t \t \treturn 'scsi' \n\t \t \telse: \n\t \t \t \treturn 'ide' \n\t \telif (device_type == 'disk'): \n\t \t \treturn 'virtio' \n\t \telif (device_type == 'floppy'): \n\t \t \treturn 'fdc' \n\telif (virt_type == 'parallels'): \n\t \tif (device_type == 'cdrom'): \n\t \t \treturn 'ide' \n\t \telif (device_type == 'disk'): \n\t \t \treturn 'scsi' \n\telse: \n\t \traise exception.UnsupportedVirtType(virt=virt_type) \n\treturn None\n", 
" \tif CONF.libvirt.disk_prefix: \n\t \treturn CONF.libvirt.disk_prefix \n\tif (disk_bus == 'ide'): \n\t \treturn 'hd' \n\telif (disk_bus == 'virtio'): \n\t \treturn 'vd' \n\telif (disk_bus == 'xen'): \n\t \treturn 'xvd' \n\telif (disk_bus == 'scsi'): \n\t \treturn 'sd' \n\telif (disk_bus == 'usb'): \n\t \treturn 'sd' \n\telif (disk_bus == 'fdc'): \n\t \treturn 'fd' \n\telif (disk_bus == 'uml'): \n\t \treturn 'ubd' \n\telif (disk_bus == 'lxc'): \n\t \treturn None \n\telif (disk_bus == 'sata'): \n\t \treturn 'sd' \n\telse: \n\t \traise exception.InternalError((_('Unable \tto \tdetermine \tdisk \tprefix \tfor \t%s') % disk_bus))\n", 
" \tdisk_dev = find_disk_dev_for_disk_bus(mapping, disk_bus, assigned_devices) \n\tinfo = {'bus': disk_bus, 'dev': disk_dev, 'type': device_type} \n\tif ((boot_index is not None) and (boot_index >= 0)): \n\t \tinfo['boot_index'] = str(boot_index) \n\treturn info\n", 
" \tmapping = {} \n\tif rescue: \n\t \trescue_info = get_next_disk_info(mapping, disk_bus, boot_index=1) \n\t \tmapping['disk.rescue'] = rescue_info \n\t \tmapping['root'] = rescue_info \n\t \tos_info = get_next_disk_info(mapping, disk_bus) \n\t \tmapping['disk'] = os_info \n\t \tif configdrive.required_by(instance): \n\t \t \tdevice_type = get_config_drive_type() \n\t \t \tdisk_bus = get_disk_bus_for_device_type(instance, virt_type, image_meta, device_type) \n\t \t \tconfig_info = get_next_disk_info(mapping, disk_bus, device_type) \n\t \t \tmapping['disk.config.rescue'] = config_info \n\t \treturn mapping \n\tpre_assigned_device_names = [block_device.strip_dev(get_device_name(bdm)) for bdm in itertools.chain(driver.block_device_info_get_ephemerals(block_device_info), [driver.block_device_info_get_swap(block_device_info)], driver.block_device_info_get_mapping(block_device_info)) if get_device_name(bdm)] \n\troot_bdm = block_device.get_root_bdm(driver.block_device_info_get_mapping(block_device_info)) \n\troot_device_name = block_device.strip_dev(driver.block_device_info_get_root(block_device_info)) \n\troot_info = get_root_info(instance, virt_type, image_meta, root_bdm, disk_bus, cdrom_bus, root_device_name) \n\tmapping['root'] = root_info \n\tif ((not root_bdm) and (not block_device.volume_in_mapping(root_info['dev'], block_device_info))): \n\t \tmapping['disk'] = root_info \n\telif root_bdm: \n\t \tupdate_bdm(root_bdm, root_info) \n\tdefault_eph = get_default_ephemeral_info(instance, disk_bus, block_device_info, mapping) \n\tif default_eph: \n\t \tmapping['disk.local'] = default_eph \n\tfor (idx, eph) in enumerate(driver.block_device_info_get_ephemerals(block_device_info)): \n\t \teph_info = get_info_from_bdm(instance, virt_type, image_meta, eph, mapping, disk_bus, assigned_devices=pre_assigned_device_names) \n\t \tmapping[get_eph_disk(idx)] = eph_info \n\t \tupdate_bdm(eph, eph_info) \n\tswap = driver.block_device_info_get_swap(block_device_info) \n\tif (swap and (swap.get('swap_size', 0) > 0)): \n\t \tswap_info = get_info_from_bdm(instance, virt_type, image_meta, swap, mapping, disk_bus) \n\t \tmapping['disk.swap'] = swap_info \n\t \tupdate_bdm(swap, swap_info) \n\telif (instance.get_flavor()['swap'] > 0): \n\t \tswap_info = get_next_disk_info(mapping, disk_bus, assigned_devices=pre_assigned_device_names) \n\t \tif (not block_device.volume_in_mapping(swap_info['dev'], block_device_info)): \n\t \t \tmapping['disk.swap'] = swap_info \n\tblock_device_mapping = driver.block_device_info_get_mapping(block_device_info) \n\tfor bdm in block_device_mapping: \n\t \tvol_info = get_info_from_bdm(instance, virt_type, image_meta, bdm, mapping, assigned_devices=pre_assigned_device_names) \n\t \tmapping[block_device.prepend_dev(vol_info['dev'])] = vol_info \n\t \tupdate_bdm(bdm, vol_info) \n\tif configdrive.required_by(instance): \n\t \tdevice_type = get_config_drive_type() \n\t \tdisk_bus = get_disk_bus_for_device_type(instance, virt_type, image_meta, device_type) \n\t \tconfig_info = get_next_disk_info(mapping, disk_bus, device_type) \n\t \tmapping['disk.config'] = config_info \n\treturn mapping\n", 
" \tdisk_bus = get_disk_bus_for_device_type(instance, virt_type, image_meta, 'disk') \n\tcdrom_bus = get_disk_bus_for_device_type(instance, virt_type, image_meta, 'cdrom') \n\tmapping = get_disk_mapping(virt_type, instance, disk_bus, cdrom_bus, image_meta, block_device_info, rescue) \n\treturn {'disk_bus': disk_bus, 'cdrom_bus': cdrom_bus, 'mapping': mapping}\n", 
" \tconf.net_type = 'vhostuser' \n\tconf.vhostuser_type = 'unix' \n\tconf.vhostuser_mode = mode \n\tconf.vhostuser_path = path\n", 
" \tconf.net_type = 'vhostuser' \n\tconf.vhostuser_type = 'unix' \n\tconf.vhostuser_mode = mode \n\tconf.vhostuser_path = path\n", 
" \tconf.net_type = 'vhostuser' \n\tconf.vhostuser_type = 'unix' \n\tconf.vhostuser_mode = mode \n\tconf.vhostuser_path = path\n", 
" \tconf.net_type = 'vhostuser' \n\tconf.vhostuser_type = 'unix' \n\tconf.vhostuser_mode = mode \n\tconf.vhostuser_path = path\n", 
" \tconf.net_type = 'vhostuser' \n\tconf.vhostuser_type = 'unix' \n\tconf.vhostuser_mode = mode \n\tconf.vhostuser_path = path\n", 
" \tconf.net_type = 'vhostuser' \n\tconf.vhostuser_type = 'unix' \n\tconf.vhostuser_mode = mode \n\tconf.vhostuser_path = path\n", 
" \tbandwidth_items = ['vif_inbound_average', 'vif_inbound_peak', 'vif_inbound_burst', 'vif_outbound_average', 'vif_outbound_peak', 'vif_outbound_burst'] \n\tfor (key, value) in inst_type.get('extra_specs', {}).items(): \n\t \tscope = key.split(':') \n\t \tif ((len(scope) > 1) and (scope[0] == 'quota')): \n\t \t \tif (scope[1] in bandwidth_items): \n\t \t \t \tsetattr(conf, scope[1], value)\n", 
" \troot_helper = utils.get_root_helper() \n\tif (not execute): \n\t \texecute = putils.execute \n\tiscsi = connector.ISCSIConnector(root_helper=root_helper, execute=execute) \n\treturn iscsi.get_initiator()\n", 
" \t_fczm_register.append(cls) \n\treturn cls\n", 
" \tinit_targ_map = build_initiator_target_map(connector, conn_info['data']['target_wwn'], lookup_service) \n\tif init_targ_map: \n\t \tconn_info['data']['initiator_target_map'] = init_targ_map\n", 
" \tmode_cmd('-o', 'deop', text, chan, conn, notice)\n", 
" \tmode_cmd('-o', 'deop', text, chan, conn, notice)\n", 
" \texecute('qemu-img', 'create', '-f', disk_format, path, size)\n", 
" \tbase_cmd = ['qemu-img', 'create', '-f', 'qcow2'] \n\tcow_opts = [] \n\tif backing_file: \n\t \tcow_opts += [('backing_file=%s' % backing_file)] \n\t \tbase_details = images.qemu_img_info(backing_file) \n\telse: \n\t \tbase_details = None \n\tif (base_details and (base_details.cluster_size is not None)): \n\t \tcow_opts += [('cluster_size=%s' % base_details.cluster_size)] \n\tif (size is not None): \n\t \tcow_opts += [('size=%s' % size)] \n\tif cow_opts: \n\t \tcsv_opts = ','.join(cow_opts) \n\t \tcow_opts = ['-o', csv_opts] \n\tcmd = ((base_cmd + cow_opts) + [path]) \n\texecute(*cmd)\n", 
" \tvg_info = get_volume_group_info(vg) \n\tfree_space = vg_info['free'] \n\tdef check_size(vg, lv, size): \n\t \tif (size > free_space): \n\t \t \traise RuntimeError((_('Insufficient \tSpace \ton \tVolume \tGroup \t%(vg)s. \tOnly \t%(free_space)db \tavailable, \tbut \t%(size)d \tbytes \trequired \tby \tvolume \t%(lv)s.') % {'vg': vg, 'free_space': free_space, 'size': size, 'lv': lv})) \n\tif sparse: \n\t \tpreallocated_space = (64 * units.Mi) \n\t \tcheck_size(vg, lv, preallocated_space) \n\t \tif (free_space < size): \n\t \t \tLOG.warning(_LW('Volume \tgroup \t%(vg)s \twill \tnot \tbe \table \tto \thold \tsparse \tvolume \t%(lv)s. \tVirtual \tvolume \tsize \tis \t%(size)d \tbytes, \tbut \tfree \tspace \ton \tvolume \tgroup \tis \tonly \t%(free_space)db.'), {'vg': vg, 'free_space': free_space, 'size': size, 'lv': lv}) \n\t \tcmd = ('lvcreate', '-L', ('%db' % preallocated_space), '--virtualsize', ('%db' % size), '-n', lv, vg) \n\telse: \n\t \tcheck_size(vg, lv, size) \n\t \tcmd = ('lvcreate', '-L', ('%db' % size), '-n', lv, vg) \n\tutils.execute(run_as_root=True, attempts=3, *cmd)\n", 
" \t(out, err) = utils.execute('vgs', '--noheadings', '--nosuffix', '--separator', '|', '--units', 'b', '-o', 'vg_size,vg_free', vg, run_as_root=True) \n\tinfo = out.split('|') \n\tif (len(info) != 2): \n\t \traise RuntimeError((_('vg \t%s \tmust \tbe \tLVM \tvolume \tgroup') % vg)) \n\treturn {'total': int(info[0]), 'free': int(info[1]), 'used': (int(info[0]) - int(info[1]))}\n", 
" \t(out, err) = utils.execute('lvs', '--noheadings', '-o', 'lv_name', vg, run_as_root=True) \n\treturn [line.strip() for line in out.splitlines()]\n", 
" \t(out, err) = utils.execute('lvs', '-o', 'vg_all,lv_all', '--separator', '|', path, run_as_root=True) \n\tinfo = [line.split('|') for line in out.splitlines()] \n\tif (len(info) != 2): \n\t \traise RuntimeError((_('Path \t%s \tmust \tbe \tLVM \tlogical \tvolume') % path)) \n\treturn dict(zip(*info))\n", 
" \ttry: \n\t \t(out, _err) = utils.execute('blockdev', '--getsize64', path, run_as_root=True) \n\texcept processutils.ProcessExecutionError: \n\t \tif (not utils.path_exists(path)): \n\t \t \traise exception.VolumeBDMPathNotFound(path=path) \n\t \telse: \n\t \t \traise \n\treturn int(out)\n", 
" \tvolume_clear = CONF.libvirt.volume_clear \n\tif (volume_clear == 'none'): \n\t \treturn \n\tvolume_clear_size = (int(CONF.libvirt.volume_clear_size) * units.Mi) \n\ttry: \n\t \tvolume_size = get_volume_size(path) \n\texcept exception.VolumeBDMPathNotFound: \n\t \tLOG.warning(_LW('ignoring \tmissing \tlogical \tvolume \t%(path)s'), {'path': path}) \n\t \treturn \n\tif ((volume_clear_size != 0) and (volume_clear_size < volume_size)): \n\t \tvolume_size = volume_clear_size \n\tif (volume_clear == 'zero'): \n\t \t_zero_volume(path, volume_size) \n\telif (volume_clear == 'shred'): \n\t \tutils.execute('shred', '-n3', ('-s%d' % volume_size), path, run_as_root=True)\n", 
" \terrors = [] \n\tfor path in paths: \n\t \tclear_volume(path) \n\t \tlvremove = ('lvremove', '-f', path) \n\t \ttry: \n\t \t \tutils.execute(attempts=3, run_as_root=True, *lvremove) \n\t \texcept processutils.ProcessExecutionError as exp: \n\t \t \terrors.append(six.text_type(exp)) \n\tif errors: \n\t \traise exception.VolumesNotRemoved(reason=', \t'.join(errors))\n", 
" \tif (CONF.libvirt.virt_type == 'xen'): \n\t \tif is_block_dev: \n\t \t \treturn 'phy' \n\t \telse: \n\t \t \tif (hypervisor_version >= 4002000): \n\t \t \t \ttry: \n\t \t \t \t \texecute('xend', 'status', run_as_root=True, check_exit_code=True) \n\t \t \t \texcept OSError as exc: \n\t \t \t \t \tif (exc.errno == errno.ENOENT): \n\t \t \t \t \t \tLOG.debug('xend \tis \tnot \tfound') \n\t \t \t \t \t \treturn 'qemu' \n\t \t \t \t \telse: \n\t \t \t \t \t \traise \n\t \t \t \texcept processutils.ProcessExecutionError: \n\t \t \t \t \tLOG.debug('xend \tis \tnot \tstarted') \n\t \t \t \t \treturn 'qemu' \n\t \t \ttry: \n\t \t \t \t(out, err) = execute('tap-ctl', 'check', check_exit_code=False) \n\t \t \t \tif (out == 'ok\\n'): \n\t \t \t \t \tif (hypervisor_version > 4000000): \n\t \t \t \t \t \treturn 'tap2' \n\t \t \t \t \telse: \n\t \t \t \t \t \treturn 'tap' \n\t \t \t \telse: \n\t \t \t \t \tLOG.info(_LI('tap-ctl \tcheck: \t%s'), out) \n\t \t \texcept OSError as exc: \n\t \t \t \tif (exc.errno == errno.ENOENT): \n\t \t \t \t \tLOG.debug('tap-ctl \ttool \tis \tnot \tinstalled') \n\t \t \t \telse: \n\t \t \t \t \traise \n\t \t \treturn 'file' \n\telif (CONF.libvirt.virt_type in ('kvm', 'qemu')): \n\t \treturn 'qemu' \n\telse: \n\t \treturn None\n", 
" \tsize = images.qemu_img_info(path, format).virtual_size \n\treturn int(size)\n", 
" \tbacking_file = images.qemu_img_info(path, format).backing_file \n\tif (backing_file and basename): \n\t \tbacking_file = os.path.basename(backing_file) \n\treturn backing_file\n", 
" \tif (not host): \n\t \texecute('cp', '-r', src, dest) \n\telse: \n\t \tif receive: \n\t \t \tsrc = ('%s:%s' % (utils.safe_ip_format(host), src)) \n\t \telse: \n\t \t \tdest = ('%s:%s' % (utils.safe_ip_format(host), dest)) \n\t \tremote_filesystem_driver = remotefs.RemoteFilesystem() \n\t \tremote_filesystem_driver.copy_file(src, dest, on_execute=on_execute, on_completion=on_completion, compression=compression)\n", 
" \tif umask: \n\t \tsaved_umask = os.umask(umask) \n\ttry: \n\t \twith open(path, 'w') as f: \n\t \t \tf.write(contents) \n\tfinally: \n\t \tif umask: \n\t \t \tos.umask(saved_umask)\n", 
" \texecute('chown', owner, path, run_as_root=True)\n", 
" \tif (dest_fmt == 'iso'): \n\t \tdest_fmt = 'raw' \n\tif (dest_fmt == 'ploop'): \n\t \tdest_fmt = 'parallels' \n\tqemu_img_cmd = ('qemu-img', 'convert', '-f', source_fmt, '-O', dest_fmt) \n\tif (CONF.libvirt.snapshot_compression and (dest_fmt == 'qcow2')): \n\t \tqemu_img_cmd += ('-c',) \n\tqemu_img_cmd += (disk_path, out_path) \n\texecute(*qemu_img_cmd)\n", 
" \tif (dest_fmt == 'iso'): \n\t \tdest_fmt = 'raw' \n\tif (dest_fmt == 'ploop'): \n\t \tdest_fmt = 'parallels' \n\tqemu_img_cmd = ('qemu-img', 'convert', '-f', source_fmt, '-O', dest_fmt) \n\tif (CONF.libvirt.snapshot_compression and (dest_fmt == 'qcow2')): \n\t \tqemu_img_cmd += ('-c',) \n\tqemu_img_cmd += (disk_path, out_path) \n\texecute(*qemu_img_cmd)\n", 
" \tif (dest_fmt == 'iso'): \n\t \tdest_fmt = 'raw' \n\tif (dest_fmt == 'ploop'): \n\t \tdest_fmt = 'parallels' \n\tqemu_img_cmd = ('qemu-img', 'convert', '-f', source_fmt, '-O', dest_fmt) \n\tif (CONF.libvirt.snapshot_compression and (dest_fmt == 'qcow2')): \n\t \tqemu_img_cmd += ('-c',) \n\tqemu_img_cmd += (disk_path, out_path) \n\texecute(*qemu_img_cmd)\n", 
" \twith open(path, 'r') as fp: \n\t \treturn fp.read()\n", 
" \treturn open(*args, **kwargs)\n", 
" \treturn os.unlink(path)\n", 
" \txml_desc = virt_dom.XMLDesc(0) \n\tdomain = etree.fromstring(xml_desc) \n\tos_type = domain.find('os/type').text \n\tdriver = None \n\tif (CONF.libvirt.virt_type == 'lxc'): \n\t \tfilesystem = domain.find('devices/filesystem') \n\t \tdriver = filesystem.find('driver') \n\t \tsource = filesystem.find('source') \n\t \tdisk_path = source.get('dir') \n\t \tdisk_path = disk_path[0:disk_path.rfind('rootfs')] \n\t \tdisk_path = os.path.join(disk_path, 'disk') \n\telif ((CONF.libvirt.virt_type == 'parallels') and (os_type == obj_fields.VMMode.EXE)): \n\t \tfilesystem = domain.find('devices/filesystem') \n\t \tdriver = filesystem.find('driver') \n\t \tsource = filesystem.find('source') \n\t \tdisk_path = source.get('file') \n\telse: \n\t \tdisk = domain.find('devices/disk') \n\t \tdriver = disk.find('driver') \n\t \tsource = disk.find('source') \n\t \tdisk_path = (source.get('file') or source.get('dev')) \n\t \tif ((not disk_path) and (CONF.libvirt.images_type == 'rbd')): \n\t \t \tdisk_path = source.get('name') \n\t \t \tif disk_path: \n\t \t \t \tdisk_path = ('rbd:' + disk_path) \n\tif (not disk_path): \n\t \traise RuntimeError(_(\"Can't \tretrieve \troot \tdevice \tpath \tfrom \tinstance \tlibvirt \tconfiguration\")) \n\tif (driver is not None): \n\t \tformat = driver.get('type') \n\t \tif (format == 'aio'): \n\t \t \tformat = 'raw' \n\telse: \n\t \tformat = None \n\treturn (disk_path, format)\n", 
" \tif path.startswith('/dev'): \n\t \treturn 'lvm' \n\telif path.startswith('rbd:'): \n\t \treturn 'rbd' \n\telif (os.path.isdir(path) and os.path.exists(os.path.join(path, 'DiskDescriptor.xml'))): \n\t \treturn 'ploop' \n\treturn None\n", 
" \thddinfo = os.statvfs(path) \n\ttotal = (hddinfo.f_frsize * hddinfo.f_blocks) \n\tfree = (hddinfo.f_frsize * hddinfo.f_bavail) \n\tused = (hddinfo.f_frsize * (hddinfo.f_blocks - hddinfo.f_bfree)) \n\treturn {'total': total, 'free': free, 'used': used}\n", 
" \timages.fetch_to_raw(context, image_id, target)\n", 
" \tif relative: \n\t \treturn instance.uuid \n\treturn os.path.join(CONF.instances_path, instance.uuid)\n", 
" \tdef str_method(self): \n\t \treturn str(self._obj) \n\tdef repr_method(self): \n\t \treturn repr(self._obj) \n\ttpool.Proxy.__str__ = str_method \n\ttpool.Proxy.__repr__ = repr_method\n", 
" \tif (not compute_driver): \n\t \tcompute_driver = CONF.compute_driver \n\tif (not compute_driver): \n\t \tLOG.error(_LE('Compute \tdriver \toption \trequired, \tbut \tnot \tspecified')) \n\t \tsys.exit(1) \n\tLOG.info(_LI(\"Loading \tcompute \tdriver \t'%s'\"), compute_driver) \n\ttry: \n\t \tdriver = importutils.import_object(('nova.virt.%s' % compute_driver), virtapi) \n\t \treturn utils.check_isinstance(driver, ComputeDriver) \n\texcept ImportError: \n\t \tLOG.exception(_LE('Unable \tto \tload \tthe \tvirtualization \tdriver')) \n\t \tsys.exit(1)\n", 
" \tproc = sp.Popen(['ssh', host], stdin=sp.PIPE) \n\tproc.stdin.write(cmd) \n\tproc.wait()\n", 
" \tif (env.user == 'root'): \n\t \tfunc = run \n\telse: \n\t \tfunc = sudo \n\treturn func(command, *args, **kwargs)\n", 
" \tconsole_prefix = 'FTP: \t' \n\tplugin_tag = 'jenkins.plugins.publish__over__ftp.BapFtpPublisherPlugin' \n\tpublisher_tag = 'jenkins.plugins.publish__over__ftp.BapFtpPublisher' \n\ttransfer_tag = 'jenkins.plugins.publish__over__ftp.BapFtpTransfer' \n\tplugin_reference_tag = 'jenkins.plugins.publish_over_ftp.BapFtpPublisherPlugin' \n\t(_, transfer_node) = base_publish_over(xml_parent, data, console_prefix, plugin_tag, publisher_tag, transfer_tag, plugin_reference_tag) \n\tXML.SubElement(transfer_node, 'asciiMode').text = 'false'\n", 
" \tparsed_url = urllib.parse.urlparse(url) \n\tfile_name = os.path.basename(parsed_url.path) \n\tserver_path = parsed_url.path.replace(file_name, '') \n\tunquoted_server_path = urllib.parse.unquote(server_path) \n\tdata = ftplib.FTP() \n\tif (parsed_url.port is not None): \n\t \tdata.connect(parsed_url.hostname, parsed_url.port) \n\telse: \n\t \tdata.connect(parsed_url.hostname) \n\tdata.login() \n\tif (len(server_path) > 1): \n\t \tdata.cwd(unquoted_server_path) \n\tdata.sendcmd('TYPE \tI') \n\tdata.sendcmd(('REST \t' + str(initial_size))) \n\tdown_cmd = ('RETR \t' + file_name) \n\tassert (file_size == data.size(file_name)) \n\tprogress = ProgressBar(file_size, initial_value=initial_size, max_chars=40, spinner=True, mesg='file_sizes', verbose_bool=verbose_bool) \n\tmode = ('ab' if (initial_size > 0) else 'wb') \n\twith open(temp_file_name, mode) as local_file: \n\t \tdef chunk_write(chunk): \n\t \t \treturn _chunk_write(chunk, local_file, progress) \n\t \tdata.retrbinary(down_cmd, chunk_write) \n\t \tdata.close() \n\tsys.stdout.write('\\n') \n\tsys.stdout.flush()\n", 
" \tret = {'name': name, 'changes': {}, 'comment': '', 'result': True} \n\tif (not results): \n\t \tret['comment'] = \"'results' \targument \tis \trequired\" \n\t \tret['result'] = False \n\t \treturn ret \n\tif isinstance(results, six.string_types): \n\t \tresults = results.split(',') \n\tneeded = _get_missing_results(results, dest_dir) \n\tif ((not force) and (not needed)): \n\t \tret['comment'] = 'All \tneeded \tpackages \texist' \n\t \treturn ret \n\tif __opts__['test']: \n\t \tret['result'] = None \n\t \tif force: \n\t \t \tret['comment'] = 'Packages \twill \tbe \tforce-built' \n\t \telse: \n\t \t \tret['comment'] = 'The \tfollowing \tpackages \tneed \tto \tbe \tbuilt: \t' \n\t \t \tret['comment'] += ', \t'.join(needed) \n\t \treturn ret \n\tif ((env is not None) and (not isinstance(env, dict))): \n\t \tret['comment'] = \"Invalidly-formatted \t'env' \tparameter. \tSee \tdocumentation.\" \n\t \tret['result'] = False \n\t \treturn ret \n\tfunc = 'pkgbuild.build' \n\tif (__grains__.get('os_family', False) not in ('RedHat', 'Suse')): \n\t \tfor res in results: \n\t \t \tif res.endswith('.rpm'): \n\t \t \t \tfunc = 'rpmbuild.build' \n\t \t \t \tbreak \n\tret['changes'] = __salt__[func](runas, tgt, dest_dir, spec, sources, deps, env, template, saltenv, log_dir) \n\tneeded = _get_missing_results(results, dest_dir) \n\tif needed: \n\t \tret['comment'] = 'The \tfollowing \tpackages \twere \tnot \tbuilt: \t' \n\t \tret['comment'] += ', \t'.join(needed) \n\t \tret['result'] = False \n\telse: \n\t \tret['comment'] = 'All \tneeded \tpackages \twere \tbuilt' \n\treturn ret\n", 
" \tcommands = [sudo_from_args(['sed', '-i', '1 \ti \tPermitRootLogin \tyes', '/etc/ssh/sshd_config'])] \n\tif is_systemd_distribution(distribution): \n\t \tcommands.append(sudo_from_args(['systemctl', 'restart', 'sshd'])) \n\telse: \n\t \tcommands.append(sudo_from_args(['service', 'ssh', 'restart'])) \n\treturn sequence(commands)\n", 
" \tif (len(p) == 3): \n\t \tp[0] = [] \n\telse: \n\t \tp[0] = p[2]\n", 
" \t(major, minor, micro, release_level, serial) = sys.version_info \n\tif (major == 2): \n\t \tif (minor != 7): \n\t \t \tmsg = 'Error: \tPython \t2.%s \tfound \tbut \tPython \t2.7 \trequired.' \n\t \t \tprint (msg % minor) \n\telif (major > 2): \n\t \tmsg = 'It \tseems \tthat \tyou \tare \trunning \tw3af \tusing \tPython3, \twhich \tis \tnot \tofficially \tsupported \tby \tthe \tw3af \tteam.\\nTo \tforce \tw3af \tto \tbe \trun \tusing \tpython2.7 \trun \tit \tas \tfollows \t(depending \ton \tyour \tOS):\\n\\n \t* \tpython2.7 \tw3af_console\\n \t* \tpython2 \tw3af_console\\n\\nTo \tmake \tthis \tchange \tpermanent \tmodify \tthe \tshebang \tline \tin \tthe \tw3af_console, \tw3af_gui \tand \tw3af_api \tscripts.' \n\t \tprint msg \n\t \tsys.exit(1)\n", 
" \tif (session_id is None): \n\t \tsession_id = random.randint(0, 18446744073709551615L) \n\tsock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM) \n\tdata = struct.pack('!BQxxxxx', 56, session_id) \n\tsock.sendto(data, (address, port)) \n\tsock.settimeout(timeout) \n\ttry: \n\t \treceived = sock.recv(2048) \n\texcept socket.timeout: \n\t \treturn False \n\tfinally: \n\t \tsock.close() \n\tfmt = '!BQxxxxxQxxxx' \n\tif (len(received) != struct.calcsize(fmt)): \n\t \tLOG.warning(_LW('Expected \tto \treceive \t%(exp)s \tbytes, \tbut \tactually \t%(act)s'), dict(exp=struct.calcsize(fmt), act=len(received))) \n\t \treturn False \n\t(identifier, server_sess, client_sess) = struct.unpack(fmt, received) \n\treturn ((identifier == 64) and (client_sess == session_id))\n", 
" \tif at_time: \n\t \tcmd = \"echo \t'{0}' \t| \tat \t{1}\".format(cmd, _cmd_quote(at_time)) \n\treturn (not bool(__salt__['cmd.retcode'](cmd, python_shell=True)))\n", 
" \terrback = (errback or _ensure_errback) \n\twith pool.acquire(block=True) as conn: \n\t \tconn.ensure_connection(errback=errback) \n\t \tchannel = conn.default_channel \n\t \trevive = partial(revive_connection, conn, on_revive=on_revive) \n\t \tinsured = conn.autoretry(fun, channel, errback=errback, on_revive=revive, **opts) \n\t \t(retval, _) = insured(*args, **dict(kwargs, connection=conn)) \n\t \treturn retval\n", 
" \tif (not unit): \n\t \tunit = CONF.instance_usage_audit_period \n\toffset = 0 \n\tif ('@' in unit): \n\t \t(unit, offset) = unit.split('@', 1) \n\t \toffset = int(offset) \n\tif (before is not None): \n\t \trightnow = before \n\telse: \n\t \trightnow = timeutils.utcnow() \n\tif (unit not in ('month', 'day', 'year', 'hour')): \n\t \traise ValueError('Time \tperiod \tmust \tbe \thour, \tday, \tmonth \tor \tyear') \n\tif (unit == 'month'): \n\t \tif (offset == 0): \n\t \t \toffset = 1 \n\t \tend = datetime.datetime(day=offset, month=rightnow.month, year=rightnow.year) \n\t \tif (end >= rightnow): \n\t \t \tyear = rightnow.year \n\t \t \tif (1 >= rightnow.month): \n\t \t \t \tyear -= 1 \n\t \t \t \tmonth = (12 + (rightnow.month - 1)) \n\t \t \telse: \n\t \t \t \tmonth = (rightnow.month - 1) \n\t \t \tend = datetime.datetime(day=offset, month=month, year=year) \n\t \tyear = end.year \n\t \tif (1 >= end.month): \n\t \t \tyear -= 1 \n\t \t \tmonth = (12 + (end.month - 1)) \n\t \telse: \n\t \t \tmonth = (end.month - 1) \n\t \tbegin = datetime.datetime(day=offset, month=month, year=year) \n\telif (unit == 'year'): \n\t \tif (offset == 0): \n\t \t \toffset = 1 \n\t \tend = datetime.datetime(day=1, month=offset, year=rightnow.year) \n\t \tif (end >= rightnow): \n\t \t \tend = datetime.datetime(day=1, month=offset, year=(rightnow.year - 1)) \n\t \t \tbegin = datetime.datetime(day=1, month=offset, year=(rightnow.year - 2)) \n\t \telse: \n\t \t \tbegin = datetime.datetime(day=1, month=offset, year=(rightnow.year - 1)) \n\telif (unit == 'day'): \n\t \tend = datetime.datetime(hour=offset, day=rightnow.day, month=rightnow.month, year=rightnow.year) \n\t \tif (end >= rightnow): \n\t \t \tend = (end - datetime.timedelta(days=1)) \n\t \tbegin = (end - datetime.timedelta(days=1)) \n\telif (unit == 'hour'): \n\t \tend = rightnow.replace(minute=offset, second=0, microsecond=0) \n\t \tif (end >= rightnow): \n\t \t \tend = (end - datetime.timedelta(hours=1)) \n\t \tbegin = (end - datetime.timedelta(hours=1)) \n\treturn (begin, end)\n", 
" \tif (length is None): \n\t \tlength = CONF.password_length \n\tr = random.SystemRandom() \n\tpassword = [r.choice(s) for s in symbolgroups] \n\tr.shuffle(password) \n\tpassword = password[:length] \n\tlength -= len(password) \n\tsymbols = ''.join(symbolgroups) \n\tpassword.extend([r.choice(symbols) for _i in range(length)]) \n\tr.shuffle(password) \n\treturn ''.join(password)\n", 
" \treturn _XHTML_ESCAPE_RE.sub((lambda match: _XHTML_ESCAPE_DICT[match.group(0)]), to_basestring(value))\n", 
" \tif ((value is None) or isinstance(value, six.binary_type)): \n\t \treturn value \n\tif (not isinstance(value, six.text_type)): \n\t \tvalue = six.text_type(value) \n\treturn value.encode('utf-8')\n", 
" \tif (not ((type(number) is types.LongType) or (type(number) is types.IntType))): \n\t \traise TypeError('You \tmust \tpass \ta \tlong \tor \tan \tint') \n\tstring = '' \n\twhile (number > 0): \n\t \tstring = ('%s%s' % (byte((number & 255)), string)) \n\t \tnumber /= 256 \n\treturn string\n", 
" \ttry: \n\t \tos.remove(fname) \n\texcept OSError: \n\t \tpass\n", 
" \treturn [api for api in apis if (api['name'] == name)]\n", 
" \titems = [] \n\tfor (k, v) in d.items(): \n\t \tnew_key = (((parent_key + '.') + k) if parent_key else k) \n\t \tif isinstance(v, collections.MutableMapping): \n\t \t \titems.extend(list(flatten_dict(v, new_key).items())) \n\t \telse: \n\t \t \titems.append((new_key, v)) \n\treturn dict(items)\n", 
" \tdata_copy = deepcopy(data) \n\tfield_names = {} \n\tfor (key, value) in data.iteritems(): \n\t \tif (key in DEFAULT_FIELD_NAMES): \n\t \t \tfield_names[key] = data_copy.pop(key) \n\treturn (field_names, data_copy)\n", 
" \treturn dict(((k.upper(), v) for (k, v) in dictionary.items()))\n", 
" \treturn dict([(k, v) for (k, v) in six.iteritems(master_dict) if (k in keys)])\n", 
" \tresult = {k: ['-'] for k in (set(orig.keys()) - set(new.keys()))} \n\tfor (key, value) in new.items(): \n\t \tif ((key not in orig) or (value != orig[key])): \n\t \t \tresult[key] = ['+', value] \n\treturn result\n", 
" \tif isinstance(obj, cls): \n\t \treturn obj \n\traise Exception((_('Expected \tobject \tof \ttype: \t%s') % str(cls)))\n", 
" \ttry: \n\t \tif netaddr.valid_ipv6(server_str): \n\t \t \treturn (server_str, '') \n\t \tif (server_str.find(']:') != (-1)): \n\t \t \t(address, port) = server_str.replace('[', '', 1).split(']:') \n\t \t \treturn (address, port) \n\t \tif (server_str.find(':') == (-1)): \n\t \t \treturn (server_str, '') \n\t \t(address, port) = server_str.split(':') \n\t \treturn (address, port) \n\texcept (ValueError, netaddr.AddrFormatError): \n\t \tLOG.error(_LE('Invalid \tserver_string: \t%s'), server_str) \n\t \treturn ('', '')\n", 
" \treturn STRING_BOOLS[string.strip().lower()]\n", 
" \treturn ((val is True) or (val in ['True', 'true', 'T', 't']))\n", 
" \ttry: \n\t \tparts = urlparse.urlparse(url) \n\t \tscheme = parts[0] \n\t \tnetloc = parts[1] \n\t \tif (scheme and netloc): \n\t \t \treturn True \n\t \telse: \n\t \t \treturn False \n\texcept: \n\t \treturn False\n", 
" \tif (not isinstance(address, (bytes, str_type))): \n\t \treturn False \n\tif (address.count('.') != 3): \n\t \treturn False \n\tfor entry in address.split('.'): \n\t \tif ((not entry.isdigit()) or (int(entry) < 0) or (int(entry) > 255)): \n\t \t \treturn False \n\t \telif ((entry[0] == '0') and (len(entry) > 1)): \n\t \t \treturn False \n\treturn True\n", 
" \tis_windows = (platform.system() == 'Windows') \n\tif (is_windows and (family == socket.AF_INET6)): \n\t \traise ValueError('Checking \tIPv6 \taddresses \tis \tnot \tsupported \ton \tWindows') \n\ttry: \n\t \tif is_windows: \n\t \t \tsocket.inet_aton(address) \n\t \telse: \n\t \t \tsocket.inet_pton(family, address) \n\texcept socket.error: \n\t \treturn False \n\treturn True\n", 
" \tif (netaddr.IPNetwork(network).version == 6): \n\t \treturn 'IPv6' \n\telif (netaddr.IPNetwork(network).version == 4): \n\t \treturn 'IPv4'\n", 
" \tif (not CONF.monkey_patch): \n\t \treturn \n\tif six.PY2: \n\t \tis_method = inspect.ismethod \n\telse: \n\t \tdef is_method(obj): \n\t \t \treturn (inspect.ismethod(obj) or inspect.isfunction(obj)) \n\tfor module_and_decorator in CONF.monkey_patch_modules: \n\t \t(module, decorator_name) = module_and_decorator.split(':') \n\t \tdecorator = importutils.import_class(decorator_name) \n\t \t__import__(module) \n\t \tmodule_data = pyclbr.readmodule_ex(module) \n\t \tfor (key, value) in module_data.items(): \n\t \t \tif isinstance(value, pyclbr.Class): \n\t \t \t \tclz = importutils.import_class(('%s.%s' % (module, key))) \n\t \t \t \tfor (method, func) in inspect.getmembers(clz, is_method): \n\t \t \t \t \tsetattr(clz, method, decorator(('%s.%s.%s' % (module, key, method)), func)) \n\t \t \tif isinstance(value, pyclbr.Function): \n\t \t \t \tfunc = importutils.import_class(('%s.%s' % (module, key))) \n\t \t \t \tsetattr(sys.modules[module], key, decorator(('%s.%s' % (module, key)), func))\n", 
" \tfor item in list_: \n\t \tif (item.get(search_field) == value): \n\t \t \treturn item.get(output_field, value) \n\treturn value\n", 
" \tend = clock() \n\ttotal = (end - START) \n\tprint('Completion \ttime: \t{0} \tseconds.'.format(total))\n", 
" \treturn (_request_ctx_stack.top is not None)\n", 
" \tpath = os.path.join(base, dev) \n\tif partition: \n\t \tpath += str(partition) \n\treturn path\n", 
" \tif hasattr(td, 'total_seconds'): \n\t \treturn td.total_seconds() \n\tms = td.microseconds \n\tsecs = (td.seconds + ((td.days * 24) * 3600)) \n\treturn ((ms + (secs * (10 ** 6))) / (10 ** 6))\n", 
" \tif six.PY3: \n\t \thostname = hostname.encode('latin-1', 'ignore') \n\t \thostname = hostname.decode('latin-1') \n\telif isinstance(hostname, six.text_type): \n\t \thostname = hostname.encode('latin-1', 'ignore') \n\thostname = re.sub('[ \t_]', '-', hostname) \n\thostname = re.sub('[^\\\\w.-]+', '', hostname) \n\thostname = hostname.lower() \n\thostname = hostname.strip('.-') \n\treturn hostname\n", 
" \tglobal _FILE_CACHE \n\tif force_reload: \n\t \tdelete_cached_file(filename) \n\treloaded = False \n\tmtime = os.path.getmtime(filename) \n\tcache_info = _FILE_CACHE.setdefault(filename, {}) \n\tif ((not cache_info) or (mtime > cache_info.get('mtime', 0))): \n\t \tLOG.debug('Reloading \tcached \tfile \t%s', filename) \n\t \twith open(filename) as fap: \n\t \t \tcache_info['data'] = fap.read() \n\t \tcache_info['mtime'] = mtime \n\t \treloaded = True \n\treturn (reloaded, cache_info['data'])\n", 
" \treturn open(*args, **kwargs)\n", 
" \treturn open(fn, 'r').read()\n", 
" \tdef is_dict_like(thing): \n\t \treturn (hasattr(thing, 'has_key') or isinstance(thing, dict)) \n\tdef get(thing, attr, default): \n\t \tif is_dict_like(thing): \n\t \t \treturn thing.get(attr, default) \n\t \telse: \n\t \t \treturn getattr(thing, attr, default) \n\tdef set_value(thing, attr, val): \n\t \tif is_dict_like(thing): \n\t \t \tthing[attr] = val \n\t \telse: \n\t \t \tsetattr(thing, attr, val) \n\tdef delete(thing, attr): \n\t \tif is_dict_like(thing): \n\t \t \tdel thing[attr] \n\t \telse: \n\t \t \tdelattr(thing, attr) \n\tNOT_PRESENT = object() \n\told_values = {} \n\tfor (attr, new_value) in kwargs.items(): \n\t \told_values[attr] = get(obj, attr, NOT_PRESENT) \n\t \tset_value(obj, attr, new_value) \n\ttry: \n\t \t(yield) \n\tfinally: \n\t \tfor (attr, old_value) in old_values.items(): \n\t \t \tif (old_value is NOT_PRESENT): \n\t \t \t \tdelete(obj, attr) \n\t \t \telse: \n\t \t \t \tset_value(obj, attr, old_value)\n", 
" \tmac = [250, 22, 62, random.randint(0, 255), random.randint(0, 255), random.randint(0, 255)] \n\treturn ':'.join(map((lambda x: ('%02x' % x)), mac))\n", 
" \ttry: \n\t \t(out, _err) = execute('cat', file_path, run_as_root=True) \n\t \treturn out \n\texcept processutils.ProcessExecutionError: \n\t \traise exception.FileNotFound(file_path=file_path)\n", 
" \tif (owner_uid is None): \n\t \towner_uid = os.getuid() \n\torig_uid = os.stat(path).st_uid \n\tif (orig_uid != owner_uid): \n\t \texecute('chown', owner_uid, path, run_as_root=True) \n\ttry: \n\t \t(yield) \n\tfinally: \n\t \tif (orig_uid != owner_uid): \n\t \t \texecute('chown', orig_uid, path, run_as_root=True)\n", 
" \tif (not encountered): \n\t \tencountered = [] \n\tfor subclass in clazz.__subclasses__(): \n\t \tif (subclass not in encountered): \n\t \t \tencountered.append(subclass) \n\t \t \tfor subsubclass in walk_class_hierarchy(subclass, encountered): \n\t \t \t \t(yield subsubclass) \n\t \t \t(yield subclass)\n", 
" \tif (fs == 'swap'): \n\t \targs = ['mkswap'] \n\telse: \n\t \targs = ['mkfs', '-t', fs] \n\tif (fs in ('ext3', 'ext4', 'ntfs')): \n\t \targs.extend(['-F']) \n\tif label: \n\t \tif (fs in ('msdos', 'vfat')): \n\t \t \tlabel_opt = '-n' \n\t \telse: \n\t \t \tlabel_opt = '-L' \n\t \targs.extend([label_opt, label]) \n\targs.append(path) \n\texecute(run_as_root=run_as_root, *args)\n", 
" \ttry: \n\t \tfile_like_object.seek((- num), os.SEEK_END) \n\texcept IOError as e: \n\t \tif (e.errno == errno.EINVAL): \n\t \t \tfile_like_object.seek(0, os.SEEK_SET) \n\t \telse: \n\t \t \traise \n\tremaining = file_like_object.tell() \n\treturn (file_like_object.read(), remaining)\n", 
" \tif ((not hasattr(function, '__closure__')) or (not function.__closure__)): \n\t \treturn function \n\tdef _get_wrapped_function(function): \n\t \tif ((not hasattr(function, '__closure__')) or (not function.__closure__)): \n\t \t \treturn None \n\t \tfor closure in function.__closure__: \n\t \t \tfunc = closure.cell_contents \n\t \t \tdeeper_func = _get_wrapped_function(func) \n\t \t \tif deeper_func: \n\t \t \t \treturn deeper_func \n\t \t \telif hasattr(closure.cell_contents, '__call__'): \n\t \t \t \treturn closure.cell_contents \n\treturn _get_wrapped_function(function)\n", 
" \tif (not isinstance(value, six.string_types)): \n\t \tif (name is None): \n\t \t \tmsg = _('The \tinput \tis \tnot \ta \tstring \tor \tunicode') \n\t \telse: \n\t \t \tmsg = (_('%s \tis \tnot \ta \tstring \tor \tunicode') % name) \n\t \traise exception.InvalidInput(message=msg) \n\tif (name is None): \n\t \tname = value \n\tif (len(value) < min_length): \n\t \tmsg = (_('%(name)s \thas \ta \tminimum \tcharacter \trequirement \tof \t%(min_length)s.') % {'name': name, 'min_length': min_length}) \n\t \traise exception.InvalidInput(message=msg) \n\tif (max_length and (len(value) > max_length)): \n\t \tmsg = (_('%(name)s \thas \tmore \tthan \t%(max_length)s \tcharacters.') % {'name': name, 'max_length': max_length}) \n\t \traise exception.InvalidInput(message=msg)\n", 
" \tif context.is_admin: \n\t \treturn True \n\t(rule, target, credentials) = _prepare_check(context, action, target, pluralized) \n\ttry: \n\t \tresult = _ENFORCER.enforce(rule, target, credentials, action=action, do_raise=True) \n\texcept policy.PolicyNotAuthorized: \n\t \twith excutils.save_and_reraise_exception(): \n\t \t \tlog_rule_list(rule) \n\t \t \tLOG.debug(\"Failed \tpolicy \tcheck \tfor \t'%s'\", action) \n\treturn result\n", 
" \tinit() \n\tcredentials = context.to_policy_values() \n\ttarget = credentials \n\treturn _ENFORCER.authorize('context_is_admin', target, credentials)\n", 
" \tca_dir = ca_folder() \n\tif (not os.path.exists(ca_path())): \n\t \tgenrootca_sh_path = os.path.abspath(os.path.join(os.path.dirname(__file__), 'CA', 'genrootca.sh')) \n\t \tfileutils.ensure_tree(ca_dir) \n\t \tutils.execute('sh', genrootca_sh_path, cwd=ca_dir)\n", 
" \tif (not CONF.crypto.use_project_ca): \n\t \tproject_id = None \n\tcrl_file_path = crl_path(project_id) \n\tif (not os.path.exists(crl_file_path)): \n\t \traise exception.CryptoCRLFileNotFound(project=project_id) \n\twith open(crl_file_path, 'r') as crlfile: \n\t \treturn crlfile.read()\n", 
" \tif isinstance(text, six.text_type): \n\t \ttext = text.encode('utf-8') \n\ttry: \n\t \tpub_bytes = ssh_public_key.encode('utf-8') \n\t \tpub_key = serialization.load_ssh_public_key(pub_bytes, backends.default_backend()) \n\t \treturn pub_key.encrypt(text, padding.PKCS1v15()) \n\texcept Exception as exc: \n\t \traise exception.EncryptionFailure(reason=six.text_type(exc))\n", 
" \tif isinstance(text, six.text_type): \n\t \ttext = text.encode('utf-8') \n\ttry: \n\t \tpub_bytes = ssh_public_key.encode('utf-8') \n\t \tpub_key = serialization.load_ssh_public_key(pub_bytes, backends.default_backend()) \n\t \treturn pub_key.encrypt(text, padding.PKCS1v15()) \n\texcept Exception as exc: \n\t \traise exception.EncryptionFailure(reason=six.text_type(exc))\n", 
" \ttry: \n\t \tutils.execute('openssl', 'ca', '-config', './openssl.cnf', '-revoke', file_name, cwd=ca_folder(project_id)) \n\t \tutils.execute('openssl', 'ca', '-gencrl', '-config', './openssl.cnf', '-out', CONF.crypto.crl_file, cwd=ca_folder(project_id)) \n\texcept OSError: \n\t \traise exception.ProjectNotFound(project_id=project_id) \n\texcept processutils.ProcessExecutionError: \n\t \traise exception.RevokeCertFailure(project_id=project_id)\n", 
" \tadmin = context.get_admin_context() \n\tfor cert in db.certificate_get_all_by_user(admin, user_id): \n\t \trevoke_cert(cert['project_id'], cert['file_name'])\n", 
" \tadmin = context.get_admin_context() \n\tfor cert in db.certificate_get_all_by_project(admin, project_id): \n\t \trevoke_cert(cert['project_id'], cert['file_name'])\n", 
" \tadmin = context.get_admin_context() \n\tfor cert in db.certificate_get_all_by_user_and_project(admin, user_id, project_id): \n\t \trevoke_cert(cert['project_id'], cert['file_name'])\n", 
" \treturn (CONF.crypto.user_cert_subject % (project_id, user_id, utils.isotime()))\n", 
" \treturn (CONF.crypto.user_cert_subject % (project_id, user_id, utils.isotime()))\n", 
" \tsubject = _user_cert_subject(user_id, project_id) \n\twith utils.tempdir() as tmpdir: \n\t \tkeyfile = os.path.abspath(os.path.join(tmpdir, 'temp.key')) \n\t \tcsrfile = os.path.abspath(os.path.join(tmpdir, 'temp.csr')) \n\t \tutils.execute('openssl', 'genrsa', '-out', keyfile, str(bits)) \n\t \tutils.execute('openssl', 'req', '-new', '-key', keyfile, '-out', csrfile, '-batch', '-subj', subject) \n\t \twith open(keyfile) as f: \n\t \t \tprivate_key = f.read() \n\t \twith open(csrfile) as f: \n\t \t \tcsr = f.read() \n\t(serial, signed_csr) = sign_csr(csr, project_id) \n\tfname = os.path.join(ca_folder(project_id), ('newcerts/%s.pem' % serial)) \n\tcert = {'user_id': user_id, 'project_id': project_id, 'file_name': fname} \n\tdb.certificate_create(context.get_admin_context(), cert) \n\treturn (private_key, signed_csr)\n", 
" \twith open(localfn, u'rb') as f: \n\t \th = hashlib.md5() \n\t \tblock = f.read(conf.compute_hash_block_size) \n\t \twhile block: \n\t \t \th.update(block) \n\t \t \tblock = f.read(conf.compute_hash_block_size) \n\treturn h.hexdigest()\n", 
" \tdef decorator(fx): \n\t \tfx._event_id = id \n\t \tfx._event_name = event \n\t \treturn fx \n\treturn decorator\n", 
" \troot_device_name = None \n\tfor bdm in properties.get('mappings', []): \n\t \tif (bdm['virtual'] == 'root'): \n\t \t \troot_device_name = bdm['device'] \n\tif ('root_device_name' in properties): \n\t \troot_device_name = properties['root_device_name'] \n\treturn root_device_name\n", 
" \tfor m in mappings: \n\t \tvirtual = m['virtual'] \n\t \tif (is_swap_or_ephemeral(virtual) and (not m['device'].startswith('/'))): \n\t \t \tm['device'] = ('/dev/' + m['device']) \n\treturn mappings\n", 
" \treturn (_dev.sub('', device_name) if device_name else device_name)\n", 
" \tdevice_name = strip_dev(device_name) \n\treturn (_pref.sub('', device_name) if device_name else device_name)\n", 
" \tmatch = re.match('(^/dev/x{0,1}[a-z]{0,1}d{0,1})([a-z]+)[0-9]*$', device) \n\tif (not match): \n\t \treturn None \n\treturn match.groups()\n", 
" \treturn feedback_domain.FeedbackMessage(message_model.id, message_model.thread_id, message_model.message_id, message_model.author_id, message_model.updated_status, message_model.updated_subject, message_model.text, message_model.created_on, message_model.last_updated)\n", 
" \treturn salt.utils.network.interface_ip(iface)\n", 
" \treturn ((numpy.asarray(inarray, 'f') / 127.5) - 1)\n", 
" \tdelta = timedelta(weeks=1) \n\tif ((datetime.utcnow() - x) > delta): \n\t \treturn x.strftime('%b \t%d, \t%Y') \n\telse: \n\t \tdate_array = date.distance_of_time_in_words(x, datetime.utcnow()).replace(',', '').split(' \t') \n\t \treturn ('~%s \t%s \tago' % (date_array[0], date_array[1]))\n", 
" \treturn ((numpy.asarray(inarray, 'f') / 127.5) - 1)\n", 
" \treturn sqlalchemy.Table(table_name, metadata, autoload=True)\n", 
" \treturn IMPL.compute_node_get_all_by_host(context, host)\n", 
" \tsecurity_group_ensure_default(context) \n\tvalues = values.copy() \n\tvalues['metadata'] = _metadata_refs(values.get('metadata'), models.InstanceMetadata) \n\tvalues['system_metadata'] = _metadata_refs(values.get('system_metadata'), models.InstanceSystemMetadata) \n\t_handle_objects_related_type_conversions(values) \n\tinstance_ref = models.Instance() \n\tif (not values.get('uuid')): \n\t \tvalues['uuid'] = uuidutils.generate_uuid() \n\tinstance_ref['info_cache'] = models.InstanceInfoCache() \n\tinfo_cache = values.pop('info_cache', None) \n\tif (info_cache is not None): \n\t \tinstance_ref['info_cache'].update(info_cache) \n\tsecurity_groups = values.pop('security_groups', []) \n\tinstance_ref['extra'] = models.InstanceExtra() \n\tinstance_ref['extra'].update({'numa_topology': None, 'pci_requests': None, 'vcpu_model': None}) \n\tinstance_ref['extra'].update(values.pop('extra', {})) \n\tinstance_ref.update(values) \n\tdef _get_sec_group_models(security_groups): \n\t \tmodels = [] \n\t \tdefault_group = _security_group_ensure_default(context) \n\t \tif ('default' in security_groups): \n\t \t \tmodels.append(default_group) \n\t \t \tsecurity_groups = [x for x in security_groups if (x != 'default')] \n\t \tif security_groups: \n\t \t \tmodels.extend(_security_group_get_by_names(context, security_groups)) \n\t \treturn models \n\tif ('hostname' in values): \n\t \t_validate_unique_server_name(context, values['hostname']) \n\tinstance_ref.security_groups = _get_sec_group_models(security_groups) \n\tcontext.session.add(instance_ref) \n\tec2_instance_create(context, instance_ref['uuid']) \n\treturn instance_ref\n", 
" \treturn salt.utils.network.interface_ip(iface)\n", 
" \treturn ((numpy.asarray(inarray, 'f') / 127.5) - 1)\n", 
" \tglobal _REPOSITORY \n\trel_path = 'migrate_repo' \n\tif (database == 'api'): \n\t \trel_path = os.path.join('api_migrations', 'migrate_repo') \n\tpath = os.path.join(os.path.abspath(os.path.dirname(__file__)), rel_path) \n\tassert os.path.exists(path) \n\tif (_REPOSITORY.get(database) is None): \n\t \t_REPOSITORY[database] = Repository(path) \n\treturn _REPOSITORY[database]\n", 
" \t_run_as_pg(('psql \t-c \t\"DROP \tUSER \t%(name)s;\"' % locals()))\n", 
" \tif (operation is not False): \n\t \ttry: \n\t \t \toldValue = getattr(self, attrib) \n\t \texcept AttributeError: \n\t \t \toldValue = None \n\t \t \tvalue = value \n\t \tif ((value is not None) and (not isinstance(value, basestring)) and (oldValue is not None) and (not isinstance(oldValue, basestring))): \n\t \t \tvalue = numpy.array(value, float) \n\t \t \tif (operation in ('', None)): \n\t \t \t \tif ((value.shape is ()) and (not isinstance(oldValue, attributeSetter))): \n\t \t \t \t \tvalue = ((oldValue * 0) + value) \n\t \t \telif (operation == '+'): \n\t \t \t \tvalue = (oldValue + value) \n\t \t \telif (operation == '*'): \n\t \t \t \tvalue = (oldValue * value) \n\t \t \telif (operation == '-'): \n\t \t \t \tvalue = (oldValue - value) \n\t \t \telif (operation == '/'): \n\t \t \t \tvalue = (oldValue / value) \n\t \t \telif (operation == '**'): \n\t \t \t \tvalue = (oldValue ** value) \n\t \t \telif (operation == '%'): \n\t \t \t \tvalue = (oldValue % value) \n\t \t \telse: \n\t \t \t \tmsg = 'Unsupported \tvalue \t\"%s\" \tfor \toperation \twhen \tsetting \t%s \tin \t%s' \n\t \t \t \tvals = (operation, attrib, self.__class__.__name__) \n\t \t \t \traise ValueError((msg % vals)) \n\t \telif (operation not in ('', None)): \n\t \t \tmsg = 'operation \t%s \tinvalid \tfor \t%s \t(old \tvalue) \tand \t%s \t(operation \tvalue)' \n\t \t \tvals = (operation.__repr__(), oldValue, value) \n\t \t \traise TypeError((msg % vals)) \n\tif stealth: \n\t \tself.__dict__[attrib] = value \n\telse: \n\t \tautoLogOrig = self.autoLog \n\t \tself.__dict__['autoLog'] = (log or (autoLogOrig and (log is None))) \n\t \tsetattr(self, attrib, value) \n\t \tif (attrib != 'autoLog'): \n\t \t \tself.__dict__['autoLog'] = autoLogOrig\n", 
" \treturn sys.modules[__name__]\n", 
" \tdef wrapper(*args, **kwargs): \n\t \tif (not is_admin_context(args[0])): \n\t \t \traise exception.AdminRequired() \n\t \treturn f(*args, **kwargs) \n\treturn wrapper\n", 
" \t@functools.wraps(f) \n\tdef wrapper(*args, **kwargs): \n\t \tnova.context.require_context(args[0]) \n\t \treturn f(*args, **kwargs) \n\treturn wrapper\n", 
" \t@functools.wraps(f) \n\tdef wrapper(context, instance_uuid, *args, **kwargs): \n\t \tinstance_get_by_uuid(context, instance_uuid) \n\t \treturn f(context, instance_uuid, *args, **kwargs) \n\treturn wrapper\n", 
" \t@functools.wraps(f) \n\tdef wrapper(context, aggregate_id, *args, **kwargs): \n\t \taggregate_get(context, aggregate_id) \n\t \treturn f(context, aggregate_id, *args, **kwargs) \n\treturn wrapper\n", 
" \t@functools.wraps(f) \n\tdef wrapped(*args, **kwargs): \n\t \twhile True: \n\t \t \ttry: \n\t \t \t \treturn f(*args, **kwargs) \n\t \t \texcept db_exc.DBDeadlock: \n\t \t \t \tLOG.warning(_LW(\"Deadlock \tdetected \twhen \trunning \t'%(func_name)s': \tRetrying...\"), dict(func_name=f.__name__)) \n\t \t \t \ttime.sleep(0.5) \n\t \t \t \tcontinue \n\tfunctools.update_wrapper(wrapped, f) \n\treturn wrapped\n", 
" \tif (read_deleted is None): \n\t \tread_deleted = context.read_deleted \n\tquery_kwargs = {} \n\tif ('no' == read_deleted): \n\t \tquery_kwargs['deleted'] = False \n\telif ('only' == read_deleted): \n\t \tquery_kwargs['deleted'] = True \n\telif ('yes' == read_deleted): \n\t \tpass \n\telse: \n\t \traise ValueError((_(\"Unrecognized \tread_deleted \tvalue \t'%s'\") % read_deleted)) \n\tquery = sqlalchemyutils.model_query(model, context.session, args, **query_kwargs) \n\tif (nova.context.is_user_context(context) and project_only): \n\t \tif (project_only == 'allow_none'): \n\t \t \tquery = query.filter(or_((model.project_id == context.project_id), (model.project_id == null()))) \n\t \telse: \n\t \t \tquery = query.filter_by(project_id=context.project_id) \n\treturn query\n", 
" \tfilter_dict = {} \n\tif (filters is None): \n\t \tfilters = {} \n\tfor (key, value) in six.iteritems(filters): \n\t \tif isinstance(value, (list, tuple, set, frozenset)): \n\t \t \tcolumn_attr = getattr(model, key) \n\t \t \tquery = query.filter(column_attr.in_(value)) \n\t \telse: \n\t \t \tfilter_dict[key] = value \n\tif filter_dict: \n\t \tquery = query.filter_by(**filter_dict) \n\treturn query\n", 
" \treturn StringSet(info, name, case_flags=make_case_flags(info))\n", 
" \tconvert_objects_related_datetimes(values) \n\tcompute_node_ref = models.ComputeNode() \n\tcompute_node_ref.update(values) \n\tcompute_node_ref.save(context.session) \n\treturn compute_node_ref\n", 
" \tcompute_ref = compute_node_get_model(context, compute_id) \n\tvalues['updated_at'] = timeutils.utcnow() \n\tconvert_objects_related_datetimes(values) \n\tcompute_ref.update(values) \n\treturn compute_ref\n", 
" \tresult = model_query(context, models.ComputeNode).filter_by(id=compute_id).soft_delete(synchronize_session=False) \n\tif (not result): \n\t \traise exception.ComputeHostNotFound(host=compute_id)\n", 
" \tengine = get_engine(context=context) \n\tservices_tbl = models.Service.__table__ \n\tinner_sel = sa.alias(_compute_node_select(context), name='inner_sel') \n\tj = sa.join(inner_sel, services_tbl, sql.and_(sql.or_((inner_sel.c.host == services_tbl.c.host), (inner_sel.c.service_id == services_tbl.c.id)), (services_tbl.c.disabled == false()), (services_tbl.c.binary == 'nova-compute'))) \n\tagg_cols = [func.count().label('count'), sql.func.sum(inner_sel.c.vcpus).label('vcpus'), sql.func.sum(inner_sel.c.memory_mb).label('memory_mb'), sql.func.sum(inner_sel.c.local_gb).label('local_gb'), sql.func.sum(inner_sel.c.vcpus_used).label('vcpus_used'), sql.func.sum(inner_sel.c.memory_mb_used).label('memory_mb_used'), sql.func.sum(inner_sel.c.local_gb_used).label('local_gb_used'), sql.func.sum(inner_sel.c.free_ram_mb).label('free_ram_mb'), sql.func.sum(inner_sel.c.free_disk_gb).label('free_disk_gb'), sql.func.sum(inner_sel.c.current_workload).label('current_workload'), sql.func.sum(inner_sel.c.running_vms).label('running_vms'), sql.func.sum(inner_sel.c.disk_available_least).label('disk_available_least')] \n\tselect = sql.select(agg_cols).select_from(j) \n\tconn = engine.connect() \n\tresults = conn.execute(select).fetchone() \n\tfields = ('count', 'vcpus', 'memory_mb', 'local_gb', 'vcpus_used', 'memory_mb_used', 'local_gb_used', 'free_ram_mb', 'free_disk_gb', 'current_workload', 'running_vms', 'disk_available_least') \n\tresults = {field: int((results[idx] or 0)) for (idx, field) in enumerate(fields)} \n\tconn.close() \n\treturn results\n", 
" \tout = [] \n\tcount = 0 \n\tfor ip in ips: \n\t \tout.append(ip['address']) \n\t \tcount += 1 \n\t \tif (count > (block_size - 1)): \n\t \t \t(yield out) \n\t \t \tout = [] \n\t \t \tcount = 0 \n\tif out: \n\t \t(yield out)\n", 
" \tadapter = get_adapter(request) \n\tif (not user.is_active): \n\t \treturn adapter.respond_user_inactive(request, user) \n\tfrom .models import EmailAddress \n\thas_verified_email = EmailAddress.objects.filter(user=user, verified=True).exists() \n\tif (email_verification == EmailVerificationMethod.NONE): \n\t \tpass \n\telif (email_verification == EmailVerificationMethod.OPTIONAL): \n\t \tif ((not has_verified_email) and signup): \n\t \t \tsend_email_confirmation(request, user, signup=signup) \n\telif (email_verification == EmailVerificationMethod.MANDATORY): \n\t \tif (not has_verified_email): \n\t \t \tsend_email_confirmation(request, user, signup=signup) \n\t \t \treturn adapter.respond_email_verification_sent(request, user) \n\ttry: \n\t \tadapter.login(request, user) \n\t \tresponse = HttpResponseRedirect(get_login_redirect_url(request, redirect_url)) \n\t \tif (signal_kwargs is None): \n\t \t \tsignal_kwargs = {} \n\t \tsignals.user_logged_in.send(sender=user.__class__, request=request, response=response, user=user, **signal_kwargs) \n\t \tadapter.add_message(request, messages.SUCCESS, 'account/messages/logged_in.txt', {'user': user}) \n\texcept ImmediateHttpResponse as e: \n\t \tresponse = e.response \n\treturn response\n", 
" \ttenant_id = request.user.tenant_id \n\tnetworks = [] \n\tif api.base.is_service_enabled(request, 'network'): \n\t \ttry: \n\t \t \tnetworks = api.neutron.network_list_for_tenant(request, tenant_id) \n\t \t \tnetworks = [(n.id, n.name_or_id) for n in networks if n['subnets']] \n\t \t \tnetworks.sort(key=(lambda obj: obj[1])) \n\t \texcept Exception as e: \n\t \t \tmsg = _('Failed \tto \tget \tnetwork \tlist \t{0}').format(six.text_type(e)) \n\t \t \texceptions.handle(request, msg) \n\tif (not networks): \n\t \tif include_empty_option: \n\t \t \treturn [('', _('No \tnetworks \tavailable'))] \n\t \treturn [] \n\tif include_empty_option: \n\t \treturn ([('', _('Select \tNetwork'))] + networks) \n\treturn networks\n", 
" \ttry: \n\t \tvif_ref = models.VirtualInterface() \n\t \tvif_ref.update(values) \n\t \tvif_ref.save(context.session) \n\texcept db_exc.DBError: \n\t \tLOG.exception(_LE('VIF \tcreation \tfailed \twith \ta \tdatabase \terror.')) \n\t \traise exception.VirtualInterfaceCreateException() \n\treturn vif_ref\n", 
" \treturn IMPL.virtual_interface_get(context, vif_id)\n", 
" \treturn IMPL.virtual_interface_get(context, vif_id)\n", 
" \treturn IMPL.virtual_interface_get(context, vif_id)\n", 
" \treturn IMPL.virtual_interface_get_by_instance_and_network(context, instance_id, network_id)\n", 
" \tvif_ref = _virtual_interface_query(context).filter_by(instance_uuid=instance_uuid).filter_by(network_id=network_id).first() \n\treturn vif_ref\n", 
" \t_virtual_interface_query(context).filter_by(instance_uuid=instance_uuid).soft_delete()\n", 
" \tvif_refs = _virtual_interface_query(context).all() \n\treturn vif_refs\n", 
" \tsecurity_group_ensure_default(context) \n\tvalues = values.copy() \n\tvalues['metadata'] = _metadata_refs(values.get('metadata'), models.InstanceMetadata) \n\tvalues['system_metadata'] = _metadata_refs(values.get('system_metadata'), models.InstanceSystemMetadata) \n\t_handle_objects_related_type_conversions(values) \n\tinstance_ref = models.Instance() \n\tif (not values.get('uuid')): \n\t \tvalues['uuid'] = uuidutils.generate_uuid() \n\tinstance_ref['info_cache'] = models.InstanceInfoCache() \n\tinfo_cache = values.pop('info_cache', None) \n\tif (info_cache is not None): \n\t \tinstance_ref['info_cache'].update(info_cache) \n\tsecurity_groups = values.pop('security_groups', []) \n\tinstance_ref['extra'] = models.InstanceExtra() \n\tinstance_ref['extra'].update({'numa_topology': None, 'pci_requests': None, 'vcpu_model': None}) \n\tinstance_ref['extra'].update(values.pop('extra', {})) \n\tinstance_ref.update(values) \n\tdef _get_sec_group_models(security_groups): \n\t \tmodels = [] \n\t \tdefault_group = _security_group_ensure_default(context) \n\t \tif ('default' in security_groups): \n\t \t \tmodels.append(default_group) \n\t \t \tsecurity_groups = [x for x in security_groups if (x != 'default')] \n\t \tif security_groups: \n\t \t \tmodels.extend(_security_group_get_by_names(context, security_groups)) \n\t \treturn models \n\tif ('hostname' in values): \n\t \t_validate_unique_server_name(context, values['hostname']) \n\tinstance_ref.security_groups = _get_sec_group_models(security_groups) \n\tcontext.session.add(instance_ref) \n\tec2_instance_create(context, instance_ref['uuid']) \n\treturn instance_ref\n", 
" \tuuids = [inst['uuid'] for inst in instances] \n\tif (manual_joins is None): \n\t \tmanual_joins = ['metadata', 'system_metadata'] \n\tmeta = collections.defaultdict(list) \n\tif ('metadata' in manual_joins): \n\t \tfor row in _instance_metadata_get_multi(context, uuids): \n\t \t \tmeta[row['instance_uuid']].append(row) \n\tsys_meta = collections.defaultdict(list) \n\tif ('system_metadata' in manual_joins): \n\t \tfor row in _instance_system_metadata_get_multi(context, uuids): \n\t \t \tsys_meta[row['instance_uuid']].append(row) \n\tpcidevs = collections.defaultdict(list) \n\tif ('pci_devices' in manual_joins): \n\t \tfor row in _instance_pcidevs_get_multi(context, uuids): \n\t \t \tpcidevs[row['instance_uuid']].append(row) \n\tfilled_instances = [] \n\tfor inst in instances: \n\t \tinst = dict(inst) \n\t \tinst['system_metadata'] = sys_meta[inst['uuid']] \n\t \tinst['metadata'] = meta[inst['uuid']] \n\t \tif ('pci_devices' in manual_joins): \n\t \t \tinst['pci_devices'] = pcidevs[inst['uuid']] \n\t \tfilled_instances.append(inst) \n\treturn filled_instances\n", 
" \treturn IMPL.instance_get_all_by_filters(context, filters, sort_key, sort_dir, limit=limit, marker=marker, columns_to_join=columns_to_join)\n", 
" \tmodel = models.Instance \n\t(safe_regex_filter, db_regexp_op) = _get_regexp_ops(CONF.database.connection) \n\tfor filter_name in filters: \n\t \ttry: \n\t \t \tcolumn_attr = getattr(model, filter_name) \n\t \texcept AttributeError: \n\t \t \tcontinue \n\t \tif ('property' == type(column_attr).__name__): \n\t \t \tcontinue \n\t \tfilter_val = filters[filter_name] \n\t \tif (not isinstance(filter_val, six.string_types)): \n\t \t \tfilter_val = str(filter_val) \n\t \tif (db_regexp_op == 'LIKE'): \n\t \t \tquery = query.filter(column_attr.op(db_regexp_op)(((u'%' + filter_val) + u'%'))) \n\t \telse: \n\t \t \tfilter_val = safe_regex_filter(filter_val) \n\t \t \tquery = query.filter(column_attr.op(db_regexp_op)(filter_val)) \n\treturn query\n", 
" \tquery = context.session.query(models.Instance) \n\tif (columns_to_join is None): \n\t \tcolumns_to_join_new = ['info_cache', 'security_groups'] \n\t \tmanual_joins = ['metadata', 'system_metadata'] \n\telse: \n\t \t(manual_joins, columns_to_join_new) = _manual_join_columns(columns_to_join) \n\tfor column in columns_to_join_new: \n\t \tif ('extra.' in column): \n\t \t \tquery = query.options(undefer(column)) \n\t \telse: \n\t \t \tquery = query.options(joinedload(column)) \n\tquery = query.filter(or_((models.Instance.terminated_at == null()), (models.Instance.terminated_at > begin))) \n\tif end: \n\t \tquery = query.filter((models.Instance.launched_at < end)) \n\tif project_id: \n\t \tquery = query.filter_by(project_id=project_id) \n\tif host: \n\t \tquery = query.filter_by(host=host) \n\tif (marker is not None): \n\t \ttry: \n\t \t \tmarker = _instance_get_by_uuid(context.elevated(read_deleted='yes'), marker) \n\t \texcept exception.InstanceNotFound: \n\t \t \traise exception.MarkerNotFound(marker=marker) \n\tquery = sqlalchemyutils.paginate_query(query, models.Instance, limit, ['project_id', 'uuid'], marker=marker) \n\treturn _instances_fill_metadata(context, query.all(), manual_joins)\n", 
" \tuuids = [] \n\tfor tuple in model_query(context, models.Instance, (models.Instance.uuid,), read_deleted='no').filter_by(host=host).all(): \n\t \tuuids.append(tuple[0]) \n\treturn uuids\n", 
" \trv = IMPL.instance_update_and_get_original(context, instance_uuid, values, columns_to_join=columns_to_join, expected=expected) \n\treturn rv\n", 
" \treturn IMPL.instance_add_security_group(context, instance_id, security_group_id)\n", 
" \treturn IMPL.instance_remove_security_group(context, instance_id, security_group_id)\n", 
" \treturn IMPL.instance_info_cache_get(context, instance_uuid)\n", 
" \treturn IMPL.instance_info_cache_update(context, instance_uuid, values)\n", 
" \treturn IMPL.instance_info_cache_delete(context, instance_uuid)\n", 
" \tdef network_query(project_filter, id=None): \n\t \tfilter_kwargs = {'project_id': project_filter} \n\t \tif (id is not None): \n\t \t \tfilter_kwargs['id'] = id \n\t \treturn model_query(context, models.Network, read_deleted='no').filter_by(**filter_kwargs).with_lockmode('update').first() \n\tif (not force): \n\t \tnetwork_ref = network_query(project_id) \n\tif (force or (not network_ref)): \n\t \tnetwork_ref = network_query(None, network_id) \n\t \tif (not network_ref): \n\t \t \traise exception.NoMoreNetworks() \n\t \tnetwork_ref['project_id'] = project_id \n\t \tcontext.session.add(network_ref) \n\treturn network_ref\n", 
" \treturn model_query(context, models.Reservation, read_deleted='no').filter(models.Reservation.uuid.in_(reservations)).with_lockmode('update')\n", 
" \tec2_volume_ref = models.VolumeIdMapping() \n\tec2_volume_ref.update({'uuid': volume_uuid}) \n\tif (id is not None): \n\t \tec2_volume_ref.update({'id': id}) \n\tec2_volume_ref.save(context.session) \n\treturn ec2_volume_ref\n", 
" \tec2_snapshot_ref = models.SnapshotIdMapping() \n\tec2_snapshot_ref.update({'uuid': snapshot_uuid}) \n\tif (id is not None): \n\t \tec2_snapshot_ref.update({'id': id}) \n\tec2_snapshot_ref.save(context.session) \n\treturn ec2_snapshot_ref\n", 
" \tquery = _security_group_get_query(context, read_deleted='no', join_rules=False).filter_by(project_id=context.project_id).filter(models.SecurityGroup.name.in_(group_names)) \n\tsg_models = query.all() \n\tif (len(sg_models) == len(group_names)): \n\t \treturn sg_models \n\tgroup_names_from_models = [x.name for x in sg_models] \n\tfor group_name in group_names: \n\t \tif (group_name not in group_names_from_models): \n\t \t \traise exception.SecurityGroupNotFoundForProject(project_id=context.project_id, security_group_id=group_name)\n", 
" \treturn IMPL.security_group_ensure_default(context)\n", 
" \tspecs = values.get('extra_specs') \n\tspecs_refs = [] \n\tif specs: \n\t \tfor (k, v) in specs.items(): \n\t \t \tspecs_ref = models.InstanceTypeExtraSpecs() \n\t \t \tspecs_ref['key'] = k \n\t \t \tspecs_ref['value'] = v \n\t \t \tspecs_refs.append(specs_ref) \n\tvalues['extra_specs'] = specs_refs \n\tinstance_type_ref = models.InstanceTypes() \n\tinstance_type_ref.update(values) \n\tif (projects is None): \n\t \tprojects = [] \n\ttry: \n\t \tinstance_type_ref.save(context.session) \n\texcept db_exc.DBDuplicateEntry as e: \n\t \tif ('flavorid' in e.columns): \n\t \t \traise exception.FlavorIdExists(flavor_id=values['flavorid']) \n\t \traise exception.FlavorExists(name=values['name']) \n\texcept Exception as e: \n\t \traise db_exc.DBError(e) \n\tfor project in set(projects): \n\t \taccess_ref = models.InstanceTypeProjects() \n\t \taccess_ref.update({'instance_type_id': instance_type_ref.id, 'project_id': project}) \n\t \taccess_ref.save(context.session) \n\treturn _dict_with_extra_specs(instance_type_ref)\n", 
" \tinst_type_dict = dict(inst_type_query) \n\textra_specs = {x['key']: x['value'] for x in inst_type_query['extra_specs']} \n\tinst_type_dict['extra_specs'] = extra_specs \n\treturn inst_type_dict\n", 
" \treturn IMPL.flavor_get_by_name(context, name)\n", 
" \tresult = _flavor_get_query(context).filter_by(name=name).first() \n\tif (not result): \n\t \traise exception.FlavorNotFoundByName(flavor_name=name) \n\treturn _dict_with_extra_specs(result)\n", 
" \tresult = _flavor_get_query(context).filter_by(name=name).first() \n\tif (not result): \n\t \traise exception.FlavorNotFoundByName(flavor_name=name) \n\treturn _dict_with_extra_specs(result)\n", 
" \tresult = _flavor_get_query(context, read_deleted=read_deleted).filter_by(flavorid=flavor_id).order_by(asc(models.InstanceTypes.deleted), asc(models.InstanceTypes.id)).first() \n\tif (not result): \n\t \traise exception.FlavorNotFound(flavor_id=flavor_id) \n\treturn _dict_with_extra_specs(result)\n", 
" \tref = model_query(context, models.InstanceTypes, read_deleted='no').filter_by(flavorid=flavor_id).first() \n\tif (not ref): \n\t \traise exception.FlavorNotFound(flavor_id=flavor_id) \n\tref.soft_delete(context.session) \n\tmodel_query(context, models.InstanceTypeExtraSpecs, read_deleted='no').filter_by(instance_type_id=ref['id']).soft_delete() \n\tmodel_query(context, models.InstanceTypeProjects, read_deleted='no').filter_by(instance_type_id=ref['id']).soft_delete()\n", 
" \tinstance_type_id_subq = _flavor_get_id_from_flavor_query(context, flavor_id) \n\taccess_refs = _flavor_access_query(context).filter_by(instance_type_id=instance_type_id_subq).all() \n\treturn access_refs\n", 
" \tinstance_type_id = _flavor_get_id_from_flavor(context, flavor_id) \n\taccess_ref = models.InstanceTypeProjects() \n\taccess_ref.update({'instance_type_id': instance_type_id, 'project_id': project_id}) \n\ttry: \n\t \taccess_ref.save(context.session) \n\texcept db_exc.DBDuplicateEntry: \n\t \traise exception.FlavorAccessExists(flavor_id=flavor_id, project_id=project_id) \n\treturn access_ref\n", 
" \tinstance_type_id = _flavor_get_id_from_flavor(context, flavor_id) \n\tcount = _flavor_access_query(context).filter_by(instance_type_id=instance_type_id).filter_by(project_id=project_id).soft_delete(synchronize_session=False) \n\tif (count == 0): \n\t \traise exception.FlavorAccessNotFound(flavor_id=flavor_id, project_id=project_id)\n", 
" \treturn IMPL.vol_get_usage_by_time(context, begin)\n", 
" \treturn IMPL.s3_image_get(context, image_id)\n", 
" \treturn IMPL.s3_image_get_by_uuid(context, image_uuid)\n", 
" \treturn IMPL.s3_image_create(context, image_uuid)\n", 
" \tfault_ref = models.InstanceFault() \n\tfault_ref.update(values) \n\tfault_ref.save(context.session) \n\treturn dict(fault_ref)\n", 
" \treturn IMPL.instance_fault_get_by_instance_uuids(context, instance_uuids, latest=latest)\n", 
" \tactions = model_query(context, models.InstanceAction).filter_by(instance_uuid=instance_uuid).order_by(desc('created_at'), desc('id')).all() \n\treturn actions\n", 
" \treturn IMPL.action_get_by_request_id(context, uuid, request_id)\n", 
" \treturn IMPL.action_event_start(context, values)\n", 
" \treturn IMPL.action_event_finish(context, values)\n", 
" \tec2_instance_ref = models.InstanceIdMapping() \n\tec2_instance_ref.update({'uuid': instance_uuid}) \n\tif (id is not None): \n\t \tec2_instance_ref.update({'id': id}) \n\tec2_instance_ref.save(context.session) \n\treturn ec2_instance_ref\n", 
" \tengine = get_engine() \n\tconn = engine.connect() \n\tmetadata = MetaData() \n\tmetadata.bind = engine \n\ttable = models.BASE.metadata.tables[tablename] \n\tshadow_tablename = (_SHADOW_TABLE_PREFIX + tablename) \n\trows_archived = 0 \n\ttry: \n\t \tshadow_table = Table(shadow_tablename, metadata, autoload=True) \n\texcept NoSuchTableError: \n\t \treturn rows_archived \n\tif (tablename == 'dns_domains'): \n\t \tcolumn = table.c.domain \n\telse: \n\t \tcolumn = table.c.id \n\tdeleted_column = table.c.deleted \n\tcolumns = [c.name for c in table.c] \n\tif (tablename in ('instance_actions', 'migrations')): \n\t \tinstances = models.BASE.metadata.tables['instances'] \n\t \tdeleted_instances = sql.select([instances.c.uuid]).where((instances.c.deleted != instances.c.deleted.default.arg)) \n\t \tupdate_statement = table.update().values(deleted=table.c.id).where(table.c.instance_uuid.in_(deleted_instances)) \n\t \tconn.execute(update_statement) \n\telif (tablename == 'instance_actions_events'): \n\t \tinstances = models.BASE.metadata.tables['instances'] \n\t \tinstance_actions = models.BASE.metadata.tables['instance_actions'] \n\t \tdeleted_instances = sql.select([instances.c.uuid]).where((instances.c.deleted != instances.c.deleted.default.arg)) \n\t \tdeleted_actions = sql.select([instance_actions.c.id]).where(instance_actions.c.instance_uuid.in_(deleted_instances)) \n\t \tupdate_statement = table.update().values(deleted=table.c.id).where(table.c.action_id.in_(deleted_actions)) \n\t \tconn.execute(update_statement) \n\tinsert = shadow_table.insert(inline=True).from_select(columns, sql.select([table], (deleted_column != deleted_column.default.arg)).order_by(column).limit(max_rows)) \n\tquery_delete = sql.select([column], (deleted_column != deleted_column.default.arg)).order_by(column).limit(max_rows) \n\tdelete_statement = DeleteFromSelect(table, query_delete, column) \n\ttry: \n\t \twith conn.begin(): \n\t \t \tconn.execute(insert) \n\t \t \tresult_delete = conn.execute(delete_statement) \n\t \trows_archived = result_delete.rowcount \n\texcept db_exc.DBReferenceError as ex: \n\t \tLOG.warning(_LW('IntegrityError \tdetected \twhen \tarchiving \ttable \t%(tablename)s: \t%(error)s'), {'tablename': tablename, 'error': six.text_type(ex)}) \n\tif (((max_rows is None) or (rows_archived < max_rows)) and ('instance_uuid' in columns)): \n\t \tinstances = models.BASE.metadata.tables['instances'] \n\t \tlimit = ((max_rows - rows_archived) if (max_rows is not None) else None) \n\t \textra = _archive_if_instance_deleted(table, shadow_table, instances, conn, limit) \n\t \trows_archived += extra \n\treturn rows_archived\n", 
" \ttable_to_rows_archived = {} \n\ttotal_rows_archived = 0 \n\tmeta = MetaData(get_engine(use_slave=True)) \n\tmeta.reflect() \n\tfor table in reversed(meta.sorted_tables): \n\t \ttablename = table.name \n\t \tif ((tablename == 'migrate_version') or tablename.startswith(_SHADOW_TABLE_PREFIX)): \n\t \t \tcontinue \n\t \trows_archived = _archive_deleted_rows_for_table(tablename, max_rows=(max_rows - total_rows_archived)) \n\t \ttotal_rows_archived += rows_archived \n\t \tif rows_archived: \n\t \t \ttable_to_rows_archived[tablename] = rows_archived \n\t \tif (total_rows_archived >= max_rows): \n\t \t \tbreak \n\treturn table_to_rows_archived\n", 
" \treturn IMPL.db_sync(version=version, database=database, context=context)\n", 
" \treturn IMPL.db_version(database=database, context=context)\n", 
" \treturn IMPL.constraint(**conditions)\n", 
" \treturn IMPL.equal_any(*values)\n", 
" \treturn IMPL.not_equal(*values)\n", 
" \treturn IMPL.service_destroy(context, service_id)\n", 
" \treturn IMPL.service_get(context, service_id)\n", 
" \treturn IMPL.service_get_by_host_and_topic(context, host, topic)\n", 
" \treturn IMPL.service_get_all(context, disabled)\n", 
" \treturn IMPL.service_get_all_by_topic(context, topic)\n", 
" \treturn IMPL.service_get_all_by_host(context, host)\n", 
" \treturn IMPL.service_get_by_compute_host(context, host)\n", 
" \tservice_instance = salt.utils.vmware.get_service_instance(host=host, username=username, password=password, protocol=protocol, port=port) \n\tvalid_services = ['DCUI', 'TSM', 'SSH', 'ssh', 'lbtd', 'lsassd', 'lwiod', 'netlogond', 'ntpd', 'sfcbd-watchdog', 'snmpd', 'vprobed', 'vpxa', 'xorg'] \n\thost_names = _check_hosts(service_instance, host, host_names) \n\tret = {} \n\tfor host_name in host_names: \n\t \tif (service_name not in valid_services): \n\t \t \tret.update({host_name: {'Error': '{0} \tis \tnot \ta \tvalid \tservice \tname.'.format(service_name)}}) \n\t \t \treturn ret \n\t \thost_ref = _get_host_ref(service_instance, host, host_name=host_name) \n\t \tservices = host_ref.configManager.serviceSystem.serviceInfo.service \n\t \tif ((service_name == 'SSH') or (service_name == 'ssh')): \n\t \t \ttemp_service_name = 'TSM-SSH' \n\t \telse: \n\t \t \ttemp_service_name = service_name \n\t \tfor service in services: \n\t \t \tif (service.key == temp_service_name): \n\t \t \t \tret.update({host_name: {service_name: service.running}}) \n\t \t \t \tbreak \n\t \t \telse: \n\t \t \t \tmsg = \"Could \tnot \tfind \tservice \t'{0}' \tfor \thost \t'{1}'.\".format(service_name, host_name) \n\t \t \t \tret.update({host_name: {'Error': msg}}) \n\t \tif (ret.get(host_name) is None): \n\t \t \tmsg = \"'vsphere.get_service_running' \tfailed \tfor \thost \t{0}.\".format(host_name) \n\t \t \tlog.debug(msg) \n\t \t \tret.update({host_name: {'Error': msg}}) \n\treturn ret\n", 
" \treturn IMPL.service_create(context, values)\n", 
" \treturn IMPL.service_update(context, service_id, values)\n", 
" \treturn IMPL.compute_node_get_all(context)\n", 
" \treturn IMPL.compute_node_get_all(context)\n", 
" \treturn IMPL.compute_node_search_by_hypervisor(context, hypervisor_match)\n", 
" \tresult = {} \n\tif (compute_node.vcpus > 0): \n\t \tresult[VCPU] = {'total': compute_node.vcpus, 'reserved': 0, 'min_unit': 1, 'max_unit': compute_node.vcpus, 'step_size': 1, 'allocation_ratio': compute_node.cpu_allocation_ratio} \n\tif (compute_node.memory_mb > 0): \n\t \tresult[MEMORY_MB] = {'total': compute_node.memory_mb, 'reserved': CONF.reserved_host_memory_mb, 'min_unit': 1, 'max_unit': compute_node.memory_mb, 'step_size': 1, 'allocation_ratio': compute_node.ram_allocation_ratio} \n\tif (compute_node.local_gb > 0): \n\t \tresult[DISK_GB] = {'total': compute_node.local_gb, 'reserved': (CONF.reserved_host_disk_mb * 1024), 'min_unit': 1, 'max_unit': compute_node.local_gb, 'step_size': 1, 'allocation_ratio': compute_node.disk_allocation_ratio} \n\treturn result\n", 
" \treturn IMPL.consistencygroup_update(context, consistencygroup_id, values)\n", 
" \tresult = model_query(context, models.ComputeNode).filter_by(id=compute_id).soft_delete(synchronize_session=False) \n\tif (not result): \n\t \traise exception.ComputeHostNotFound(host=compute_id)\n", 
" \treturn IMPL.certificate_create(context, values)\n", 
" \treturn IMPL.certificate_get_all_by_project(context, project_id)\n", 
" \treturn IMPL.certificate_get_all_by_user(context, user_id)\n", 
" \treturn IMPL.certificate_get_all_by_user_and_project(context, user_id, project_id)\n", 
" \treturn IMPL.floating_ip_get_pools(context)\n", 
" \treturn IMPL.floating_ip_allocate_address(context, project_id, pool, auto_assigned)\n", 
" \treturn IMPL.floating_ip_bulk_create(context, ips, want_result=want_result)\n", 
" \treturn IMPL.floating_ip_bulk_destroy(context, ips)\n", 
" \treturn IMPL.floating_ip_create(context, values)\n", 
" \treturn IMPL.floating_ip_get_all_by_project(context, project_id)\n", 
" \treturn IMPL.floating_ip_deallocate(context, address)\n", 
" \treturn IMPL.floating_ip_destroy(context, address)\n", 
" \treturn IMPL.floating_ip_disassociate(context, address)\n", 
" \treturn IMPL.floating_ip_fixed_ip_associate(context, floating_address, fixed_address, host)\n", 
" \treturn IMPL.floating_ip_get_all(context)\n", 
" \treturn IMPL.floating_ip_get_all_by_host(context, host)\n", 
" \treturn IMPL.floating_ip_get_all_by_project(context, project_id)\n", 
" \treturn IMPL.floating_ip_get_by_address(context, address)\n", 
" \treturn IMPL.floating_ip_get_by_fixed_ip_id(context, fixed_ip_id)\n", 
" \treturn IMPL.floating_ip_get_by_fixed_ip_id(context, fixed_ip_id)\n", 
" \treturn IMPL.floating_ip_update(context, address, values)\n", 
" \tif (call != 'function'): \n\t \traise SaltCloudSystemExit('The \tfloating_ip_list \taction \tmust \tbe \tcalled \twith \t-f \tor \t--function') \n\tconn = get_conn() \n\treturn conn.floating_ip_list()\n", 
" \tcursor.execute('SHOW \tTABLES') \n\treturn [row[0] for row in cursor.fetchall()]\n", 
" \treturn IMPL.dnsdomain_register_for_zone(context, fqdomain, zone)\n", 
" \treturn IMPL.dnsdomain_register_for_project(context, fqdomain, project)\n", 
" \treturn IMPL.dnsdomain_unregister(context, fqdomain)\n", 
" \treturn IMPL.dnsdomain_get(context, fqdomain)\n", 
" \treturn IMPL.migration_update(context, id, values)\n", 
" \treturn IMPL.migration_create(context, values)\n", 
" \treturn IMPL.migration_get(context, migration_id)\n", 
" \treturn IMPL.migration_get_by_instance_and_status(context, instance_uuid, status)\n", 
" \treturn IMPL.migration_get_unconfirmed_by_dest_compute(context, confirm_window, dest_compute)\n", 
" \treturn IMPL.migration_get_in_progress_by_host_and_node(context, host, node)\n", 
" \treturn IMPL.fixed_ip_associate(context, address, instance_uuid, network_id, reserved, virtual_interface_id)\n", 
" \treturn IMPL.fixed_ip_associate_pool(context, network_id, instance_uuid, host, virtual_interface_id)\n", 
" \treturn IMPL.fixed_ip_update(context, address, values)\n", 
" \treturn IMPL.fixed_ip_bulk_create(context, ips)\n", 
" \treturn IMPL.fixed_ip_disassociate(context, address)\n", 
" \treturn IMPL.fixed_ip_disassociate_all_by_timeout(context, host, time)\n", 
" \treturn IMPL.fixed_ip_get(context, id, get_network)\n", 
" \treturn IMPL.fixed_ip_get_all(context)\n", 
" \treturn IMPL.fixed_ip_get_by_address(context, address, columns_to_join=columns_to_join)\n", 
" \treturn IMPL.fixed_ip_get_by_address(context, address, columns_to_join=columns_to_join)\n", 
" \treturn IMPL.floating_ip_get_by_fixed_ip_id(context, fixed_ip_id)\n", 
" \treturn IMPL.fixed_ip_get_by_instance(context, instance_uuid)\n", 
" \treturn IMPL.fixed_ip_get_by_host(context, host)\n", 
" \treturn IMPL.fixed_ip_get_by_network_host(context, network_uuid, host)\n", 
" \treturn IMPL.fixed_ips_by_virtual_interface(context, vif_id)\n", 
" \treturn IMPL.fixed_ip_update(context, address, values)\n", 
" \treturn IMPL.fixed_ip_get_by_host(context, host)\n", 
" \treturn IMPL.virtual_interface_update(context, address, values)\n", 
" \treturn IMPL.virtual_interface_get(context, vif_id)\n", 
" \treturn IMPL.virtual_interface_get_by_address(context, address)\n", 
" \treturn IMPL.virtual_interface_get_by_uuid(context, vif_uuid)\n", 
" \treturn IMPL.virtual_interface_get_by_instance(context, instance_id)\n", 
" \treturn IMPL.virtual_interface_get_by_instance_and_network(context, instance_id, network_id)\n", 
" \treturn IMPL.virtual_interface_delete_by_instance(context, instance_id)\n", 
" \treturn IMPL.virtual_interface_get_all(context)\n", 
" \treturn IMPL.instance_create(context, values)\n", 
" \ttry: \n\t \tprojects = conn.ex_list_projects() \n\texcept AttributeError: \n\t \tlog.warning('Cannot \tget \tprojects, \tyou \tmay \tneed \tto \tupdate \tlibcloud \tto \t0.15 \tor \tlater') \n\t \treturn False \n\tprojid = config.get_cloud_config_value('projectid', vm_, __opts__) \n\tif (not projid): \n\t \treturn False \n\tfor project in projects: \n\t \tif (str(projid) in (str(project.id), str(project.name))): \n\t \t \treturn project \n\tlog.warning(\"Couldn't \tfind \tproject \t{0} \tin \tprojects\".format(projid)) \n\treturn False\n", 
" \treturn IMPL.instance_destroy(context, instance_uuid, constraint)\n", 
" \treturn IMPL.instance_get(context, instance_id, columns_to_join=columns_to_join)\n", 
" \treturn IMPL.instance_get(context, instance_id, columns_to_join=columns_to_join)\n", 
" \treturn IMPL.instance_get_all(context, columns_to_join=columns_to_join)\n", 
" \treturn IMPL.instance_get_all_by_filters(context, filters, sort_key, sort_dir, limit=limit, marker=marker, columns_to_join=columns_to_join)\n", 
" \treturn IMPL.instance_get_active_by_window_joined(context, begin, end, project_id, host, columns_to_join=columns_to_join, limit=limit, marker=marker)\n", 
" \treturn IMPL.instance_get_all_by_host(context, host, columns_to_join)\n", 
" \treturn IMPL.instance_get_all_by_host_and_node(context, host, node, columns_to_join=columns_to_join)\n", 
" \treturn IMPL.instance_get_all_by_host_and_not_type(context, host, type_id)\n", 
" \treturn IMPL.instance_floating_address_get_all(context, instance_uuid)\n", 
" \treturn IMPL.instance_floating_address_get_all(context, instance_uuid)\n", 
" \treturn IMPL.instance_get_all_hung_in_rebooting(context, reboot_window)\n", 
" \treturn isinstance(object, types.MethodType)\n", 
" \treturn IMPL.instance_update(context, instance_uuid, values, expected=expected)\n", 
" \trv = IMPL.instance_update_and_get_original(context, instance_uuid, values, columns_to_join=columns_to_join, expected=expected) \n\treturn rv\n", 
" \treturn IMPL.instance_add_security_group(context, instance_id, security_group_id)\n", 
" \treturn IMPL.instance_remove_security_group(context, instance_id, security_group_id)\n", 
" \treturn IMPL.instance_info_cache_get(context, instance_uuid)\n", 
" \treturn IMPL.instance_info_cache_update(context, instance_uuid, values)\n", 
" \treturn IMPL.instance_info_cache_delete(context, instance_uuid)\n", 
" \treturn IMPL.key_pair_create(context, values)\n", 
" \treturn IMPL.key_pair_destroy(context, user_id, name)\n", 
" \treturn IMPL.key_pair_get(context, user_id, name)\n", 
" \treturn IMPL.key_pair_get_all_by_user(context, user_id, limit=limit, marker=marker)\n", 
" \treturn IMPL.key_pair_count_by_user(context, user_id)\n", 
" \treturn IMPL.network_associate(context, project_id, network_id, force)\n", 
" \treturn IMPL.network_count_reserved_ips(context, network_id)\n", 
" \treturn IMPL.network_create_safe(context, values)\n", 
" \treturn IMPL.network_delete_safe(context, network_id)\n", 
" \tif args.host_only: \n\t \tcs.networks.disassociate(args.network, True, False) \n\telif args.project_only: \n\t \tcs.networks.disassociate(args.network, False, True) \n\telse: \n\t \tcs.networks.disassociate(args.network, True, True)\n", 
" \treturn IMPL.network_get(context, network_id, project_only=project_only)\n", 
" \treturn IMPL.network_get_all(context, project_only)\n", 
" \treturn IMPL.network_get_all_by_uuids(context, network_uuids, project_only=project_only)\n", 
" \treturn IMPL.network_in_use_on_host(context, network_id, host)\n", 
" \treturn IMPL.network_get_associated_fixed_ips(context, network_id, host)\n", 
" \treturn IMPL.network_get_by_uuid(context, uuid)\n", 
" \treturn IMPL.network_get_by_cidr(context, cidr)\n", 
" \treturn IMPL.fixed_ip_get_by_instance(context, instance_uuid)\n", 
" \treturn IMPL.network_get_all_by_host(context, host)\n", 
" \treturn IMPL.network_set_host(context, network_id, host_id)\n", 
" \treturn IMPL.network_update(context, network_id, values)\n", 
" \treturn IMPL.quota_create(context, project_id, resource, limit, user_id=user_id)\n", 
" \treturn IMPL.quota_get(context, project_id, resource, user_id=user_id)\n", 
" \treturn IMPL.quota_get_all_by_project(context, project_id)\n", 
" \treturn IMPL.quota_update(context, project_id, resource, limit, user_id=user_id)\n", 
" \treturn IMPL.quota_class_create(context, class_name, resource, limit)\n", 
" \treturn IMPL.quota_class_get(context, class_name, resource)\n", 
" \treturn IMPL.quota_class_get_all_by_name(context, class_name)\n", 
" \treturn IMPL.quota_class_update(context, class_name, resource, limit)\n", 
" \treturn IMPL.quota_usage_get(context, project_id, resource, user_id=user_id)\n", 
" \treturn IMPL.quota_usage_get_all_by_project(context, project_id)\n", 
" \treturn IMPL.quota_usage_update(context, project_id, user_id, resource, **kwargs)\n", 
" \treturn IMPL.quota_create(context, project_id, resource, limit, user_id=user_id)\n", 
" \treturn IMPL.quota_get(context, project_id, resource, user_id=user_id)\n", 
" \treturn IMPL.quota_reserve(context, resources, quotas, user_quotas, deltas, expire, until_refresh, max_age, project_id=project_id, user_id=user_id)\n", 
" \treturn IMPL.reservation_commit(context, reservations, project_id=project_id, user_id=user_id)\n", 
" \treturn IMPL.reservation_rollback(context, reservations, project_id=project_id, user_id=user_id)\n", 
" \treturn IMPL.quota_destroy_all_by_project(context, project_id)\n", 
" \treturn IMPL.reservation_expire(context)\n", 
" \treturn IMPL.block_device_mapping_create(context, values, legacy)\n", 
" \treturn IMPL.block_device_mapping_update_or_create(context, values, legacy)\n", 
" \treturn IMPL.block_device_mapping_update_or_create(context, values, legacy)\n", 
" \treturn IMPL.block_device_mapping_get_all_by_instance(context, instance_uuid)\n", 
" \treturn IMPL.block_device_mapping_destroy_by_instance_and_volume(context, instance_uuid, volume_id)\n", 
" \treturn IMPL.block_device_mapping_destroy_by_instance_and_volume(context, instance_uuid, volume_id)\n", 
" \treturn IMPL.block_device_mapping_destroy_by_instance_and_volume(context, instance_uuid, volume_id)\n", 
" \treturn IMPL.security_group_get_all(context)\n", 
" \treturn IMPL.security_group_get(context, security_group_id, columns_to_join)\n", 
" \treturn IMPL.security_group_get_by_name(context, project_id, group_name, columns_to_join=None)\n", 
" \treturn IMPL.security_group_get_by_project(context, project_id)\n", 
" \treturn IMPL.security_group_get_by_instance(context, instance_uuid)\n", 
" \twith settings(hide('running', 'stdout', 'warnings'), warn_only=True): \n\t \treturn run(('getent \tgroup \t%(name)s' % locals())).succeeded\n", 
" \treturn IMPL.security_group_in_use(context, group_id)\n", 
" \tconn = _auth(profile) \n\treturn conn.create_security_group(name, description)\n", 
" \treturn IMPL.security_group_ensure_default(context)\n", 
" \tconn = _get_conn(region=region, key=key, keyid=keyid, profile=profile) \n\tgroup = _get_group(conn, name=name, vpc_id=vpc_id, vpc_name=vpc_name, group_id=group_id, region=region, key=key, keyid=keyid, profile=profile) \n\tif group: \n\t \tdeleted = conn.delete_security_group(group_id=group.id) \n\t \tif deleted: \n\t \t \tlog.info('Deleted \tsecurity \tgroup \t{0} \twith \tid \t{1}.'.format(group.name, group.id)) \n\t \t \treturn True \n\t \telse: \n\t \t \tmsg = 'Failed \tto \tdelete \tsecurity \tgroup \t{0}.'.format(name) \n\t \t \tlog.error(msg) \n\t \t \treturn False \n\telse: \n\t \tlog.debug('Security \tgroup \tnot \tfound.') \n\t \treturn False\n", 
" \treturn IMPL.security_group_rule_count_by_group(context, security_group_id)\n", 
" \tconn = _auth(profile) \n\treturn conn.create_security_group(name, description)\n", 
" \treturn IMPL.security_group_rule_get_by_security_group(context, security_group_id, columns_to_join=columns_to_join)\n", 
" \treturn IMPL.security_group_rule_get_by_security_group(context, security_group_id, columns_to_join=columns_to_join)\n", 
" \treturn IMPL.security_group_rule_destroy(context, security_group_rule_id)\n", 
" \treturn IMPL.security_group_rule_get(context, security_group_rule_id)\n", 
" \treturn IMPL.security_group_rule_count_by_group(context, security_group_id)\n", 
" \treturn IMPL.provider_fw_rule_create(context, rule)\n", 
" \treturn IMPL.provider_fw_rule_get_all(context)\n", 
" \treturn IMPL.provider_fw_rule_destroy(context, rule_id)\n", 
" \treturn IMPL.project_get_networks(context, project_id, associate)\n", 
" \treturn IMPL.console_pool_create(context, values)\n", 
" \treturn IMPL.console_pool_get_by_host_type(context, compute_host, proxy_host, console_type)\n", 
" \treturn IMPL.console_pool_get_all_by_host_type(context, host, console_type)\n", 
" \treturn IMPL.console_create(context, values)\n", 
" \treturn IMPL.console_delete(context, console_id)\n", 
" \treturn IMPL.console_get_by_pool_instance(context, pool_id, instance_uuid)\n", 
" \treturn IMPL.console_get_all_by_instance(context, instance_uuid, columns_to_join)\n", 
" \treturn IMPL.console_get(context, console_id, instance_uuid)\n", 
" \treturn IMPL.flavor_create(context, values, projects=projects)\n", 
" \treturn IMPL.instance_get_all(context, columns_to_join=columns_to_join)\n", 
" \treturn IMPL.flavor_get(context, id)\n", 
" \treturn IMPL.flavor_get_by_name(context, name)\n", 
" \treturn IMPL.flavor_get_by_flavor_id(context, id, read_deleted)\n", 
" \treturn IMPL.flavor_destroy(context, flavor_id)\n", 
" \treturn IMPL.flavor_access_get_by_flavor_id(context, flavor_id)\n", 
" \treturn IMPL.flavor_access_add(context, flavor_id, project_id)\n", 
" \treturn IMPL.flavor_access_remove(context, flavor_id, project_id)\n", 
" \treturn IMPL.cell_create(context, values)\n", 
" \treturn IMPL.cell_update(context, cell_name, values)\n", 
" \treturn IMPL.cell_delete(context, cell_name)\n", 
" \treturn IMPL.cell_get(context, cell_name)\n", 
" \treturn IMPL.cell_get_all(context)\n", 
" \treturn IMPL.instance_metadata_get(context, instance_uuid)\n", 
" \tIMPL.instance_metadata_delete(context, instance_uuid, key)\n", 
" \tIMPL.instance_system_metadata_update(context, instance_uuid, metadata, delete)\n", 
" \treturn IMPL.instance_system_metadata_get(context, instance_uuid)\n", 
" \tIMPL.instance_system_metadata_update(context, instance_uuid, metadata, delete)\n", 
" \treturn IMPL.agent_build_create(context, values)\n", 
" \treturn IMPL.agent_build_get_by_triple(context, hypervisor, os, architecture)\n", 
" \treturn IMPL.agent_build_get_all(context, hypervisor)\n", 
" \tIMPL.agent_build_destroy(context, agent_update_id)\n", 
" \tIMPL.agent_build_update(context, agent_build_id, values)\n", 
" \treturn IMPL.bw_usage_get(context, uuid, start_period, mac)\n", 
" \treturn IMPL.bw_usage_get_by_uuids(context, uuids, start_period)\n", 
" \trv = IMPL.bw_usage_update(context, uuid, mac, start_period, bw_in, bw_out, last_ctr_in, last_ctr_out, last_refreshed=last_refreshed) \n\tif update_cells: \n\t \ttry: \n\t \t \tcells_rpcapi.CellsAPI().bw_usage_update_at_top(context, uuid, mac, start_period, bw_in, bw_out, last_ctr_in, last_ctr_out, last_refreshed) \n\t \texcept Exception: \n\t \t \tLOG.exception(_LE('Failed \tto \tnotify \tcells \tof \tbw_usage \tupdate')) \n\treturn rv\n", 
" \treturn IMPL.flavor_extra_specs_get(context, flavor_id)\n", 
" \tIMPL.flavor_extra_specs_delete(context, flavor_id, key)\n", 
" \tIMPL.flavor_extra_specs_update_or_create(context, flavor_id, extra_specs)\n", 
" \treturn IMPL.vol_get_usage_by_time(context, begin)\n", 
" \treturn IMPL.vol_usage_update(context, id, rd_req, rd_bytes, wr_req, wr_bytes, instance_id, project_id, user_id, availability_zone, update_totals=update_totals)\n", 
" \treturn IMPL.s3_image_get(context, image_id)\n", 
" \treturn IMPL.s3_image_get_by_uuid(context, image_uuid)\n", 
" \treturn IMPL.s3_image_create(context, image_uuid)\n", 
" \treturn IMPL.aggregate_create(context, values, metadata)\n", 
" \treturn IMPL.aggregate_get(context, aggregate_id)\n", 
" \treturn IMPL.aggregate_get_by_host(context, host, key)\n", 
" \treturn IMPL.aggregate_metadata_get_by_host(context, host, key)\n", 
" \treturn IMPL.aggregate_metadata_get_by_host(context, host, key)\n", 
" \treturn IMPL.aggregate_update(context, aggregate_id, values)\n", 
" \treturn IMPL.aggregate_delete(context, aggregate_id)\n", 
" \treturn IMPL.aggregate_get_all(context)\n", 
" \tIMPL.aggregate_metadata_add(context, aggregate_id, metadata, set_delete)\n", 
" \treturn IMPL.aggregate_metadata_get(context, aggregate_id)\n", 
" \tIMPL.aggregate_metadata_delete(context, aggregate_id, key)\n", 
" \tIMPL.aggregate_host_add(context, aggregate_id, host)\n", 
" \treturn IMPL.aggregate_host_get_all(context, aggregate_id)\n", 
" \tIMPL.aggregate_host_delete(context, aggregate_id, host)\n", 
" \treturn IMPL.instance_fault_create(context, values)\n", 
" \treturn IMPL.instance_fault_get_by_instance_uuids(context, instance_uuids, latest=latest)\n", 
" \treturn IMPL.action_start(context, values)\n", 
" \treturn IMPL.action_finish(context, values)\n", 
" \treturn IMPL.actions_get(context, uuid)\n", 
" \treturn IMPL.action_get_by_request_id(context, uuid, request_id)\n", 
" \treturn IMPL.action_event_start(context, values)\n", 
" \treturn IMPL.action_event_finish(context, values)\n", 
" \treturn IMPL.action_events_get(context, action_id)\n", 
" \treturn IMPL.get_instance_uuid_by_ec2_id(context, ec2_id)\n", 
" \treturn IMPL.get_instance_uuid_by_ec2_id(context, ec2_id)\n", 
" \treturn IMPL.ec2_instance_create(context, instance_uuid, id)\n", 
" \treturn IMPL.task_log_end_task(context, task_name, period_beginning, period_ending, host, errors, message)\n", 
" \treturn IMPL.task_log_begin_task(context, task_name, period_beginning, period_ending, host, task_items, message)\n", 
" \treturn IMPL.archive_deleted_rows(max_rows=max_rows)\n", 
" \treturn IMPL.archive_deleted_rows(max_rows=max_rows)\n", 
" \treturn CellFilterHandler().get_all_classes()\n", 
" \tchanged = False \n\tdata = get_all_config() \n\tfor key in c: \n\t \tif (key in data): \n\t \t \tif (data[key] != c[key]): \n\t \t \t \tchanged = True \n\tif changed: \n\t \treload_config()\n", 
" \tcfg = config.load() \n\tif (not kind): \n\t \tkind = cfg.search_backend.lower() \n\tif (kind == 'sqlalchemy'): \n\t \tfrom . import db \n\t \treturn db.SQLAlchemyIndex() \n\ttry: \n\t \tmodule = importlib.import_module(kind) \n\texcept ImportError: \n\t \tpass \n\telse: \n\t \treturn module.Index() \n\traise NotImplementedError('Unknown \tindex \ttype \t{0!r}'.format(kind))\n", 
" \treturn CellWeightHandler().get_all_classes()\n", 
" \treturn sum((edge[2].get(attr, default) for edge in G.edges(data=True)))\n", 
" \treturn (lsb + (msb << 7))\n", 
" \ttry: \n\t \timages = get_available_images(request, request.user.project_id) \n\texcept Exception: \n\t \texceptions.handle(request, _('Unable \tto \tretrieve \timages')) \n\timages.sort(key=(lambda c: c.name)) \n\timages_list = [('', _('Select \tImage'))] \n\tfor image in images: \n\t \timage_label = u'{} \t({})'.format(image.name, filesizeformat(image.size)) \n\t \timages_list.append((image.id, image_label)) \n\tif (not images): \n\t \treturn [('', _('No \timages \tavailable'))] \n\treturn images_list\n", 
" \tgroup.update({'host': host, 'updated_at': timeutils.utcnow(), 'cluster_name': cluster_name}) \n\tgroup.save() \n\treturn group\n", 
" \tif (stubmap is None): \n\t \tstubmap = apiproxy \n\tstub = stubmap.GetStub(service) \n\tassert stub, ('No \tapi \tproxy \tfound \tfor \tservice \t\"%s\"' % service) \n\tassert hasattr(stub, 'CreateRPC'), (('The \tservice \t\"%s\" \tdoesn\\'t \thave \t' + 'a \tCreateRPC \tmethod.') % service) \n\treturn stub.CreateRPC()\n", 
" \treturn IMPL.flavor_create(context, values, projects=projects)\n", 
" \tif (id is None): \n\t \tmsg = _('id \tcannot \tbe \tNone') \n\t \traise exception.InvalidGroupType(reason=msg) \n\telse: \n\t \televated = (context if context.is_admin else context.elevated()) \n\t \tdb.group_type_destroy(elevated, id)\n", 
" \tfor key in system_metadata_flavor_props.keys(): \n\t \tfor prefix in prefixes: \n\t \t \tto_key = ('%sinstance_type_%s' % (prefix, key)) \n\t \t \tdel metadata[to_key] \n\tfor key in list(metadata.keys()): \n\t \tfor prefix in prefixes: \n\t \t \tif key.startswith(('%sinstance_type_extra_' % prefix)): \n\t \t \t \tdel metadata[key] \n\treturn metadata\n", 
" \tname = CONF.default_group_type \n\tgrp_type = {} \n\tif (name is not None): \n\t \tctxt = context.get_admin_context() \n\t \ttry: \n\t \t \tgrp_type = get_group_type_by_name(ctxt, name) \n\t \texcept exception.GroupTypeNotFoundByName: \n\t \t \tLOG.exception(_LE('Default \tgroup \ttype \tis \tnot \tfound. \tPlease \tcheck \tdefault_group_type \tconfig.')) \n\treturn grp_type\n", 
" \tif (id is None): \n\t \tmsg = _('id \tcannot \tbe \tNone') \n\t \traise exception.InvalidGroupType(reason=msg) \n\tif (ctxt is None): \n\t \tctxt = context.get_admin_context() \n\treturn db.group_type_get(ctxt, id, expected_fields=expected_fields)\n", 
" \tif (name is None): \n\t \tmsg = _('name \tcannot \tbe \tNone') \n\t \traise exception.InvalidGroupType(reason=msg) \n\treturn db.group_type_get_by_name(context, name)\n", 
" \tif (ctxt is None): \n\t \tctxt = context.get_admin_context(read_deleted=read_deleted) \n\treturn objects.Flavor.get_by_flavor_id(ctxt, flavorid, read_deleted)\n", 
" \tif (ctxt is None): \n\t \tctxt = context.get_admin_context() \n\tflavor = objects.Flavor.get_by_flavor_id(ctxt, flavorid) \n\treturn flavor.projects\n", 
" \treturn IMPL.group_type_access_add(context, type_id, project_id)\n", 
" \treturn IMPL.group_type_access_remove(context, type_id, project_id)\n", 
" \tflavor = objects.Flavor() \n\tsys_meta = utils.instance_sys_meta(instance) \n\tif (not sys_meta): \n\t \treturn None \n\tfor key in system_metadata_flavor_props.keys(): \n\t \ttype_key = ('%sinstance_type_%s' % (prefix, key)) \n\t \tsetattr(flavor, key, sys_meta[type_key]) \n\textra_specs = [(k, v) for (k, v) in sys_meta.items() if k.startswith(('%sinstance_type_extra_' % prefix))] \n\tif extra_specs: \n\t \tflavor.extra_specs = {} \n\t \tfor (key, value) in extra_specs: \n\t \t \textra_key = key[len(('%sinstance_type_extra_' % prefix)):] \n\t \t \tflavor.extra_specs[extra_key] = value \n\treturn flavor\n", 
" \tfor key in system_metadata_flavor_props.keys(): \n\t \tto_key = ('%sinstance_type_%s' % (prefix, key)) \n\t \tmetadata[to_key] = instance_type[key] \n\textra_specs = instance_type.get('extra_specs', {}) \n\tfor extra_prefix in system_metadata_flavor_extra_props: \n\t \tfor key in extra_specs: \n\t \t \tif key.startswith(extra_prefix): \n\t \t \t \tto_key = ('%sinstance_type_extra_%s' % (prefix, key)) \n\t \t \t \tmetadata[to_key] = extra_specs[key] \n\treturn metadata\n", 
" \tfor key in system_metadata_flavor_props.keys(): \n\t \tfor prefix in prefixes: \n\t \t \tto_key = ('%sinstance_type_%s' % (prefix, key)) \n\t \t \tdel metadata[to_key] \n\tfor key in list(metadata.keys()): \n\t \tfor prefix in prefixes: \n\t \t \tif key.startswith(('%sinstance_type_extra_' % prefix)): \n\t \t \t \tdel metadata[key] \n\treturn metadata\n", 
" \tcache_key = _cache_get_key() \n\ttry: \n\t \treturn __context__[cache_key] \n\texcept KeyError: \n\t \tpass \n\tconn = _get_conn(region=region, key=key, keyid=keyid, profile=profile) \n\t__context__[cache_key] = {} \n\ttopics = conn.get_all_topics() \n\tfor t in topics['ListTopicsResponse']['ListTopicsResult']['Topics']: \n\t \tshort_name = t['TopicArn'].split(':')[(-1)] \n\t \t__context__[cache_key][short_name] = t['TopicArn'] \n\treturn __context__[cache_key]\n", 
" \tfault_obj = objects.InstanceFault(context=context) \n\tfault_obj.host = CONF.host \n\tfault_obj.instance_uuid = instance.uuid \n\tfault_obj.update(exception_to_dict(fault, message=fault_message)) \n\tcode = fault_obj.code \n\tfault_obj.details = _get_fault_details(exc_info, code) \n\tfault_obj.create()\n", 
" \treq_prefix = None \n\treq_letter = None \n\tif device: \n\t \ttry: \n\t \t \t(req_prefix, req_letter) = block_device.match_device(device) \n\t \texcept (TypeError, AttributeError, ValueError): \n\t \t \traise exception.InvalidDevicePath(path=device) \n\tif (not root_device_name): \n\t \troot_device_name = block_device.DEFAULT_ROOT_DEV_NAME \n\ttry: \n\t \tprefix = block_device.match_device(block_device.prepend_dev(root_device_name))[0] \n\texcept (TypeError, AttributeError, ValueError): \n\t \traise exception.InvalidDevicePath(path=root_device_name) \n\tif driver.is_xenapi(): \n\t \tprefix = '/dev/xvd' \n\tif (req_prefix != prefix): \n\t \tLOG.debug('Using \t%(prefix)s \tinstead \tof \t%(req_prefix)s', {'prefix': prefix, 'req_prefix': req_prefix}) \n\tused_letters = set() \n\tfor device_path in device_name_list: \n\t \tletter = block_device.get_device_letter(device_path) \n\t \tused_letters.add(letter) \n\tif driver.is_xenapi(): \n\t \tflavor = instance.get_flavor() \n\t \tif flavor.ephemeral_gb: \n\t \t \tused_letters.add('b') \n\t \tif flavor.swap: \n\t \t \tused_letters.add('c') \n\tif (not req_letter): \n\t \treq_letter = _get_unused_letter(used_letters) \n\tif (req_letter in used_letters): \n\t \traise exception.DevicePathInUse(path=device) \n\treturn (prefix + req_letter)\n", 
" \t(audit_start, audit_end) = notifications.audit_period_bounds(current_period) \n\tbw = notifications.bandwidth_usage(instance_ref, audit_start, ignore_missing_network_data) \n\tif (system_metadata is None): \n\t \tsystem_metadata = utils.instance_sys_meta(instance_ref) \n\timage_meta = notifications.image_meta(system_metadata) \n\textra_info = dict(audit_period_beginning=str(audit_start), audit_period_ending=str(audit_end), bandwidth=bw, image_meta=image_meta) \n\tif extra_usage_info: \n\t \textra_info.update(extra_usage_info) \n\tnotify_about_instance_usage(notifier, context, instance_ref, 'exists', system_metadata=system_metadata, extra_usage_info=extra_info)\n", 
" \tif (not extra_usage_info): \n\t \textra_usage_info = {} \n\tusage_info = notifications.info_from_instance(context, instance, network_info, system_metadata, **extra_usage_info) \n\tif fault: \n\t \tfault_payload = exception_to_dict(fault) \n\t \tLOG.debug(fault_payload['message'], instance=instance) \n\t \tusage_info.update(fault_payload) \n\tif event_suffix.endswith('error'): \n\t \tmethod = notifier.error \n\telse: \n\t \tmethod = notifier.info \n\tmethod(context, ('compute.instance.%s' % event_suffix), usage_info)\n", 
" \t@functools.wraps(function) \n\tdef decorated_function(self, context, *args, **kwargs): \n\t \ttry: \n\t \t \treturn function(self, context, *args, **kwargs) \n\t \texcept exception.UnexpectedTaskStateError as e: \n\t \t \twith excutils.save_and_reraise_exception(): \n\t \t \t \tLOG.info(_LI('Task \tpossibly \tpreempted: \t%s'), e.format_message()) \n\t \texcept Exception: \n\t \t \twith excutils.save_and_reraise_exception(): \n\t \t \t \twrapped_func = safe_utils.get_wrapped_function(function) \n\t \t \t \tkeyed_args = inspect.getcallargs(wrapped_func, self, context, *args, **kwargs) \n\t \t \t \tinstance = keyed_args['instance'] \n\t \t \t \toriginal_task_state = instance.task_state \n\t \t \t \ttry: \n\t \t \t \t \tself._instance_update(context, instance, task_state=None) \n\t \t \t \t \tLOG.info(_LI('Successfully \treverted \ttask \tstate \tfrom \t%s \ton \tfailure \tfor \tinstance.'), original_task_state, instance=instance) \n\t \t \t \texcept exception.InstanceNotFound: \n\t \t \t \t \tpass \n\t \t \t \texcept Exception as e: \n\t \t \t \t \tmsg = _LW('Failed \tto \trevert \ttask \tstate \tfor \tinstance. \tError: \t%s') \n\t \t \t \t \tLOG.warning(msg, e, instance=instance) \n\treturn decorated_function\n", 
" \t@functools.wraps(function) \n\tdef decorated_function(self, context, *args, **kwargs): \n\t \ttry: \n\t \t \treturn function(self, context, *args, **kwargs) \n\t \texcept exception.InstanceNotFound: \n\t \t \traise \n\t \texcept Exception as e: \n\t \t \tkwargs.update(dict(zip(function.__code__.co_varnames[2:], args))) \n\t \t \twith excutils.save_and_reraise_exception(): \n\t \t \t \tcompute_utils.add_instance_fault_from_exc(context, kwargs['instance'], e, sys.exc_info()) \n\treturn decorated_function\n", 
" \t@utils.expects_func_args('instance') \n\tdef helper(function): \n\t \t@functools.wraps(function) \n\t \tdef decorated_function(self, context, *args, **kwargs): \n\t \t \twrapped_func = safe_utils.get_wrapped_function(function) \n\t \t \tkeyed_args = inspect.getcallargs(wrapped_func, self, context, *args, **kwargs) \n\t \t \tinstance_uuid = keyed_args['instance']['uuid'] \n\t \t \tevent_name = '{0}_{1}'.format(prefix, function.__name__) \n\t \t \twith EventReporter(context, event_name, instance_uuid): \n\t \t \t \treturn function(self, context, *args, **kwargs) \n\t \treturn decorated_function \n\treturn helper\n", 
" \tcompute_api_class_name = _get_compute_api_class_name() \n\tcompute_api_class = importutils.import_class(compute_api_class_name) \n\tclass_name = (compute_api_class.__module__ + '.HostAPI') \n\treturn importutils.import_object(class_name, *args, **kwargs)\n", 
" \tcompute_api_class_name = _get_compute_api_class_name() \n\tcompute_api_class = importutils.import_class(compute_api_class_name) \n\tclass_name = (compute_api_class.__module__ + '.InstanceActionAPI') \n\treturn importutils.import_object(class_name, *args, **kwargs)\n", 
" \tif ((vm_state is not None) and (not isinstance(vm_state, set))): \n\t \tvm_state = set(vm_state) \n\tif ((task_state is not None) and (not isinstance(task_state, set))): \n\t \ttask_state = set(task_state) \n\tdef outer(f): \n\t \t@six.wraps(f) \n\t \tdef inner(self, context, instance, *args, **kw): \n\t \t \tif ((vm_state is not None) and (instance.vm_state not in vm_state)): \n\t \t \t \traise exception.InstanceInvalidState(attr='vm_state', instance_uuid=instance.uuid, state=instance.vm_state, method=f.__name__) \n\t \t \tif ((task_state is not None) and (instance.task_state not in task_state)): \n\t \t \t \traise exception.InstanceInvalidState(attr='task_state', instance_uuid=instance.uuid, state=instance.task_state, method=f.__name__) \n\t \t \tif (must_have_launched and (not instance.launched_at)): \n\t \t \t \traise exception.InstanceInvalidState(attr='launched_at', instance_uuid=instance.uuid, state=instance.launched_at, method=f.__name__) \n\t \t \treturn f(self, context, instance, *args, **kw) \n\t \treturn inner \n\treturn outer\n", 
" \t@functools.wraps(func) \n\tdef wrapped(self, context, target_obj, *args, **kwargs): \n\t \tcheck_policy(context, func.__name__, target_obj) \n\t \treturn func(self, context, target_obj, *args, **kwargs) \n\treturn wrapped\n", 
" \tif preferred: \n\t \tunordered.sort(key=(lambda i: preferred.index(accessor(i))))\n", 
" \treturn os.path.basename(sys.argv[0])[:16].replace(' \t', '_')\n", 
" \tif (CONF.metadata_host != '127.0.0.1'): \n\t \tiptables_manager.ipv4['nat'].add_rule('PREROUTING', ('-s \t0.0.0.0/0 \t-d \t169.254.169.254/32 \t-p \ttcp \t-m \ttcp \t--dport \t80 \t-j \tDNAT \t--to-destination \t%s:%s' % (CONF.metadata_host, CONF.metadata_port))) \n\telse: \n\t \tiptables_manager.ipv4['nat'].add_rule('PREROUTING', ('-s \t0.0.0.0/0 \t-d \t169.254.169.254/32 \t-p \ttcp \t-m \ttcp \t--dport \t80 \t-j \tREDIRECT \t--to-ports \t%s' % CONF.metadata_port)) \n\tiptables_manager.apply()\n", 
" \trule = ('-p \ttcp \t-m \ttcp \t--dport \t%s \t%s \t-j \tACCEPT' % (CONF.metadata_port, _iptables_dest(CONF.metadata_host))) \n\tif (netaddr.IPAddress(CONF.metadata_host).version == 4): \n\t \tiptables_manager.ipv4['filter'].add_rule('INPUT', rule) \n\telse: \n\t \tiptables_manager.ipv6['filter'].add_rule('INPUT', rule) \n\tiptables_manager.apply()\n", 
" \tadd_snat_rule(ip_range, is_external) \n\trules = [] \n\tif is_external: \n\t \tfor snat_range in CONF.force_snat_range: \n\t \t \trules.append(('PREROUTING \t-p \tipv4 \t--ip-src \t%s \t--ip-dst \t%s \t-j \tredirect \t--redirect-target \tACCEPT' % (ip_range, snat_range))) \n\tif rules: \n\t \tensure_ebtables_rules(rules, 'nat') \n\tiptables_manager.ipv4['nat'].add_rule('POSTROUTING', ('-s \t%s \t-d \t%s/32 \t-j \tACCEPT' % (ip_range, CONF.metadata_host))) \n\tfor dmz in CONF.dmz_cidr: \n\t \tiptables_manager.ipv4['nat'].add_rule('POSTROUTING', ('-s \t%s \t-d \t%s \t-j \tACCEPT' % (ip_range, dmz))) \n\tiptables_manager.ipv4['nat'].add_rule('POSTROUTING', ('-s \t%(range)s \t-d \t%(range)s \t-m \tconntrack \t! \t--ctstate \tDNAT \t-j \tACCEPT' % {'range': ip_range})) \n\tiptables_manager.apply()\n", 
" \t_execute('ip', 'addr', 'add', (str(floating_ip) + '/32'), 'dev', device, run_as_root=True, check_exit_code=[0, 2, 254]) \n\tif (CONF.send_arp_for_ha and (CONF.send_arp_for_ha_count > 0)): \n\t \tsend_arp_for_ip(floating_ip, device, CONF.send_arp_for_ha_count)\n", 
" \t_execute('ip', 'addr', 'del', (str(floating_ip) + '/32'), 'dev', device, run_as_root=True, check_exit_code=[0, 2, 254])\n", 
" \t_execute('ip', 'addr', 'add', '169.254.169.254/32', 'scope', 'link', 'dev', 'lo', run_as_root=True, check_exit_code=[0, 2, 254])\n", 
" \tiptables_manager.ipv4['filter'].add_rule('FORWARD', ('-d \t%s \t-p \tudp \t--dport \t1194 \t-j \tACCEPT' % private_ip)) \n\tiptables_manager.ipv4['nat'].add_rule('PREROUTING', ('-d \t%s \t-p \tudp \t--dport \t%s \t-j \tDNAT \t--to \t%s:1194' % (public_ip, port, private_ip))) \n\tiptables_manager.ipv4['nat'].add_rule('OUTPUT', ('-d \t%s \t-p \tudp \t--dport \t%s \t-j \tDNAT \t--to \t%s:1194' % (public_ip, port, private_ip))) \n\tiptables_manager.apply()\n", 
" \tregex = ('.*\\\\s+%s(/32|\\\\s+|$)' % floating_ip) \n\tnum_rules = iptables_manager.ipv4['nat'].remove_rules_regex(regex) \n\tif num_rules: \n\t \tmsg = _LW('Removed \t%(num)d \tduplicate \trules \tfor \tfloating \tIP \t%(float)s') \n\t \tLOG.warning(msg, {'num': num_rules, 'float': floating_ip}) \n\tfor (chain, rule) in floating_forward_rules(floating_ip, fixed_ip, device): \n\t \tiptables_manager.ipv4['nat'].add_rule(chain, rule) \n\tiptables_manager.apply() \n\tif (device != network['bridge']): \n\t \tensure_ebtables_rules(*floating_ebtables_rules(fixed_ip, network))\n", 
" \tfor (chain, rule) in floating_forward_rules(floating_ip, fixed_ip, device): \n\t \tiptables_manager.ipv4['nat'].remove_rule(chain, rule) \n\tiptables_manager.apply() \n\tif (device != network['bridge']): \n\t \tremove_ebtables_rules(*floating_ebtables_rules(fixed_ip, network))\n", 
" \treturn ([('PREROUTING \t--logical-in \t%s \t-p \tipv4 \t--ip-src \t%s \t! \t--ip-dst \t%s \t-j \tredirect \t--redirect-target \tACCEPT' % (network['bridge'], fixed_ip, network['cidr']))], 'nat')\n", 
" \thosts = [] \n\thost = None \n\tif network_ref['multi_host']: \n\t \thost = CONF.host \n\tfor fixedip in objects.FixedIPList.get_by_network(context, network_ref, host=host): \n\t \tif fixedip.leased: \n\t \t \thosts.append(_host_lease(fixedip)) \n\treturn '\\n'.join(hosts)\n", 
" \thosts = [] \n\tmacs = set() \n\tfor fixedip in fixedips: \n\t \tif fixedip.allocated: \n\t \t \tif (fixedip.virtual_interface.address not in macs): \n\t \t \t \thosts.append(_host_dhcp(fixedip)) \n\t \t \t \tmacs.add(fixedip.virtual_interface.address) \n\treturn '\\n'.join(hosts)\n", 
" \thosts = [] \n\tfor fixedip in objects.FixedIPList.get_by_network(context, network_ref): \n\t \tif fixedip.allocated: \n\t \t \thosts.append(_host_dns(fixedip)) \n\treturn '\\n'.join(hosts)\n", 
" \ttable = iptables_manager.ipv4['filter'] \n\tfor port in [67, 53]: \n\t \tfor proto in ['udp', 'tcp']: \n\t \t \targs = {'dev': dev, 'port': port, 'proto': proto} \n\t \t \ttable.add_rule('INPUT', ('-i \t%(dev)s \t-p \t%(proto)s \t-m \t%(proto)s \t--dport \t%(port)s \t-j \tACCEPT' % args)) \n\tiptables_manager.apply()\n", 
" \ttable = iptables_manager.ipv4['filter'] \n\tfor port in [67, 53]: \n\t \tfor proto in ['udp', 'tcp']: \n\t \t \targs = {'dev': dev, 'port': port, 'proto': proto} \n\t \t \ttable.remove_rule('INPUT', ('-i \t%(dev)s \t-p \t%(proto)s \t-m \t%(proto)s \t--dport \t%(port)s \t-j \tACCEPT' % args)) \n\tiptables_manager.apply()\n", 
" \tgateway = network_ref['gateway'] \n\tif (network_ref['multi_host'] and (not (network_ref['share_address'] or CONF.share_dhcp_address))): \n\t \tgateway = network_ref['dhcp_server'] \n\thosts = [] \n\tif CONF.use_single_default_gateway: \n\t \tfor fixedip in fixedips: \n\t \t \tif fixedip.allocated: \n\t \t \t \tvif_id = fixedip.virtual_interface_id \n\t \t \t \tif fixedip.default_route: \n\t \t \t \t \thosts.append(_host_dhcp_opts(vif_id, gateway)) \n\t \t \t \telse: \n\t \t \t \t \thosts.append(_host_dhcp_opts(vif_id)) \n\telse: \n\t \thosts.append(_host_dhcp_opts(None, gateway)) \n\treturn '\\n'.join(hosts)\n", 
" \tconffile = _dhcp_file(dev, 'conf') \n\toptsfile = _dhcp_file(dev, 'opts') \n\twrite_to_file(optsfile, get_dhcp_opts(context, network_ref, fixedips)) \n\tos.chmod(optsfile, 420) \n\t_add_dhcp_mangle_rule(dev) \n\tos.chmod(conffile, 420) \n\tpid = _dnsmasq_pid_for(dev) \n\tif pid: \n\t \tif is_pid_cmdline_correct(pid, conffile.split('/')[(-1)]): \n\t \t \ttry: \n\t \t \t \t_execute('kill', '-HUP', pid, run_as_root=True) \n\t \t \t \t_add_dnsmasq_accept_rules(dev) \n\t \t \t \treturn \n\t \t \texcept Exception as exc: \n\t \t \t \tLOG.error(_LE('kill \t-HUP \tdnsmasq \tthrew \t%s'), exc) \n\t \telse: \n\t \t \tLOG.debug('Pid \t%d \tis \tstale, \trelaunching \tdnsmasq', pid) \n\tcmd = ['env', ('CONFIG_FILE=%s' % jsonutils.dumps(CONF.dhcpbridge_flagfile)), ('NETWORK_ID=%s' % str(network_ref['id'])), 'dnsmasq', '--strict-order', '--bind-interfaces', ('--conf-file=%s' % CONF.dnsmasq_config_file), ('--pid-file=%s' % _dhcp_file(dev, 'pid')), ('--dhcp-optsfile=%s' % _dhcp_file(dev, 'opts')), ('--listen-address=%s' % network_ref['dhcp_server']), '--except-interface=lo', ('--dhcp-range=set:%s,%s,static,%s,%ss' % (network_ref['label'], network_ref['dhcp_start'], network_ref['netmask'], CONF.dhcp_lease_time)), ('--dhcp-lease-max=%s' % len(netaddr.IPNetwork(network_ref['cidr']))), ('--dhcp-hostsfile=%s' % _dhcp_file(dev, 'conf')), ('--dhcp-script=%s' % CONF.dhcpbridge), '--no-hosts', '--leasefile-ro'] \n\tif CONF.dhcp_domain: \n\t \tcmd.append(('--domain=%s' % CONF.dhcp_domain)) \n\tdns_servers = CONF.dns_server \n\tif CONF.use_network_dns_servers: \n\t \tif network_ref.get('dns1'): \n\t \t \tdns_servers.append(network_ref.get('dns1')) \n\t \tif network_ref.get('dns2'): \n\t \t \tdns_servers.append(network_ref.get('dns2')) \n\tif network_ref['multi_host']: \n\t \tcmd.append(('--addn-hosts=%s' % _dhcp_file(dev, 'hosts'))) \n\tif dns_servers: \n\t \tcmd.append('--no-resolv') \n\tfor dns_server in dns_servers: \n\t \tcmd.append(('--server=%s' % dns_server)) \n\t_execute(run_as_root=True, *cmd) \n\t_add_dnsmasq_accept_rules(dev)\n", 
" \ttimestamp = timeutils.utcnow() \n\tseconds_since_epoch = calendar.timegm(timestamp.utctimetuple()) \n\treturn ('%d \t%s \t%s \t%s \t*' % ((seconds_since_epoch + CONF.dhcp_lease_time), fixedip.virtual_interface.address, fixedip.address, (fixedip.instance.hostname or '*')))\n", 
" \thostname = fixedip.instance.hostname \n\tif (len(hostname) > 63): \n\t \tLOG.warning(_LW('hostname \t%s \ttoo \tlong, \ttruncating.'), hostname) \n\t \thostname = ((fixedip.instance.hostname[:2] + '-') + fixedip.instance.hostname[(-60):]) \n\tif CONF.use_single_default_gateway: \n\t \tnet = _host_dhcp_network(fixedip.virtual_interface_id) \n\t \treturn ('%s,%s.%s,%s,net:%s' % (fixedip.virtual_interface.address, hostname, CONF.dhcp_domain, fixedip.address, net)) \n\telse: \n\t \treturn ('%s,%s.%s,%s' % (fixedip.virtual_interface.address, hostname, CONF.dhcp_domain, fixedip.address))\n", 
" \tvalues = [] \n\tif (vif_id is not None): \n\t \tvalues.append(_host_dhcp_network(vif_id)) \n\tvalues.append('3') \n\tif gateway: \n\t \tvalues.append(('%s' % gateway)) \n\treturn ','.join(values)\n", 
" \tif CONF.fake_network: \n\t \tLOG.debug('FAKE \tNET: \t%s', ' \t'.join(map(str, cmd))) \n\t \treturn ('fake', 0) \n\telse: \n\t \treturn utils.execute(*cmd, **kwargs)\n", 
" \treturn os.path.exists(('/sys/class/net/%s' % device))\n", 
" \tfileutils.ensure_tree(CONF.networks_path) \n\treturn os.path.abspath(('%s/nova-%s.%s' % (CONF.networks_path, dev, kind)))\n", 
" \tfileutils.ensure_tree(CONF.networks_path) \n\treturn os.path.abspath(('%s/nova-ra-%s.%s' % (CONF.networks_path, dev, kind)))\n", 
" \tpid_file = _dhcp_file(dev, 'pid') \n\tif os.path.exists(pid_file): \n\t \ttry: \n\t \t \twith open(pid_file, 'r') as f: \n\t \t \t \treturn int(f.read()) \n\t \texcept (ValueError, IOError): \n\t \t \treturn None\n", 
" \tpid_file = _ra_file(dev, 'pid') \n\tif os.path.exists(pid_file): \n\t \twith open(pid_file, 'r') as f: \n\t \t \treturn int(f.read())\n", 
" \tcmd = ['ip', 'addr', action] \n\tcmd.extend(params) \n\tcmd.extend(['dev', device]) \n\treturn cmd\n", 
" \tfor dev in [dev1_name, dev2_name]: \n\t \tdelete_net_dev(dev) \n\tutils.execute('ip', 'link', 'add', dev1_name, 'type', 'veth', 'peer', 'name', dev2_name, run_as_root=True) \n\tfor dev in [dev1_name, dev2_name]: \n\t \tutils.execute('ip', 'link', 'set', dev, 'up', run_as_root=True) \n\t \tutils.execute('ip', 'link', 'set', dev, 'promisc', 'on', run_as_root=True) \n\t \t_set_device_mtu(dev, mtu)\n", 
" \targspec = inspect.getargspec(f) \n\t@functools.wraps(f) \n\tdef wrapper(self, context, *args, **kwargs): \n\t \ttry: \n\t \t \tinstance = kwargs.get('instance') \n\t \t \tif (not instance): \n\t \t \t \tinstance = args[(argspec.args.index('instance') - 2)] \n\t \texcept ValueError: \n\t \t \tmsg = _('instance \tis \ta \trequired \targument \tto \tuse \t@refresh_cache') \n\t \t \traise Exception(msg) \n\t \twith lockutils.lock(('refresh_cache-%s' % instance.uuid)): \n\t \t \tres = f(self, context, *args, **kwargs) \n\t \t \tupdate_instance_cache_with_nw_info(self, context, instance, nw_info=res) \n\t \treturn res \n\treturn wrapper\n", 
" \t@functools.wraps(func) \n\tdef wrapped(self, context, target_obj, *args, **kwargs): \n\t \tcheck_policy(context, func.__name__, target_obj) \n\t \treturn func(self, context, target_obj, *args, **kwargs) \n\treturn wrapped\n", 
" \tif os.path.isdir('/sys/class/scsi_host/host{0}'.format(host)): \n\t \tcmd = 'echo \t\"- \t- \t-\" \t> \t/sys/class/scsi_host/host{0}/scan'.format(host) \n\telse: \n\t \treturn 'Host \t{0} \tdoes \tnot \texist'.format(host) \n\treturn __salt__['cmd.run'](cmd).splitlines()\n", 
" \tcmd = 'multipath \t-l' \n\treturn __salt__['cmd.run'](cmd).splitlines()\n", 
" \treturn IMPL.volume_glance_metadata_copy_to_snapshot(context, snapshot_id, volume_id)\n", 
" \tbase_url = url \n\tdef web_request(url, method=None, body=None): \n\t \treq = webob.Request.blank(('%s%s' % (base_url, url))) \n\t \tif method: \n\t \t \treq.content_type = 'application/json' \n\t \t \treq.method = method \n\t \tif body: \n\t \t \treq.body = jsonutils.dump_as_bytes(body) \n\t \treturn req \n\treturn web_request\n", 
" \treturn compare_tree_to_dict(actual, expected, ('rel', 'href', 'type'))\n", 
" \treturn compare_tree_to_dict(actual, expected, ('base', 'type'))\n", 
" \tfor (elem, data) in zip(actual, expected): \n\t \tfor key in keys: \n\t \t \tif (elem.get(key) != data.get(key)): \n\t \t \t \treturn False \n\treturn True\n", 
" \tto_delete = ('id', 'created_at', 'updated_at', 'deleted_at', 'deleted', 'action_id') \n\tfor key in to_delete: \n\t \tif (key in event): \n\t \t \tdel event[key] \n\tif ('start_time' in event): \n\t \tevent['start_time'] = str(event['start_time'].replace(tzinfo=None)) \n\tif ('finish_time' in event): \n\t \tevent['finish_time'] = str(event['finish_time'].replace(tzinfo=None)) \n\treturn event\n", 
" \tto_delete = ('id', 'created_at', 'updated_at', 'deleted_at', 'deleted', 'action_id') \n\tfor key in to_delete: \n\t \tif (key in event): \n\t \t \tdel event[key] \n\tif ('start_time' in event): \n\t \tevent['start_time'] = str(event['start_time'].replace(tzinfo=None)) \n\tif ('finish_time' in event): \n\t \tevent['finish_time'] = str(event['finish_time'].replace(tzinfo=None)) \n\treturn event\n", 
" \tresults = {True: 'enabled', False: 'disabled'} \n\tif (host_name == 'notimplemented'): \n\t \traise NotImplementedError() \n\telif (host_name == 'dummydest'): \n\t \traise exception.ComputeHostNotFound(host=host_name) \n\telif (host_name == 'service_not_available'): \n\t \traise exception.ComputeServiceUnavailable(host=host_name) \n\telif (host_name == 'host_c2'): \n\t \treturn results[(not enabled)] \n\telse: \n\t \treturn results[enabled]\n", 
" \tflavor = flavors.get_flavor_by_name('m1.tiny') \n\tnet_info = model.NetworkInfo([]) \n\tinfo_cache = objects.InstanceInfoCache(network_info=net_info) \n\tinst = objects.Instance(context=context, image_ref=uuids.fake_image_ref, reservation_id='r-fakeres', user_id=user_id, project_id=project_id, instance_type_id=flavor.id, flavor=flavor, old_flavor=None, new_flavor=None, system_metadata={}, ami_launch_index=0, root_gb=0, ephemeral_gb=0, info_cache=info_cache) \n\tif params: \n\t \tinst.update(params) \n\tinst.create() \n\treturn inst\n", 
" \tinst = {} \n\tinst['image_ref'] = 'cedef40a-ed67-4d10-800e-17455edce175' \n\tinst['reservation_id'] = 'r-fakeres' \n\tinst['user_id'] = kwargs.get('user_id', 'admin') \n\tinst['project_id'] = kwargs.get('project_id', 'fake') \n\tinst['instance_type_id'] = '1' \n\tif ('host' in kwargs): \n\t \tinst['host'] = kwargs.get('host') \n\tinst['vcpus'] = kwargs.get('vcpus', 1) \n\tinst['memory_mb'] = kwargs.get('memory_mb', 20) \n\tinst['root_gb'] = kwargs.get('root_gb', 30) \n\tinst['ephemeral_gb'] = kwargs.get('ephemeral_gb', 30) \n\tinst['vm_state'] = kwargs.get('vm_state', vm_states.ACTIVE) \n\tinst['power_state'] = kwargs.get('power_state', power_state.RUNNING) \n\tinst['task_state'] = kwargs.get('task_state', None) \n\tinst['availability_zone'] = kwargs.get('availability_zone', None) \n\tinst['ami_launch_index'] = 0 \n\tinst['launched_on'] = kwargs.get('launched_on', 'dummy') \n\treturn inst\n", 
" \treturn urllib.parse.quote(domain.replace('.', '%2E'))\n", 
" \tclass HTTPConnectionDecorator(object, ): \n\t \t'Decorator \tto \tmock \tthe \tHTTPConecction \tclass.\\n\\n \t \t \t \t \t \t \t \tWraps \tthe \treal \tHTTPConnection \tclass \tso \tthat \twhen \tyou \tinstantiate\\n \t \t \t \t \t \t \t \tthe \tclass \tyou \tmight \tinstead \tget \ta \tfake \tinstance.\\n \t \t \t \t \t \t \t \t' \n\t \tdef __init__(self, wrapped): \n\t \t \tself.wrapped = wrapped \n\t \tdef __call__(self, connection_host, *args, **kwargs): \n\t \t \tif (connection_host == host): \n\t \t \t \treturn FakeHttplibConnection(app, host) \n\t \t \telse: \n\t \t \t \treturn self.wrapped(connection_host, *args, **kwargs) \n\toldHTTPConnection = http_client.HTTPConnection \n\tnew_http_connection = HTTPConnectionDecorator(http_client.HTTPConnection) \n\thttp_client.HTTPConnection = new_http_connection \n\treturn oldHTTPConnection\n", 
" \tinstances = orig_func(*args, **kwargs) \n\tcontext = args[0] \n\tfake_device = objects.PciDevice.get_by_dev_addr(context, 1, 'a') \n\tdef _info_cache_for(instance): \n\t \tinfo_cache = dict(test_instance_info_cache.fake_info_cache, network_info=_get_fake_cache(), instance_uuid=instance['uuid']) \n\t \tif isinstance(instance, obj_base.NovaObject): \n\t \t \t_info_cache = objects.InstanceInfoCache(context) \n\t \t \tobjects.InstanceInfoCache._from_db_object(context, _info_cache, info_cache) \n\t \t \tinfo_cache = _info_cache \n\t \tinstance['info_cache'] = info_cache \n\tif isinstance(instances, (list, obj_base.ObjectListBase)): \n\t \tfor instance in instances: \n\t \t \t_info_cache_for(instance) \n\t \t \tfake_device.claim(instance.uuid) \n\t \t \tfake_device.allocate(instance) \n\telse: \n\t \t_info_cache_for(instances) \n\t \tfake_device.claim(instances.uuid) \n\t \tfake_device.allocate(instances) \n\treturn instances\n", 
" \tinstances = orig_func(*args, **kwargs) \n\tcontext = args[0] \n\tfake_device = objects.PciDevice.get_by_dev_addr(context, 1, 'a') \n\tdef _info_cache_for(instance): \n\t \tinfo_cache = dict(test_instance_info_cache.fake_info_cache, network_info=_get_fake_cache(), instance_uuid=instance['uuid']) \n\t \tif isinstance(instance, obj_base.NovaObject): \n\t \t \t_info_cache = objects.InstanceInfoCache(context) \n\t \t \tobjects.InstanceInfoCache._from_db_object(context, _info_cache, info_cache) \n\t \t \tinfo_cache = _info_cache \n\t \tinstance['info_cache'] = info_cache \n\tif isinstance(instances, (list, obj_base.ObjectListBase)): \n\t \tfor instance in instances: \n\t \t \t_info_cache_for(instance) \n\t \t \tfake_device.claim(instance.uuid) \n\t \t \tfake_device.allocate(instance) \n\telse: \n\t \t_info_cache_for(instances) \n\t \tfake_device.claim(instances.uuid) \n\t \tfake_device.allocate(instances) \n\treturn instances\n", 
" \tif (template_name is None): \n\t \ttemplate_name = ('403.html', 'authority/403.html') \n\tcontext = {'request_path': request.path} \n\tif extra_context: \n\t \tcontext.update(extra_context) \n\treturn HttpResponseForbidden(loader.render_to_string(template_name, context, context_instance=RequestContext(request)))\n", 
" \t@functools.wraps(function) \n\tdef decorated_function(self, *args, **kwargs): \n\t \t@contextlib.contextmanager \n\t \tdef fake_vdi_attached(*args, **kwargs): \n\t \t \tfake_dev = 'fakedev' \n\t \t \t(yield fake_dev) \n\t \tdef fake_image_download(*args, **kwargs): \n\t \t \tpass \n\t \torig_vdi_attached = vm_utils.vdi_attached \n\t \torig_image_download = fake_image._FakeImageService.download \n\t \ttry: \n\t \t \tvm_utils.vdi_attached = fake_vdi_attached \n\t \t \tfake_image._FakeImageService.download = fake_image_download \n\t \t \treturn function(self, *args, **kwargs) \n\t \tfinally: \n\t \t \tfake_image._FakeImageService.download = orig_image_download \n\t \t \tvm_utils.vdi_attached = orig_vdi_attached \n\treturn decorated_function\n", 
" \treturn fake.FakeVim()\n", 
" \treturn isinstance(module, fake.FakeVim)\n", 
" \ttest.stub_out('nova.virt.vmwareapi.network_util.get_network_with_the_name', fake.fake_get_network) \n\ttest.stub_out('nova.virt.vmwareapi.images.upload_image_stream_optimized', fake.fake_upload_image) \n\ttest.stub_out('nova.virt.vmwareapi.images.fetch_image', fake.fake_fetch_image) \n\ttest.stub_out('nova.virt.vmwareapi.driver.VMwareAPISession.vim', fake_vim_prop) \n\ttest.stub_out('nova.virt.vmwareapi.driver.VMwareAPISession._is_vim_object', fake_is_vim_object) \n\tif CONF.use_neutron: \n\t \ttest.stub_out('nova.network.neutronv2.api.API.update_instance_vnic_index', (lambda *args, **kwargs: None))\n", 
" \tdef _create_instance_type(**updates): \n\t \tinstance_type = {'id': 2, 'name': 'm1.tiny', 'memory_mb': 512, 'vcpus': 1, 'vcpu_weight': None, 'root_gb': 0, 'ephemeral_gb': 10, 'flavorid': 1, 'rxtx_factor': 1.0, 'swap': 0, 'deleted_at': None, 'created_at': datetime.datetime(2014, 8, 8, 0, 0, 0), 'updated_at': None, 'deleted': False, 'disabled': False, 'is_public': True, 'extra_specs': {}} \n\t \tif updates: \n\t \t \tinstance_type.update(updates) \n\t \treturn instance_type \n\tINSTANCE_TYPES = {'m1.tiny': _create_instance_type(id=2, name='m1.tiny', memory_mb=512, vcpus=1, vcpu_weight=None, root_gb=0, ephemeral_gb=10, flavorid=1, rxtx_factor=1.0, swap=0), 'm1.small': _create_instance_type(id=5, name='m1.small', memory_mb=2048, vcpus=1, vcpu_weight=None, root_gb=20, ephemeral_gb=0, flavorid=2, rxtx_factor=1.0, swap=1024), 'm1.medium': _create_instance_type(id=1, name='m1.medium', memory_mb=4096, vcpus=2, vcpu_weight=None, root_gb=40, ephemeral_gb=40, flavorid=3, rxtx_factor=1.0, swap=0), 'm1.large': _create_instance_type(id=3, name='m1.large', memory_mb=8192, vcpus=4, vcpu_weight=10, root_gb=80, ephemeral_gb=80, flavorid=4, rxtx_factor=1.0, swap=0), 'm1.xlarge': _create_instance_type(id=4, name='m1.xlarge', memory_mb=16384, vcpus=8, vcpu_weight=None, root_gb=160, ephemeral_gb=160, flavorid=5, rxtx_factor=1.0, swap=0)} \n\tfixed_ip_fields = {'address': '10.0.0.3', 'address_v6': 'fe80::a00:3', 'network_id': 'fake_flat'} \n\tdef fake_flavor_get_all(*a, **k): \n\t \treturn INSTANCE_TYPES.values() \n\t@classmethod \n\tdef fake_flavor_get_by_name(cls, context, name): \n\t \treturn INSTANCE_TYPES[name] \n\t@classmethod \n\tdef fake_flavor_get(cls, context, id): \n\t \tfor (name, inst_type) in six.iteritems(INSTANCE_TYPES): \n\t \t \tif (str(inst_type['id']) == str(id)): \n\t \t \t \treturn inst_type \n\t \treturn None \n\tdef fake_fixed_ip_get_by_instance(context, instance_id): \n\t \treturn [FakeModel(fixed_ip_fields)] \n\tfuncs = {'nova.objects.flavor._flavor_get_all_from_db': fake_flavor_get_all, 'nova.objects.Flavor._flavor_get_by_name_from_db': fake_flavor_get_by_name, 'nova.objects.Flavor._flavor_get_from_db': fake_flavor_get, 'nova.db.api.fixed_ip_get_by_instance': fake_fixed_ip_get_by_instance} \n\tstub_out(test, funcs)\n", 
" \tdef wrapped_func(*args, **kwarg): \n\t \tCALLED_FUNCTION.append(name) \n\t \treturn function(*args, **kwarg) \n\treturn wrapped_func\n", 
" \tdef _create_instance_type(**updates): \n\t \tinstance_type = {'id': 2, 'name': 'm1.tiny', 'memory_mb': 512, 'vcpus': 1, 'vcpu_weight': None, 'root_gb': 0, 'ephemeral_gb': 10, 'flavorid': 1, 'rxtx_factor': 1.0, 'swap': 0, 'deleted_at': None, 'created_at': datetime.datetime(2014, 8, 8, 0, 0, 0), 'updated_at': None, 'deleted': False, 'disabled': False, 'is_public': True, 'extra_specs': {}} \n\t \tif updates: \n\t \t \tinstance_type.update(updates) \n\t \treturn instance_type \n\tINSTANCE_TYPES = {'m1.tiny': _create_instance_type(id=2, name='m1.tiny', memory_mb=512, vcpus=1, vcpu_weight=None, root_gb=0, ephemeral_gb=10, flavorid=1, rxtx_factor=1.0, swap=0), 'm1.small': _create_instance_type(id=5, name='m1.small', memory_mb=2048, vcpus=1, vcpu_weight=None, root_gb=20, ephemeral_gb=0, flavorid=2, rxtx_factor=1.0, swap=1024), 'm1.medium': _create_instance_type(id=1, name='m1.medium', memory_mb=4096, vcpus=2, vcpu_weight=None, root_gb=40, ephemeral_gb=40, flavorid=3, rxtx_factor=1.0, swap=0), 'm1.large': _create_instance_type(id=3, name='m1.large', memory_mb=8192, vcpus=4, vcpu_weight=10, root_gb=80, ephemeral_gb=80, flavorid=4, rxtx_factor=1.0, swap=0), 'm1.xlarge': _create_instance_type(id=4, name='m1.xlarge', memory_mb=16384, vcpus=8, vcpu_weight=None, root_gb=160, ephemeral_gb=160, flavorid=5, rxtx_factor=1.0, swap=0)} \n\tfixed_ip_fields = {'address': '10.0.0.3', 'address_v6': 'fe80::a00:3', 'network_id': 'fake_flat'} \n\tdef fake_flavor_get_all(*a, **k): \n\t \treturn INSTANCE_TYPES.values() \n\t@classmethod \n\tdef fake_flavor_get_by_name(cls, context, name): \n\t \treturn INSTANCE_TYPES[name] \n\t@classmethod \n\tdef fake_flavor_get(cls, context, id): \n\t \tfor (name, inst_type) in six.iteritems(INSTANCE_TYPES): \n\t \t \tif (str(inst_type['id']) == str(id)): \n\t \t \t \treturn inst_type \n\t \treturn None \n\tdef fake_fixed_ip_get_by_instance(context, instance_id): \n\t \treturn [FakeModel(fixed_ip_fields)] \n\tfuncs = {'nova.objects.flavor._flavor_get_all_from_db': fake_flavor_get_all, 'nova.objects.Flavor._flavor_get_by_name_from_db': fake_flavor_get_by_name, 'nova.objects.Flavor._flavor_get_from_db': fake_flavor_get, 'nova.db.api.fixed_ip_get_by_instance': fake_fixed_ip_get_by_instance} \n\tstub_out(test, funcs)\n", 
" \tstubs.Set(session.XenAPISession, '_create_session', (lambda s, url: cls(url, **opt_args))) \n\tstubs.Set(session.XenAPISession, '_get_product_version_and_brand', (lambda s: (product_version, product_brand))) \n\tstubs.Set(session.XenAPISession, '_get_platform_version', (lambda s: platform_version))\n", 
" \tdef f(*args): \n\t \treturn False \n\tstubs.Set(vm_utils, '_determine_is_pv_objectstore', f)\n", 
" \treturn True\n", 
" \tdef f(_1, _2, _3, _4): \n\t \traise Exception('Test \tException \traised \tby \tfake \tlookup_image') \n\tstubs.Set(vm_utils, 'lookup_image', f)\n", 
" \tdef _fake_fetch_disk_image(context, session, instance, name_label, image, image_type): \n\t \tif raise_failure: \n\t \t \traise XenAPI.Failure('Test \tException \traised \tby \tfake \tfetch_image_glance_disk') \n\t \telif (image_type == vm_utils.ImageType.KERNEL): \n\t \t \tfilename = 'kernel' \n\t \telif (image_type == vm_utils.ImageType.RAMDISK): \n\t \t \tfilename = 'ramdisk' \n\t \telse: \n\t \t \tfilename = 'unknown' \n\t \tvdi_type = vm_utils.ImageType.to_string(image_type) \n\t \treturn {vdi_type: dict(uuid=None, file=filename)} \n\tstubs.Set(vm_utils, '_fetch_disk_image', _fake_fetch_disk_image)\n", 
" \tdef f(*args): \n\t \traise XenAPI.Failure('Test \tException \traised \tby \tfake \tcreate_vm') \n\tstubs.Set(vm_utils, 'create_vm', f)\n", 
" \tdef f(*args): \n\t \traise XenAPI.Failure('Test \tException \traised \tby \tfake \t_attach_disks') \n\tstubs.Set(vmops.VMOps, '_attach_disks', f)\n", 
" \tglobal _fake_execute_repliers \n\t_fake_execute_repliers = repliers\n", 
" \treturn ('', '')\n", 
" \tglobal _fake_execute_repliers \n\tprocess_input = kwargs.get('process_input', None) \n\tcheck_exit_code = kwargs.get('check_exit_code', 0) \n\tdelay_on_retry = kwargs.get('delay_on_retry', True) \n\tattempts = kwargs.get('attempts', 1) \n\trun_as_root = kwargs.get('run_as_root', False) \n\tcmd_str = ' \t'.join((str(part) for part in cmd_parts)) \n\tLOG.debug('Faking \texecution \tof \tcmd \t(subprocess): \t%s', cmd_str) \n\t_fake_execute_log.append(cmd_str) \n\treply_handler = fake_execute_default_reply_handler \n\tfor fake_replier in _fake_execute_repliers: \n\t \tif re.match(fake_replier[0], cmd_str): \n\t \t \treply_handler = fake_replier[1] \n\t \t \tLOG.debug('Faked \tcommand \tmatched \t%s', fake_replier[0]) \n\t \t \tbreak \n\tif isinstance(reply_handler, six.string_types): \n\t \treply = (reply_handler, '') \n\telse: \n\t \ttry: \n\t \t \treply = reply_handler(cmd_parts, process_input=process_input, delay_on_retry=delay_on_retry, attempts=attempts, run_as_root=run_as_root, check_exit_code=check_exit_code) \n\t \texcept processutils.ProcessExecutionError as e: \n\t \t \tLOG.debug('Faked \tcommand \traised \tan \texception \t%s', e) \n\t \t \traise \n\tLOG.debug(\"Reply \tto \tfaked \tcommand \tis \tstdout='%(stdout)s' \tstderr='%(stderr)s'\", {'stdout': reply[0], 'stderr': reply[1]}) \n\tgreenthread.sleep(0) \n\treturn reply\n", 
" \tfor (module, func) in funcs.items(): \n\t \ttest.stub_out(module, func)\n", 
" \tdef _create_instance_type(**updates): \n\t \tinstance_type = {'id': 2, 'name': 'm1.tiny', 'memory_mb': 512, 'vcpus': 1, 'vcpu_weight': None, 'root_gb': 0, 'ephemeral_gb': 10, 'flavorid': 1, 'rxtx_factor': 1.0, 'swap': 0, 'deleted_at': None, 'created_at': datetime.datetime(2014, 8, 8, 0, 0, 0), 'updated_at': None, 'deleted': False, 'disabled': False, 'is_public': True, 'extra_specs': {}} \n\t \tif updates: \n\t \t \tinstance_type.update(updates) \n\t \treturn instance_type \n\tINSTANCE_TYPES = {'m1.tiny': _create_instance_type(id=2, name='m1.tiny', memory_mb=512, vcpus=1, vcpu_weight=None, root_gb=0, ephemeral_gb=10, flavorid=1, rxtx_factor=1.0, swap=0), 'm1.small': _create_instance_type(id=5, name='m1.small', memory_mb=2048, vcpus=1, vcpu_weight=None, root_gb=20, ephemeral_gb=0, flavorid=2, rxtx_factor=1.0, swap=1024), 'm1.medium': _create_instance_type(id=1, name='m1.medium', memory_mb=4096, vcpus=2, vcpu_weight=None, root_gb=40, ephemeral_gb=40, flavorid=3, rxtx_factor=1.0, swap=0), 'm1.large': _create_instance_type(id=3, name='m1.large', memory_mb=8192, vcpus=4, vcpu_weight=10, root_gb=80, ephemeral_gb=80, flavorid=4, rxtx_factor=1.0, swap=0), 'm1.xlarge': _create_instance_type(id=4, name='m1.xlarge', memory_mb=16384, vcpus=8, vcpu_weight=None, root_gb=160, ephemeral_gb=160, flavorid=5, rxtx_factor=1.0, swap=0)} \n\tfixed_ip_fields = {'address': '10.0.0.3', 'address_v6': 'fe80::a00:3', 'network_id': 'fake_flat'} \n\tdef fake_flavor_get_all(*a, **k): \n\t \treturn INSTANCE_TYPES.values() \n\t@classmethod \n\tdef fake_flavor_get_by_name(cls, context, name): \n\t \treturn INSTANCE_TYPES[name] \n\t@classmethod \n\tdef fake_flavor_get(cls, context, id): \n\t \tfor (name, inst_type) in six.iteritems(INSTANCE_TYPES): \n\t \t \tif (str(inst_type['id']) == str(id)): \n\t \t \t \treturn inst_type \n\t \treturn None \n\tdef fake_fixed_ip_get_by_instance(context, instance_id): \n\t \treturn [FakeModel(fixed_ip_fields)] \n\tfuncs = {'nova.objects.flavor._flavor_get_all_from_db': fake_flavor_get_all, 'nova.objects.Flavor._flavor_get_by_name_from_db': fake_flavor_get_by_name, 'nova.objects.Flavor._flavor_get_from_db': fake_flavor_get, 'nova.db.api.fixed_ip_get_by_instance': fake_fixed_ip_get_by_instance} \n\tstub_out(test, funcs)\n", 
" \treturn FakeLDAP()\n", 
" \tinner = query[1:(-1)] \n\tif inner.startswith('&'): \n\t \t(l, r) = _paren_groups(inner[1:]) \n\t \treturn (_match_query(l, attrs) and _match_query(r, attrs)) \n\tif inner.startswith('|'): \n\t \t(l, r) = _paren_groups(inner[1:]) \n\t \treturn (_match_query(l, attrs) or _match_query(r, attrs)) \n\tif inner.startswith('!'): \n\t \treturn (not _match_query(query[2:(-1)], attrs)) \n\t(k, _sep, v) = inner.partition('=') \n\treturn _match(k, v, attrs)\n", 
" \tcount = 0 \n\tstart = 0 \n\tresult = [] \n\tfor pos in range(len(source)): \n\t \tif (source[pos] == '('): \n\t \t \tif (count == 0): \n\t \t \t \tstart = pos \n\t \t \tcount += 1 \n\t \tif (source[pos] == ')'): \n\t \t \tcount -= 1 \n\t \t \tif (count == 0): \n\t \t \t \tresult.append(source[start:(pos + 1)]) \n\treturn result\n", 
" \tif (key not in attrs): \n\t \treturn False \n\tif (value == '*'): \n\t \treturn True \n\tif (key != 'objectclass'): \n\t \treturn (value in attrs[key]) \n\tvalues = _subs(value) \n\tfor v in values: \n\t \tif (v in attrs[key]): \n\t \t \treturn True \n\treturn False\n", 
" \tsubs = {'groupOfNames': ['novaProject']} \n\tif (value in subs): \n\t \treturn ([value] + subs[value]) \n\treturn [value]\n", 
" \treturn [str(x) for x in jsonutils.loads(encoded)]\n", 
" \treturn jsonutils.dumps(list(unencoded))\n", 
" \tinstances = orig_func(*args, **kwargs) \n\tcontext = args[0] \n\tfake_device = objects.PciDevice.get_by_dev_addr(context, 1, 'a') \n\tdef _info_cache_for(instance): \n\t \tinfo_cache = dict(test_instance_info_cache.fake_info_cache, network_info=_get_fake_cache(), instance_uuid=instance['uuid']) \n\t \tif isinstance(instance, obj_base.NovaObject): \n\t \t \t_info_cache = objects.InstanceInfoCache(context) \n\t \t \tobjects.InstanceInfoCache._from_db_object(context, _info_cache, info_cache) \n\t \t \tinfo_cache = _info_cache \n\t \tinstance['info_cache'] = info_cache \n\tif isinstance(instances, (list, obj_base.ObjectListBase)): \n\t \tfor instance in instances: \n\t \t \t_info_cache_for(instance) \n\t \t \tfake_device.claim(instance.uuid) \n\t \t \tfake_device.allocate(instance) \n\telse: \n\t \t_info_cache_for(instances) \n\t \tfake_device.claim(instances.uuid) \n\t \tfake_device.allocate(instances) \n\treturn instances\n", 
" \treturn ''.join((random.choice((string.ascii_uppercase + string.digits)) for _x in range(length)))\n", 
" \treturn ''.join((random.choice(string.digits) for _x in range(length)))\n", 
" \twhile True: \n\t \tif numeric: \n\t \t \tcandidate = (prefix + generate_random_numeric(8)) \n\t \telse: \n\t \t \tcandidate = (prefix + generate_random_alphanumeric(8)) \n\t \tif (candidate not in items): \n\t \t \treturn candidate \n\t \tLOG.debug(('Random \tcollision \ton \t%s' % candidate))\n", 
" \tif (key in data): \n\t \tdata[key].add(value) \n\telse: \n\t \tdata[key] = set([value])\n", 
" \treturn Table(name, MetaData(bind=session.bind), autoload=True)\n", 
" \tdef wrapped_func(self, *args, **kwargs): \n\t \ttry: \n\t \t \treturn f(self, *args, **kwargs) \n\t \texcept NotImplementedError: \n\t \t \tframe = traceback.extract_tb(sys.exc_info()[2])[(-1)] \n\t \t \tLOG.error(('%(driver)s \tdoes \tnot \timplement \t%(method)s \trequired \tfor \ttest \t%(test)s' % {'driver': type(self.connection), 'method': frame[2], 'test': f.__name__})) \n\twrapped_func.__name__ = f.__name__ \n\twrapped_func.__doc__ = f.__doc__ \n\treturn wrapped_func\n", 
" \tenabled_services = objects.ServiceList.get_all(context, disabled=False, set_zones=True) \n\tavailable_zones = [] \n\tfor (zone, host) in [(service['availability_zone'], service['host']) for service in enabled_services]: \n\t \tif ((not with_hosts) and (zone not in available_zones)): \n\t \t \tavailable_zones.append(zone) \n\t \telif with_hosts: \n\t \t \t_available_zones = dict(available_zones) \n\t \t \tzone_hosts = _available_zones.setdefault(zone, set()) \n\t \t \tzone_hosts.add(host) \n\t \t \tavailable_zones = list(_available_zones.items()) \n\tif (not get_only_available): \n\t \tdisabled_services = objects.ServiceList.get_all(context, disabled=True, set_zones=True) \n\t \tnot_available_zones = [] \n\t \tazs = (available_zones if (not with_hosts) else dict(available_zones)) \n\t \tzones = [(service['availability_zone'], service['host']) for service in disabled_services if (service['availability_zone'] not in azs)] \n\t \tfor (zone, host) in zones: \n\t \t \tif ((not with_hosts) and (zone not in not_available_zones)): \n\t \t \t \tnot_available_zones.append(zone) \n\t \t \telif with_hosts: \n\t \t \t \t_not_available_zones = dict(not_available_zones) \n\t \t \t \tzone_hosts = _not_available_zones.setdefault(zone, set()) \n\t \t \t \tzone_hosts.add(host) \n\t \t \t \tnot_available_zones = list(_not_available_zones.items()) \n\t \treturn (available_zones, not_available_zones) \n\telse: \n\t \treturn available_zones\n", 
" \tdef _get_paginated_instances(context, filters, shuffle, limit, marker): \n\t \tinstances = objects.InstanceList.get_by_filters(context, filters, sort_key='deleted', sort_dir='asc', limit=limit, marker=marker) \n\t \tif (len(instances) > 0): \n\t \t \tmarker = instances[(-1)]['uuid'] \n\t \t \tinstances = list(instances) \n\t \t \tif shuffle: \n\t \t \t \trandom.shuffle(instances) \n\t \treturn (instances, marker) \n\tfilters = {} \n\tif (updated_since is not None): \n\t \tfilters['changes-since'] = updated_since \n\tif (project_id is not None): \n\t \tfilters['project_id'] = project_id \n\tif (not deleted): \n\t \tfilters['deleted'] = False \n\tlimit = CONF.cells.instance_update_sync_database_limit \n\tmarker = None \n\tinstances = [] \n\twhile True: \n\t \tif (not instances): \n\t \t \t(instances, marker) = _get_paginated_instances(context, filters, shuffle, limit, marker) \n\t \tif (not instances): \n\t \t \tbreak \n\t \tinstance = instances.pop(0) \n\t \tif uuids_only: \n\t \t \t(yield instance.uuid) \n\t \telse: \n\t \t \t(yield instance)\n", 
" \tif (cell_name is None): \n\t \treturn item \n\treturn ((cell_name + _CELL_ITEM_SEP) + str(item))\n", 
" \tresult = cell_and_item.rsplit(_CELL_ITEM_SEP, 1) \n\tif (len(result) == 1): \n\t \treturn (None, cell_and_item) \n\telse: \n\t \treturn result\n", 
" \tcompute_proxy = ComputeNodeProxy(compute_node, cell_name) \n\treturn compute_proxy\n", 
" \tservice_proxy = ServiceProxy(service, cell_name) \n\treturn service_proxy\n", 
" \ttask_log['id'] = cell_with_item(cell_name, task_log['id']) \n\ttask_log['host'] = cell_with_item(cell_name, task_log['host'])\n", 
" \t@functools.wraps(f) \n\tdef wrapper(self, *args, **kwargs): \n\t \tself._cell_data_sync() \n\t \treturn f(self, *args, **kwargs) \n\treturn wrapper\n", 
" \tpath_parts = path.split(_PATH_CELL_SEP) \n\tpath_parts.reverse() \n\treturn _PATH_CELL_SEP.join(path_parts)\n", 
" \tpath = _reverse_path(routing_path) \n\tif ((not neighbor_only) or (len(path) == 1)): \n\t \treturn path \n\treturn _PATH_CELL_SEP.join(path.split(_PATH_CELL_SEP)[:2])\n", 
" \td = {} \n\td['id'] = vol.id \n\td['status'] = vol.status \n\td['size'] = vol.size \n\td['availability_zone'] = vol.availability_zone \n\td['created_at'] = vol.created_at \n\td['attach_time'] = '' \n\td['mountpoint'] = '' \n\td['multiattach'] = getattr(vol, 'multiattach', False) \n\tif vol.attachments: \n\t \td['attachments'] = collections.OrderedDict() \n\t \tfor attachment in vol.attachments: \n\t \t \ta = {attachment['server_id']: {'attachment_id': attachment.get('attachment_id'), 'mountpoint': attachment.get('device')}} \n\t \t \td['attachments'].update(a.items()) \n\t \td['attach_status'] = 'attached' \n\telse: \n\t \td['attach_status'] = 'detached' \n\td['display_name'] = vol.name \n\td['display_description'] = vol.description \n\td['volume_type_id'] = vol.volume_type \n\td['snapshot_id'] = vol.snapshot_id \n\td['bootable'] = strutils.bool_from_string(vol.bootable) \n\td['volume_metadata'] = {} \n\tfor (key, value) in vol.metadata.items(): \n\t \td['volume_metadata'][key] = value \n\tif hasattr(vol, 'volume_image_metadata'): \n\t \td['volume_image_metadata'] = copy.deepcopy(vol.volume_image_metadata) \n\treturn d\n", 
" \td = {} \n\td['id'] = snapshot.id \n\td['status'] = snapshot.status \n\td['progress'] = snapshot.progress \n\td['size'] = snapshot.size \n\td['created_at'] = snapshot.created_at \n\td['display_name'] = snapshot.name \n\td['display_description'] = snapshot.description \n\td['volume_id'] = snapshot.volume_id \n\td['project_id'] = snapshot.project_id \n\td['volume_size'] = snapshot.size \n\treturn d\n", 
" \tproperty_name_to_values = {} \n\tfor entity in entities: \n\t \tfor (property_name, value) in entity.iteritems(): \n\t \t \tproperty_name_to_values.setdefault(property_name, []).append(value) \n\treturn property_name_to_values\n", 
" \tpath = src.get(attrib) \n\tif (not path): \n\t \treturn [src] \n\tpath = encoder.MaybeNarrowPath(path) \n\tpathlist = glob.glob(path) \n\tif (not pathlist): \n\t \treturn [src] \n\tif (type(src) != types.DictionaryType): \n\t \ttmp = {} \n\t \tfor key in src.keys(): \n\t \t \ttmp[key] = src[key] \n\t \tsrc = tmp \n\tretval = [] \n\tfor path in pathlist: \n\t \tdst = src.copy() \n\t \tdst[attrib] = path \n\t \tretval.append(dst) \n\treturn retval\n", 
" \t(a, axis) = _chk_asarray(a, axis) \n\ts = np.sum(a, axis) \n\tif (not np.isscalar(s)): \n\t \treturn (s.astype(float) * s) \n\telse: \n\t \treturn (float(s) * s)\n", 
" \tn = (float(sum(v.values())) or 1.0) \n\ts = dict.__setitem__ \n\tfor f in v: \n\t \ts(v, f, (v[f] / n)) \n\treturn v\n", 
" \timport networkx as nx \n\timport os.path as op \n\tiflogger.info(u'Creating \taverage \tnetwork \tfor \tgroup: \t{grp}'.format(grp=group_id)) \n\tmatlab_network_list = [] \n\tif (len(in_files) == 1): \n\t \tavg_ntwk = read_unknown_ntwk(in_files[0]) \n\telse: \n\t \tcount_to_keep_edge = np.round((len(in_files) / 2.0)) \n\t \tiflogger.info(u'Number \tof \tnetworks: \t{L}, \tan \tedge \tmust \toccur \tin \tat \tleast \t{c} \tto \tremain \tin \tthe \taverage \tnetwork'.format(L=len(in_files), c=count_to_keep_edge)) \n\t \tntwk_res_file = read_unknown_ntwk(ntwk_res_file) \n\t \tiflogger.info(u'{n} \tNodes \tfound \tin \tnetwork \tresolution \tfile'.format(n=ntwk_res_file.number_of_nodes())) \n\t \tntwk = remove_all_edges(ntwk_res_file) \n\t \tcounting_ntwk = ntwk.copy() \n\t \tfor (index, subject) in enumerate(in_files): \n\t \t \ttmp = nx.read_gpickle(subject) \n\t \t \tiflogger.info(u'File \t{s} \thas \t{n} \tedges'.format(s=subject, n=tmp.number_of_edges())) \n\t \t \tedges = tmp.edges_iter() \n\t \t \tfor edge in edges: \n\t \t \t \tdata = {} \n\t \t \t \tdata = tmp.edge[edge[0]][edge[1]] \n\t \t \t \tdata[u'count'] = 1 \n\t \t \t \tif ntwk.has_edge(edge[0], edge[1]): \n\t \t \t \t \tcurrent = {} \n\t \t \t \t \tcurrent = ntwk.edge[edge[0]][edge[1]] \n\t \t \t \t \tdata = add_dicts_by_key(current, data) \n\t \t \t \tntwk.add_edge(edge[0], edge[1], data) \n\t \t \tnodes = tmp.nodes_iter() \n\t \t \tfor node in nodes: \n\t \t \t \tdata = {} \n\t \t \t \tdata = ntwk.node[node] \n\t \t \t \tif (u'value' in tmp.node[node]): \n\t \t \t \t \tdata[u'value'] = (data[u'value'] + tmp.node[node][u'value']) \n\t \t \t \tntwk.add_node(node, data) \n\t \tnodes = ntwk.nodes_iter() \n\t \tedges = ntwk.edges_iter() \n\t \tiflogger.info(u'Total \tnetwork \thas \t{n} \tedges'.format(n=ntwk.number_of_edges())) \n\t \tavg_ntwk = nx.Graph() \n\t \tnewdata = {} \n\t \tfor node in nodes: \n\t \t \tdata = ntwk.node[node] \n\t \t \tnewdata = data \n\t \t \tif (u'value' in data): \n\t \t \t \tnewdata[u'value'] = (data[u'value'] / len(in_files)) \n\t \t \t \tntwk.node[node][u'value'] = newdata \n\t \t \tavg_ntwk.add_node(node, newdata) \n\t \tedge_dict = {} \n\t \tedge_dict[u'count'] = np.zeros((avg_ntwk.number_of_nodes(), avg_ntwk.number_of_nodes())) \n\t \tfor edge in edges: \n\t \t \tdata = ntwk.edge[edge[0]][edge[1]] \n\t \t \tif (ntwk.edge[edge[0]][edge[1]][u'count'] >= count_to_keep_edge): \n\t \t \t \tfor key in list(data.keys()): \n\t \t \t \t \tif (not (key == u'count')): \n\t \t \t \t \t \tdata[key] = (data[key] / len(in_files)) \n\t \t \t \tntwk.edge[edge[0]][edge[1]] = data \n\t \t \t \tavg_ntwk.add_edge(edge[0], edge[1], data) \n\t \t \tedge_dict[u'count'][(edge[0] - 1)][(edge[1] - 1)] = ntwk.edge[edge[0]][edge[1]][u'count'] \n\t \tiflogger.info(u'After \tthresholding, \tthe \taverage \tnetwork \thas \thas \t{n} \tedges'.format(n=avg_ntwk.number_of_edges())) \n\t \tavg_edges = avg_ntwk.edges_iter() \n\t \tfor edge in avg_edges: \n\t \t \tdata = avg_ntwk.edge[edge[0]][edge[1]] \n\t \t \tfor key in list(data.keys()): \n\t \t \t \tif (not (key == u'count')): \n\t \t \t \t \tedge_dict[key] = np.zeros((avg_ntwk.number_of_nodes(), avg_ntwk.number_of_nodes())) \n\t \t \t \t \tedge_dict[key][(edge[0] - 1)][(edge[1] - 1)] = data[key] \n\t \tfor key in list(edge_dict.keys()): \n\t \t \ttmp = {} \n\t \t \tnetwork_name = (((group_id + u'_') + key) + u'_average.mat') \n\t \t \tmatlab_network_list.append(op.abspath(network_name)) \n\t \t \ttmp[key] = edge_dict[key] \n\t \t \tsio.savemat(op.abspath(network_name), tmp) \n\t \t \tiflogger.info(u'Saving \taverage \tnetwork \tfor \tkey: \t{k} \tas \t{out}'.format(k=key, out=op.abspath(network_name))) \n\tnetwork_name = (group_id + u'_average.pck') \n\tnx.write_gpickle(avg_ntwk, op.abspath(network_name)) \n\tiflogger.info(u'Saving \taverage \tnetwork \tas \t{out}'.format(out=op.abspath(network_name))) \n\tavg_ntwk = fix_keys_for_gexf(avg_ntwk) \n\tnetwork_name = (group_id + u'_average.gexf') \n\tnx.write_gexf(avg_ntwk, op.abspath(network_name)) \n\tiflogger.info(u'Saving \taverage \tnetwork \tas \t{out}'.format(out=op.abspath(network_name))) \n\treturn (network_name, matlab_network_list)\n", 
" \tassert (len(inputs) == len(targets)) \n\tn_loads = ((batch_size * stride) + (seq_length - stride)) \n\tfor start_idx in range(0, ((len(inputs) - n_loads) + 1), (batch_size * stride)): \n\t \tseq_inputs = np.zeros(((batch_size, seq_length) + inputs.shape[1:]), dtype=inputs.dtype) \n\t \tseq_targets = np.zeros(((batch_size, seq_length) + targets.shape[1:]), dtype=targets.dtype) \n\t \tfor b_idx in xrange(batch_size): \n\t \t \tstart_seq_idx = (start_idx + (b_idx * stride)) \n\t \t \tend_seq_idx = (start_seq_idx + seq_length) \n\t \t \tseq_inputs[b_idx] = inputs[start_seq_idx:end_seq_idx] \n\t \t \tseq_targets[b_idx] = targets[start_seq_idx:end_seq_idx] \n\t \tflatten_inputs = seq_inputs.reshape((((-1),) + inputs.shape[1:])) \n\t \tflatten_targets = seq_targets.reshape((((-1),) + targets.shape[1:])) \n\t \t(yield (flatten_inputs, flatten_targets))\n", 
" \tresult = [] \n\tfor elt in lst: \n\t \tif isinstance(elt, cls): \n\t \t \tresult.extend(elt) \n\t \telse: \n\t \t \tresult.append(elt) \n\treturn result\n", 
" \tgshared = [theano.shared((p.get_value() * 0.0), name=('%s_grad' % k)) for (k, p) in tparams.items()] \n\tgsup = [(gs, g) for (gs, g) in zip(gshared, grads)] \n\tf_grad_shared = theano.function([x, mask, y], cost, updates=gsup, name='sgd_f_grad_shared') \n\tpup = [(p, (p - (lr * g))) for (p, g) in zip(tparams.values(), gshared)] \n\tf_update = theano.function([lr], [], updates=pup, name='sgd_f_update') \n\treturn (f_grad_shared, f_update)\n", 
" \tiflogger.info(u'Computing \tmeasures \twhich \treturn \ta \tdictionary:') \n\tmeasures = {} \n\tiflogger.info(u'...Computing \trich \tclub \tcoefficient...') \n\tmeasures[u'rich_club_coef'] = nx.rich_club_coefficient(ntwk) \n\treturn measures\n", 
" \tgshared = [theano.shared((p.get_value() * 0.0), name=('%s_grad' % k)) for (k, p) in tparams.items()] \n\tgsup = [(gs, g) for (gs, g) in zip(gshared, grads)] \n\tf_grad_shared = theano.function([x, mask, y], cost, updates=gsup, name='sgd_f_grad_shared') \n\tpup = [(p, (p - (lr * g))) for (p, g) in zip(tparams.values(), gshared)] \n\tf_update = theano.function([lr], [], updates=pup, name='sgd_f_update') \n\treturn (f_grad_shared, f_update)\n", 
" \treturn call_talib_with_ds(ds, count, talib.MOM, timeperiod)\n", 
" \tgshared = [theano.shared((p.get_value() * 0.0), name=('%s_grad' % k)) for (k, p) in tparams.items()] \n\tgsup = [(gs, g) for (gs, g) in zip(gshared, grads)] \n\tf_grad_shared = theano.function([x, mask, y], cost, updates=gsup, name='sgd_f_grad_shared') \n\tpup = [(p, (p - (lr * g))) for (p, g) in zip(tparams.values(), gshared)] \n\tf_update = theano.function([lr], [], updates=pup, name='sgd_f_update') \n\treturn (f_grad_shared, f_update)\n", 
" \tif ((config is None) and (state is None)): \n\t \traise ValueError('adagrad \trequires \ta \tdictionary \tto \tretain \tstate \tbetween \titerations') \n\tstate = (state if (state is not None) else config) \n\tlr = config.get('learningRate', 0.001) \n\tlrd = config.get('learningRateDecay', 0) \n\twd = config.get('weightDecay', 0) \n\tstate['evalCounter'] = state.get('evalCounter', 0) \n\t(fx, dfdx) = opfunc(x) \n\tif (wd != 0): \n\t \tdfdx.add_(wd, x) \n\tclr = (lr / (1 + (state['evalCounter'] * lrd))) \n\tif (not ('paramVariance' in state)): \n\t \tstate['paramVariance'] = x.new().resize_as_(dfdx).zero_() \n\t \tstate['paramStd'] = x.new().resize_as_(dfdx) \n\tstate['paramVariance'].addcmul_(1, dfdx, dfdx) \n\tstate['paramStd'].resize_as_(state['paramVariance']).copy_(state['paramVariance']).sqrt_() \n\tx.addcdiv_((- clr), dfdx, state['paramStd'].add_(1e-10)) \n\tstate['evalCounter'] += 1 \n\treturn (x, fx)\n", 
" \tgrads = get_or_compute_grads(loss_or_grads, params) \n\tupdates = OrderedDict() \n\tfor (param, grad) in zip(params, grads): \n\t \tvalue = param.get_value(borrow=True) \n\t \tacc_grad = theano.shared(np.zeros(value.shape, dtype=value.dtype), broadcastable=param.broadcastable) \n\t \tacc_grad_new = ((rho * acc_grad) + ((1 - rho) * grad)) \n\t \tacc_rms = theano.shared(np.zeros(value.shape, dtype=value.dtype), broadcastable=param.broadcastable) \n\t \tacc_rms_new = ((rho * acc_rms) + ((1 - rho) * (grad ** 2))) \n\t \tupdates[acc_grad] = acc_grad_new \n\t \tupdates[acc_rms] = acc_rms_new \n\t \tupdates[param] = (param - (learning_rate * (grad / T.sqrt(((acc_rms_new - (acc_grad_new ** 2)) + epsilon))))) \n\treturn updates\n", 
" \tupdates = OrderedDict({}) \n\texp_sqr_grads = OrderedDict({}) \n\texp_sqr_ups = OrderedDict({}) \n\tgparams = [] \n\tfor param in params: \n\t \tempty = np.zeros_like(param.get_value()) \n\t \texp_sqr_grads[param] = theano.shared(value=as_floatX(empty), name=('exp_grad_%s' % param.name)) \n\t \tgp = T.grad(cost, param) \n\t \texp_sqr_ups[param] = theano.shared(value=as_floatX(empty), name=('exp_grad_%s' % param.name)) \n\t \tgparams.append(gp) \n\tfor (param, gp) in zip(params, gparams): \n\t \texp_sg = exp_sqr_grads[param] \n\t \texp_su = exp_sqr_ups[param] \n\t \tup_exp_sg = ((rho * exp_sg) + ((1 - rho) * T.sqr(gp))) \n\t \tupdates[exp_sg] = up_exp_sg \n\t \tstep = ((- (T.sqrt((exp_su + epsilon)) / T.sqrt((up_exp_sg + epsilon)))) * gp) \n\t \tupdates[exp_su] = ((rho * exp_su) + ((1 - rho) * T.sqr(step))) \n\t \tstepped_param = (param + step) \n\t \tif ((param.get_value(borrow=True).ndim == 2) and (param.name != 'Words')): \n\t \t \tcol_norms = T.sqrt(T.sum(T.sqr(stepped_param), axis=0)) \n\t \t \tdesired_norms = T.clip(col_norms, 0, T.sqrt(norm_lim)) \n\t \t \tscale = (desired_norms / (1e-07 + col_norms)) \n\t \t \tupdates[param] = (stepped_param * scale) \n\t \telse: \n\t \t \tupdates[param] = stepped_param \n\treturn updates\n", 
" \tX_sqr = T.sqr(X).sum(axis=1).dimshuffle(0, 'x') \n\tc_sqr = T.sqr(centroids).sum(axis=1).dimshuffle('x', 0) \n\tc_sqr.name = 'c_sqr' \n\tXc = T.dot(X, centroids.T) \n\tXc.name = 'Xc' \n\tsq_dists = ((c_sqr + X_sqr) - (2.0 * Xc)) \n\tsq_dists_safe = T.clip(sq_dists, 0.0, 1e+30) \n\tZ = T.sqrt(sq_dists_safe) \n\tZ.name = 'Z' \n\tmu = Z.mean(axis=1) \n\tmu.name = 'mu' \n\tmu = mu.dimshuffle(0, 'x') \n\tmu.name = 'mu_broadcasted' \n\trval = T.clip((mu - Z), 0.0, 1e+30) \n\trval.name = 'triangle_code' \n\treturn rval\n", 
" \tassert (not isinstance(batch, list)) \n\tif ((batch is None) or (isinstance(batch, tuple) and (len(batch) == 0))): \n\t \treturn True \n\tif isinstance(batch, tuple): \n\t \tsubbatch_results = tuple((_is_batch_all(b, predicate) for b in batch)) \n\t \tresult = all(subbatch_results) \n\t \tassert (result == any(subbatch_results)), 'composite \tbatch \thad \ta \tmixture \tof \tnumeric \tand \tsymbolic \tsubbatches. \tThis \tshould \tnever \thappen.' \n\t \treturn result \n\telse: \n\t \treturn predicate(batch)\n", 
" \tassert (upper_bound > 0) \n\twith tf.op_scope([t, upper_bound], name, 'batch_clip_by_l2norm') as name: \n\t \tsaved_shape = tf.shape(t) \n\t \tbatch_size = tf.slice(saved_shape, [0], [1]) \n\t \tt2 = tf.reshape(t, tf.concat(0, [batch_size, [(-1)]])) \n\t \tupper_bound_inv = tf.fill(tf.slice(saved_shape, [0], [1]), tf.constant((1.0 / upper_bound))) \n\t \tl2norm_inv = tf.rsqrt((tf.reduce_sum((t2 * t2), [1]) + 1e-06)) \n\t \tscale = (tf.minimum(l2norm_inv, upper_bound_inv) * upper_bound) \n\t \tclipped_t = tf.matmul(tf.diag(scale), t2) \n\t \tclipped_t = tf.reshape(clipped_t, saved_shape, name=name) \n\treturn clipped_t\n", 
" \tif (not hasattr(tensor, '_keras_history')): \n\t \treturn tensor \n\tif ((layer is None) or node_index): \n\t \t(layer, node_index, _) = tensor._keras_history \n\tif (not layer.inbound_nodes): \n\t \treturn [tensor] \n\telse: \n\t \tnode = layer.inbound_nodes[node_index] \n\t \tif (not node.inbound_layers): \n\t \t \treturn node.input_tensors \n\t \telse: \n\t \t \tsource_tensors = [] \n\t \t \tfor i in range(len(node.inbound_layers)): \n\t \t \t \tx = node.input_tensors[i] \n\t \t \t \tlayer = node.inbound_layers[i] \n\t \t \t \tnode_index = node.node_indices[i] \n\t \t \t \tprevious_sources = get_source_inputs(x, layer, node_index) \n\t \t \t \tfor x in previous_sources: \n\t \t \t \t \tif (x not in source_tensors): \n\t \t \t \t \t \tsource_tensors.append(x) \n\t \t \treturn source_tensors\n", 
" \treturn to_dense(x).eval()\n", 
" \tassert (input.ndim in (2, 3)) \n\tassert (filters.ndim in (2, 3)) \n\tif (filter_shape and image_shape): \n\t \tif (input.ndim == 3): \n\t \t \tbsize = image_shape[0] \n\t \telse: \n\t \t \tbsize = 1 \n\t \timshp = ((1,) + tuple(image_shape[(-2):])) \n\t \tif (filters.ndim == 3): \n\t \t \tnkern = filter_shape[0] \n\t \telse: \n\t \t \tnkern = 1 \n\t \tkshp = filter_shape[(-2):] \n\telse: \n\t \t(nkern, kshp) = (None, None) \n\t \t(bsize, imshp) = (None, None) \n\tif (input.ndim == 3): \n\t \tsym_bsize = input.shape[0] \n\telse: \n\t \tsym_bsize = 1 \n\tif (filters.ndim == 3): \n\t \tsym_nkern = filters.shape[0] \n\telse: \n\t \tsym_nkern = 1 \n\tnew_input_shape = tensor.join(0, tensor.stack([sym_bsize, 1]), input.shape[(-2):]) \n\tinput4D = tensor.reshape(input, new_input_shape, ndim=4) \n\tnew_filter_shape = tensor.join(0, tensor.stack([sym_nkern, 1]), filters.shape[(-2):]) \n\tfilters4D = tensor.reshape(filters, new_filter_shape, ndim=4) \n\top = conv.ConvOp(output_mode=border_mode, dx=subsample[0], dy=subsample[1], imshp=imshp, kshp=kshp, nkern=nkern, bsize=bsize, **kargs) \n\toutput = op(input4D, filters4D) \n\tif ((input.ndim == 2) and (filters.ndim == 2)): \n\t \tif theano.config.warn.signal_conv2d_interface: \n\t \t \twarnings.warn('theano.tensor.signal.conv2d() \tnow \toutputs \ta \t2d \ttensor \twhen \tboth \tinputs \tare \t2d. \tTo \tdisable \tthis \twarning, \tset \tthe \tTheano \tflag \twarn.signal_conv2d_interface \tto \tFalse', stacklevel=3) \n\t \toutput = tensor.flatten(output.T, outdim=2).T \n\telif ((input.ndim == 2) or (filters.ndim == 2)): \n\t \toutput = tensor.flatten(output.T, outdim=3).T \n\treturn output\n", 
" \t(nb_filter, nb_channel, kh, kw) = teacher_w.shape \n\tstudent_w = np.zeros((nb_filter, nb_filter, kh, kw)) \n\tfor i in xrange(nb_filter): \n\t \tstudent_w[(i, i, ((kh - 1) / 2), ((kw - 1) / 2))] = 1.0 \n\tstudent_b = np.zeros(nb_filter) \n\treturn (student_w, student_b)\n", 
" \t(nb_filter, nb_channel, kh, kw) = teacher_w.shape \n\tstudent_w = np.zeros((nb_filter, nb_filter, kh, kw)) \n\tfor i in xrange(nb_filter): \n\t \tstudent_w[(i, i, ((kh - 1) / 2), ((kw - 1) / 2))] = 1.0 \n\tstudent_b = np.zeros(nb_filter) \n\treturn (student_w, student_b)\n", 
" \tif (stride == 1): \n\t \treturn slim.conv2d(inputs, num_outputs, kernel_size, stride=1, rate=rate, padding='SAME', scope=scope) \n\telse: \n\t \tkernel_size_effective = (kernel_size + ((kernel_size - 1) * (rate - 1))) \n\t \tpad_total = (kernel_size_effective - 1) \n\t \tpad_beg = (pad_total // 2) \n\t \tpad_end = (pad_total - pad_beg) \n\t \tinputs = tf.pad(inputs, [[0, 0], [pad_beg, pad_end], [pad_beg, pad_end], [0, 0]]) \n\t \treturn slim.conv2d(inputs, num_outputs, kernel_size, stride=stride, rate=rate, padding='VALID', scope=scope)\n", 
" \treturn reduce((lambda x, y: np.dot(y, x)), arrs[::(-1)])\n", 
" \tint_rad = 0.06 \n\tnoise = _ad_hoc_noise(coils, ch_type) \n\tif (mode == 'fast'): \n\t \tn_coeff = 50 \n\t \tlut_fun = _get_legen_lut_fast \n\telse: \n\t \tn_coeff = 100 \n\t \tlut_fun = _get_legen_lut_accurate \n\t(lut, n_fact) = _get_legen_table(ch_type, False, n_coeff, verbose=False) \n\tlut_fun = partial(lut_fun, lut=lut) \n\treturn (int_rad, noise, lut_fun, n_fact)\n", 
" \tid = '' \n\tfor n in range(6): \n\t \tletter = random.choice(letter_choices) \n\t \tid += letter \n\treturn id\n", 
" \tdefault_generator.set_state(new_state)\n", 
" \tfor pattern in ignore_patterns: \n\t \tif fnmatch.fnmatchcase(path, pattern): \n\t \t \treturn True \n\treturn False\n", 
" \ttry: \n\t \treturn X.astype(theano.config.floatX) \n\texcept AttributeError: \n\t \treturn np.asarray(X, dtype=theano.config.floatX)\n", 
" \traise NotImplementedError('TODO: \timplement \tthe \tfunction')\n", 
" \tif (not theano): \n\t \traise ImportError('theano \tis \trequired \tfor \ttheano_function') \n\tcache = ({} if (cache == None) else cache) \n\tbroadcastables = dim_handling(inputs, **kwargs) \n\tif (sys.version_info < (3,)): \n\t \tdim_names = inspect.getargspec(dim_handling)[0] \n\telse: \n\t \tparam = inspect.signature(dim_handling).parameters.items() \n\t \tdim_names = [n for (n, p) in param if (p.kind == p.POSITIONAL_OR_KEYWORD)] \n\ttheano_kwargs = dict(((k, v) for (k, v) in kwargs.items() if (k not in dim_names))) \n\tcode = partial(theano_code, cache=cache, dtypes=dtypes, broadcastables=broadcastables) \n\ttinputs = list(map(code, inputs)) \n\ttoutputs = list(map(code, outputs)) \n\ttoutputs = (toutputs[0] if (len(toutputs) == 1) else toutputs) \n\treturn theano.function(tinputs, toutputs, **theano_kwargs)\n", 
" \tglobal scputimes \n\twith open_binary(('%s/stat' % procfs_path)) as f: \n\t \tvalues = f.readline().split()[1:] \n\tfields = ['user', 'nice', 'system', 'idle', 'iowait', 'irq', 'softirq'] \n\tvlen = len(values) \n\tif (vlen >= 8): \n\t \tfields.append('steal') \n\tif (vlen >= 9): \n\t \tfields.append('guest') \n\tif (vlen >= 10): \n\t \tfields.append('guest_nice') \n\tscputimes = namedtuple('scputimes', fields) \n\treturn scputimes\n", 
" \tlabels = _validate_labels(labels, ndim=1) \n\tlabels_ = labels.copy() \n\tuniq = np.unique(labels_) \n\tfor (i, e) in enumerate(uniq): \n\t \tlabels_[(labels_ == e)] = i \n\tif (simplify_binary and (len(uniq) == 2)): \n\t \treturn (labels_.reshape((labels_.shape[0], 1)), uniq) \n\telse: \n\t \treturn (OneHotFormatter(len(uniq), dtype=dtype).format(labels_, mode=mode, sparse=sparse), uniq)\n", 
" \tres = [] \n\tfor el in iterable: \n\t \tif (el not in res): \n\t \t \tres.append(el) \n\treturn res\n", 
" \tif isinstance(value, BeautifulSoup): \n\t \treturn value.find() \n\tif isinstance(value, Tag): \n\t \treturn value \n\tif isinstance(value, list): \n\t \treturn [ensure_soup(item, parser=parser) for item in value] \n\tparsed = BeautifulSoup(value, features=parser) \n\treturn parsed.find()\n", 
" \tif ((ord is None) or (ord == 'fro')): \n\t \tord = 2 \n\tif (ord == np.inf): \n\t \treturn max(abs(a), axis=axis, keepdims=keepdims, split_every=split_every) \n\telif (ord == (- np.inf)): \n\t \treturn min(abs(a), axis=axis, keepdims=keepdims, split_every=split_every) \n\telif (ord == 1): \n\t \treturn sum(abs(a), axis=axis, dtype=dtype, keepdims=keepdims, split_every=split_every) \n\telif ((ord % 2) == 0): \n\t \treturn (sum((a ** ord), axis=axis, dtype=dtype, keepdims=keepdims, split_every=split_every) ** (1.0 / ord)) \n\telse: \n\t \treturn (sum((abs(a) ** ord), axis=axis, dtype=dtype, keepdims=keepdims, split_every=split_every) ** (1.0 / ord))\n", 
" \ttparams = OrderedDict() \n\tfor (kk, pp) in params.iteritems(): \n\t \ttparams[kk] = theano.shared(params[kk], name=kk) \n\treturn tparams\n", 
" \tif balance: \n\t \traise NotImplementedError \n\tif (tol is None): \n\t \tu_d = (2 ** (-53)) \n\t \ttol = u_d \n\tF = B \n\teta = np.exp(((t * mu) / float(s))) \n\tfor i in range(s): \n\t \tc1 = _exact_inf_norm(B) \n\t \tfor j in range(m_star): \n\t \t \tcoeff = (t / float((s * (j + 1)))) \n\t \t \tB = (coeff * A.dot(B)) \n\t \t \tc2 = _exact_inf_norm(B) \n\t \t \tF = (F + B) \n\t \t \tif ((c1 + c2) <= (tol * _exact_inf_norm(F))): \n\t \t \t \tbreak \n\t \t \tc1 = c2 \n\t \tF = (eta * F) \n\t \tB = F \n\treturn F\n", 
" \tsummed = [tensor.sqr(tensor.as_tensor_variable(t)).sum() for t in tensors] \n\tjoined = tensor.stack(summed, axis=0) \n\treturn (joined.sum() if squared else tensor.sqrt(joined.sum()))\n", 
" \tsummed = [tensor.sqr(tensor.as_tensor_variable(t)).sum() for t in tensors] \n\tjoined = tensor.stack(summed, axis=0) \n\treturn (joined.sum() if squared else tensor.sqrt(joined.sum()))\n", 
" \treturn sum(((flowDict[u][v] * d.get(weight, 0)) for (u, v, d) in G.edges(data=True)))\n", 
" \tassert ((cost is not None) or (start is not None)) \n\tassert isinstance(end, list) \n\tassert isinstance(wrt, list) \n\tif (start is not None): \n\t \tassert isinstance(start, dict) \n\tparams = list(set((wrt + end))) \n\tstart_grads = None \n\tcost_grads = None \n\tif (start is not None): \n\t \tstart_grads = list(theano.grad(cost=None, wrt=params, known_grads=start, consider_constant=end, disconnected_inputs='ignore')) \n\tif (cost is not None): \n\t \tcost_grads = list(theano.grad(cost=cost, wrt=params, consider_constant=end, disconnected_inputs='ignore')) \n\tgrads = None \n\tif (start is None): \n\t \tgrads = cost_grads \n\telse: \n\t \tgrads = start_grads \n\t \tif (cost_grads is not None): \n\t \t \tfor i in range(len(grads)): \n\t \t \t \tgrads[i] += cost_grads[i] \n\tpgrads = OrderedDict(izip(params, grads)) \n\twrt_grads = list((pgrads[k] for k in wrt)) \n\tend_grads = list((pgrads[k] for k in end)) \n\tif details: \n\t \treturn (wrt_grads, end_grads, start_grads, cost_grads) \n\treturn (wrt_grads, end_grads)\n", 
" \tassert ((cost is not None) or (start is not None)) \n\tassert isinstance(end, list) \n\tassert isinstance(wrt, list) \n\tif (start is not None): \n\t \tassert isinstance(start, dict) \n\tparams = list(set((wrt + end))) \n\tstart_grads = None \n\tcost_grads = None \n\tif (start is not None): \n\t \tstart_grads = list(theano.grad(cost=None, wrt=params, known_grads=start, consider_constant=end, disconnected_inputs='ignore')) \n\tif (cost is not None): \n\t \tcost_grads = list(theano.grad(cost=cost, wrt=params, consider_constant=end, disconnected_inputs='ignore')) \n\tgrads = None \n\tif (start is None): \n\t \tgrads = cost_grads \n\telse: \n\t \tgrads = start_grads \n\t \tif (cost_grads is not None): \n\t \t \tfor i in range(len(grads)): \n\t \t \t \tgrads[i] += cost_grads[i] \n\tpgrads = OrderedDict(izip(params, grads)) \n\twrt_grads = list((pgrads[k] for k in wrt)) \n\tend_grads = list((pgrads[k] for k in end)) \n\tif details: \n\t \treturn (wrt_grads, end_grads, start_grads, cost_grads) \n\treturn (wrt_grads, end_grads)\n", 
" \tassert ((cost is not None) or (start is not None)) \n\tassert isinstance(end, list) \n\tassert isinstance(wrt, list) \n\tif (start is not None): \n\t \tassert isinstance(start, dict) \n\tparams = list(set((wrt + end))) \n\tstart_grads = None \n\tcost_grads = None \n\tif (start is not None): \n\t \tstart_grads = list(theano.grad(cost=None, wrt=params, known_grads=start, consider_constant=end, disconnected_inputs='ignore')) \n\tif (cost is not None): \n\t \tcost_grads = list(theano.grad(cost=cost, wrt=params, consider_constant=end, disconnected_inputs='ignore')) \n\tgrads = None \n\tif (start is None): \n\t \tgrads = cost_grads \n\telse: \n\t \tgrads = start_grads \n\t \tif (cost_grads is not None): \n\t \t \tfor i in range(len(grads)): \n\t \t \t \tgrads[i] += cost_grads[i] \n\tpgrads = OrderedDict(izip(params, grads)) \n\twrt_grads = list((pgrads[k] for k in wrt)) \n\tend_grads = list((pgrads[k] for k in end)) \n\tif details: \n\t \treturn (wrt_grads, end_grads, start_grads, cost_grads) \n\treturn (wrt_grads, end_grads)\n", 
" \trpc = crop_async(image_data, left_x, top_y, right_x, bottom_y, output_encoding=output_encoding, quality=quality, correct_orientation=correct_orientation, rpc=rpc, transparent_substitution_rgb=transparent_substitution_rgb) \n\treturn rpc.get_result()\n", 
" \tif (not image_list): \n\t \traise ValueError('Empty \timage_list.') \n\trank_assertions = [] \n\tfor i in range(len(image_list)): \n\t \timage_rank = tf.rank(image_list[i]) \n\t \trank_assert = tf.Assert(tf.equal(image_rank, 3), ['Wrong \trank \tfor \ttensor \t \t%s \t[expected] \t[actual]', image_list[i].name, 3, image_rank]) \n\t \trank_assertions.append(rank_assert) \n\timage_shape = control_flow_ops.with_dependencies([rank_assertions[0]], tf.shape(image_list[0])) \n\timage_height = image_shape[0] \n\timage_width = image_shape[1] \n\tcrop_size_assert = tf.Assert(tf.logical_and(tf.greater_equal(image_height, crop_height), tf.greater_equal(image_width, crop_width)), ['Crop \tsize \tgreater \tthan \tthe \timage \tsize.']) \n\tasserts = [rank_assertions[0], crop_size_assert] \n\tfor i in range(1, len(image_list)): \n\t \timage = image_list[i] \n\t \tasserts.append(rank_assertions[i]) \n\t \tshape = control_flow_ops.with_dependencies([rank_assertions[i]], tf.shape(image)) \n\t \theight = shape[0] \n\t \twidth = shape[1] \n\t \theight_assert = tf.Assert(tf.equal(height, image_height), ['Wrong \theight \tfor \ttensor \t%s \t[expected][actual]', image.name, height, image_height]) \n\t \twidth_assert = tf.Assert(tf.equal(width, image_width), ['Wrong \twidth \tfor \ttensor \t%s \t[expected][actual]', image.name, width, image_width]) \n\t \tasserts.extend([height_assert, width_assert]) \n\tmax_offset_height = control_flow_ops.with_dependencies(asserts, tf.reshape(((image_height - crop_height) + 1), [])) \n\tmax_offset_width = control_flow_ops.with_dependencies(asserts, tf.reshape(((image_width - crop_width) + 1), [])) \n\toffset_height = tf.random_uniform([], maxval=max_offset_height, dtype=tf.int32) \n\toffset_width = tf.random_uniform([], maxval=max_offset_width, dtype=tf.int32) \n\treturn [_crop(image, offset_height, offset_width, crop_height, crop_width) for image in image_list]\n", 
" \tif (None in [image_shape, kernel_shape, border_mode, subsample, dilation]): \n\t \treturn None \n\tdil_kernel_shape = (((kernel_shape - 1) * dilation) + 1) \n\tif (border_mode == 'half'): \n\t \tpad = (dil_kernel_shape // 2) \n\telif (border_mode == 'full'): \n\t \tpad = (dil_kernel_shape - 1) \n\telif (border_mode == 'valid'): \n\t \tpad = 0 \n\telse: \n\t \tpad = border_mode \n\t \tif (pad < 0): \n\t \t \traise ValueError('border_mode \tmust \tbe \t>= \t0') \n\tif (pad == 0): \n\t \tout_shp = (image_shape - dil_kernel_shape) \n\telse: \n\t \tout_shp = ((image_shape + (2 * pad)) - dil_kernel_shape) \n\tif (subsample != 1): \n\t \tout_shp = (out_shp // subsample) \n\tout_shp = (out_shp + 1) \n\treturn out_shp\n", 
" \tif (None in [image_shape, kernel_shape, border_mode, subsample, dilation]): \n\t \treturn None \n\tdil_kernel_shape = (((kernel_shape - 1) * dilation) + 1) \n\tif (border_mode == 'half'): \n\t \tpad = (dil_kernel_shape // 2) \n\telif (border_mode == 'full'): \n\t \tpad = (dil_kernel_shape - 1) \n\telif (border_mode == 'valid'): \n\t \tpad = 0 \n\telse: \n\t \tpad = border_mode \n\t \tif (pad < 0): \n\t \t \traise ValueError('border_mode \tmust \tbe \t>= \t0') \n\tif (pad == 0): \n\t \tout_shp = (image_shape - dil_kernel_shape) \n\telse: \n\t \tout_shp = ((image_shape + (2 * pad)) - dil_kernel_shape) \n\tif (subsample != 1): \n\t \tout_shp = (out_shp // subsample) \n\tout_shp = (out_shp + 1) \n\treturn out_shp\n", 
" \tif (None in [image_shape, kernel_shape, border_mode, subsample, dilation]): \n\t \treturn None \n\tdil_kernel_shape = (((kernel_shape - 1) * dilation) + 1) \n\tif (border_mode == 'half'): \n\t \tpad = (dil_kernel_shape // 2) \n\telif (border_mode == 'full'): \n\t \tpad = (dil_kernel_shape - 1) \n\telif (border_mode == 'valid'): \n\t \tpad = 0 \n\telse: \n\t \tpad = border_mode \n\t \tif (pad < 0): \n\t \t \traise ValueError('border_mode \tmust \tbe \t>= \t0') \n\tif (pad == 0): \n\t \tout_shp = (image_shape - dil_kernel_shape) \n\telse: \n\t \tout_shp = ((image_shape + (2 * pad)) - dil_kernel_shape) \n\tif (subsample != 1): \n\t \tout_shp = (out_shp // subsample) \n\tout_shp = (out_shp + 1) \n\treturn out_shp\n", 
" \traise NotImplementedError('TODO: \timplement \tthis \tfunction.')\n", 
" \tevoked = read_evokeds(fname, condition=0, proj=True) \n\tdrop_ch = evoked.ch_names[:3] \n\tch_names = evoked.ch_names[3:] \n\tch_names_orig = evoked.ch_names \n\tdummy = evoked.copy().drop_channels(drop_ch) \n\tassert_equal(ch_names, dummy.ch_names) \n\tassert_equal(ch_names_orig, evoked.ch_names) \n\tassert_equal(len(ch_names_orig), len(evoked.data)) \n\tdummy2 = evoked.copy().drop_channels([drop_ch[0]]) \n\tassert_equal(dummy2.ch_names, ch_names_orig[1:]) \n\tevoked.drop_channels(drop_ch) \n\tassert_equal(ch_names, evoked.ch_names) \n\tassert_equal(len(ch_names), len(evoked.data)) \n\tfor ch_names in ([1, 2], 'fake', ['fake']): \n\t \tassert_raises(ValueError, evoked.drop_channels, ch_names)\n", 
" \tT = current.T \n\tdb = current.db \n\ts3db = current.s3db \n\tftable = s3db.gis_layer_feature \n\tltable = s3db.gis_layer_config \n\tstable = db.gis_style \n\tconfig = GIS.get_config() \n\tconfig_id = config.id \n\tpostgres = (current.deployment_settings.get_database_type() == 'postgres') \n\tlayers_feature_resource = [] \n\tappend = layers_feature_resource.append \n\tfor layer in feature_resources: \n\t \tname = s3_str(layer['name']) \n\t \t_layer = dict(name=name) \n\t \t_id = layer.get('id') \n\t \tif _id: \n\t \t \t_id = str(_id) \n\t \telse: \n\t \t \t_id = name \n\t \t_id = re.sub('\\\\W', '_', _id) \n\t \t_layer['id'] = _id \n\t \tlayer_id = layer.get('layer_id', None) \n\t \tif layer_id: \n\t \t \tquery = (ftable.layer_id == layer_id) \n\t \t \tleft = [ltable.on(((ltable.layer_id == layer_id) & (ltable.config_id == config_id))), stable.on(((((stable.layer_id == layer_id) & ((stable.config_id == config_id) | (stable.config_id == None))) & (stable.record_id == None)) & (stable.aggregate == False)))] \n\t \t \tif postgres: \n\t \t \t \torderby = stable.config_id \n\t \t \telse: \n\t \t \t \torderby = (~ stable.config_id) \n\t \t \trow = db(query).select(ftable.layer_id, ftable.controller, ftable.function, ftable.filter, ftable.aggregate, ftable.trackable, ftable.use_site, ftable.popup_fields, ftable.popup_label, ftable.cluster_attribute, ltable.dir, stable.marker_id, stable.opacity, stable.popup_format, stable.cluster_distance, stable.cluster_threshold, stable.style, left=left, limitby=(0, 1), orderby=orderby).first() \n\t \t \t_dir = layer.get('dir', row['gis_layer_config.dir']) \n\t \t \t_style = row['gis_style'] \n\t \t \trow = row['gis_layer_feature'] \n\t \t \tif row.use_site: \n\t \t \t \tmaxdepth = 1 \n\t \t \telse: \n\t \t \t \tmaxdepth = 0 \n\t \t \topacity = (layer.get('opacity', _style.opacity) or 1) \n\t \t \tcluster_attribute = (layer.get('cluster_attribute', row.cluster_attribute) or CLUSTER_ATTRIBUTE) \n\t \t \tcluster_distance = (layer.get('cluster_distance', _style.cluster_distance) or CLUSTER_DISTANCE) \n\t \t \tcluster_threshold = layer.get('cluster_threshold', _style.cluster_threshold) \n\t \t \tif (cluster_threshold is None): \n\t \t \t \tcluster_threshold = CLUSTER_THRESHOLD \n\t \t \tstyle = layer.get('style', None) \n\t \t \tif style: \n\t \t \t \ttry: \n\t \t \t \t \tstyle = json.loads(style) \n\t \t \t \texcept: \n\t \t \t \t \tcurrent.log.error(('Invalid \tStyle: \t%s' % style)) \n\t \t \t \t \tstyle = None \n\t \t \telse: \n\t \t \t \tstyle = _style.style \n\t \t \taggregate = layer.get('aggregate', row.aggregate) \n\t \t \tif aggregate: \n\t \t \t \turl = ('%s.geojson?layer=%i&show_ids=true' % (URL(c=row.controller, f=row.function, args='report'), row.layer_id)) \n\t \t \t \turl_format = ('%s/{id}.plain' % URL(c='gis', f='location')) \n\t \t \telse: \n\t \t \t \t_url = URL(c=row.controller, f=row.function) \n\t \t \t \turl = ('%s.geojson?layer=%i&components=None&show_ids=true&maxdepth=%s' % (_url, row.layer_id, maxdepth)) \n\t \t \t \turl_format = ('%s/{id}.plain' % _url) \n\t \t \t_filter = layer.get('filter', row.filter) \n\t \t \tif _filter: \n\t \t \t \turl = ('%s&%s' % (url, _filter)) \n\t \t \tif row.trackable: \n\t \t \t \turl = ('%s&track=1' % url) \n\t \t \tif (not style): \n\t \t \t \tmarker = layer.get('marker') \n\t \t \t \tif marker: \n\t \t \t \t \tmarker = Marker(marker).as_json_dict() \n\t \t \t \telif _style.marker_id: \n\t \t \t \t \tmarker = Marker(marker_id=_style.marker_id).as_json_dict() \n\t \t \tpopup_format = _style.popup_format \n\t \t \tif (not popup_format): \n\t \t \t \tpopup_fields = row['popup_fields'] \n\t \t \t \tif popup_fields: \n\t \t \t \t \tpopup_label = row['popup_label'] \n\t \t \t \t \tif popup_label: \n\t \t \t \t \t \tpopup_format = ('{%s} \t(%s)' % (popup_fields[0], current.T(popup_label))) \n\t \t \t \t \telse: \n\t \t \t \t \t \tpopup_format = ('%s' % popup_fields[0]) \n\t \t \t \t \tfor f in popup_fields[1:]: \n\t \t \t \t \t \tpopup_format = ('%s<br \t/>{%s}' % (popup_format, f)) \n\t \telse: \n\t \t \turl = layer['url'] \n\t \t \ttablename = layer['tablename'] \n\t \t \ttable = s3db[tablename] \n\t \t \tif ('location_id' in table.fields): \n\t \t \t \tmaxdepth = 0 \n\t \t \telif ('site_id' in table.fields): \n\t \t \t \tmaxdepth = 1 \n\t \t \telif (tablename == 'gis_location'): \n\t \t \t \tmaxdepth = 0 \n\t \t \telse: \n\t \t \t \tcontinue \n\t \t \toptions = ('components=None&maxdepth=%s&show_ids=true' % maxdepth) \n\t \t \tif ('?' in url): \n\t \t \t \turl = ('%s&%s' % (url, options)) \n\t \t \telse: \n\t \t \t \turl = ('%s?%s' % (url, options)) \n\t \t \topacity = layer.get('opacity', 1) \n\t \t \tcluster_attribute = layer.get('cluster_attribute', CLUSTER_ATTRIBUTE) \n\t \t \tcluster_distance = layer.get('cluster_distance', CLUSTER_DISTANCE) \n\t \t \tcluster_threshold = layer.get('cluster_threshold', CLUSTER_THRESHOLD) \n\t \t \t_dir = layer.get('dir', None) \n\t \t \tstyle = layer.get('style', None) \n\t \t \tif style: \n\t \t \t \ttry: \n\t \t \t \t \tstyle = json.loads(style) \n\t \t \t \texcept: \n\t \t \t \t \tcurrent.log.error(('Invalid \tStyle: \t%s' % style)) \n\t \t \t \t \tstyle = None \n\t \t \tif (not style): \n\t \t \t \tmarker = layer.get('marker', None) \n\t \t \t \tif marker: \n\t \t \t \t \tmarker = Marker(marker).as_json_dict() \n\t \t \tpopup_format = layer.get('popup_format') \n\t \t \turl_format = layer.get('url_format') \n\t \tif (('active' in layer) and (not layer['active'])): \n\t \t \t_layer['visibility'] = False \n\t \tif (opacity != 1): \n\t \t \t_layer['opacity'] = ('%.1f' % opacity) \n\t \tif popup_format: \n\t \t \tif ('T(' in popup_format): \n\t \t \t \titems = regex_translate.findall(popup_format) \n\t \t \t \tfor item in items: \n\t \t \t \t \ttitem = str(T(item[1:(-1)])) \n\t \t \t \t \tpopup_format = popup_format.replace(('T(%s)' % item), titem) \n\t \t \t_layer['popup_format'] = popup_format \n\t \tif url_format: \n\t \t \t_layer['url_format'] = url_format \n\t \tif (cluster_attribute != CLUSTER_ATTRIBUTE): \n\t \t \t_layer['cluster_attribute'] = cluster_attribute \n\t \tif (cluster_distance != CLUSTER_DISTANCE): \n\t \t \t_layer['cluster_distance'] = cluster_distance \n\t \tif (cluster_threshold != CLUSTER_THRESHOLD): \n\t \t \t_layer['cluster_threshold'] = cluster_threshold \n\t \tif _dir: \n\t \t \t_layer['dir'] = _dir \n\t \tif style: \n\t \t \t_layer['style'] = style \n\t \telif marker: \n\t \t \t_layer['marker'] = marker \n\t \telse: \n\t \t \turl = ('%s&markers=1' % url) \n\t \t_layer['url'] = url \n\t \tappend(_layer) \n\treturn layers_feature_resource\n", 
" \tinputs_shape = inputs.get_shape() \n\twith tf.variable_scope(scope, 'BatchNorm', [inputs], reuse=reuse): \n\t \taxis = list(range((len(inputs_shape) - 1))) \n\t \tparams_shape = inputs_shape[(-1):] \n\t \t(beta, gamma) = (None, None) \n\t \tif center: \n\t \t \tbeta = variables.variable('beta', params_shape, initializer=tf.zeros_initializer(), trainable=trainable, restore=restore) \n\t \tif scale: \n\t \t \tgamma = variables.variable('gamma', params_shape, initializer=tf.ones_initializer(), trainable=trainable, restore=restore) \n\t \tmoving_collections = [moving_vars, tf.GraphKeys.MOVING_AVERAGE_VARIABLES] \n\t \tmoving_mean = variables.variable('moving_mean', params_shape, initializer=tf.zeros_initializer(), trainable=False, restore=restore, collections=moving_collections) \n\t \tmoving_variance = variables.variable('moving_variance', params_shape, initializer=tf.ones_initializer(), trainable=False, restore=restore, collections=moving_collections) \n\t \tif is_training: \n\t \t \t(mean, variance) = tf.nn.moments(inputs, axis) \n\t \t \tupdate_moving_mean = moving_averages.assign_moving_average(moving_mean, mean, decay) \n\t \t \ttf.add_to_collection(UPDATE_OPS_COLLECTION, update_moving_mean) \n\t \t \tupdate_moving_variance = moving_averages.assign_moving_average(moving_variance, variance, decay) \n\t \t \ttf.add_to_collection(UPDATE_OPS_COLLECTION, update_moving_variance) \n\t \telse: \n\t \t \tmean = moving_mean \n\t \t \tvariance = moving_variance \n\t \toutputs = tf.nn.batch_normalization(inputs, mean, variance, beta, gamma, epsilon) \n\t \toutputs.set_shape(inputs.get_shape()) \n\t \tif activation: \n\t \t \toutputs = activation(outputs) \n\t \treturn outputs\n", 
" \tmodule = import_string(import_path) \n\tpath = getattr(module, '__path__', None) \n\tif (path is None): \n\t \traise ValueError(('%r \tis \tnot \ta \tpackage' % import_path)) \n\tbasename = (module.__name__ + '.') \n\tfor (importer, modname, ispkg) in pkgutil.iter_modules(path): \n\t \tmodname = (basename + modname) \n\t \tif ispkg: \n\t \t \tif include_packages: \n\t \t \t \t(yield modname) \n\t \t \tif recursive: \n\t \t \t \tfor item in find_modules(modname, include_packages, True): \n\t \t \t \t \t(yield item) \n\t \telse: \n\t \t \t(yield modname)\n", 
" \tins = defaultdict(list) \n\touts = defaultdict(list) \n\tlookup = {repr(layer): index for (index, layer) in enumerate(network)} \n\tfor current_layer in network: \n\t \tif hasattr(current_layer, 'input_layers'): \n\t \t \tlayer_ins = current_layer.input_layers \n\t \telif hasattr(current_layer, 'input_layer'): \n\t \t \tlayer_ins = [current_layer.input_layer] \n\t \telse: \n\t \t \tlayer_ins = [] \n\t \tins[lookup[repr(current_layer)]].extend([lookup[repr(l)] for l in layer_ins]) \n\t \tfor l in layer_ins: \n\t \t \touts[lookup[repr(l)]].append(lookup[repr(current_layer)]) \n\treturn (ins, outs)\n", 
" \treturn np.ones(shape)\n", 
" \traise NotImplementedError('TODO: \timplement \tthe \tfunction')\n", 
" \tif (layer_set is None): \n\t \tlayer_set = set() \n\ttrainable_count = 0 \n\tnon_trainable_count = 0 \n\tfor layer in layers: \n\t \tif (layer in layer_set): \n\t \t \tcontinue \n\t \tlayer_set.add(layer) \n\t \tif isinstance(layer, (Model, Sequential)): \n\t \t \t(t, nt) = count_total_params(layer.layers, layer_set) \n\t \t \ttrainable_count += t \n\t \t \tnon_trainable_count += nt \n\t \telse: \n\t \t \ttrainable_count += sum([K.count_params(p) for p in layer.trainable_weights]) \n\t \t \tnon_trainable_count += sum([K.count_params(p) for p in layer.non_trainable_weights]) \n\treturn (trainable_count, non_trainable_count)\n", 
" \tif (layer_set is None): \n\t \tlayer_set = set() \n\ttrainable_count = 0 \n\tnon_trainable_count = 0 \n\tfor layer in layers: \n\t \tif (layer in layer_set): \n\t \t \tcontinue \n\t \tlayer_set.add(layer) \n\t \tif isinstance(layer, (Model, Sequential)): \n\t \t \t(t, nt) = count_total_params(layer.layers, layer_set) \n\t \t \ttrainable_count += t \n\t \t \tnon_trainable_count += nt \n\t \telse: \n\t \t \ttrainable_count += sum([K.count_params(p) for p in layer.trainable_weights]) \n\t \t \tnon_trainable_count += sum([K.count_params(p) for p in layer.non_trainable_weights]) \n\treturn (trainable_count, non_trainable_count)\n", 
" \tregex = (('^\\\\s*' + re_func_decl) + '\\\\s*{') \n\tfuncs = [] \n\twhile True: \n\t \tm = re.search(regex, code, re.M) \n\t \tif (m is None): \n\t \t \treturn funcs \n\t \t(rtype, name, args) = m.groups()[:3] \n\t \tif ((args == 'void') or (args.strip() == '')): \n\t \t \targs = [] \n\t \telse: \n\t \t \targs = [tuple(arg.strip().split(' \t')) for arg in args.split(',')] \n\t \tfuncs.append((name, args, rtype)) \n\t \tcode = code[m.end():]\n", 
" \tdef _offset(*args): \n\t \targs2 = list(map(add, args, offset)) \n\t \treturn func(*args2) \n\twith ignoring(Exception): \n\t \t_offset.__name__ = ('offset_' + func.__name__) \n\treturn _offset\n", 
" \t(count, sum) = _stats(input, labels, index) \n\treturn sum\n", 
" \tif (node_attr is None): \n\t \tvalue = (lambda u: u) \n\telif (not hasattr(node_attr, '__call__')): \n\t \tvalue = (lambda u: G.node[u][node_attr]) \n\telse: \n\t \tvalue = node_attr \n\treturn value\n", 
" \tout = np.zeros_like(distsq) \n\tmask = (distsq > 0) \n\tvalid = distsq[mask] \n\tout[mask] = (valid * np.log(valid)) \n\treturn out\n", 
" \tassert hasattr(Y_hat, 'owner') \n\towner = Y_hat.owner \n\tassert (owner is not None) \n\top = owner.op \n\tif isinstance(op, Print): \n\t \tassert (len(owner.inputs) == 1) \n\t \t(Y_hat,) = owner.inputs \n\t \towner = Y_hat.owner \n\t \top = owner.op \n\tsuccess = False \n\tif isinstance(op, T.Elemwise): \n\t \tif isinstance(op.scalar_op, T.nnet.sigm.ScalarSigmoid): \n\t \t \tsuccess = True \n\tif (not success): \n\t \traise TypeError(((('Expected \tY_hat \tto \tbe \tthe \toutput \tof \ta \tsigmoid, \tbut \tit \tappears \tto \tbe \tthe \toutput \tof \t' + str(op)) + ' \tof \ttype \t') + str(type(op)))) \n\t(z,) = owner.inputs \n\tassert (z.ndim == 2) \n\treturn z\n", 
" \treturn tf.get_default_graph().get_tensor_by_name(('%s:0' % layer))\n", 
" \tinputs_shape = inputs.get_shape() \n\twith tf.variable_scope(scope, 'BatchNorm', [inputs], reuse=reuse): \n\t \taxis = list(range((len(inputs_shape) - 1))) \n\t \tparams_shape = inputs_shape[(-1):] \n\t \t(beta, gamma) = (None, None) \n\t \tif center: \n\t \t \tbeta = variables.variable('beta', params_shape, initializer=tf.zeros_initializer(), trainable=trainable, restore=restore) \n\t \tif scale: \n\t \t \tgamma = variables.variable('gamma', params_shape, initializer=tf.ones_initializer(), trainable=trainable, restore=restore) \n\t \tmoving_collections = [moving_vars, tf.GraphKeys.MOVING_AVERAGE_VARIABLES] \n\t \tmoving_mean = variables.variable('moving_mean', params_shape, initializer=tf.zeros_initializer(), trainable=False, restore=restore, collections=moving_collections) \n\t \tmoving_variance = variables.variable('moving_variance', params_shape, initializer=tf.ones_initializer(), trainable=False, restore=restore, collections=moving_collections) \n\t \tif is_training: \n\t \t \t(mean, variance) = tf.nn.moments(inputs, axis) \n\t \t \tupdate_moving_mean = moving_averages.assign_moving_average(moving_mean, mean, decay) \n\t \t \ttf.add_to_collection(UPDATE_OPS_COLLECTION, update_moving_mean) \n\t \t \tupdate_moving_variance = moving_averages.assign_moving_average(moving_variance, variance, decay) \n\t \t \ttf.add_to_collection(UPDATE_OPS_COLLECTION, update_moving_variance) \n\t \telse: \n\t \t \tmean = moving_mean \n\t \t \tvariance = moving_variance \n\t \toutputs = tf.nn.batch_normalization(inputs, mean, variance, beta, gamma, epsilon) \n\t \toutputs.set_shape(inputs.get_shape()) \n\t \tif activation: \n\t \t \toutputs = activation(outputs) \n\t \treturn outputs\n", 
" \tif any(map(isiterable, x)): \n\t \treturn np.vectorize(func)(*x) \n\telse: \n\t \treturn func(*x)\n", 
" \twith tf.name_scope(name): \n\t \treturn tf.reduce_mean((- ((target * tf.log((output + epsilon))) + ((1.0 - target) * tf.log(((1.0 - output) + epsilon))))))\n", 
" \twith tf.name_scope(name): \n\t \treturn tf.reduce_mean((- ((target * tf.log((output + epsilon))) + ((1.0 - target) * tf.log(((1.0 - output) + epsilon))))))\n", 
" \treturn tf.equal(x, y)\n", 
" \treturn [(element1 + element2) for (element1, element2) in zip(row1, row2)]\n", 
" \twith tf.name_scope('HingeLoss'): \n\t \treturn tf.reduce_mean(tf.maximum((1.0 - (y_true * y_pred)), 0.0))\n", 
" \treturn Hinge(norm)(x, t)\n", 
" \treturn BinaryAccuracy()(y, t)\n", 
" \tassert (len(logits) == len(labels)) \n\tif (len(np.shape(logits)) > 1): \n\t \tpredicted_labels = np.argmax(logits, axis=1) \n\telse: \n\t \tassert (len(np.shape(logits)) == 1) \n\t \tpredicted_labels = logits \n\tcorrect = np.sum((predicted_labels == labels.reshape(len(labels)))) \n\taccuracy = (float(correct) / len(labels)) \n\treturn accuracy\n", 
" \treturn Sigmoid(use_cudnn)(x)\n", 
" \treturn tf.nn.softmax(x)\n", 
" \treturn tensor.tanh(x)\n", 
" \treturn ReLU(use_cudnn)(x)\n", 
" \treturn ReLU(use_cudnn)(x)\n", 
" \tx = tf.convert_to_tensor(x) \n\tdependencies = [tf.assert_positive(x), tf.assert_less(x, 1.0)] \n\tx = control_flow_ops.with_dependencies(dependencies, x) \n\treturn (tf.log(x) - tf.log((1.0 - x)))\n", 
" \t(C1, C2, C3, C4) = get_numbered_constants(eq, num=4) \n\ta = (- r['a1']) \n\tb = (- r['a2']) \n\tc = (- r['b2']) \n\td = (- r['a3']) \n\tk = (- r['b3']) \n\tp = (- r['c3']) \n\tsol1 = (C1 * exp((a * t))) \n\tsol2 = ((((b * C1) * exp((a * t))) / (a - c)) + (C2 * exp((c * t)))) \n\tsol3 = (((((C1 * (d + ((b * k) / (a - c)))) * exp((a * t))) / (a - p)) + (((k * C2) * exp((c * t))) / (c - p))) + (C3 * exp((p * t)))) \n\treturn [Eq(x(t), sol1), Eq(y(t), sol2), Eq(z(t), sol3)]\n", 
" \tif (stride == 1): \n\t \treturn slim.conv2d(inputs, num_outputs, kernel_size, stride=1, rate=rate, padding='SAME', scope=scope) \n\telse: \n\t \tkernel_size_effective = (kernel_size + ((kernel_size - 1) * (rate - 1))) \n\t \tpad_total = (kernel_size_effective - 1) \n\t \tpad_beg = (pad_total // 2) \n\t \tpad_end = (pad_total - pad_beg) \n\t \tinputs = tf.pad(inputs, [[0, 0], [pad_beg, pad_end], [pad_beg, pad_end], [0, 0]]) \n\t \treturn slim.conv2d(inputs, num_outputs, kernel_size, stride=stride, rate=rate, padding='VALID', scope=scope)\n", 
" \tinput_shape = utils.get_incoming_shape(incoming) \n\tassert (len(input_shape) == 4), 'Incoming \tTensor \tshape \tmust \tbe \t4-D' \n\tfilter_size = utils.autoformat_filter_conv2d(filter_size, input_shape[(-1)], nb_filter) \n\tstrides = utils.autoformat_kernel_2d(strides) \n\tpadding = utils.autoformat_padding(padding) \n\ttry: \n\t \tvscope = tf.variable_scope(scope, default_name=name, values=[incoming], reuse=reuse) \n\texcept Exception: \n\t \tvscope = tf.variable_op_scope([incoming], scope, name, reuse=reuse) \n\twith vscope as scope: \n\t \tname = scope.name \n\t \tW_init = weights_init \n\t \tif isinstance(weights_init, str): \n\t \t \tW_init = initializations.get(weights_init)() \n\t \tW_regul = None \n\t \tif regularizer: \n\t \t \tW_regul = (lambda x: losses.get(regularizer)(x, weight_decay)) \n\t \tW = vs.variable('W', shape=filter_size, regularizer=W_regul, initializer=W_init, trainable=trainable, restore=restore) \n\t \ttf.add_to_collection(((tf.GraphKeys.LAYER_VARIABLES + '/') + name), W) \n\t \tbias_init = initializations.get(bias_init)() \n\t \tb = vs.variable('b', shape=nb_filter, initializer=bias_init, trainable=trainable, restore=restore) \n\t \ttf.add_to_collection(((tf.GraphKeys.LAYER_VARIABLES + '/') + name), b) \n\t \tW_T = vs.variable('W_T', shape=nb_filter, regularizer=None, initializer=W_init, trainable=trainable, restore=restore) \n\t \ttf.add_to_collection(((tf.GraphKeys.LAYER_VARIABLES + '/') + name), W_T) \n\t \tb_T = vs.variable('b_T', shape=nb_filter, initializer=tf.constant_initializer((-3)), trainable=trainable, restore=restore) \n\t \ttf.add_to_collection(((tf.GraphKeys.LAYER_VARIABLES + '/') + name), b_T) \n\t \tif isinstance(activation, str): \n\t \t \tactivation = activations.get(activation) \n\t \telif hasattr(activation, '__call__'): \n\t \t \tactivation = activation \n\t \telse: \n\t \t \traise ValueError('Invalid \tActivation.') \n\t \tconvolved = tf.nn.conv2d(incoming, W, strides, padding) \n\t \tH = activation((convolved + b)) \n\t \tT = tf.sigmoid((tf.mul(convolved, W_T) + b_T)) \n\t \tC = tf.sub(1.0, T) \n\t \tinference = tf.add(tf.mul(H, T), tf.mul(convolved, C)) \n\t \ttf.add_to_collection(tf.GraphKeys.ACTIVATIONS, inference) \n\tinference.scope = scope \n\tinference.W = W \n\tinference.W_T = W_T \n\tinference.b = b \n\tinference.b_T = b_T \n\ttf.add_to_collection(((tf.GraphKeys.LAYER_TENSOR + '/') + name), inference) \n\treturn inference\n", 
" \tndim = len(x.shape[2:]) \n\tfunc = ConvolutionND(ndim, stride, pad, use_cudnn, cover_all) \n\tif (b is None): \n\t \treturn func(x, W) \n\telse: \n\t \treturn func(x, W, b)\n", 
" \treturn a.flatten()[flatnonzero(a)]\n"

]