[" \tif (sig == signal.SIG_DFL): \n\t \ttry: \n\t \t \tproc_file = ('%s/%d/cmdline' % (PROC_DIR, pid)) \n\t \t \tif os.path.exists(proc_file): \n\t \t \t \twith open(proc_file, 'r') as fd: \n\t \t \t \t \tif (name not in fd.read()): \n\t \t \t \t \t \traise InvalidPidFileException() \n\t \texcept IOError: \n\t \t \tpass \n\tos.kill(pid, sig)\n", 
" \tif (pb_value is not None): \n\t \treturn pb_value.decode('utf-8') \n\treturn None\n", 
" \tif (len(s1) != len(s2)): \n\t \treturn False \n\tb = True \n\tfor (ch1, ch2) in zip(s1, s2): \n\t \tif (ch1 != ch2): \n\t \t \tb = False \n\treturn b\n", 
" \tassert isinstance(s, basestring) \n\treturn [int(i) for i in s.split()]\n", 
" \tif isinstance(obj, basestring): \n\t \treturn obj.encode('utf-8') \n\telse: \n\t \treturn obj\n", 
" \t_local_dict_stack.pop()\n", 
" \t_local_dict_stack.pop()\n", 
" \ttry: \n\t \tp = Popen(arglist, stdout=PIPE, stderr=PIPE) \n\t \t(output, errors) = p.communicate() \n\texcept OSError as e: \n\t \treturn (1, '') \n\treturn (p.returncode, output)\n", 
" \treturn sep.join((v for v in l if v))\n", 
" \t((x1, y1), (x2, y2)) = s1 \n\t((x3, y3), (x4, y4)) = s2 \n\tden = (((y4 - y3) * (x2 - x1)) - ((x4 - x3) * (y2 - y1))) \n\tn1 = (((x4 - x3) * (y1 - y3)) - ((y4 - y3) * (x1 - x3))) \n\tn2 = (((x2 - x1) * (y1 - y3)) - ((y2 - y1) * (x1 - x3))) \n\tif (den == 0): \n\t \treturn False \n\tu1 = (n1 / den) \n\tu2 = (n2 / den) \n\treturn ((0.0 <= u1 <= 1.0) and (0.0 <= u2 <= 1.0))\n", 
" \treturn request.headers.get(key, default)\n", 
" \tmatch = date_re.match(value) \n\tif match: \n\t \tkw = dict(((k, int(v)) for (k, v) in six.iteritems(match.groupdict()))) \n\t \treturn datetime.date(**kw)\n", 
" \treturn [chunk for line in str.split('\\n') for chunk in textwrap.wrap(line, length)]\n", 
" \ts = re.sub('(?<!\\\\r)\\\\n', CRLF, s) \n\ts = re.sub('\\\\r(?!\\\\n)', CRLF, s) \n\treturn s\n", 
" \treturn sep.join((v for v in l if v))\n", 
" \tif isinstance(objects, QuerySet): \n\t \tmodel = objects.model \n\t \tpks = [force_text(pk) for pk in objects.values_list(u'pk', flat=True)] \n\t \tctype = get_content_type(model) \n\telse: \n\t \tpks = [] \n\t \tfor (idx, obj) in enumerate(objects): \n\t \t \tif (not idx): \n\t \t \t \tmodel = type(obj) \n\t \t \t \tctype = get_content_type(model) \n\t \t \tpks.append(force_text(obj.pk)) \n\treturn (pks, model, ctype)\n", 
" \t(count, sum) = _stats(input, labels, index) \n\treturn sum\n", 
" \tif (actions is not None): \n\t \tfor action in actions: \n\t \t \tif (action is not None): \n\t \t \t \taction.setEnabled(enable)\n", 
" \treturn sep.join((v for v in l if v))\n", 
" \tenv = copy.deepcopy(os.environ) \n\tenv['PATH'] = _env_path \n\tout = subprocess.Popen([_pyexe, '-c', pycode], env=env, stdout=subprocess.PIPE, shell=False).stdout.read() \n\tout = out.decode('ascii').strip() \n\treturn out\n", 
" \ttry: \n\t \tformat = format.split()[1] \n\texcept IndexError: \n\t \tpass \n\tconverted = format \n\treplacements = {'%H': 'hh', '%I': 'HH', '%M': 'ii', '%S': 'ss'} \n\tfor (search, replace) in replacements.items(): \n\t \tconverted = converted.replace(search, replace) \n\treturn converted.strip()\n", 
" \tm = FLOAT_REGEX.search(value) \n\tif (m is not None): \n\t \treturn float(value) \n\treturn long(value)\n", 
" \ttext = '' \n\tassert (endian in (LITTLE_ENDIAN, BIG_ENDIAN)) \n\tassert (0 <= value) \n\tfor index in xrange(size): \n\t \tif ((value & 1) == 1): \n\t \t \ttext += '1' \n\t \telse: \n\t \t \ttext += '0' \n\t \tvalue >>= 1 \n\tif (endian is LITTLE_ENDIAN): \n\t \ttext = text[::(-1)] \n\tresult = '' \n\twhile (len(text) != 0): \n\t \tif (len(result) != 0): \n\t \t \tresult += ' \t' \n\t \tif classic_mode: \n\t \t \tresult += text[7::(-1)] \n\t \telse: \n\t \t \tresult += text[:8] \n\t \ttext = text[8:] \n\treturn result\n", 
" \tenv = dict(os.environ) \n\tenv['PYTHONPATH'] = os.pathsep.join(sys.path) \n\treturn env\n", 
" \ttry: \n\t \ttext = re.sub('[^\\\\w \t]', '', text) \n\t \treturn [x.strip('.').lower() for x in text.split()] \n\texcept TypeError: \n\t \treturn None\n", 
" \tfilename = os.path.expanduser(filename) \n\tfilename = os.path.abspath(filename) \n\tdirname = os.path.dirname(filename) \n\tif (not os.path.exists(dirname)): \n\t \tos.makedirs(dirname) \n\texists = os.path.exists(filename) \n\tlog_fp = open(filename, mode) \n\tif exists: \n\t \tlog_fp.write(('%s\\n' % ('-' * 60))) \n\t \tlog_fp.write(('%s \trun \ton \t%s\\n' % (sys.argv[0], time.strftime('%c')))) \n\treturn log_fp\n", 
" \t_test_fetch('http://google.com')\n", 
" \tensure_dirs(filename) \n\tresponse = requests.get(url, headers=headers, stream=True) \n\tif (response.status_code == 200): \n\t \twith open(filename, 'wb') as f: \n\t \t \tfor chunk in response.iter_content((16 * 1024)): \n\t \t \t \tf.write(chunk)\n", 
" \t_test_fetch('http://google.com')\n", 
" \t_test_fetch('http://google.com')\n", 
" \t_test_fetch('http://google.com')\n", 
" \tgroup = parser.add_argument_group(u'Daemonization \tOptions') \n\t(group.add_argument(u'-f', u'--logfile', default=default_logfile),) \n\t(group.add_argument(u'--pidfile', default=default_pidfile),) \n\t(group.add_argument(u'--uid', default=None),) \n\t(group.add_argument(u'--gid', default=None),) \n\t(group.add_argument(u'--umask', default=None),) \n\t(group.add_argument(u'--executable', default=None),)\n", 
" \tif _PY3: \n\t \treturn list(d.keys()) \n\telse: \n\t \treturn d.keys()\n", 
" \tcol_name_map = collections.defaultdict((lambda : ([None] * len(arrays)))) \n\tcol_name_list = [] \n\tif (table_names is None): \n\t \ttable_names = [six.text_type((ii + 1)) for ii in range(len(arrays))] \n\tfor (idx, array) in enumerate(arrays): \n\t \ttable_name = table_names[idx] \n\t \tfor name in array.colnames: \n\t \t \tout_name = name \n\t \t \tif (name in common_names): \n\t \t \t \tif (name not in col_name_list): \n\t \t \t \t \tcol_name_list.append(name) \n\t \t \telse: \n\t \t \t \tothers = list(arrays) \n\t \t \t \tothers.pop(idx) \n\t \t \t \tif any(((name in other.colnames) for other in others)): \n\t \t \t \t \tout_name = uniq_col_name.format(table_name=table_name, col_name=name) \n\t \t \t \tcol_name_list.append(out_name) \n\t \t \tcol_name_map[out_name][idx] = name \n\tcol_name_count = Counter(col_name_list) \n\trepeated_names = [name for (name, count) in six.iteritems(col_name_count) if (count > 1)] \n\tif repeated_names: \n\t \traise TableMergeError(u'Merging \tcolumn \tnames \tresulted \tin \tduplicates: \t{0}. \t \tChange \tuniq_col_name \tor \ttable_names \targs \tto \tfix \tthis.'.format(repeated_names)) \n\tcol_name_map = OrderedDict(((name, col_name_map[name]) for name in col_name_list)) \n\treturn col_name_map\n", 
" \treturn [word.strip() for word in string.split(sep) if word.strip()]\n", 
" \tparser = argparse.ArgumentParser() \n\trules = shlex.split(rule) \n\trules.pop(0) \n\tparser.add_argument('--vckeymap', dest='vckeymap', action='store') \n\tparser.add_argument('--xlayouts', dest='xlayouts', action='store') \n\tparser.add_argument('--switch', dest='switch', action='store') \n\tparser.add_argument('keyboard') \n\targs = clean_args(vars(parser.parse_args(rules))) \n\tif (('keyboard' in args) and ('xlayouts' not in args)): \n\t \targs['xlayouts'] = args['keyboard'] \n\tparser = None \n\treturn args\n", 
" \tif (app_name is None): \n\t \tapp_name = DefaultConfig.PROJECT \n\tapp = Flask(app_name, instance_path=INSTANCE_FOLDER_PATH, instance_relative_config=True) \n\tconfigure_app(app, config) \n\tconfigure_hook(app) \n\tconfigure_blueprints(app) \n\tconfigure_extensions(app) \n\tconfigure_logging(app) \n\tconfigure_template_filters(app) \n\tconfigure_error_handlers(app) \n\tconfigure_cli(app) \n\treturn app\n", 
" \ttry: \n\t \tf = open('version.txt') \n\texcept IOError: \n\t \tos.system('./version.sh \t> \tversion.txt') \n\t \tf = open('version.txt') \n\tversion = ''.join(f.readlines()).rstrip() \n\tf.close() \n\treturn version\n", 
" \tif isinstance(product[0], Matrix): \n\t \treturn _sympy_tensor_product(*product) \n\telif isinstance(product[0], numpy_ndarray): \n\t \treturn _numpy_tensor_product(*product) \n\telif isinstance(product[0], scipy_sparse_matrix): \n\t \treturn _scipy_sparse_tensor_product(*product)\n", 
" \traarr = (np.ones(arrshape) * 10.6847929) \n\tdecarr = (np.ones(arrshape) * 41.269065) \n\tif (distance is not None): \n\t \tdistance = (np.ones(arrshape) * distance) \n\tprint(raarr, decarr, distance) \n\tc = ICRS(ra=(raarr * u.deg), dec=(decarr * u.deg), distance=distance) \n\tg = c.transform_to(Galactic) \n\tassert (g.l.shape == arrshape) \n\tnpt.assert_array_almost_equal(g.l.degree, 121.17440967) \n\tnpt.assert_array_almost_equal(g.b.degree, (-21.57299631)) \n\tif (distance is not None): \n\t \tassert (g.distance.unit == c.distance.unit) \n\tc2 = c.transform_to(FK5).transform_to(ICRS) \n\tnpt.assert_array_almost_equal(c.ra.radian, c2.ra.radian) \n\tnpt.assert_array_almost_equal(c.dec.radian, c2.dec.radian) \n\tassert (c2.ra.shape == arrshape) \n\tif (distance is not None): \n\t \tassert (c2.distance.unit == c.distance.unit) \n\tfk4 = c.transform_to(FK4) \n\tnpt.assert_array_almost_equal(fk4.ra.degree, 10.0004, decimal=4) \n\tnpt.assert_array_almost_equal(fk4.dec.degree, 40.9953, decimal=4) \n\tassert (fk4.ra.shape == arrshape) \n\tif (distance is not None): \n\t \tassert (fk4.distance.unit == c.distance.unit) \n\tcfk4 = fk4.transform_to(ICRS) \n\tassert (cfk4.ra.shape == arrshape)\n", 
" \tif (not text): \n\t \treturn '' \n\tif text[(-1)].isspace(): \n\t \treturn '' \n\telse: \n\t \tregex = cleanup_regex[include] \n\t \tmatches = regex.search(text) \n\t \tif matches: \n\t \t \treturn matches.group(0) \n\t \telse: \n\t \t \treturn ''\n", 
" \treturn (np.isscalar(x) or (isdense(x) and (x.ndim == 0)))\n", 
" \tif (not isinstance(seq, (list, tuple))): \n\t \treturn seq \n\telse: \n\t \treturn deepfirst(seq[0])\n", 
" \tif (not isinstance(hexnum, six.string_types)): \n\t \tnbits = (intsize * 8) \n\t \thexnum = ('0x%x' % ((hexnum + (1 << nbits)) % (1 << nbits))) \n\ts = hexnum[2:] \n\tif ((len(s) % 2) != 0): \n\t \ts = ('0' + s) \n\tresult = codecs.decode(s, 'hex')[::(-1)] \n\treturn result\n", 
" \t_list.sort((lambda a, b: (a[indexkey] < b[indexkey]))) \n\treturn _list\n", 
" \treturn subprocess.call(cmd, shell=True)\n", 
" \treturn (l if ((l is None) or is_list(l, scalars)) else [l])\n", 
" \tif isinstance(lst, str): \n\t \treturn lst \n\telse: \n\t \treturn ', \t'.join(lst)\n", 
" \treturn where((arr == 1))[0][0]\n", 
" \tw_len = len(word) \n\tw_len_plus_1 = (w_len + 1) \n\ti = 0 \n\twhile (i < w_len): \n\t \tj = (i + 2) \n\t \twhile (j < w_len_plus_1): \n\t \t \t(yield word[i:j]) \n\t \t \tj += 1 \n\t \tif from_beginning_only: \n\t \t \treturn \n\t \ti += 1\n", 
" \tsegment = None \n\toffset = 0 \n\tif start_byte: \n\t \tfh.seek(start_byte) \n\telse: \n\t \tfh.seek(0, os.SEEK_END) \n\ttotal_size = remaining_size = fh.tell() \n\twhile (remaining_size > 0): \n\t \toffset = min(total_size, (offset + buf_size)) \n\t \tfh.seek((- offset), os.SEEK_END) \n\t \tbuf = fh.read(min(remaining_size, buf_size)) \n\t \tremaining_size -= buf_size \n\t \tlines = buf.decode(sys.getfilesystemencoding()).split(u'\\n') \n\t \tif (segment is not None): \n\t \t \tif (buf[(-1)] is not u'\\n'): \n\t \t \t \tlines[(-1)] += segment \n\t \t \telse: \n\t \t \t \t(yield segment) \n\t \tsegment = lines[0] \n\t \tfor index in range((len(lines) - 1), 0, (-1)): \n\t \t \tif len(lines[index]): \n\t \t \t \t(yield lines[index]) \n\t(yield segment)\n", 
" \tminutes = int(round((hours * 60))) \n\treturn ('%02d:%02d' % divmod(minutes, 60))\n", 
" \tprefix = ('-' if (total_seconds < 0) else '') \n\t(hours, rem) = divmod(abs(round(total_seconds)), 3600) \n\t(minutes, seconds) = divmod(rem, 60) \n\tchunks = [] \n\tif hours: \n\t \tchunks.append(str(hours)) \n\t \tmin_format = '{:02}' \n\telse: \n\t \tmin_format = '{}' \n\tchunks.append(min_format.format(minutes)) \n\tchunks.append('{:02}'.format(seconds)) \n\treturn (prefix + ':'.join(chunks))\n", 
" \tt = (t or time.localtime(t)) \n\treturn (time.localtime(t).tm_wday + 1)\n", 
" \t(a, axis) = _chk_asarray(a, axis) \n\treturn np.sum((a * a), axis)\n", 
" \tif isinstance(data, DataFrame): \n\t \tif (columns is not None): \n\t \t \tarrays = [data._ixs(i, axis=1).values for (i, col) in enumerate(data.columns) if (col in columns)] \n\t \telse: \n\t \t \tcolumns = data.columns \n\t \t \tarrays = [data._ixs(i, axis=1).values for i in range(len(columns))] \n\t \treturn (arrays, columns) \n\tif (not len(data)): \n\t \tif isinstance(data, np.ndarray): \n\t \t \tcolumns = data.dtype.names \n\t \t \tif (columns is not None): \n\t \t \t \treturn (([[]] * len(columns)), columns) \n\t \treturn ([], []) \n\tif isinstance(data[0], (list, tuple)): \n\t \treturn _list_to_arrays(data, columns, coerce_float=coerce_float, dtype=dtype) \n\telif isinstance(data[0], collections.Mapping): \n\t \treturn _list_of_dict_to_arrays(data, columns, coerce_float=coerce_float, dtype=dtype) \n\telif isinstance(data[0], Series): \n\t \treturn _list_of_series_to_arrays(data, columns, coerce_float=coerce_float, dtype=dtype) \n\telif isinstance(data[0], Categorical): \n\t \tif (columns is None): \n\t \t \tcolumns = _default_index(len(data)) \n\t \treturn (data, columns) \n\telif (isinstance(data, (np.ndarray, Series, Index)) and (data.dtype.names is not None)): \n\t \tcolumns = list(data.dtype.names) \n\t \tarrays = [data[k] for k in columns] \n\t \treturn (arrays, columns) \n\telse: \n\t \tdata = lmap(tuple, data) \n\t \treturn _list_to_arrays(data, columns, coerce_float=coerce_float, dtype=dtype)\n", 
" \tencoded = binascii.b2a_base64(s)[:(-1)] \n\tif (altchars is not None): \n\t \treturn encoded.translate(string.maketrans('+/', altchars[:2])) \n\treturn encoded\n", 
" \t_list.sort((lambda a, b: (a[indexkey] < b[indexkey]))) \n\treturn _list\n", 
" \tif atomp(items): \n\t \treturn items \n\tif (len(s) == 0): \n\t \treturn items[0] \n\tlst = ([0] * s[0]) \n\tstride = s[0] \n\tfor i in range(s[0]): \n\t \tlst[i] = _fa(items[i::stride], s[1:]) \n\treturn lst\n", 
" \tmatch = app.url_map.bind('').match(url, method=request.method) \n\tresponse = app.view_functions[match[0]](**match[1]) \n\treturn make_response(response)\n", 
" \td = defaultdict(list) \n\tfor (k, v) in ordered_pairs: \n\t \td[k].append(v) \n\tfor (k, v) in d.items(): \n\t \tif (len(v) == 1): \n\t \t \td[k] = v[0] \n\treturn dict(d)\n", 
" \tmodname = os.path.splitext(os.path.basename(__file__))[0] \n\treturn (fix_ext_py(__file__), modname)\n", 
" \tsys.path.insert(0, os.path.join(os.path.dirname(__file__), 'libs'))\n", 
" \tif (not profile): \n\t \treturn None \n\t(_, cur, table) = _connect(profile) \n\tq = profile.get('get_query', 'SELECT \tvalue \tFROM \t{0} \tWHERE \tkey=:key'.format(table)) \n\tres = cur.execute(q, {'key': key}) \n\tres = res.fetchone() \n\tif (not res): \n\t \treturn None \n\treturn msgpack.unpackb(res[0])\n", 
" \tif (lst == []): \n\t \treturn lst \n\tif atomp(lst): \n\t \treturn [lst] \n\treturn (_flatten(lst[0]) + _flatten(lst[1:]))\n", 
" \treturn next(iter(s))\n", 
" \tattr = GetNodeAnnotation(node, annotation, set()) \n\tattr.add(value) \n\tSetNodeAnnotation(node, annotation, attr)\n", 
" \ttry: \n\t \tfile = open(fileName, writeMode) \n\t \tfile.write(fileText) \n\t \tfile.close() \n\texcept IOError: \n\t \tprint (('The \tfile \t' + fileName) + ' \tcan \tnot \tbe \twritten \tto.')\n", 
" \ttry: \n\t \tfile = open(fileName, writeMode) \n\t \tfile.write(fileText) \n\t \tfile.close() \n\texcept IOError: \n\t \tprint (('The \tfile \t' + fileName) + ' \tcan \tnot \tbe \twritten \tto.')\n", 
" \treturn _compile(pattern, 0).split(string, maxsplit)\n", 
" \tindexOfCharacter = getIndexOfStartingWithSecond(character, splitLine) \n\tif (indexOfCharacter < 0): \n\t \treturn None \n\treturn splitLine[indexOfCharacter][1:]\n", 
" \t_test_fetch('ftp://speedtest.tele2.net/1KB.zip')\n", 
" \tcaret_tester.js.load('position_caret/invisible.html', style=style) \n\tcaret_tester.check()\n", 
" \tretval = {} \n\tcounter = 0 \n\twhile (counter < len(list)): \n\t \tretval[list[counter]] = list[(counter + 1)] \n\t \tcounter += 2 \n\t__debug('__options2hash \treturning:', retval) \n\treturn retval\n", 
" \treturn next(iter(seq))\n", 
" \treturn (os.path.join(basename, filename) for (basename, dirnames, filenames) in os.walk(folder) for filename in filenames if filename.endswith(suffix))\n", 
" \ttry: \n\t \tmatch = re.match('^[0-9a-f]{32}$', request.POST['md5']) \n\t \tif match: \n\t \t \tmd5 = request.POST['md5'] \n\t \t \tquery = request.POST['q'] \n\t \t \tcode = request.POST['code'] \n\t \t \tmatches = [] \n\t \t \tif (code == 'java'): \n\t \t \t \tsrc = os.path.join(settings.UPLD_DIR, (md5 + '/java_source/')) \n\t \t \t \text = '.java' \n\t \t \telif (code == 'smali'): \n\t \t \t \tsrc = os.path.join(settings.UPLD_DIR, (md5 + '/smali_source/')) \n\t \t \t \text = '.smali' \n\t \t \telse: \n\t \t \t \treturn HttpResponseRedirect('/error/') \n\t \t \tfor (dir_name, sub_dir, files) in os.walk(src): \n\t \t \t \tfor jfile in files: \n\t \t \t \t \tif jfile.endswith(ext): \n\t \t \t \t \t \tfile_path = os.path.join(src, dir_name, jfile) \n\t \t \t \t \t \tif ('+' in jfile): \n\t \t \t \t \t \t \tfp2 = os.path.join(src, dir_name, jfile.replace('+', 'x')) \n\t \t \t \t \t \t \tshutil.move(file_path, fp2) \n\t \t \t \t \t \t \tfile_path = fp2 \n\t \t \t \t \t \tfileparam = file_path.replace(src, '') \n\t \t \t \t \t \twith io.open(file_path, mode='r', encoding='utf8', errors='ignore') as file_pointer: \n\t \t \t \t \t \t \tdat = file_pointer.read() \n\t \t \t \t \t \tif (query in dat): \n\t \t \t \t \t \t \tmatches.append(((((((\"<a \thref='../ViewSource/?file=\" + escape(fileparam)) + '&md5=') + md5) + \"&type=apk'>\") + escape(fileparam)) + '</a>')) \n\t \tflz = len(matches) \n\t \tcontext = {'title': 'Search \tResults', 'matches': matches, 'term': query, 'found': str(flz)} \n\t \ttemplate = 'general/search.html' \n\t \treturn render(request, template, context) \n\texcept: \n\t \tPrintException('[ERROR] \tSearching \tFailed') \n\t \treturn HttpResponseRedirect('/error/')\n", 
" \treturn random.randint(start, end)\n", 
" \tnow = time.strftime(format, time.localtime()) \n\ttry: \n\t \tnow_decoded = codecs.utf_8_decode(now)[0] \n\texcept UnicodeDecodeError: \n\t \tnow_decoded = time.strftime('%Y_%m_%d_%H%M', time.localtime()) \n\treturn now_decoded\n", 
" \tconfig = convert_config_from_json(config) \n\tif (config is None): \n\t \tlogging.error('Invalid \tconfiguration \tfor \tapplication') \n\t \treturn BAD_PID \n\tif (not misc.is_app_name_valid(config['app_name'])): \n\t \tlogging.error(('Invalid \tapp \tname \tfor \tapplication: \t' + config['app_name'])) \n\t \treturn BAD_PID \n\tlogging.info(('Starting \t%s \tapplication \t%s' % (config['language'], config['app_name']))) \n\tenv_vars = config['env_vars'] \n\tenv_vars['GOPATH'] = '/root/appscale/AppServer/gopath/' \n\tenv_vars['GOROOT'] = '/root/appscale/AppServer/goroot/' \n\twatch = ('app___' + config['app_name']) \n\tmatch_cmd = '' \n\tif ((config['language'] == constants.PYTHON27) or (config['language'] == constants.GO) or (config['language'] == constants.PHP)): \n\t \tstart_cmd = create_python27_start_cmd(config['app_name'], config['load_balancer_ip'], config['app_port'], config['load_balancer_ip'], config['xmpp_ip']) \n\t \tstop_cmd = create_python27_stop_cmd(config['app_port']) \n\t \tenv_vars.update(create_python_app_env(config['load_balancer_ip'], config['app_name'])) \n\telif (config['language'] == constants.JAVA): \n\t \tremove_conflicting_jars(config['app_name']) \n\t \tcopy_successful = copy_modified_jars(config['app_name']) \n\t \tif (not copy_successful): \n\t \t \treturn BAD_PID \n\t \tmax_heap = (config['max_memory'] - 250) \n\t \tif (max_heap <= 0): \n\t \t \treturn BAD_PID \n\t \tstart_cmd = create_java_start_cmd(config['app_name'], config['app_port'], config['load_balancer_ip'], max_heap) \n\t \tmatch_cmd = 'java \t-ea \t-cp.*--port={}.*{}'.format(str(config['app_port']), os.path.dirname(locate_dir((('/var/apps/' + config['app_name']) + '/app/'), 'WEB-INF'))) \n\t \tstop_cmd = create_java_stop_cmd(config['app_port']) \n\t \tenv_vars.update(create_java_app_env(config['app_name'])) \n\telse: \n\t \tlogging.error(('Unknown \tapplication \tlanguage \t%s \tfor \tappname \t%s' % (config['language'], config['app_name']))) \n\t \treturn BAD_PID \n\tlogging.info(('Start \tcommand: \t' + str(start_cmd))) \n\tlogging.info(('Stop \tcommand: \t' + str(stop_cmd))) \n\tlogging.info(('Environment \tvariables: \t' + str(env_vars))) \n\tsyslog_server = '' \n\tif ('syslog_server' in config): \n\t \tsyslog_server = config['syslog_server'] \n\tmonit_app_configuration.create_config_file(str(watch), str(start_cmd), str(stop_cmd), [config['app_port']], env_vars, config['max_memory'], syslog_server, appscale_info.get_private_ip(), match_cmd=match_cmd) \n\tfull_watch = '{}-{}'.format(str(watch), str(config['app_port'])) \n\tif (not monit_interface.start(full_watch, is_group=False)): \n\t \tlogging.warning('Monit \twas \tunable \tto \tstart \t{}:{}'.format(str(config['app_name']), config['app_port'])) \n\t \treturn BAD_PID \n\tthreading.Thread(target=add_routing, args=(config['app_name'], config['app_port'])).start() \n\tif ('log_size' in config.keys()): \n\t \tlog_size = config['log_size'] \n\telif (config['app_name'] == APPSCALE_DASHBOARD_ID): \n\t \tlog_size = DASHBOARD_LOG_SIZE \n\telse: \n\t \tlog_size = APP_LOG_SIZE \n\tif (not setup_logrotate(config['app_name'], watch, log_size)): \n\t \tlogging.error('Error \twhile \tsetting \tup \tlog \trotation \tfor \tapplication: \t{}'.format(config['app_name'])) \n\treturn 0\n", 
" \treturn _randone(parse(regex_string), limit)\n", 
" \tlen_col = len(col_names) \n\ttry: \n\t \tif (data.shape != (len(row_names), len_col)): \n\t \t \traise ValueError((\"Data \tshape \tof \t%s \tdoesn't \tmatch \theader \tsizes \t%s \t%s\" % (data.shape, len(row_names), len(col_names)))) \n\texcept AttributeError: \n\t \ttry: \n\t \t \tif ((not numpy.all([(len_col == len(row)) for row in data])) or (len(row_names) != len(data))): \n\t \t \t \traise ValueError((\"Data \tshape \tdoesn't \tmatch \theader \tsizes \t%s \t%s\" % (len(row_names), len(col_names)))) \n\t \texcept: \n\t \t \traise ValueError('Unsupported \tdata \ttype \tfor \tformat_matrix') \n\tlines = [] \n\trow_names = map(str, row_names) \n\tcol_names = map(str, col_names) \n\tlines.append(' DCTB '.join(([''] + col_names))) \n\tfor (row_name, values) in zip(row_names, data): \n\t \tline = [row_name] \n\t \tfor (col_name, value) in zip(col_names, values): \n\t \t \tif (convert_matching_names_to_zero and (col_name == row_name) and numpy.isclose(value, 0.0)): \n\t \t \t \tvalue = 0.0 \n\t \t \tline.append(str(value)) \n\t \tlines.append(' DCTB '.join(line)) \n\treturn '\\n'.join(lines)\n", 
" \tws = '' \n\tconversion_data = ((np.bool, np.int8, np.int8), (np.uint8, np.int8, np.int16), (np.uint16, np.int16, np.int32), (np.uint32, np.int32, np.int64)) \n\tfloat32_max = struct.unpack('<f', '\\xff\\xff\\xff~')[0] \n\tfloat64_max = struct.unpack('<d', '\\xff\\xff\\xff\\xff\\xff\\xff\\xdf\\x7f')[0] \n\tfor col in data: \n\t \tdtype = data[col].dtype \n\t \tfor c_data in conversion_data: \n\t \t \tif (dtype == c_data[0]): \n\t \t \t \tif (data[col].max() <= np.iinfo(c_data[1]).max): \n\t \t \t \t \tdtype = c_data[1] \n\t \t \t \telse: \n\t \t \t \t \tdtype = c_data[2] \n\t \t \t \tif (c_data[2] == np.float64): \n\t \t \t \t \tif (data[col].max() >= (2 ** 53)): \n\t \t \t \t \t \tws = (precision_loss_doc % ('uint64', 'float64')) \n\t \t \t \tdata[col] = data[col].astype(dtype) \n\t \tif (dtype == np.int8): \n\t \t \tif ((data[col].max() > 100) or (data[col].min() < (-127))): \n\t \t \t \tdata[col] = data[col].astype(np.int16) \n\t \telif (dtype == np.int16): \n\t \t \tif ((data[col].max() > 32740) or (data[col].min() < (-32767))): \n\t \t \t \tdata[col] = data[col].astype(np.int32) \n\t \telif (dtype == np.int64): \n\t \t \tif ((data[col].max() <= 2147483620) and (data[col].min() >= (-2147483647))): \n\t \t \t \tdata[col] = data[col].astype(np.int32) \n\t \t \telse: \n\t \t \t \tdata[col] = data[col].astype(np.float64) \n\t \t \t \tif ((data[col].max() >= (2 ** 53)) or (data[col].min() <= (- (2 ** 53)))): \n\t \t \t \t \tws = (precision_loss_doc % ('int64', 'float64')) \n\t \telif (dtype in (np.float32, np.float64)): \n\t \t \tvalue = data[col].max() \n\t \t \tif np.isinf(value): \n\t \t \t \tmsg = 'Column \t{0} \thas \ta \tmaximum \tvalue \tof \tinfinity \twhich \tis \toutside \tthe \trange \tsupported \tby \tStata.' \n\t \t \t \traise ValueError(msg.format(col)) \n\t \t \tif ((dtype == np.float32) and (value > float32_max)): \n\t \t \t \tdata[col] = data[col].astype(np.float64) \n\t \t \telif (dtype == np.float64): \n\t \t \t \tif (value > float64_max): \n\t \t \t \t \tmsg = 'Column \t{0} \thas \ta \tmaximum \tvalue \t({1}) \toutside \tthe \trange \tsupported \tby \tStata \t({1})' \n\t \t \t \t \traise ValueError(msg.format(col, value, float64_max)) \n\tif ws: \n\t \timport warnings \n\t \twarnings.warn(ws, PossiblePrecisionLoss) \n\treturn data\n", 
" \tpenalties = [] \n\tfor key in distance.keys(): \n\t \tkey = key.replace('album_', '') \n\t \tkey = key.replace('track_', '') \n\t \tkey = key.replace('_', ' \t') \n\t \tpenalties.append(key) \n\tif penalties: \n\t \tif (limit and (len(penalties) > limit)): \n\t \t \tpenalties = (penalties[:limit] + ['...']) \n\t \treturn ui.colorize('text_warning', (u'(%s)' % ', \t'.join(penalties)))\n", 
" \tpenalties = [] \n\tfor key in distance.keys(): \n\t \tkey = key.replace('album_', '') \n\t \tkey = key.replace('track_', '') \n\t \tkey = key.replace('_', ' \t') \n\t \tpenalties.append(key) \n\tif penalties: \n\t \tif (limit and (len(penalties) > limit)): \n\t \t \tpenalties = (penalties[:limit] + ['...']) \n\t \treturn ui.colorize('text_warning', (u'(%s)' % ', \t'.join(penalties)))\n", 
" \tgoal = GoldRevenueGoalByDate.get(date) \n\tif (not goal): \n\t \treturn 0 \n\treturn float(goal)\n", 
" \tassert (byteorder == 'big') \n\tassert (not signed) \n\tif ((len(data) % 4) != 0): \n\t \tdata = (('\\x00' * (4 - (len(data) % 4))) + data) \n\tresult = 0 \n\twhile (len(data) > 0): \n\t \t(digit,) = struct.unpack('>I', data[:4]) \n\t \tresult = ((result << 32) + digit) \n\t \tdata = data[4:] \n\treturn result\n", 
" \ts = '{0:0{1}b}'.format(abs(as_int(k)), as_int(abs((n_bits or 0)))) \n\treturn list(map(int, s))\n", 
" \tif (type(l) not in (types.IntType, types.LongType)): \n\t \traise ValueError, 'the \tinput \tmust \tbe \tan \tinteger' \n\tif (l < 0): \n\t \traise ValueError, 'the \tinput \tmust \tbe \tgreater \tthan \t0' \n\ts = '' \n\twhile l: \n\t \ts = (s + chr((l & 255L))) \n\t \tl >>= 8 \n\treturn s\n", 
" \tfor char in SPECIAL_CHARS: \n\t \t(yield (get_char_description(char), char)) \n\tcode = language.code.replace(u'_', u'-').split(u'-')[0] \n\tif (code in EXTRA_CHARS): \n\t \tfor char in EXTRA_CHARS[code]: \n\t \t \t(yield (get_char_description(char), char)) \n\t(yield get_quote(code, DOUBLE_OPEN, _(u'Opening \tdouble \tquote'))) \n\t(yield get_quote(code, DOUBLE_CLOSE, _(u'Closing \tdouble \tquote'))) \n\t(yield get_quote(code, SINGLE_OPEN, _(u'Opening \tsingle \tquote'))) \n\t(yield get_quote(code, SINGLE_CLOSE, _(u'Closing \tsingle \tquote'))) \n\tif (code in HYPHEN_LANGS): \n\t \t(yield (_(u'Hyphen'), u'-')) \n\tif (code in EN_DASH_LANGS): \n\t \t(yield (_(u'En \tdash'), u'\\u2013')) \n\tif (code in EM_DASH_LANGS): \n\t \t(yield (_(u'Em \tdash'), u'\\u2014'))\n", 
" \tfor item in iterable: \n\t \tif predicate(item): \n\t \t \treturn item\n", 
" \tif isinstance(s, str): \n\t \treturn ''.join(s.split()) \n\telse: \n\t \treturn b('').join(s.split())\n", 
" \treturn int(_count_nonzero(x))\n", 
" \thandler_type = url_map.GetHandlerType() \n\tif (handler_type == 'static_files'): \n\t \treturn (url_map.upload + '$') \n\telif (handler_type == 'static_dir'): \n\t \tpath = url_map.static_dir.rstrip(os.path.sep) \n\t \treturn ((path + re.escape(os.path.sep)) + '(.*)') \n\tassert False, 'This \tproperty \tonly \tapplies \tto \tstatic \thandlers.'\n", 
" \tif str_in.startswith(('\"', \"'\")): \n\t \treturn shlex.split(str_in) \n\telse: \n\t \tcomponents = str_in.split(' \t', 1) \n\t \tif (len(components) > 1): \n\t \t \treturn ([components[0]] + SplitIntoComponents(components[1])) \n\t \telse: \n\t \t \treturn components\n", 
" \tif (not (is_string_like(repl) or callable(repl))): \n\t \traise TypeError('repl \tmust \tbe \ta \tstring \tor \tcallable') \n\tuse_re = ((not case) or (len(pat) > 1) or flags or callable(repl)) \n\tif use_re: \n\t \tif (not case): \n\t \t \tflags |= re.IGNORECASE \n\t \tregex = re.compile(pat, flags=flags) \n\t \tn = (n if (n >= 0) else 0) \n\t \tdef f(x): \n\t \t \treturn regex.sub(repl, x, count=n) \n\telse: \n\t \tf = (lambda x: x.replace(pat, repl, n)) \n\treturn _na_map(f, arr)\n", 
" \twith open(filename, 'rb') as fp: \n\t \tdata = fp.read() \n\tencodings = ['utf-8', locale.getpreferredencoding(False), 'latin1'] \n\tfor enc in encodings: \n\t \ttry: \n\t \t \tdata = data.decode(enc) \n\t \texcept UnicodeDecodeError: \n\t \t \tcontinue \n\t \tbreak \n\tassert (type(data) != bytes) \n\treturn data\n", 
" \tdef getter(self): \n\t \treturn getattr(self, name) \n\tdef setter(self, value): \n\t \tif isinstance(value, basestring): \n\t \t \ttry: \n\t \t \t \tsetattr(self, name, datetime.strptime(value, u'%Y-%m-%d')) \n\t \t \texcept ValueError: \n\t \t \t \tsetattr(self, name, None) \n\t \telse: \n\t \t \tsetattr(self, name, value) \n\treturn synonym(name, descriptor=property(getter, setter))\n", 
" \tfor (i, x) in enumerate(iterable): \n\t \tif condition(x): \n\t \t \treturn i \n\ttry: \n\t \treturn (i + 1) \n\texcept NameError: \n\t \traise ValueError('iterable \tmust \tbe \tnon-empty')\n", 
" \ttext = re.sub(\"[-'_\\\\s]\", '_', text) \n\ttext = re.sub('_+', '_', text).strip('_') \n\tpat = '([^,\\\\(]*)\\\\((.*?)\\\\)' \n\ttext = re.sub(pat, '\\\\g<1>', text).strip() \n\ttry: \n\t \ttext = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore') \n\t \ttext = unicode(re.sub('[-\\\\s]+', ' \t', text)) \n\texcept UnicodeDecodeError: \n\t \tlog.exception(u\"Failing \tto \tnormalize \t'{0}'\".format(text)) \n\treturn text\n", 
" \ttext = re.sub(\"[-'_\\\\s]\", '_', text) \n\ttext = re.sub('_+', '_', text).strip('_') \n\tpat = '([^,\\\\(]*)\\\\((.*?)\\\\)' \n\ttext = re.sub(pat, '\\\\g<1>', text).strip() \n\ttry: \n\t \ttext = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore') \n\t \ttext = unicode(re.sub('[-\\\\s]+', ' \t', text)) \n\texcept UnicodeDecodeError: \n\t \tlog.exception(u\"Failing \tto \tnormalize \t'{0}'\".format(text)) \n\treturn text\n", 
" \trv = eye(3) \n\trv[(2, 0)] = x \n\trv[(2, 1)] = y \n\treturn rv\n", 
" \trv = eye(3) \n\trv[(2, 0)] = x \n\trv[(2, 1)] = y \n\treturn rv\n", 
" \tsubsets_list = [[]] \n\tfor x in l: \n\t \tsubsets_list += [([x] + t) for t in subsets_list] \n\treturn subsets_list\n", 
" \tlocale = Locale.parse(locale) \n\tif (not lst): \n\t \treturn '' \n\tif (len(lst) == 1): \n\t \treturn lst[0] \n\tif (len(lst) == 2): \n\t \treturn locale.list_patterns['2'].format(*lst) \n\tresult = locale.list_patterns['start'].format(lst[0], lst[1]) \n\tfor elem in lst[2:(-1)]: \n\t \tresult = locale.list_patterns['middle'].format(result, elem) \n\tresult = locale.list_patterns['end'].format(result, lst[(-1)]) \n\treturn result\n", 
" \treturn [elem for elem in lst if (not predicate(elem))]\n", 
" \tmatches = findLinksRel(link_attrs_list, target_rel) \n\tif (not matches): \n\t \treturn None \n\tfirst = matches[0] \n\treturn first.get('href')\n", 
" \tfrom numbers import Number \n\tfor vector in array: \n\t \tif isinstance(vector[0], Number): \n\t \t \tif (not all((isinstance(item, Number) for item in vector))): \n\t \t \t \traise exceptions.PlotlyError('Error \tin \tdataframe. \tMake \tsure \tall \tentries \tof \teach \tcolumn \tare \teither \tnumbers \tor \tstrings.') \n\t \telif isinstance(vector[0], str): \n\t \t \tif (not all((isinstance(item, str) for item in vector))): \n\t \t \t \traise exceptions.PlotlyError('Error \tin \tdataframe. \tMake \tsure \tall \tentries \tof \teach \tcolumn \tare \teither \tnumbers \tor \tstrings.')\n", 
" \tresult = Seq(*tuple(map(Char, s))) \n\tresult.str = ('Str(%s)' % repr(s)) \n\treturn result\n", 
" \tif ((n is None) or (n == 0)): \n\t \tn = (-1) \n\tf = (lambda x: x.rsplit(pat, n)) \n\tres = _na_map(f, arr) \n\treturn res\n", 
" \tp = sabnzbd.misc.real_path(root, value) \n\tif os.path.exists(p): \n\t \treturn (None, value) \n\telse: \n\t \treturn ((T('Folder \t\"%s\" \tdoes \tnot \texist') % p), None)\n", 
" \tfunc = ((use_sudo and run_as_root) or run) \n\twith settings(hide('running', 'warnings'), warn_only=True): \n\t \treturn func(('[ \t-d \t\"%(path)s\" \t]' % locals())).succeeded\n", 
" \tfunc = ((use_sudo and run_as_root) or run) \n\twith settings(hide('running', 'warnings'), warn_only=True): \n\t \treturn func(('[ \t-d \t\"%(path)s\" \t]' % locals())).succeeded\n", 
" \tfunc = ((use_sudo and run_as_root) or run) \n\twith settings(hide('running', 'warnings'), warn_only=True): \n\t \treturn func(('[ \t-d \t\"%(path)s\" \t]' % locals())).succeeded\n", 
" \treturn domhelpers.findElements(document, (lambda n, m=re.compile('h[23]$').match: m(n.nodeName)))\n", 
" \treturn random.choice(string.ascii_letters)\n", 
" \tfrom sympy.matrices import DeferredVector \n\tfrom sympy import Dummy, sympify, Symbol, Function, flatten \n\tif (printer is not None): \n\t \tif inspect.isfunction(printer): \n\t \t \tlambdarepr = printer \n\t \telif inspect.isclass(printer): \n\t \t \tlambdarepr = (lambda expr: printer().doprint(expr)) \n\t \telse: \n\t \t \tlambdarepr = (lambda expr: printer.doprint(expr)) \n\telse: \n\t \tfrom sympy.printing.lambdarepr import lambdarepr \n\tdef sub_args(args, dummies_dict): \n\t \tif isinstance(args, str): \n\t \t \treturn args \n\t \telif isinstance(args, DeferredVector): \n\t \t \treturn str(args) \n\t \telif iterable(args): \n\t \t \tdummies = flatten([sub_args(a, dummies_dict) for a in args]) \n\t \t \treturn ','.join((str(a) for a in dummies)) \n\t \telif isinstance(args, (Function, Symbol)): \n\t \t \tdummies = Dummy() \n\t \t \tdummies_dict.update({args: dummies}) \n\t \t \treturn str(dummies) \n\t \telse: \n\t \t \treturn str(args) \n\tdef sub_expr(expr, dummies_dict): \n\t \ttry: \n\t \t \texpr = sympify(expr).xreplace(dummies_dict) \n\t \texcept Exception: \n\t \t \tif isinstance(expr, DeferredVector): \n\t \t \t \tpass \n\t \t \telif isinstance(expr, dict): \n\t \t \t \tk = [sub_expr(sympify(a), dummies_dict) for a in expr.keys()] \n\t \t \t \tv = [sub_expr(sympify(a), dummies_dict) for a in expr.values()] \n\t \t \t \texpr = dict(zip(k, v)) \n\t \t \telif isinstance(expr, tuple): \n\t \t \t \texpr = tuple((sub_expr(sympify(a), dummies_dict) for a in expr)) \n\t \t \telif isinstance(expr, list): \n\t \t \t \texpr = [sub_expr(sympify(a), dummies_dict) for a in expr] \n\t \treturn expr \n\tdef isiter(l): \n\t \treturn iterable(l, exclude=(str, DeferredVector, NotIterable)) \n\tif (isiter(args) and any((isiter(i) for i in args))): \n\t \tfrom sympy.utilities.iterables import flatten \n\t \timport re \n\t \tdum_args = [str(Dummy(str(i))) for i in range(len(args))] \n\t \titer_args = ','.join([(i if isiter(a) else i) for (i, a) in zip(dum_args, args)]) \n\t \tlstr = lambdastr(flatten(args), expr, printer=printer, dummify=dummify) \n\t \tflat = '__flatten_args__' \n\t \trv = ('lambda \t%s: \t(%s)(*list(%s([%s])))' % (','.join(dum_args), lstr, flat, iter_args)) \n\t \tif (len(re.findall(('\\\\b%s\\\\b' % flat), rv)) > 1): \n\t \t \traise ValueError(('the \tname \t%s \tis \treserved \tby \tlambdastr' % flat)) \n\t \treturn rv \n\tdummies_dict = {} \n\tif dummify: \n\t \targs = sub_args(args, dummies_dict) \n\telif isinstance(args, str): \n\t \tpass \n\telif iterable(args, exclude=DeferredVector): \n\t \targs = ','.join((str(a) for a in args)) \n\tif dummify: \n\t \tif isinstance(expr, str): \n\t \t \tpass \n\t \telse: \n\t \t \texpr = sub_expr(expr, dummies_dict) \n\texpr = lambdarepr(expr) \n\treturn ('lambda \t%s: \t(%s)' % (args, expr))\n", 
" \tif ((soup is None) or (soup.name != 'table')): \n\t \treturn False \n\telif ('table_id' not in htmldict): \n\t \treturn (numtable == 1) \n\ttable_id = htmldict['table_id'] \n\tif isinstance(table_id, six.string_types): \n\t \treturn (('id' in soup.attrs) and (soup['id'] == table_id)) \n\telif isinstance(table_id, int): \n\t \treturn (table_id == numtable) \n\treturn False\n", 
" \twhile 1: \n\t \tdata = fileobj.read(CHUNK) \n\t \ttry: \n\t \t \tparser.feed(data) \n\t \texcept EndOfHeadError: \n\t \t \tbreak \n\t \tif (len(data) != CHUNK): \n\t \t \tbreak \n\treturn parser.http_equiv\n", 
" \tout = np.empty(len(list_of_arrays), dtype=object) \n\tout[:] = list_of_arrays \n\treturn out\n", 
" \treturn (((a0 * a1) + (c0 * b1)), ((b0 * a1) + (d0 * b1)), ((a0 * c1) + (c0 * d1)), ((b0 * c1) + (d0 * d1)), (((a0 * e1) + (c0 * f1)) + e0), (((b0 * e1) + (d0 * f1)) + f0))\n", 
" \treturn read_in(os.path.join(dire, 'FILEPATHS'))\n", 
" \tresult = by(expr, count=expr.count()) \n\tif sort: \n\t \tresult = result.sort('count', ascending=False) \n\treturn result\n", 
" \tresult = by(expr, count=expr.count()) \n\tif sort: \n\t \tresult = result.sort('count', ascending=False) \n\treturn result\n", 
" \tresult = by(expr, count=expr.count()) \n\tif sort: \n\t \tresult = result.sort('count', ascending=False) \n\treturn result\n", 
" \tresult = by(expr, count=expr.count()) \n\tif sort: \n\t \tresult = result.sort('count', ascending=False) \n\treturn result\n", 
" \tresult = by(expr, count=expr.count()) \n\tif sort: \n\t \tresult = result.sort('count', ascending=False) \n\treturn result\n", 
" \tif (not os.path.exists(src)): \n\t \traise IOError((\"No \tsuch \tfile \tor \tdirectory: \t'%s'\" % src)) \n\tif mkdirs: \n\t \tmakedirs(os.path.dirname(dst)) \n\tos.symlink(os.path.abspath(src), dst)\n", 
" \treturn a.nonzero()\n", 
" \tfor (key, value) in dictionary.iteritems(): \n\t \tif (element is value): \n\t \t \treturn key\n", 
" \treturn int(_count_nonzero(x))\n", 
" \tx = queue.pop(index) \n\tqueue.insert(0, x) \n\treturn x\n", 
" \tfrom .dense import Matrix \n\tc = (r if (c is None) else c) \n\tr = as_int(r) \n\tc = as_int(c) \n\treturn Matrix(r, c, (([S.One] * r) * c))\n", 
" \tfor key in keys: \n\t \ttry: \n\t \t \tdel d[key] \n\t \texcept KeyError: \n\t \t \tpass\n", 
" \treturn time.mktime(dt.timetuple())\n", 
" \tkey_to_error = {} \n\tfor error in errors: \n\t \tkey = _time_sort_key(error) \n\t \tkey_to_error.setdefault(key, {}) \n\t \tkey_to_error[key].update(error) \n\tdef sort_key(key_and_error): \n\t \t(key, error) = key_and_error \n\t \treturn (key[0], bool(error.get('task_error')), key[1:]) \n\treturn [error for (key, error) in sorted(key_to_error.items(), key=sort_key, reverse=True)]\n", 
" \ts = s.lstrip() \n\ti = 0 \n\twhile s.startswith((ZERO, '0')): \n\t \ts = re.sub(('^(0|%s)\\\\s*' % ZERO), '', s, 1) \n\t \ti = (i + 1) \n\treturn (s, i)\n", 
" \tg = _prep_create_using(create_using) \n\tsrc_i = df.columns.get_loc(source) \n\ttar_i = df.columns.get_loc(target) \n\tif edge_attr: \n\t \tif (edge_attr is True): \n\t \t \tedge_i = [] \n\t \t \tfor (i, col) in enumerate(df.columns): \n\t \t \t \tif ((col is not source) and (col is not target)): \n\t \t \t \t \tedge_i.append((col, i)) \n\t \telif isinstance(edge_attr, (list, tuple)): \n\t \t \tedge_i = [(i, df.columns.get_loc(i)) for i in edge_attr] \n\t \telse: \n\t \t \tedge_i = [(edge_attr, df.columns.get_loc(edge_attr))] \n\t \tfor row in df.values: \n\t \t \t(s, t) = (row[src_i], row[tar_i]) \n\t \t \tif g.is_multigraph(): \n\t \t \t \tg.add_edge(s, t) \n\t \t \t \tkey = max(g[s][t]) \n\t \t \t \tg[s][t][key].update(((i, row[j]) for (i, j) in edge_i)) \n\t \t \telse: \n\t \t \t \tg.add_edge(s, t) \n\t \t \t \tg[s][t].update(((i, row[j]) for (i, j) in edge_i)) \n\telse: \n\t \tfor row in df.values: \n\t \t \tg.add_edge(row[src_i], row[tar_i]) \n\treturn g\n", 
" \tif (not isinstance(x, (tuple, list))): \n\t \treturn x \n\treturn tuple(map(to_tuple, x))\n", 
" \tif (not isinstance(x, (tuple, list))): \n\t \treturn x \n\treturn tuple(map(to_tuple, x))\n", 
" \tif (not isinstance(x, (tuple, list))): \n\t \treturn x \n\treturn tuple(map(to_tuple, x))\n", 
" \tif outputTo.endswith('stderr'): \n\t \tsys.stderr.write(text) \n\t \tsys.stderr.write('\\n') \n\t \tsys.stderr.flush() \n\t \treturn \n\tif outputTo.endswith('stdout'): \n\t \tsys.stdout.write(text) \n\t \tsys.stdout.write('\\n') \n\t \tsys.stdout.flush() \n\t \treturn \n\tarchive.writeFileText(outputTo, text)\n", 
" \tif (col == '_uuid'): \n\t \tval = row.uuid \n\telse: \n\t \tval = getattr(row, col) \n\tif (isinstance(val, list) and len(val)): \n\t \tif isinstance(val[0], idl.Row): \n\t \t \tval = [v.uuid for v in val] \n\t \tcol_type = row._table.columns[col].type \n\t \tif col_type.is_optional(): \n\t \t \tval = val[0] \n\treturn val\n", 
" \tif (prefix is None): \n\t \tprefix = find_best_blas_type((A, B))[0] \n\tcopy = prefix_copy_index_matrix_map[prefix] \n\tif (not inplace): \n\t \tB = np.copy(B, order='F') \n\ttry: \n\t \tif (not A.is_f_contig()): \n\t \t \traise ValueError() \n\texcept: \n\t \tA = np.asfortranarray(A) \n\tcopy(A, B, np.asfortranarray(index), index_rows, index_cols, is_diagonal) \n\treturn B\n", 
" \tif (prefix is None): \n\t \tprefix = find_best_blas_type((A, B))[0] \n\tcopy = prefix_copy_index_matrix_map[prefix] \n\tif (not inplace): \n\t \tB = np.copy(B, order='F') \n\ttry: \n\t \tif (not A.is_f_contig()): \n\t \t \traise ValueError() \n\texcept: \n\t \tA = np.asfortranarray(A) \n\tcopy(A, B, np.asfortranarray(index), index_rows, index_cols, is_diagonal) \n\treturn B\n", 
" \tif (col == '_uuid'): \n\t \tval = row.uuid \n\telse: \n\t \tval = getattr(row, col) \n\tif (isinstance(val, list) and len(val)): \n\t \tif isinstance(val[0], idl.Row): \n\t \t \tval = [v.uuid for v in val] \n\t \tcol_type = row._table.columns[col].type \n\t \tif col_type.is_optional(): \n\t \t \tval = val[0] \n\treturn val\n", 
" \ttry: \n\t \tos.chdir(directory) \n\texcept Exception as exc: \n\t \terror = DaemonOSEnvironmentError(('Unable \tto \tchange \tworking \tdirectory \t(%(exc)s)' % vars())) \n\t \traise error\n", 
" \treturn ((sql_create(app, style, connection) + sql_custom(app, style, connection)) + sql_indexes(app, style, connection))\n", 
" \treturn binascii.a2b_base64(s)\n", 
" \tif (encoding is None): \n\t \tencoding = select_best_encoding() \n\tif (errors is None): \n\t \terrors = BEHAVE_UNICODE_ERRORS \n\tif isinstance(value, six.text_type): \n\t \treturn value \n\telif isinstance(value, six.binary_type): \n\t \ttry: \n\t \t \treturn six.text_type(value, encoding, errors) \n\t \texcept UnicodeError: \n\t \t \treturn six.u(value) \n\telse: \n\t \ttry: \n\t \t \tif six.PY2: \n\t \t \t \ttry: \n\t \t \t \t \ttext2 = six.text_type(value) \n\t \t \t \texcept UnicodeError as e: \n\t \t \t \t \tdata = str(value) \n\t \t \t \t \ttext2 = six.text_type(data, 'unicode-escape', 'replace') \n\t \t \telse: \n\t \t \t \ttext2 = six.text_type(value) \n\t \texcept UnicodeError as e: \n\t \t \ttext2 = six.text_type(e) \n\t \treturn text2\n", 
" \tif (encoding is None): \n\t \tencoding = select_best_encoding() \n\tif (errors is None): \n\t \terrors = BEHAVE_UNICODE_ERRORS \n\tif isinstance(value, six.text_type): \n\t \treturn value \n\telif isinstance(value, six.binary_type): \n\t \ttry: \n\t \t \treturn six.text_type(value, encoding, errors) \n\t \texcept UnicodeError: \n\t \t \treturn six.u(value) \n\telse: \n\t \ttry: \n\t \t \tif six.PY2: \n\t \t \t \ttry: \n\t \t \t \t \ttext2 = six.text_type(value) \n\t \t \t \texcept UnicodeError as e: \n\t \t \t \t \tdata = str(value) \n\t \t \t \t \ttext2 = six.text_type(data, 'unicode-escape', 'replace') \n\t \t \telse: \n\t \t \t \ttext2 = six.text_type(value) \n\t \texcept UnicodeError as e: \n\t \t \ttext2 = six.text_type(e) \n\t \treturn text2\n", 
" \treturn [word.strip() for word in string.split(sep) if word.strip()]\n", 
" \treturn [(x - n), (y + n), (z - n), (x - n), (y + n), (z + n), (x + n), (y + n), (z + n), (x + n), (y + n), (z - n), (x - n), (y - n), (z - n), (x + n), (y - n), (z - n), (x + n), (y - n), (z + n), (x - n), (y - n), (z + n), (x - n), (y - n), (z - n), (x - n), (y - n), (z + n), (x - n), (y + n), (z + n), (x - n), (y + n), (z - n), (x + n), (y - n), (z + n), (x + n), (y - n), (z - n), (x + n), (y + n), (z - n), (x + n), (y + n), (z + n), (x - n), (y - n), (z + n), (x + n), (y - n), (z + n), (x + n), (y + n), (z + n), (x - n), (y + n), (z + n), (x + n), (y - n), (z - n), (x - n), (y - n), (z - n), (x - n), (y + n), (z - n), (x + n), (y + n), (z - n)]\n", 
" \tfor (mu, sigma, marker) in [((-0.5), 0.75, 'o'), (0.75, 1.0, 's')]: \n\t \t(x, y) = prng.normal(loc=mu, scale=sigma, size=(2, nb_samples)) \n\t \tax.plot(x, y, ls='none', marker=marker) \n\tax.set_xlabel('X-label') \n\treturn ax\n", 
" \tif (backend == 'requests'): \n\t \treq = requests.get(url, headers={'User-Agent': UA}) \n\t \thtml_doc = req.text \n\t \tsoup = BeautifulSoup(html_doc) \n\t \tif (soup.find('div', attrs={'id': 'gs_ab_md'}) is None): \n\t \t \tprint 'Falling \tback \ton \tto \tselenium \tbackend \tdue \tto \tcaptcha.' \n\t \t \tbackend = 'selenium' \n\tif (backend == 'selenium'): \n\t \tfrom selenium import webdriver \n\t \timport selenium.webdriver.support.ui as ui \n\t \tdriver = webdriver.Firefox() \n\t \twait = ui.WebDriverWait(driver, 200) \n\t \tdriver.get(url) \n\t \twait.until((lambda driver: driver.find_elements_by_id('gs_ab_md'))) \n\t \thtml_doc = driver.page_source \n\t \tsoup = BeautifulSoup(html_doc) \n\t \tdriver.close() \n\treturn soup\n", 
" \treturn (((prefix + 'm') + str((row + 1))) + str((column + 1)))\n", 
" \ttry: \n\t \treturn _STRING_COL_CACHE[idx] \n\texcept KeyError: \n\t \traise ValueError('Invalid \tcolumn \tindex \t{0}'.format(idx))\n", 
" \tresult = [] \n\tfor i in range(len(matlist[index1])): \n\t \tmatlist[index1][i] = (matlist[index1][i] + (k * matlist[index2][i])) \n\treturn matlist\n", 
" \tws = '' \n\tconversion_data = ((np.bool, np.int8, np.int8), (np.uint8, np.int8, np.int16), (np.uint16, np.int16, np.int32), (np.uint32, np.int32, np.int64)) \n\tfloat32_max = struct.unpack('<f', '\\xff\\xff\\xff~')[0] \n\tfloat64_max = struct.unpack('<d', '\\xff\\xff\\xff\\xff\\xff\\xff\\xdf\\x7f')[0] \n\tfor col in data: \n\t \tdtype = data[col].dtype \n\t \tfor c_data in conversion_data: \n\t \t \tif (dtype == c_data[0]): \n\t \t \t \tif (data[col].max() <= np.iinfo(c_data[1]).max): \n\t \t \t \t \tdtype = c_data[1] \n\t \t \t \telse: \n\t \t \t \t \tdtype = c_data[2] \n\t \t \t \tif (c_data[2] == np.float64): \n\t \t \t \t \tif (data[col].max() >= (2 ** 53)): \n\t \t \t \t \t \tws = (precision_loss_doc % ('uint64', 'float64')) \n\t \t \t \tdata[col] = data[col].astype(dtype) \n\t \tif (dtype == np.int8): \n\t \t \tif ((data[col].max() > 100) or (data[col].min() < (-127))): \n\t \t \t \tdata[col] = data[col].astype(np.int16) \n\t \telif (dtype == np.int16): \n\t \t \tif ((data[col].max() > 32740) or (data[col].min() < (-32767))): \n\t \t \t \tdata[col] = data[col].astype(np.int32) \n\t \telif (dtype == np.int64): \n\t \t \tif ((data[col].max() <= 2147483620) and (data[col].min() >= (-2147483647))): \n\t \t \t \tdata[col] = data[col].astype(np.int32) \n\t \t \telse: \n\t \t \t \tdata[col] = data[col].astype(np.float64) \n\t \t \t \tif ((data[col].max() >= (2 ** 53)) or (data[col].min() <= (- (2 ** 53)))): \n\t \t \t \t \tws = (precision_loss_doc % ('int64', 'float64')) \n\t \telif (dtype in (np.float32, np.float64)): \n\t \t \tvalue = data[col].max() \n\t \t \tif np.isinf(value): \n\t \t \t \tmsg = 'Column \t{0} \thas \ta \tmaximum \tvalue \tof \tinfinity \twhich \tis \toutside \tthe \trange \tsupported \tby \tStata.' \n\t \t \t \traise ValueError(msg.format(col)) \n\t \t \tif ((dtype == np.float32) and (value > float32_max)): \n\t \t \t \tdata[col] = data[col].astype(np.float64) \n\t \t \telif (dtype == np.float64): \n\t \t \t \tif (value > float64_max): \n\t \t \t \t \tmsg = 'Column \t{0} \thas \ta \tmaximum \tvalue \t({1}) \toutside \tthe \trange \tsupported \tby \tStata \t({1})' \n\t \t \t \t \traise ValueError(msg.format(col, value, float64_max)) \n\tif ws: \n\t \timport warnings \n\t \twarnings.warn(ws, PossiblePrecisionLoss) \n\treturn data\n", 
" \ttry: \n\t \tbits = [] \n\t \tfor x in arg.split(u':'): \n\t \t \tif (len(x) == 0): \n\t \t \t \tbits.append(None) \n\t \t \telse: \n\t \t \t \tbits.append(int(x)) \n\t \treturn value[slice(*bits)] \n\texcept (ValueError, TypeError): \n\t \treturn value\n", 
" \treturn _na_map(sep.join, arr)\n", 
" \tif (n == 0): \n\t \treturn (0, 0, 0, 0) \n\tv = multiplicity(4, n) \n\tn //= (4 ** v) \n\tif ((n % 8) == 7): \n\t \td = 2 \n\t \tn = (n - 4) \n\telif (((n % 8) == 6) or ((n % 8) == 2)): \n\t \td = 1 \n\t \tn = (n - 1) \n\telse: \n\t \td = 0 \n\t(x, y, z) = sum_of_three_squares(n) \n\treturn _sorted_tuple(((2 ** v) * d), ((2 ** v) * x), ((2 ** v) * y), ((2 ** v) * z))\n", 
" \tif isinstance(value, list): \n\t \tvalue = u', \t'.join([v.strip() for v in value]) \n\treturn value\n", 
" \tpayload = pkt.getlayer(Raw).load \n\t(usr, pswd) = (None, None) \n\tif (('username' in payload) or ('password' in payload)): \n\t \tusr = re.search('username=(.*?)(&|$| \t)', payload) \n\t \tpswd = re.search('password=(.*?)(&|$| \t)', payload) \n\t \tif (usr is not None): \n\t \t \tusr = usr.groups(0)[0] \n\t \tif (pswd is not None): \n\t \t \tpswd = pswd.groups(0)[0] \n\telif ('Authorization:' in payload): \n\t \tpw = re.search('Authorization: \tBasic \t(.*)', payload) \n\t \tif (pw.groups(0) is not None): \n\t \t \tusr = b64decode(pw.groups(0)[0]) \n\treturn (usr, pswd)\n", 
" \tif (not text): \n\t \treturn '' \n\tif text[(-1)].isspace(): \n\t \treturn '' \n\telse: \n\t \tregex = cleanup_regex[include] \n\t \tmatches = regex.search(text) \n\t \tif matches: \n\t \t \treturn matches.group(0) \n\t \telse: \n\t \t \treturn ''\n", 
" \treturn obj[a:b]\n", 
" \tfor item in list_: \n\t \tif (item.get(search_field) == value): \n\t \t \treturn item.get(output_field, value) \n\treturn value\n", 
" \ttry: \n\t \treturn os.path.split(path)[1] \n\texcept: \n\t \treturn ''\n", 
" \treturn dict([(x, i) for (i, x) in enumerate(arr)])\n", 
" \tif ((not hasattr(node, 'op')) or (not isinstance(node.op, T.Elemwise)) or (node.op == T.fill)): \n\t \treturn False \n\tmodels = [] \n\tinputs = [] \n\tfor input in node.inputs: \n\t \tif (input.owner and (input.owner.op == T.fill)): \n\t \t \tmodels.append(input.owner.inputs[0]) \n\t \t \tinputs.append(input.owner.inputs[1]) \n\t \telse: \n\t \t \tinputs.append(input) \n\tif (not models): \n\t \treturn False \n\tc = node.op(*inputs) \n\tfor model in models: \n\t \tif (model.type != c.type): \n\t \t \tc = T.fill(model, c) \n\treplacements = {node.outputs[0]: c} \n\tfor (client, cl_idx) in node.outputs[0].clients: \n\t \tif (hasattr(client, 'op') and isinstance(client.op, T.Elemwise) and (not (client.op == T.fill))): \n\t \t \tclient_inputs = client.inputs[:] \n\t \t \tclient_inputs[cl_idx] = c \n\t \t \tnew_client = client.op(*client_inputs) \n\t \t \tnew_client.owner.outputs[0].clients = client.outputs[0].clients \n\t \t \tr = local_fill_sink.transform(new_client.owner) \n\t \t \tif (not r): \n\t \t \t \tcontinue \n\t \t \treplacements.update(r) \n\treturn replacements\n", 
" \tassert (row1 <= row2) \n\tassert (col1 <= col2) \n\treturn ((rowcol_to_cell(row1, col1, row1_abs, col1_abs) + ':') + rowcol_to_cell(row2, col2, row2_abs, col2_abs))\n", 
" \tvalue = re.sub('\\\\r\\\\n|\\\\r|\\\\n', '\\n', force_unicode(value)) \n\tparas = re.split('\\n{2,}', value) \n\tif autoescape: \n\t \tparas = [(u'<p>%s</p>' % escape(p).replace('\\n', '<br \t/>')) for p in paras] \n\telse: \n\t \tparas = [(u'<p>%s</p>' % p.replace('\\n', '<br \t/>')) for p in paras] \n\treturn u'\\n\\n'.join(paras)\n", 
" \tarr = arr.fillna('') \n\ttry: \n\t \tarr = ((sep + arr) + sep) \n\texcept TypeError: \n\t \tarr = ((sep + arr.astype(str)) + sep) \n\ttags = set() \n\tfor ts in arr.str.split(sep): \n\t \ttags.update(ts) \n\ttags = sorted((tags - set(['']))) \n\tdummies = np.empty((len(arr), len(tags)), dtype=np.int64) \n\tfor (i, t) in enumerate(tags): \n\t \tpat = ((sep + t) + sep) \n\t \tdummies[:, i] = lib.map_infer(arr.values, (lambda x: (pat in x))) \n\treturn (dummies, tags)\n", 
" \tif (not seq): \n\t \treturn [] \n\t(current, groups) = ([seq[0]], []) \n\tfor elem in seq[1:]: \n\t \tif (elem == current[(-1)]): \n\t \t \tcurrent.append(elem) \n\t \telse: \n\t \t \tgroups.append(current) \n\t \t \tcurrent = [elem] \n\tgroups.append(current) \n\tif multiple: \n\t \treturn groups \n\tfor (i, current) in enumerate(groups): \n\t \tgroups[i] = (current[0], len(current)) \n\treturn groups\n", 
" \tresult = {} \n\tfor match in re.finditer('^([A-Za-z_][A-Za-z0-9_]*) \t*= \t*([^#]*)', content, re.MULTILINE): \n\t \tresult[match.group(1)] = match.group(2).strip() \n\treturn result\n", 
" \treturn map((lambda x: x[0]), IMAGE_URL_RE.findall(text))\n", 
" \treturn map((lambda x: x[0]), IMAGE_URL_RE.findall(text))\n", 
" \tif all: \n\t \treturn PUNCTUATION_REGEX.sub('', s.strip()) \n\telse: \n\t \treturn s.strip().strip(string.punctuation)\n", 
" \tif (len(cdate) < 29): \n\t \tif (len(cdate) < 5): \n\t \t \treturn utc_now() \n\tcdate = cdate[5:] \n\ttry: \n\t \tt = time.strptime(cdate, '%d-%b-%Y \t%H:%M:%S \t%Z') \n\texcept ValueError: \n\t \ttry: \n\t \t \tt = time.strptime(cdate, '%d-%b-%y \t%H:%M:%S \t%Z') \n\t \texcept ValueError: \n\t \t \ttry: \n\t \t \t \tt = time.strptime(cdate, '%d \t%b \t%Y \t%H:%M:%S \t%Z') \n\t \t \texcept ValueError: \n\t \t \t \traise Exception, ((('ValueError: \tDate \t\"{0}\" \tdoes \tnot \tmatch \tany \tof \t'.format(cdate) + '\"%d-%b-%Y \t%H:%M:%S \t%Z\", \t') + '\"%d-%b-%y \t%H:%M:%S \t%Z\", \t') + '\"%d \t%b \t%Y \t%H:%M:%S \t%Z\".') \n\treturn calendar.timegm(t)\n", 
" \tfrom .dense import Matrix \n\tc = (r if (c is None) else c) \n\tr = as_int(r) \n\tc = as_int(c) \n\treturn Matrix(r, c, (([S.One] * r) * c))\n", 
" \tlabel = name.replace('_', ' \t') \n\tif capitalize: \n\t \tlabel = label.capitalize() \n\treturn label\n", 
" \tif (not isabs(path)): \n\t \tpath = join(os.getcwd(), path) \n\treturn normpath(path)\n", 
" \tfor c in ['\\n', ' DCTB ', ' \t']: \n\t \tt = t.replace(c, '') \n\treturn t\n", 
" \treturn os.path.splitext(fileName)[0]\n", 
" \targs = [list(iterable) for iterable in args] \n\tn = max(map(len, args)) \n\tv = kwargs.get('default', None) \n\treturn _zip(*[(i + ([v] * (n - len(i)))) for i in args])\n", 
" \tcommon.validate_file_path(src) \n\tcommon.validate_file_path(dst) \n\tif (metadata is None): \n\t \tmetadata = {} \n\t \tcopy_meta = 'COPY' \n\telse: \n\t \tcopy_meta = 'REPLACE' \n\tmetadata.update({'x-goog-copy-source': src, 'x-goog-metadata-directive': copy_meta}) \n\tapi = storage_api._get_storage_api(retry_params=retry_params) \n\t(status, resp_headers, content) = api.put_object(api_utils._quote_filename(dst), headers=metadata) \n\terrors.check_status(status, [200], src, metadata, resp_headers, body=content)\n", 
" \tnames = list(r.dtype.names) \n\tarrays = [r[name] for name in names] \n\tfor (attr, func, outname) in summaryfuncs: \n\t \tnames.append(outname) \n\t \tarrays.append(np.asarray(func(r[attr]))) \n\treturn np.rec.fromarrays(arrays, names=names)\n", 
" \tresult = pd.merge(lhs, rhs, left_on=t.on_left, right_on=t.on_right, how=t.how, suffixes=t.suffixes) \n\treturn result.reset_index()[t.fields]\n", 
" \tif (key in item): \n\t \treturn item[key] \n\tnested_item = item \n\tfor subkey in key.split('.'): \n\t \tif (not subkey): \n\t \t \traise ValueError(('empty \tsubkey \tin \t%r' % key)) \n\t \ttry: \n\t \t \tnested_item = nested_item[subkey] \n\t \texcept KeyError as e: \n\t \t \traise KeyError(('%r \t- \tlooking \tup \tkey \t%r \tin \t%r' % (e, key, nested_item))) \n\telse: \n\t \treturn nested_item\n", 
" \treason = (reason or predicate.__name__) \n\tdef decorate(fn): \n\t \tfn_name = fn.__name__ \n\t \tdef maybe(*args, **kw): \n\t \t \tif predicate(): \n\t \t \t \tmsg = (\"'%s' \tskipped: \t%s\" % (fn_name, reason)) \n\t \t \t \traise SkipTest(msg) \n\t \t \telse: \n\t \t \t \treturn fn(*args, **kw) \n\t \treturn function_named(maybe, fn_name) \n\treturn decorate\n", 
" \tif (CONF.config_drive_format == 'iso9660'): \n\t \tconfig_drive_type = 'cdrom' \n\telif (CONF.config_drive_format == 'vfat'): \n\t \tconfig_drive_type = 'disk' \n\telse: \n\t \traise exception.ConfigDriveUnknownFormat(format=CONF.config_drive_format) \n\treturn config_drive_type\n", 
" \tfor row in query: \n\t \tprint_(((u' \t' * 2) + row['key']))\n", 
" \tif (not num): \n\t \treturn None \n\tif num.isdigit(): \n\t \treturn int(num) \n\tif (not re.match('\\\\w\\\\w:\\\\w\\\\w:\\\\w\\\\w', num)): \n\t \treturn None \n\ttry: \n\t \treturn int(num.replace(':', ''), 16) \n\texcept ValueError: \n\t \treturn None\n", 
" \tif (not column_names): \n\t \treturn [] \n\tif ((not ids) and (not excluded_columns)): \n\t \treturn range(len(column_names)) \n\tif ids: \n\t \tcolumns = [] \n\t \tfor c in ids.split(','): \n\t \t \ttry: \n\t \t \t \tcolumns.append(match_column_identifier(column_names, c, column_offset)) \n\t \t \texcept ColumnIdentifierError: \n\t \t \t \tif (':' in c): \n\t \t \t \t \t(a, b) = c.split(':', 1) \n\t \t \t \telif ('-' in c): \n\t \t \t \t \t(a, b) = c.split('-', 1) \n\t \t \t \telse: \n\t \t \t \t \traise \n\t \t \t \ttry: \n\t \t \t \t \tif a: \n\t \t \t \t \t \ta = int(a) \n\t \t \t \t \telse: \n\t \t \t \t \t \ta = 1 \n\t \t \t \t \tif b: \n\t \t \t \t \t \tb = (int(b) + 1) \n\t \t \t \t \telse: \n\t \t \t \t \t \tb = (len(column_names) + 1) \n\t \t \t \texcept ValueError: \n\t \t \t \t \traise ColumnIdentifierError('Invalid \trange \t%s. \tRanges \tmust \tbe \ttwo \tintegers \tseparated \tby \ta \t- \tor \t: \tcharacter.') \n\t \t \t \tfor x in range(a, b): \n\t \t \t \t \tcolumns.append(match_column_identifier(column_names, x, column_offset)) \n\telse: \n\t \tcolumns = range(len(column_names)) \n\texcludes = [] \n\tif excluded_columns: \n\t \tfor c in excluded_columns.split(','): \n\t \t \ttry: \n\t \t \t \texcludes.append(match_column_identifier(column_names, c, column_offset)) \n\t \t \texcept ColumnIdentifierError: \n\t \t \t \tif (':' in c): \n\t \t \t \t \t(a, b) = c.split(':', 1) \n\t \t \t \telif ('-' in c): \n\t \t \t \t \t(a, b) = c.split('-', 1) \n\t \t \t \telse: \n\t \t \t \t \traise \n\t \t \t \ttry: \n\t \t \t \t \tif a: \n\t \t \t \t \t \ta = int(a) \n\t \t \t \t \telse: \n\t \t \t \t \t \ta = 1 \n\t \t \t \t \tif b: \n\t \t \t \t \t \tb = (int(b) + 1) \n\t \t \t \t \telse: \n\t \t \t \t \t \tb = len(column_names) \n\t \t \t \texcept ValueError: \n\t \t \t \t \traise ColumnIdentifierError('Invalid \trange \t%s. \tRanges \tmust \tbe \ttwo \tintegers \tseparated \tby \ta \t- \tor \t: \tcharacter.') \n\t \t \t \tfor x in range(a, b): \n\t \t \t \t \texcludes.append(match_column_identifier(column_names, x, column_offset)) \n\treturn [c for c in columns if (c not in excludes)]\n", 
" \tif (not sort_parts): \n\t \tsort = query.NullSort() \n\telif (len(sort_parts) == 1): \n\t \tsort = construct_sort_part(model_cls, sort_parts[0]) \n\telse: \n\t \tsort = query.MultipleSort() \n\t \tfor part in sort_parts: \n\t \t \tsort.add_sort(construct_sort_part(model_cls, part)) \n\treturn sort\n", 
" \tdef _by_version(name): \n\t \t'\\n \t \t \t \t \t \t \t \tParse \teach \tcomponent \tof \tthe \tfilename\\n \t \t \t \t \t \t \t \t' \n\t \t(name, ext) = os.path.splitext(name) \n\t \tparts = itertools.chain(name.split('-'), [ext]) \n\t \treturn [packaging.version.parse(part) for part in parts] \n\treturn sorted(names, key=_by_version, reverse=True)\n", 
" \treturn text[::(-1)]\n", 
" \tvar_resolve = Variable(arg).resolve \n\tdecorated = [(var_resolve(item), item) for item in value] \n\tdecorated.sort() \n\tdecorated.reverse() \n\treturn [item[1] for item in decorated]\n", 
" \tmove_entry(blocked_list, blocked, unblocked_list, unblocked)\n", 
" \tmove_entry(blocked_list, blocked, unblocked_list, unblocked)\n", 
" \tmove_entry(blocked_list, blocked, unblocked_list, unblocked)\n", 
" \tm = FLOAT_REGEX.search(value) \n\tif (m is not None): \n\t \treturn float(value) \n\treturn long(value)\n", 
" \tg = _prep_create_using(create_using) \n\tsrc_i = df.columns.get_loc(source) \n\ttar_i = df.columns.get_loc(target) \n\tif edge_attr: \n\t \tif (edge_attr is True): \n\t \t \tedge_i = [] \n\t \t \tfor (i, col) in enumerate(df.columns): \n\t \t \t \tif ((col is not source) and (col is not target)): \n\t \t \t \t \tedge_i.append((col, i)) \n\t \telif isinstance(edge_attr, (list, tuple)): \n\t \t \tedge_i = [(i, df.columns.get_loc(i)) for i in edge_attr] \n\t \telse: \n\t \t \tedge_i = [(edge_attr, df.columns.get_loc(edge_attr))] \n\t \tfor row in df.values: \n\t \t \t(s, t) = (row[src_i], row[tar_i]) \n\t \t \tif g.is_multigraph(): \n\t \t \t \tg.add_edge(s, t) \n\t \t \t \tkey = max(g[s][t]) \n\t \t \t \tg[s][t][key].update(((i, row[j]) for (i, j) in edge_i)) \n\t \t \telse: \n\t \t \t \tg.add_edge(s, t) \n\t \t \t \tg[s][t].update(((i, row[j]) for (i, j) in edge_i)) \n\telse: \n\t \tfor row in df.values: \n\t \t \tg.add_edge(row[src_i], row[tar_i]) \n\treturn g\n", 
" \t(infiles_lists, out_filepaths) = parse_tmp_to_final_filepath_map_file(f) \n\tfor (infiles_list, out_filepath) in zip(infiles_lists, out_filepaths): \n\t \ttry: \n\t \t \tof = open(out_filepath, 'w') \n\t \texcept IOError: \n\t \t \traise IOError(((\"Poller \tcan't \topen \tfinal \toutput \tfile: \t%s\" % out_filepath) + '\\nLeaving \tindividual \tjobs \toutput.\\n \tDo \tyou \thave \twrite \taccess?')) \n\t \tfor fp in infiles_list: \n\t \t \tfor line in open(fp): \n\t \t \t \tof.write(('%s\\n' % line.strip('\\n'))) \n\t \tof.close() \n\treturn True\n", 
" \ttry: \n\t \treturn len(value) \n\texcept (ValueError, TypeError): \n\t \treturn ''\n", 
" \ttry: \n\t \ttext = re.sub('[^\\\\w \t]', '', text) \n\t \treturn [x.strip('.').lower() for x in text.split()] \n\texcept TypeError: \n\t \treturn None\n", 
" \tfunc = ((use_sudo and sudo) or run) \n\tif escape: \n\t \ttext = _escape_for_regex(text) \n\t \tif exact: \n\t \t \ttext = ('^%s$' % text) \n\twith settings(hide('everything'), warn_only=True): \n\t \tegrep_cmd = ('egrep \t\"%s\" \t%s' % (text, _expand_path(filename))) \n\t \tif (not case_sensitive): \n\t \t \tegrep_cmd = egrep_cmd.replace('egrep', 'egrep \t-i', 1) \n\t \treturn func(egrep_cmd, shell=shell).succeeded\n", 
" \treduced_paths = [] \n\tfor p in paths: \n\t \tnp = os.path.normpath(p) \n\t \tif (np not in reduced_paths): \n\t \t \treduced_paths.append(np) \n\treturn reduced_paths\n", 
" \treduced_paths = [] \n\tfor p in paths: \n\t \tnp = os.path.normpath(p) \n\t \tif (np not in reduced_paths): \n\t \t \treduced_paths.append(np) \n\treturn reduced_paths\n", 
" \treduced_paths = [] \n\tfor p in paths: \n\t \tnp = os.path.normpath(p) \n\t \tif (np not in reduced_paths): \n\t \t \treduced_paths.append(np) \n\treturn reduced_paths\n", 
" \tif (axis is None): \n\t \ta = ravel(a) \n\t \toutaxis = 0 \n\telse: \n\t \ta = asarray(a) \n\t \toutaxis = axis \n\treturn (a, outaxis)\n", 
" \tif (isinstance(arr, np.ndarray) or hasattr(arr, 'data')): \n\t \tdata = (arr.data if sparse.issparse(arr) else arr) \n\t \treturn (data.flat[0], data.flat[(-1)]) \n\telse: \n\t \treturn (arr[(0, 0)], arr[((-1), (-1))])\n", 
" \tif (tag is None): \n\t \treturn default \n\tt = tag.find(name) \n\tif ((t is not None) and (t.text is not None)): \n\t \treturn t.text \n\telse: \n\t \treturn default\n", 
" \tf = (lambda x: (x[i] if (len(x) > i) else np.nan)) \n\treturn _na_map(f, arr)\n", 
" \tf = (lambda x: (x[i] if (len(x) > i) else np.nan)) \n\treturn _na_map(f, arr)\n", 
" \tif hasattr(request, '_current_page_cache'): \n\t \treturn request._current_page_cache \n\tdraft = use_draft(request) \n\tpreview = ('preview' in request.GET) \n\tif (use_path is not None): \n\t \tpath = use_path \n\telse: \n\t \tpath = request.path_info \n\t \tpages_root = unquote(reverse('pages-root')) \n\t \tif is_installed('django.contrib.admin'): \n\t \t \tadmin_base = admin_reverse('index') \n\t \telse: \n\t \t \tadmin_base = None \n\t \tif (path.startswith(pages_root) and ((not admin_base) or (not path.startswith(admin_base)))): \n\t \t \tpath = path[len(pages_root):] \n\t \tif path.endswith('/'): \n\t \t \tpath = path[:(-1)] \n\tpage = get_page_from_path(path, preview, draft) \n\tif (draft and page and (not user_can_change_page(request.user, page))): \n\t \tpage = get_page_from_path(path, preview, draft=False) \n\tif (page and (not draft)): \n\t \tancestors = page.get_ancestors().filter((Q(publication_date__gt=timezone.now()) | Q(publication_end_date__lt=timezone.now()))) \n\t \tif ancestors.exists(): \n\t \t \tpage = None \n\trequest._current_page_cache = page \n\treturn page\n", 
" \tregex = re.compile(regex) \n\tif str[0].isdigit(): \n\t \tstr = (replace + str) \n\treturn regex.sub(replace, str)\n", 
" \tif a: \n\t \ta = distutils.version.LooseVersion(a) \n\t \tb = distutils.version.LooseVersion(b) \n\t \tif (a >= b): \n\t \t \treturn True \n\t \telse: \n\t \t \treturn False \n\telse: \n\t \treturn False\n", 
" \tbuild_heap(seq) \n\theap_size = (len(seq) - 1) \n\tfor x in range(heap_size, 0, (-1)): \n\t \t(seq[0], seq[x]) = (seq[x], seq[0]) \n\t \theap_size = (heap_size - 1) \n\t \tmax_heapify(seq, 0, heap_size) \n\treturn seq\n", 
" \tkeys = list(x.keys()) \n\tidx = np.argsort([str(k) for k in keys]) \n\tkeys = [keys[ii] for ii in idx] \n\treturn keys\n", 
" \t_list.sort((lambda a, b: (a[indexkey] < b[indexkey]))) \n\treturn _list\n", 
" \tif (not num): \n\t \treturn None \n\tif num.isdigit(): \n\t \treturn int(num) \n\tif (not re.match('\\\\w\\\\w:\\\\w\\\\w:\\\\w\\\\w', num)): \n\t \treturn None \n\ttry: \n\t \treturn int(num.replace(':', ''), 16) \n\texcept ValueError: \n\t \treturn None\n", 
" \tif (not num): \n\t \treturn None \n\tif num.isdigit(): \n\t \treturn int(num) \n\tif (not re.match('\\\\w\\\\w:\\\\w\\\\w:\\\\w\\\\w', num)): \n\t \treturn None \n\ttry: \n\t \treturn int(num.replace(':', ''), 16) \n\texcept ValueError: \n\t \treturn None\n", 
" \ti = symbols('i', below_fermi=True, cls=Dummy) \n\ta = symbols('a', above_fermi=True, cls=Dummy) \n\tt_ai = AntiSymmetricTensor('t', (a,), (i,)) \n\tai = NO((Fd(a) * F(i))) \n\t(i, j) = symbols('i,j', below_fermi=True, cls=Dummy) \n\t(a, b) = symbols('a,b', above_fermi=True, cls=Dummy) \n\tt_abij = AntiSymmetricTensor('t', (a, b), (i, j)) \n\tabji = NO((((Fd(a) * Fd(b)) * F(j)) * F(i))) \n\tT1 = (t_ai * ai) \n\tT2 = ((Rational(1, 4) * t_abij) * abji) \n\treturn (T1, T2)\n", 
" \tif echo: \n\t \tprint command \n\tsys.stdout.flush() \n\tstdin = (subprocess.PIPE if input else None) \n\tprocess = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, stdin=stdin, shell=True, close_fds=True) \n\tfl = fcntl.fcntl(process.stdout, fcntl.F_GETFL) \n\tfcntl.fcntl(process.stdout, fcntl.F_SETFL, (fl | os.O_NONBLOCK)) \n\tfl = fcntl.fcntl(process.stderr, fcntl.F_GETFL) \n\tfcntl.fcntl(process.stderr, fcntl.F_SETFL, (fl | os.O_NONBLOCK)) \n\tif stdin: \n\t \tprocess.stdin.write(input) \n\t \tprocess.stdin.close() \n\tout = [] \n\terr = [] \n\techo_out = (sys.stdout if echo else None) \n\techo_err = (sys.stderr if echo else None) \n\twhile (__collect_from_stream(process.stdout, out, echo_out) or __collect_from_stream(process.stderr, err, echo_err) or (process.poll() is None)): \n\t \tpass \n\t__collect_from_stream(process.stdout, out, echo_out) \n\t__collect_from_stream(process.stderr, err, echo_err) \n\treturn RunResult(process.returncode, ''.join(out), ''.join(err))\n", 
" \tif echo: \n\t \tprint command \n\tsys.stdout.flush() \n\tstdin = (subprocess.PIPE if input else None) \n\tprocess = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, stdin=stdin, shell=True, close_fds=True) \n\tfl = fcntl.fcntl(process.stdout, fcntl.F_GETFL) \n\tfcntl.fcntl(process.stdout, fcntl.F_SETFL, (fl | os.O_NONBLOCK)) \n\tfl = fcntl.fcntl(process.stderr, fcntl.F_GETFL) \n\tfcntl.fcntl(process.stderr, fcntl.F_SETFL, (fl | os.O_NONBLOCK)) \n\tif stdin: \n\t \tprocess.stdin.write(input) \n\t \tprocess.stdin.close() \n\tout = [] \n\terr = [] \n\techo_out = (sys.stdout if echo else None) \n\techo_err = (sys.stderr if echo else None) \n\twhile (__collect_from_stream(process.stdout, out, echo_out) or __collect_from_stream(process.stderr, err, echo_err) or (process.poll() is None)): \n\t \tpass \n\t__collect_from_stream(process.stdout, out, echo_out) \n\t__collect_from_stream(process.stderr, err, echo_err) \n\treturn RunResult(process.returncode, ''.join(out), ''.join(err))\n", 
" \tif (len(l1) != len(l2)): \n\t \treturn False \n\tfor (x1, x2) in zip(l1, l2): \n\t \tU = unify_walk(x1, x2, U) \n\t \tif (U is False): \n\t \t \treturn False \n\treturn U\n", 
" \ttobj = type(obj) \n\tif isinstance(obj, Index): \n\t \tif isinstance(obj, RangeIndex): \n\t \t \treturn {u'typ': u'range_index', u'klass': u(obj.__class__.__name__), u'name': getattr(obj, 'name', None), u'start': getattr(obj, '_start', None), u'stop': getattr(obj, '_stop', None), u'step': getattr(obj, '_step', None)} \n\t \telif isinstance(obj, PeriodIndex): \n\t \t \treturn {u'typ': u'period_index', u'klass': u(obj.__class__.__name__), u'name': getattr(obj, 'name', None), u'freq': u_safe(getattr(obj, 'freqstr', None)), u'dtype': u(obj.dtype.name), u'data': convert(obj.asi8), u'compress': compressor} \n\t \telif isinstance(obj, DatetimeIndex): \n\t \t \ttz = getattr(obj, 'tz', None) \n\t \t \tif (tz is not None): \n\t \t \t \ttz = u(tz.zone) \n\t \t \t \tobj = obj.tz_convert('UTC') \n\t \t \treturn {u'typ': u'datetime_index', u'klass': u(obj.__class__.__name__), u'name': getattr(obj, 'name', None), u'dtype': u(obj.dtype.name), u'data': convert(obj.asi8), u'freq': u_safe(getattr(obj, 'freqstr', None)), u'tz': tz, u'compress': compressor} \n\t \telif isinstance(obj, MultiIndex): \n\t \t \treturn {u'typ': u'multi_index', u'klass': u(obj.__class__.__name__), u'names': getattr(obj, 'names', None), u'dtype': u(obj.dtype.name), u'data': convert(obj.values), u'compress': compressor} \n\t \telse: \n\t \t \treturn {u'typ': u'index', u'klass': u(obj.__class__.__name__), u'name': getattr(obj, 'name', None), u'dtype': u(obj.dtype.name), u'data': convert(obj.values), u'compress': compressor} \n\telif isinstance(obj, Categorical): \n\t \treturn {u'typ': u'category', u'klass': u(obj.__class__.__name__), u'name': getattr(obj, 'name', None), u'codes': obj.codes, u'categories': obj.categories, u'ordered': obj.ordered, u'compress': compressor} \n\telif isinstance(obj, Series): \n\t \tif isinstance(obj, SparseSeries): \n\t \t \traise NotImplementedError('msgpack \tsparse \tseries \tis \tnot \timplemented') \n\t \telse: \n\t \t \treturn {u'typ': u'series', u'klass': u(obj.__class__.__name__), u'name': getattr(obj, 'name', None), u'index': obj.index, u'dtype': u(obj.dtype.name), u'data': convert(obj.values), u'compress': compressor} \n\telif issubclass(tobj, NDFrame): \n\t \tif isinstance(obj, SparseDataFrame): \n\t \t \traise NotImplementedError('msgpack \tsparse \tframe \tis \tnot \timplemented') \n\t \telse: \n\t \t \tdata = obj._data \n\t \t \tif (not data.is_consolidated()): \n\t \t \t \tdata = data.consolidate() \n\t \t \treturn {u'typ': u'block_manager', u'klass': u(obj.__class__.__name__), u'axes': data.axes, u'blocks': [{u'locs': b.mgr_locs.as_array, u'values': convert(b.values), u'shape': b.values.shape, u'dtype': u(b.dtype.name), u'klass': u(b.__class__.__name__), u'compress': compressor} for b in data.blocks]} \n\telif isinstance(obj, (datetime, date, np.datetime64, timedelta, np.timedelta64, NaTType)): \n\t \tif isinstance(obj, Timestamp): \n\t \t \ttz = obj.tzinfo \n\t \t \tif (tz is not None): \n\t \t \t \ttz = u(tz.zone) \n\t \t \tfreq = obj.freq \n\t \t \tif (freq is not None): \n\t \t \t \tfreq = u(freq.freqstr) \n\t \t \treturn {u'typ': u'timestamp', u'value': obj.value, u'freq': freq, u'tz': tz} \n\t \tif isinstance(obj, NaTType): \n\t \t \treturn {u'typ': u'nat'} \n\t \telif isinstance(obj, np.timedelta64): \n\t \t \treturn {u'typ': u'timedelta64', u'data': obj.view('i8')} \n\t \telif isinstance(obj, timedelta): \n\t \t \treturn {u'typ': u'timedelta', u'data': (obj.days, obj.seconds, obj.microseconds)} \n\t \telif isinstance(obj, np.datetime64): \n\t \t \treturn {u'typ': u'datetime64', u'data': u(str(obj))} \n\t \telif isinstance(obj, datetime): \n\t \t \treturn {u'typ': u'datetime', u'data': u(obj.isoformat())} \n\t \telif isinstance(obj, date): \n\t \t \treturn {u'typ': u'date', u'data': u(obj.isoformat())} \n\t \traise Exception(('cannot \tencode \tthis \tdatetimelike \tobject: \t%s' % obj)) \n\telif isinstance(obj, Period): \n\t \treturn {u'typ': u'period', u'ordinal': obj.ordinal, u'freq': u(obj.freq)} \n\telif isinstance(obj, BlockIndex): \n\t \treturn {u'typ': u'block_index', u'klass': u(obj.__class__.__name__), u'blocs': obj.blocs, u'blengths': obj.blengths, u'length': obj.length} \n\telif isinstance(obj, IntIndex): \n\t \treturn {u'typ': u'int_index', u'klass': u(obj.__class__.__name__), u'indices': obj.indices, u'length': obj.length} \n\telif isinstance(obj, np.ndarray): \n\t \treturn {u'typ': u'ndarray', u'shape': obj.shape, u'ndim': obj.ndim, u'dtype': u(obj.dtype.name), u'data': convert(obj), u'compress': compressor} \n\telif isinstance(obj, np.number): \n\t \tif np.iscomplexobj(obj): \n\t \t \treturn {u'typ': u'np_scalar', u'sub_typ': u'np_complex', u'dtype': u(obj.dtype.name), u'real': u(obj.real.__repr__()), u'imag': u(obj.imag.__repr__())} \n\t \telse: \n\t \t \treturn {u'typ': u'np_scalar', u'dtype': u(obj.dtype.name), u'data': u(obj.__repr__())} \n\telif isinstance(obj, complex): \n\t \treturn {u'typ': u'np_complex', u'real': u(obj.real.__repr__()), u'imag': u(obj.imag.__repr__())} \n\treturn obj\n", 
" \tif (type(str) != type('')): \n\t \treturn map(ascii, str) \n\trv = '' \n\tfor c in str: \n\t \tif ((c in (' DCTB ', '\\n', '\\r')) or (' \t' <= c < chr(127))): \n\t \t \trv = (rv + c) \n\t \telse: \n\t \t \trv = ((rv + '\\\\') + ('x%02.2x' % ord(c))) \n\treturn rv\n", 
" \tlen_col = len(col_names) \n\ttry: \n\t \tif (data.shape != (len(row_names), len_col)): \n\t \t \traise ValueError((\"Data \tshape \tof \t%s \tdoesn't \tmatch \theader \tsizes \t%s \t%s\" % (data.shape, len(row_names), len(col_names)))) \n\texcept AttributeError: \n\t \ttry: \n\t \t \tif ((not numpy.all([(len_col == len(row)) for row in data])) or (len(row_names) != len(data))): \n\t \t \t \traise ValueError((\"Data \tshape \tdoesn't \tmatch \theader \tsizes \t%s \t%s\" % (len(row_names), len(col_names)))) \n\t \texcept: \n\t \t \traise ValueError('Unsupported \tdata \ttype \tfor \tformat_matrix') \n\tlines = [] \n\trow_names = map(str, row_names) \n\tcol_names = map(str, col_names) \n\tlines.append(' DCTB '.join(([''] + col_names))) \n\tfor (row_name, values) in zip(row_names, data): \n\t \tline = [row_name] \n\t \tfor (col_name, value) in zip(col_names, values): \n\t \t \tif (convert_matching_names_to_zero and (col_name == row_name) and numpy.isclose(value, 0.0)): \n\t \t \t \tvalue = 0.0 \n\t \t \tline.append(str(value)) \n\t \tlines.append(' DCTB '.join(line)) \n\treturn '\\n'.join(lines)\n", 
" \tdef getter(value): \n\t \treturn _getattr(value, attr, default) \n\treturn transform(getter)\n", 
" \tfor item in list_: \n\t \tif (item.get(search_field) == value): \n\t \t \treturn item.get(output_field, value) \n\treturn value\n", 
" \tif (isinstance(name, basestring) and (':' in name)): \n\t \treturn tuple(name.split(':', 1)) \n\telse: \n\t \treturn (None, name)\n", 
" \tcode3 = max(code1, ord('A')) \n\tcode4 = min(code2, (ord('Z') + 1)) \n\tif (code3 < code4): \n\t \td = (ord('a') - ord('A')) \n\t \treturn ((code3 + d), (code4 + d)) \n\telse: \n\t \treturn None\n", 
" \ti = 0 \n\tlast_sep_index = (-1) \n\tinside_word = False \n\tfor char in string: \n\t \tif ((ord(char) < 128) and (char in seps)): \n\t \t \tif inside_word: \n\t \t \t \t(yield _Word(span=((last_sep_index + 1), i), value=string[(last_sep_index + 1):i])) \n\t \t \tinside_word = False \n\t \t \tlast_sep_index = i \n\t \telse: \n\t \t \tinside_word = True \n\t \ti += 1 \n\tif inside_word: \n\t \t(yield _Word(span=((last_sep_index + 1), i), value=string[(last_sep_index + 1):i]))\n", 
" \tresult = [] \n\tif ((not _addtail) and (self.text is not None)): \n\t \tresult.append(self.text) \n\tfor elem in self: \n\t \tresult.extend(elem.textlist(True)) \n\tif (_addtail and (self.tail is not None)): \n\t \tresult.append(self.tail) \n\treturn result\n", 
" \tlength = len(instr) \n\tindex = int(math.floor((length * 0.75))) \n\treturn '{0}{1}'.format(instr[:index], ('X' * (length - index)))\n", 
" \tif (not isinstance(X, sp.csc_matrix)): \n\t \traise TypeError(('Expected \tmatrix \tof \tCSC \tformat, \tgot \t%s' % X.format)) \n\tindptr = X.indptr \n\t(n_samples, n_features) = X.shape \n\tmedian = np.zeros(n_features) \n\tfor (f_ind, (start, end)) in enumerate(zip(indptr[:(-1)], indptr[1:])): \n\t \tdata = np.copy(X.data[start:end]) \n\t \tnz = (n_samples - data.size) \n\t \tmedian[f_ind] = _get_median(data, nz) \n\treturn median\n", 
" \tif (len(args) < 1): \n\t \tprint 'usage: \tscript \t<filename>' \n\t \treturn \n\trun_script(args[0])\n", 
" \tdef _is_executable_file_or_link(exe): \n\t \treturn (os.access(exe, os.X_OK) and (os.path.isfile(exe) or os.path.islink(exe))) \n\tif exe: \n\t \tif _is_executable_file_or_link(exe): \n\t \t \treturn exe \n\t \text_list = os.environ.get('PATHEXT', '.EXE').split(';') \n\t \t@real_memoize \n\t \tdef _exe_has_ext(): \n\t \t \t'\\n \t \t \t \t \t \t \t \t \t \t \t \tDo \ta \tcase \tinsensitive \ttest \tif \texe \thas \ta \tfile \textension \tmatch \tin\\n \t \t \t \t \t \t \t \t \t \t \t \tPATHEXT\\n \t \t \t \t \t \t \t \t \t \t \t \t' \n\t \t \tfor ext in ext_list: \n\t \t \t \ttry: \n\t \t \t \t \tpattern = (('.*\\\\.' + ext.lstrip('.')) + '$') \n\t \t \t \t \tre.match(pattern, exe, re.I).groups() \n\t \t \t \t \treturn True \n\t \t \t \texcept AttributeError: \n\t \t \t \t \tcontinue \n\t \t \treturn False \n\t \tsearch_path = ((os.environ.get('PATH') and os.environ['PATH'].split(os.pathsep)) or list()) \n\t \tfor default_path in ['/bin', '/sbin', '/usr/bin', '/usr/sbin', '/usr/local/bin']: \n\t \t \tif (default_path not in search_path): \n\t \t \t \tsearch_path.append(default_path) \n\t \tos.environ['PATH'] = os.pathsep.join(search_path) \n\t \tfor path in search_path: \n\t \t \tfull_path = os.path.join(path, exe) \n\t \t \tif _is_executable_file_or_link(full_path): \n\t \t \t \treturn full_path \n\t \t \telif (is_windows() and (not _exe_has_ext())): \n\t \t \t \tfor ext in ext_list: \n\t \t \t \t \tif _is_executable_file_or_link((full_path + ext)): \n\t \t \t \t \t \treturn (full_path + ext) \n\t \tlog.trace(\"'{0}' \tcould \tnot \tbe \tfound \tin \tthe \tfollowing \tsearch \tpath: \t'{1}'\".format(exe, search_path)) \n\telse: \n\t \tlog.error('No \texecutable \twas \tpassed \tto \tbe \tsearched \tby \tsalt.utils.which()') \n\treturn None\n", 
" \tprint_('Server \tID,Sponsor,Server \tName,Timestamp,Distance,Ping,Download,Upload') \n\tsys.exit(0)\n", 
" \twords = text.split() \n\ttail = words \n\tresult = [] \n\tbuf = '' \n\twhile len(tail): \n\t \t(curWord, tail) = (tail[0], tail[1:]) \n\t \tif (((len(buf) + len(curWord)) + 1) > width): \n\t \t \tif (buf == ''): \n\t \t \t \trow = curWord \n\t \t \t \tbuf = '' \n\t \t \telse: \n\t \t \t \trow = buf \n\t \t \t \tbuf = curWord \n\t \t \trow += (' \t' * (width - len(row))) \n\t \t \tresult.append(row) \n\t \telse: \n\t \t \tif len(buf): \n\t \t \t \tbuf += ' \t' \n\t \t \tbuf += curWord \n\tif len(buf): \n\t \tresult.append((buf + (' \t' * (width - len(buf))))) \n\treturn result\n", 
" \tdef getter(self): \n\t \treturn getattr(self, name) \n\tdef setter(self, value): \n\t \tif isinstance(value, basestring): \n\t \t \ttry: \n\t \t \t \tsetattr(self, name, datetime.strptime(value, u'%Y-%m-%d')) \n\t \t \texcept ValueError: \n\t \t \t \tsetattr(self, name, None) \n\t \telse: \n\t \t \tsetattr(self, name, value) \n\treturn synonym(name, descriptor=property(getter, setter))\n", 
" \txmlfile = None \n\tif Backend.isDbms(DBMS.MSSQL): \n\t \txmlfile = paths.MSSQL_XML \n\telif Backend.isDbms(DBMS.MYSQL): \n\t \txmlfile = paths.MYSQL_XML \n\telif Backend.isDbms(DBMS.ORACLE): \n\t \txmlfile = paths.ORACLE_XML \n\telif Backend.isDbms(DBMS.PGSQL): \n\t \txmlfile = paths.PGSQL_XML \n\tif (not xmlfile): \n\t \treturn \n\tif Backend.isDbms(DBMS.MSSQL): \n\t \thandler = MSSQLBannerHandler(banner, kb.bannerFp) \n\t \tparseXmlFile(xmlfile, handler) \n\t \thandler = FingerprintHandler(banner, kb.bannerFp) \n\t \tparseXmlFile(paths.GENERIC_XML, handler) \n\telse: \n\t \thandler = FingerprintHandler(banner, kb.bannerFp) \n\t \tparseXmlFile(xmlfile, handler) \n\t \tparseXmlFile(paths.GENERIC_XML, handler)\n", 
" \ts = array('c', ('.' * len(arr))) \n\tfor i in xrange(len(arr)): \n\t \tif (arr[i] == 1): \n\t \t \ts[i] = '*' \n\treturn s\n", 
" \t(valid, _value, errmsg) = (False, value, 'space-delimited \tstring') \n\ttry: \n\t \tif hasattr(value, '__iter__'): \n\t \t \tvalid = True \n\t \telse: \n\t \t \t_value = value.split() \n\t \t \tif (_value == []): \n\t \t \t \traise ValueError \n\t \t \tvalid = True \n\texcept AttributeError: \n\t \tpass \n\texcept ValueError: \n\t \tpass \n\treturn (valid, _value, errmsg)\n", 
" \treturn max((len(line) for line in code.splitlines()))\n", 
" \trc = re.compile('|'.join(map(re.escape, word_dic))) \n\tdef translate(match): \n\t \treturn word_dic[match.group(0)] \n\treturn rc.sub(translate, text)\n", 
" \ttry: \n\t \toutfile = open(fname) \n\texcept IOError: \n\t \treturn [] \n\telse: \n\t \tout = outfile.readlines() \n\t \toutfile.close() \n\t \treturn out\n", 
" \treturn dict([(value, key) for (key, value) in mapping.iteritems()])\n", 
" \tif is_binary(filename): \n\t \treturn 'wb' \n\treturn 'w'\n", 
" \tif (iter(seq) is seq): \n\t \tseen = set() \n\t \tseen_add = seen.add \n\t \tfor item in seq: \n\t \t \tif (item in seen): \n\t \t \t \treturn False \n\t \t \tseen_add(item) \n\t \treturn True \n\telse: \n\t \treturn (len(seq) == len(set(seq)))\n", 
" \tkey = next(six.iterkeys(_dict)) \n\tvalue = _dict[key] \n\treturn (key, value)\n", 
" \troles = Role.objects.filter(course_id=course_id).exclude(name=FORUM_ROLE_STUDENT) \n\treturn dict([(role.name, list(role.users.values_list('id', flat=True))) for role in roles])\n", 
" \tws = '' \n\tconversion_data = ((np.bool, np.int8, np.int8), (np.uint8, np.int8, np.int16), (np.uint16, np.int16, np.int32), (np.uint32, np.int32, np.int64)) \n\tfloat32_max = struct.unpack('<f', '\\xff\\xff\\xff~')[0] \n\tfloat64_max = struct.unpack('<d', '\\xff\\xff\\xff\\xff\\xff\\xff\\xdf\\x7f')[0] \n\tfor col in data: \n\t \tdtype = data[col].dtype \n\t \tfor c_data in conversion_data: \n\t \t \tif (dtype == c_data[0]): \n\t \t \t \tif (data[col].max() <= np.iinfo(c_data[1]).max): \n\t \t \t \t \tdtype = c_data[1] \n\t \t \t \telse: \n\t \t \t \t \tdtype = c_data[2] \n\t \t \t \tif (c_data[2] == np.float64): \n\t \t \t \t \tif (data[col].max() >= (2 ** 53)): \n\t \t \t \t \t \tws = (precision_loss_doc % ('uint64', 'float64')) \n\t \t \t \tdata[col] = data[col].astype(dtype) \n\t \tif (dtype == np.int8): \n\t \t \tif ((data[col].max() > 100) or (data[col].min() < (-127))): \n\t \t \t \tdata[col] = data[col].astype(np.int16) \n\t \telif (dtype == np.int16): \n\t \t \tif ((data[col].max() > 32740) or (data[col].min() < (-32767))): \n\t \t \t \tdata[col] = data[col].astype(np.int32) \n\t \telif (dtype == np.int64): \n\t \t \tif ((data[col].max() <= 2147483620) and (data[col].min() >= (-2147483647))): \n\t \t \t \tdata[col] = data[col].astype(np.int32) \n\t \t \telse: \n\t \t \t \tdata[col] = data[col].astype(np.float64) \n\t \t \t \tif ((data[col].max() >= (2 ** 53)) or (data[col].min() <= (- (2 ** 53)))): \n\t \t \t \t \tws = (precision_loss_doc % ('int64', 'float64')) \n\t \telif (dtype in (np.float32, np.float64)): \n\t \t \tvalue = data[col].max() \n\t \t \tif np.isinf(value): \n\t \t \t \tmsg = 'Column \t{0} \thas \ta \tmaximum \tvalue \tof \tinfinity \twhich \tis \toutside \tthe \trange \tsupported \tby \tStata.' \n\t \t \t \traise ValueError(msg.format(col)) \n\t \t \tif ((dtype == np.float32) and (value > float32_max)): \n\t \t \t \tdata[col] = data[col].astype(np.float64) \n\t \t \telif (dtype == np.float64): \n\t \t \t \tif (value > float64_max): \n\t \t \t \t \tmsg = 'Column \t{0} \thas \ta \tmaximum \tvalue \t({1}) \toutside \tthe \trange \tsupported \tby \tStata \t({1})' \n\t \t \t \t \traise ValueError(msg.format(col, value, float64_max)) \n\tif ws: \n\t \timport warnings \n\t \twarnings.warn(ws, PossiblePrecisionLoss) \n\treturn data\n", 
" \tprint(' DCTB '.join((str(f) for f in args)), file=out_file)\n", 
" \tif isinstance(s, unicode): \n\t \treturn s.encode('utf-8') \n\telse: \n\t \treturn str(s)\n", 
" \tfrom unicodedata import name \n\tconverted = [] \n\tfor unich in iter(unicode_string): \n\t \ttry: \n\t \t \tch = unich.decode('ascii') \n\t \texcept UnicodeDecodeError: \n\t \t \twhat = name(unich) \n\t \t \tif (what in _UNICODE_MAP): \n\t \t \t \tch = _UNICODE_MAP[what] \n\t \t \telse: \n\t \t \t \twhat = what.split() \n\t \t \t \tif ((what[0] == 'LATIN') and (what[2] == 'LETTER') and (len(what[3]) == 1)): \n\t \t \t \t \tch = (what[3].lower() if (what[1] == 'SMALL') else what[3].upper()) \n\t \t \t \telse: \n\t \t \t \t \tch = default \n\t \tconverted.append(chr(ord(ch))) \n\treturn ''.join(converted)\n", 
" \tthisurl = 'http://www.google.fr/images/srpr/logo3w.png' \n\timg = display.Image(url=thisurl, width=200, height=200) \n\tnt.assert_equal((u'<img \tsrc=\"%s\" \twidth=\"200\" \theight=\"200\"/>' % thisurl), img._repr_html_()) \n\timg = display.Image(url=thisurl, width=200) \n\tnt.assert_equal((u'<img \tsrc=\"%s\" \twidth=\"200\"/>' % thisurl), img._repr_html_()) \n\timg = display.Image(url=thisurl) \n\tnt.assert_equal((u'<img \tsrc=\"%s\"/>' % thisurl), img._repr_html_()) \n\timg = display.Image(url=thisurl, unconfined=True) \n\tnt.assert_equal((u'<img \tsrc=\"%s\" \tclass=\"unconfined\"/>' % thisurl), img._repr_html_())\n", 
" \tfrom nibabel import load \n\timport numpy as np \n\tif (which.lower() == u'first'): \n\t \tidx = 0 \n\telif (which.lower() == u'middle'): \n\t \tidx = int(np.ceil((load(filename).shape[3] / 2))) \n\telse: \n\t \traise Exception((u'unknown \tvalue \tfor \tvolume \tselection \t: \t%s' % which)) \n\treturn idx\n", 
" \tgc = sum((seq.count(x) for x in ['G', 'C', 'g', 'c', 'S', 's'])) \n\ttry: \n\t \treturn ((gc * 100.0) / len(seq)) \n\texcept ZeroDivisionError: \n\t \treturn 0.0\n", 
" \t_list.sort((lambda a, b: (a[indexkey] < b[indexkey]))) \n\treturn _list\n", 
" \tkeys = list(x.keys()) \n\tidx = np.argsort([str(k) for k in keys]) \n\tkeys = [keys[ii] for ii in idx] \n\treturn keys\n", 
" \t(data, headers, comments) = parse_mapping_file(mapping_file) \n\ttry: \n\t \tcolumn = headers.index(field) \n\texcept ValueError: \n\t \traise ValueError(('Column \t(%s) \tnot \tfound \tin \tmapping \tfile \theaders:\\n \t%s' % (field, ' \t'.join(headers)))) \n\tresults = [(e[0], field_type_f(e[column])) for e in data] \n\tresults.sort(key=itemgetter(1)) \n\treturn results\n", 
" \tif (len(l1) != len(l2)): \n\t \treturn False \n\tfor (x1, x2) in zip(l1, l2): \n\t \tU = unify_walk(x1, x2, U) \n\t \tif (U is False): \n\t \t \treturn False \n\treturn U\n", 
" \tif (makeelement is None): \n\t \tmakeelement = html.html_parser.makeelement \n\troot = _convert_tree(beautiful_soup_tree, makeelement) \n\tchildren = root.getchildren() \n\tfor child in children: \n\t \troot.remove(child) \n\treturn children\n", 
" \tif (if_exists not in ('fail', 'replace', 'append')): \n\t \traise ValueError(\"'{0}' \tis \tnot \tvalid \tfor \tif_exists\".format(if_exists)) \n\tpandas_sql = pandasSQL_builder(con, schema=schema, flavor=flavor) \n\tif isinstance(frame, Series): \n\t \tframe = frame.to_frame() \n\telif (not isinstance(frame, DataFrame)): \n\t \traise NotImplementedError(\"'frame' \targument \tshould \tbe \teither \ta \tSeries \tor \ta \tDataFrame\") \n\tpandas_sql.to_sql(frame, name, if_exists=if_exists, index=index, index_label=index_label, schema=schema, chunksize=chunksize, dtype=dtype)\n", 
" \tif (not getIsBracketed(bracketedString)): \n\t \treturn None \n\tbracketedString = bracketedString.strip().replace('[', '').replace(']', '').replace('(', '').replace(')', '') \n\tif (len(bracketedString) < 1): \n\t \treturn [] \n\tsplitLine = bracketedString.split(',') \n\tfloatList = [] \n\tfor word in splitLine: \n\t \tevaluatedFloat = euclidean.getFloatFromValue(word) \n\t \tif (evaluatedFloat != None): \n\t \t \tfloatList.append(evaluatedFloat) \n\treturn floatList\n", 
" \tseen = {} \n\treturn [seen.setdefault(e, e) for e in l if (e not in seen)]\n", 
" \twhile 1: \n\t \tline = f.readline() \n\t \tif (not line): \n\t \t \tbreak \n\t \t(yield line[:(-1)])\n", 
" \tif (not sequence): \n\t \treturn '[ \t]' \n\treturn ('[ \t%s \t]' % ' \t| \t'.join((unic(item) for item in sequence)))\n", 
" \tif (not sequence): \n\t \treturn '[ \t]' \n\treturn ('[ \t%s \t]' % ' \t| \t'.join((unic(item) for item in sequence)))\n", 
" \tif (not sequence): \n\t \treturn '[ \t]' \n\treturn ('[ \t%s \t]' % ' \t| \t'.join((unic(item) for item in sequence)))\n", 
" \tif (not sequence): \n\t \treturn '[ \t]' \n\treturn ('[ \t%s \t]' % ' \t| \t'.join((unic(item) for item in sequence)))\n", 
" \tif (not sequence): \n\t \treturn '[ \t]' \n\treturn ('[ \t%s \t]' % ' \t| \t'.join((unic(item) for item in sequence)))\n", 
" \tfor nvitem in thelist: \n\t \tif isinstance(nvitem, dict): \n\t \t \t(name, value) = next(six.iteritems(nvitem)) \n\t \t \tif ((names is None) or (name in names)): \n\t \t \t \t(yield (nvitem, name, value))\n", 
" \tfor nvitem in thelist: \n\t \tif isinstance(nvitem, dict): \n\t \t \t(name, value) = next(six.iteritems(nvitem)) \n\t \t \tif ((names is None) or (name in names)): \n\t \t \t \t(yield (nvitem, name, value))\n", 
" \tb = as_int(b) \n\tn = as_int(n) \n\tif (b <= 1): \n\t \traise ValueError('b \tmust \tbe \t>= \t2') \n\telse: \n\t \t(x, y) = (abs(n), []) \n\t \twhile (x >= b): \n\t \t \t(x, r) = divmod(x, b) \n\t \t \ty.append(r) \n\t \ty.append(x) \n\t \ty.append(((- b) if (n < 0) else b)) \n\t \ty.reverse() \n\t \treturn y\n", 
" \ts = (sorted(iterable) if (sort is True) else list(iterable)) \n\tn = len(s) \n\t(f, i) = modf(((a + ((b + n) * p)) - 1)) \n\tif (n == 0): \n\t \traise ValueError('quantile() \targ \tis \tan \tempty \tsequence') \n\tif (f == 0): \n\t \treturn float(s[int(i)]) \n\tif (i < 0): \n\t \treturn float(s[int(i)]) \n\tif (i >= n): \n\t \treturn float(s[(-1)]) \n\ti = int(floor(i)) \n\treturn (s[i] + ((s[(i + 1)] - s[i]) * (c + (d * f))))\n", 
" \ttime.sleep(seconds)\n", 
" \tif (not match('^[,.0-9 \t()\\\\[\\\\]]*$', s)): \n\t \traise Exception('Invalid \tcharacters \tin \tstring \tfor \ttuple \tconversion') \n\tif (s.count('(') != s.count(')')): \n\t \traise Exception('Invalid \tcount \tof \t( \tand \t)') \n\tif (s.count('[') != s.count(']')): \n\t \traise Exception('Invalid \tcount \tof \t[ \tand \t]') \n\tr = eval(s) \n\tif (type(r) not in (list, tuple)): \n\t \traise Exception('Conversion \tfailed') \n\treturn r\n", 
" \treturn getattr(settings, name, default)\n", 
" \tomitNone = False \n\tif (NoneIsLast is None): \n\t \tNoneIsLast = True \n\t \tomitNone = True \n\tn = len(x) \n\tix = range(n) \n\tif (None not in x): \n\t \tix.sort(reverse=decreasing, key=(lambda j: x[j])) \n\telse: \n\t \tdef key(i, x=x): \n\t \t \telem = x[i] \n\t \t \tif (decreasing == NoneIsLast): \n\t \t \t \treturn ((not (elem is None)), elem) \n\t \t \telse: \n\t \t \t \treturn ((elem is None), elem) \n\t \tix = range(n) \n\t \tix.sort(key=key, reverse=decreasing) \n\tif omitNone: \n\t \tn = len(x) \n\t \tfor i in range((n - 1), (-1), (-1)): \n\t \t \tif (x[ix[i]] is None): \n\t \t \t \tn -= 1 \n\t \treturn ix[:n] \n\treturn ix\n", 
" \tif (ss == 'H'): \n\t \treturn 0 \n\tif (ss == 'E'): \n\t \treturn 1 \n\tif (ss == 'C'): \n\t \treturn 2 \n\tassert 0\n", 
" \tif (extra_context is None): \n\t \textra_context = {} \n\tqueryset = queryset._clone() \n\tif paginate_by: \n\t \tpaginator = ObjectPaginator(queryset, paginate_by) \n\t \tif (not page): \n\t \t \tpage = request.GET.get('page', 1) \n\t \ttry: \n\t \t \tpage = int(page) \n\t \t \tobject_list = paginator.get_page((page - 1)) \n\t \texcept (InvalidPage, ValueError): \n\t \t \tif ((page == 1) and allow_empty): \n\t \t \t \tobject_list = [] \n\t \t \telse: \n\t \t \t \traise Http404 \n\t \tc = RequestContext(request, {('%s_list' % template_object_name): object_list, 'is_paginated': (paginator.pages > 1), 'results_per_page': paginate_by, 'has_next': paginator.has_next_page((page - 1)), 'has_previous': paginator.has_previous_page((page - 1)), 'page': page, 'next': (page + 1), 'previous': (page - 1), 'last_on_page': paginator.last_on_page((page - 1)), 'first_on_page': paginator.first_on_page((page - 1)), 'pages': paginator.pages, 'hits': paginator.hits}, context_processors) \n\telse: \n\t \tc = RequestContext(request, {('%s_list' % template_object_name): queryset, 'is_paginated': False}, context_processors) \n\t \tif ((not allow_empty) and (len(queryset) == 0)): \n\t \t \traise Http404 \n\tfor (key, value) in extra_context.items(): \n\t \tif callable(value): \n\t \t \tc[key] = value() \n\t \telse: \n\t \t \tc[key] = value \n\tif (not template_name): \n\t \tmodel = queryset.model \n\t \ttemplate_name = ('%s/%s_list.html' % (model._meta.app_label, model._meta.object_name.lower())) \n\tt = template_loader.get_template(template_name) \n\treturn HttpResponse(t.render(c), mimetype=mimetype)\n", 
" \tnew_mentions = set(re.findall('\\\\[[@|\\\\+].*?\\\\]\\\\(htt[ps]{1,2}:\\\\/\\\\/[a-z\\\\d:.]+?\\\\/([a-z\\\\d]{5})\\\\/\\\\)', comment.content)) \n\tnew_mentions = [m for m in new_mentions if ((m not in comment.ever_mentioned) and validate_contributor(m, contributors))] \n\treturn new_mentions\n", 
" \tif (name == 'stdout'): \n\t \treturn sys.stdout \n\telif (name == 'stderr'): \n\t \treturn sys.stderr \n\telif (name == 'stdin'): \n\t \treturn sys.stdin \n\telse: \n\t \tif (encoding is not None): \n\t \t \timport codecs \n\t \t \tf = codecs.open(name, mode, encoding) \n\t \telse: \n\t \t \tf = open(name, mode) \n\t \tif ('w' in mode): \n\t \t \tos.chmod(name, 384) \n\t \treturn f\n", 
" \to = setattr \n\tif hasattr(obj, '__setitem__'): \n\t \to = type(obj).__setitem__ \n\t[o(obj, k, v) for (k, v) in attrs.iteritems()]\n", 
" \tlc = string.lowercase \n\tlc_len = len(lc) \n\treturn [{'name': (lc[(i % lc_len)] * int((math.ceil((i / lc_len)) + 2)))} for i in range(0, n)]\n", 
" \tlc = string.lowercase \n\tlc_len = len(lc) \n\treturn [{'name': (lc[(i % lc_len)] * int((math.ceil((i / lc_len)) + 2)))} for i in range(0, n)]\n", 
" \tws = '' \n\tconversion_data = ((np.bool, np.int8, np.int8), (np.uint8, np.int8, np.int16), (np.uint16, np.int16, np.int32), (np.uint32, np.int32, np.int64)) \n\tfloat32_max = struct.unpack('<f', '\\xff\\xff\\xff~')[0] \n\tfloat64_max = struct.unpack('<d', '\\xff\\xff\\xff\\xff\\xff\\xff\\xdf\\x7f')[0] \n\tfor col in data: \n\t \tdtype = data[col].dtype \n\t \tfor c_data in conversion_data: \n\t \t \tif (dtype == c_data[0]): \n\t \t \t \tif (data[col].max() <= np.iinfo(c_data[1]).max): \n\t \t \t \t \tdtype = c_data[1] \n\t \t \t \telse: \n\t \t \t \t \tdtype = c_data[2] \n\t \t \t \tif (c_data[2] == np.float64): \n\t \t \t \t \tif (data[col].max() >= (2 ** 53)): \n\t \t \t \t \t \tws = (precision_loss_doc % ('uint64', 'float64')) \n\t \t \t \tdata[col] = data[col].astype(dtype) \n\t \tif (dtype == np.int8): \n\t \t \tif ((data[col].max() > 100) or (data[col].min() < (-127))): \n\t \t \t \tdata[col] = data[col].astype(np.int16) \n\t \telif (dtype == np.int16): \n\t \t \tif ((data[col].max() > 32740) or (data[col].min() < (-32767))): \n\t \t \t \tdata[col] = data[col].astype(np.int32) \n\t \telif (dtype == np.int64): \n\t \t \tif ((data[col].max() <= 2147483620) and (data[col].min() >= (-2147483647))): \n\t \t \t \tdata[col] = data[col].astype(np.int32) \n\t \t \telse: \n\t \t \t \tdata[col] = data[col].astype(np.float64) \n\t \t \t \tif ((data[col].max() >= (2 ** 53)) or (data[col].min() <= (- (2 ** 53)))): \n\t \t \t \t \tws = (precision_loss_doc % ('int64', 'float64')) \n\t \telif (dtype in (np.float32, np.float64)): \n\t \t \tvalue = data[col].max() \n\t \t \tif np.isinf(value): \n\t \t \t \tmsg = 'Column \t{0} \thas \ta \tmaximum \tvalue \tof \tinfinity \twhich \tis \toutside \tthe \trange \tsupported \tby \tStata.' \n\t \t \t \traise ValueError(msg.format(col)) \n\t \t \tif ((dtype == np.float32) and (value > float32_max)): \n\t \t \t \tdata[col] = data[col].astype(np.float64) \n\t \t \telif (dtype == np.float64): \n\t \t \t \tif (value > float64_max): \n\t \t \t \t \tmsg = 'Column \t{0} \thas \ta \tmaximum \tvalue \t({1}) \toutside \tthe \trange \tsupported \tby \tStata \t({1})' \n\t \t \t \t \traise ValueError(msg.format(col, value, float64_max)) \n\tif ws: \n\t \timport warnings \n\t \twarnings.warn(ws, PossiblePrecisionLoss) \n\treturn data\n", 
" \tif (len(multi_inputs) == 0): \n\t \treturn [single_inputs] \n\tmatched_multi_inputs = [] \n\tfirst_multi_input_key = multi_inputs.keys()[0] \n\tfirst_multi_value = multi_inputs.get(first_multi_input_key) \n\tfor value in first_multi_value: \n\t \tnew_inputs = __copy_and_extend_inputs(single_inputs, first_multi_input_key, value) \n\t \tmatched_multi_inputs.append(new_inputs) \n\tfor (multi_input_key, multi_input_values) in multi_inputs.iteritems(): \n\t \tif (multi_input_key == first_multi_input_key): \n\t \t \tcontinue \n\t \tif (len(multi_input_values) != len(first_multi_value)): \n\t \t \traise InputMatchedException() \n\t \tfor (index, value) in enumerate(multi_input_values): \n\t \t \tmatched_multi_inputs[index][multi_input_key] = value \n\treturn matched_multi_inputs\n", 
" \tif (len(multi_inputs) == 0): \n\t \treturn [single_inputs] \n\tmatched_multi_inputs = [] \n\tfirst_multi_input_key = multi_inputs.keys()[0] \n\tfirst_multi_value = multi_inputs.get(first_multi_input_key) \n\tfor value in first_multi_value: \n\t \tnew_inputs = __copy_and_extend_inputs(single_inputs, first_multi_input_key, value) \n\t \tmatched_multi_inputs.append(new_inputs) \n\tfor (multi_input_key, multi_input_values) in multi_inputs.iteritems(): \n\t \tif (multi_input_key == first_multi_input_key): \n\t \t \tcontinue \n\t \tif (len(multi_input_values) != len(first_multi_value)): \n\t \t \traise InputMatchedException() \n\t \tfor (index, value) in enumerate(multi_input_values): \n\t \t \tmatched_multi_inputs[index][multi_input_key] = value \n\treturn matched_multi_inputs\n", 
" \tfor i in unzipIter(filename, directory, overwrite): \n\t \tpass\n", 
" \tfor i in unzipIter(filename, directory, overwrite): \n\t \tpass\n", 
" \tif (len(multi_inputs) == 0): \n\t \treturn [single_inputs] \n\tmatched_multi_inputs = [] \n\tfirst_multi_input_key = multi_inputs.keys()[0] \n\tfirst_multi_value = multi_inputs.get(first_multi_input_key) \n\tfor value in first_multi_value: \n\t \tnew_inputs = __copy_and_extend_inputs(single_inputs, first_multi_input_key, value) \n\t \tmatched_multi_inputs.append(new_inputs) \n\tfor (multi_input_key, multi_input_values) in multi_inputs.iteritems(): \n\t \tif (multi_input_key == first_multi_input_key): \n\t \t \tcontinue \n\t \tif (len(multi_input_values) != len(first_multi_value)): \n\t \t \traise InputMatchedException() \n\t \tfor (index, value) in enumerate(multi_input_values): \n\t \t \tmatched_multi_inputs[index][multi_input_key] = value \n\treturn matched_multi_inputs\n", 
" \tif (n == 0): \n\t \treturn (0, 0, 0, 0) \n\tv = multiplicity(4, n) \n\tn //= (4 ** v) \n\tif ((n % 8) == 7): \n\t \td = 2 \n\t \tn = (n - 4) \n\telif (((n % 8) == 6) or ((n % 8) == 2)): \n\t \td = 1 \n\t \tn = (n - 1) \n\telse: \n\t \td = 0 \n\t(x, y, z) = sum_of_three_squares(n) \n\treturn _sorted_tuple(((2 ** v) * d), ((2 ** v) * x), ((2 ** v) * y), ((2 ** v) * z))\n", 
" \tif isinstance(text, unicode): \n\t \treturn text \n\treturn unicode(text, encoding, errors=errors)\n", 
" \titems = d.items() \n\tbackitems = [[v[1], v[0]] for v in items] \n\tbackitems.sort() \n\tbackitems.reverse() \n\treturn [backitems[i][1] for i in range(0, len(backitems))]\n", 
" \tif (key in dictionary): \n\t \tdictionary[key].append(value) \n\telse: \n\t \tdictionary[key] = [value]\n", 
" \tif (key in dictionary): \n\t \tdictionary[key].append(value) \n\telse: \n\t \tdictionary[key] = [value]\n", 
" \tif (key in dictionary): \n\t \tdictionary[key].append(value) \n\telse: \n\t \tdictionary[key] = [value]\n", 
" \tmatching = set() \n\tnodes = set() \n\tfor (u, v) in G.edges(): \n\t \tif ((u not in nodes) and (v not in nodes) and (u != v)): \n\t \t \tmatching.add((u, v)) \n\t \t \tnodes.add(u) \n\t \t \tnodes.add(v) \n\treturn matching\n", 
" \tprecision = int(precision) \n\trounded = (((int(number) + (precision / 2)) // precision) * precision) \n\treturn rounded\n", 
" \timax = (4294967295 >> prefixLen) \n\tassert (i <= imax), 'Not \tenough \tIP \taddresses \tin \tthe \tsubnet' \n\tmask = (4294967295 ^ imax) \n\tipnum = ((ipBaseNum & mask) + i) \n\treturn ipStr(ipnum)\n", 
" \tmax_branch = (max_branch or 32) \n\tn = df.npartitions \n\tstages = int(math.ceil((math.log(n) / math.log(max_branch)))) \n\tif (stages > 1): \n\t \tk = int(math.ceil((n ** (1 / stages)))) \n\telse: \n\t \tk = n \n\tgroups = [] \n\tsplits = [] \n\tjoins = [] \n\tinputs = [tuple((digit(i, j, k) for j in range(stages))) for i in range((k ** stages))] \n\ttoken = tokenize(df, column, max_branch) \n\tstart = dict((((('shuffle-join-' + token), 0, inp), ((df._name, i) if (i < df.npartitions) else df._meta)) for (i, inp) in enumerate(inputs))) \n\tfor stage in range(1, (stages + 1)): \n\t \tgroup = dict((((('shuffle-group-' + token), stage, inp), (shuffle_group, (('shuffle-join-' + token), (stage - 1), inp), column, (stage - 1), k, n)) for inp in inputs)) \n\t \tsplit = dict((((('shuffle-split-' + token), stage, i, inp), (getitem, (('shuffle-group-' + token), stage, inp), i)) for i in range(k) for inp in inputs)) \n\t \tjoin = dict((((('shuffle-join-' + token), stage, inp), (_concat, [(('shuffle-split-' + token), stage, inp[(stage - 1)], insert(inp, (stage - 1), j)) for j in range(k)])) for inp in inputs)) \n\t \tgroups.append(group) \n\t \tsplits.append(split) \n\t \tjoins.append(join) \n\tend = dict((((('shuffle-' + token), i), (('shuffle-join-' + token), stages, inp)) for (i, inp) in enumerate(inputs))) \n\tdsk = merge(df.dask, start, end, *((groups + splits) + joins)) \n\tdf2 = DataFrame(dsk, ('shuffle-' + token), df, df.divisions) \n\tif ((npartitions is not None) and (npartitions != df.npartitions)): \n\t \tparts = [(i % df.npartitions) for i in range(npartitions)] \n\t \ttoken = tokenize(df2, npartitions) \n\t \tdsk = {(('repartition-group-' + token), i): (shuffle_group_2, k, column) for (i, k) in enumerate(df2._keys())} \n\t \tfor p in range(npartitions): \n\t \t \tdsk[(('repartition-get-' + token), p)] = (shuffle_group_get, (('repartition-group-' + token), parts[p]), p) \n\t \tdf3 = DataFrame(merge(df2.dask, dsk), ('repartition-get-' + token), df2, ([None] * (npartitions + 1))) \n\telse: \n\t \tdf3 = df2 \n\t \tdf3.divisions = ((None,) * (df.npartitions + 1)) \n\treturn df3\n", 
" \tfor path in job_paths: \n\t \tif (path and clip_path(path).lower().startswith(cfg.download_dir.get_path().lower())): \n\t \t \tremove_all(path, recursive=True)\n", 
" \tdef read_from_file(config_filename): \n\t \t'\\n \t \t \t \t \t \t \t \tReads \ta \tsources.lst \tfile \tfrom \ta \tgiven \tlocation\\n\\n \t \t \t \t \t \t \t \t:param \tconfig_filename: \tthe \tconfiguration \tfile \tto \tread\\n \t \t \t \t \t \t \t \t' \n\t \tglobal packages_sources \n\t \ttry: \n\t \t \tf = open(config_filename, 'r') \n\t \texcept Exception as e: \n\t \t \tpass \n\t \telse: \n\t \t \tfor line in f: \n\t \t \t \tt = line.rstrip().split(' \t') \n\t \t \t \tpackages_sources[t[0]] = this_package = package_info(config_filename, t[0], t[1], t[2], urllib.unquote(t[3]), None) \n\tif super_powers: \n\t \tread_from_file(os.path.join(dataset_conf_path, dataset_sources)) \n\telse: \n\t \tpaths = [os.path.join(root_conf_path, dataset_sources), os.path.join(user_conf_path, dataset_sources)] \n\t \ttry: \n\t \t \tpaths += [os.path.join(x, dataset_sources) for x in re.split(':|;', os.environ['PYLEARN2_DATA_PATH'])] \n\t \texcept Exception: \n\t \t \tpass \n\tfor path in paths: \n\t \tread_from_file(path) \n\tif (len(packages_sources) == 0): \n\t \traise RuntimeError('[cf] \tfatal: \tcould \tnot \tfind/read \tsources.lst \t(unexpected!)')\n", 
" \tmeans_men = (20, 35, 30, 35, 27) \n\tmeans_women = (25, 32, 34, 20, 25) \n\tindex = np.arange(len(means_men)) \n\tbar_width = 0.35 \n\tplt.bar(index, means_men, width=bar_width, alpha=0.2, color='b', label='Men') \n\tplt.bar((index + bar_width), means_women, width=bar_width, alpha=0.8, color='r', label='Women') \n\tplt.legend(loc='upper \tright', shadow=True) \n\tfor (x, y) in zip(index, means_men): \n\t \tplt.text((x + (bar_width / 2)), (y + 0.3), y, ha='center', va='bottom') \n\tfor (x, y) in zip(index, means_women): \n\t \tplt.text(((x + bar_width) + (bar_width / 2)), (y + 0.3), y, ha='center', va='bottom') \n\tplt.ylim(0, 45) \n\tplt.xlabel('Group') \n\tplt.ylabel('Scores') \n\tplt.xticks((index + bar_width), ('A\\xe7\\xbb\\x84', 'B\\xe7\\xbb\\x84', 'C\\xe7\\xbb\\x84', 'D\\xe7\\xbb\\x84', 'E\\xe7\\xbb\\x84')) \n\tplt.show() \n\treturn\n", 
" \tif (array.size > 0): \n\t \tmode = stats.mode(array) \n\t \tmost_frequent_value = mode[0][0] \n\t \tmost_frequent_count = mode[1][0] \n\telse: \n\t \tmost_frequent_value = 0 \n\t \tmost_frequent_count = 0 \n\tif ((most_frequent_count == 0) and (n_repeat == 0)): \n\t \treturn np.nan \n\telif (most_frequent_count < n_repeat): \n\t \treturn extra_value \n\telif (most_frequent_count > n_repeat): \n\t \treturn most_frequent_value \n\telif (most_frequent_count == n_repeat): \n\t \tif (most_frequent_value < extra_value): \n\t \t \treturn most_frequent_value \n\t \telse: \n\t \t \treturn extra_value\n", 
" \tflags = value.flags \n\tif (flags == 0): \n\t \treturn ((('\\x0b' + name) + _make_c_string_check(value.pattern)) + '\\x00') \n\telif (flags == re.UNICODE): \n\t \treturn ((('\\x0b' + name) + _make_c_string_check(value.pattern)) + 'u\\x00') \n\telse: \n\t \tsflags = '' \n\t \tif (flags & re.IGNORECASE): \n\t \t \tsflags += 'i' \n\t \tif (flags & re.LOCALE): \n\t \t \tsflags += 'l' \n\t \tif (flags & re.MULTILINE): \n\t \t \tsflags += 'm' \n\t \tif (flags & re.DOTALL): \n\t \t \tsflags += 's' \n\t \tif (flags & re.UNICODE): \n\t \t \tsflags += 'u' \n\t \tif (flags & re.VERBOSE): \n\t \t \tsflags += 'x' \n\t \tsflags += '\\x00' \n\t \treturn ((('\\x0b' + name) + _make_c_string_check(value.pattern)) + sflags)\n", 
" \tsequence = [((quote + unic(item)) + quote) for item in sequence] \n\tif (not sequence): \n\t \treturn '' \n\tif (len(sequence) == 1): \n\t \treturn sequence[0] \n\tlast_two = lastsep.join(sequence[(-2):]) \n\treturn sep.join((sequence[:(-2)] + [last_two]))\n", 
" \tnow = now_in_timezone(timezone_name) \n\toffset = now.tzinfo.utcoffset(now) \n\tminutes = ((offset.seconds / 60) + ((offset.days * 24) * 60)) \n\treturn minutes\n", 
" \tdiff = [] \n\tfor x in list2: \n\t \tif (x not in list1): \n\t \t \tdiff.append(x) \n\treturn diff\n", 
" \tdiff = [] \n\tfor x in list2: \n\t \tif (x not in list1): \n\t \t \tdiff.append(x) \n\treturn diff\n", 
" \tresult = JsonDumpForScriptContext(dump_object) \n\tif xssi_protection: \n\t \tresult = (')]}\\n' + result) \n\tresponse = http.HttpResponse(result, content_type='application/json; \tcharset=utf-8') \n\tresponse['Content-Disposition'] = 'attachment; \tfilename=response.json' \n\tresponse['X-Content-Type-Options'] = 'nosniff' \n\treturn response\n", 
" \tif (not getIsBracketed(bracketedString)): \n\t \treturn None \n\tbracketedString = bracketedString.strip().replace('[', '').replace(']', '').replace('(', '').replace(')', '') \n\tif (len(bracketedString) < 1): \n\t \treturn [] \n\tsplitLine = bracketedString.split(',') \n\tfloatList = [] \n\tfor word in splitLine: \n\t \tevaluatedFloat = euclidean.getFloatFromValue(word) \n\t \tif (evaluatedFloat != None): \n\t \t \tfloatList.append(evaluatedFloat) \n\treturn floatList\n", 
" \tparen_count = 0 \n\tbrace_count = 0 \n\tsq_bracket_count = 0 \n\tprevious_token = tok.previous_token \n\twhile previous_token: \n\t \tif (previous_token.value == ')'): \n\t \t \tparen_count -= 1 \n\t \telif (previous_token.value == '}'): \n\t \t \tbrace_count -= 1 \n\t \telif (previous_token.value == ']'): \n\t \t \tsq_bracket_count -= 1 \n\t \tif (previous_token.value == '('): \n\t \t \tif (paren_count == 0): \n\t \t \t \treturn True \n\t \t \tparen_count += 1 \n\t \telif (previous_token.value == '{'): \n\t \t \tif (brace_count == 0): \n\t \t \t \treturn True \n\t \t \tbrace_count += 1 \n\t \telif (previous_token.value == '['): \n\t \t \tif (sq_bracket_count == 0): \n\t \t \t \treturn True \n\t \t \tsq_bracket_count += 1 \n\t \tprevious_token = previous_token.previous_token \n\treturn False\n", 
" \tp = ('%s(.*?)%s' % (a, b)) \n\tp = re.compile(p, (re.DOTALL | re.I)) \n\treturn [m for m in p.findall(string)]\n", 
" \tmatcher = regex.match(value) \n\tif (not matcher): \n\t \traise validation.ValidationError(message) \n\treturn matcher\n", 
" \treturn ((('(?P<%s>' % name) + '|'.join(alternates)) + ')')\n", 
" \treturn '.'.join(path)\n", 
" \tencoded_args = [encode(arg) for arg in args] \n\treturn subprocess.Popen(encoded_args, cwd=cwd).pid\n", 
" \twhile 1: \n\t \tdata = fileobj.read(CHUNK) \n\t \ttry: \n\t \t \tparser.feed(data) \n\t \texcept EndOfHeadError: \n\t \t \tbreak \n\t \tif (len(data) != CHUNK): \n\t \t \tbreak \n\treturn parser.http_equiv\n", 
" \treturn dict(items(sobject))\n", 
" \tif isinstance(data, DataFrame): \n\t \tif (columns is not None): \n\t \t \tarrays = [data._ixs(i, axis=1).values for (i, col) in enumerate(data.columns) if (col in columns)] \n\t \telse: \n\t \t \tcolumns = data.columns \n\t \t \tarrays = [data._ixs(i, axis=1).values for i in range(len(columns))] \n\t \treturn (arrays, columns) \n\tif (not len(data)): \n\t \tif isinstance(data, np.ndarray): \n\t \t \tcolumns = data.dtype.names \n\t \t \tif (columns is not None): \n\t \t \t \treturn (([[]] * len(columns)), columns) \n\t \treturn ([], []) \n\tif isinstance(data[0], (list, tuple)): \n\t \treturn _list_to_arrays(data, columns, coerce_float=coerce_float, dtype=dtype) \n\telif isinstance(data[0], collections.Mapping): \n\t \treturn _list_of_dict_to_arrays(data, columns, coerce_float=coerce_float, dtype=dtype) \n\telif isinstance(data[0], Series): \n\t \treturn _list_of_series_to_arrays(data, columns, coerce_float=coerce_float, dtype=dtype) \n\telif isinstance(data[0], Categorical): \n\t \tif (columns is None): \n\t \t \tcolumns = _default_index(len(data)) \n\t \treturn (data, columns) \n\telif (isinstance(data, (np.ndarray, Series, Index)) and (data.dtype.names is not None)): \n\t \tcolumns = list(data.dtype.names) \n\t \tarrays = [data[k] for k in columns] \n\t \treturn (arrays, columns) \n\telse: \n\t \tdata = lmap(tuple, data) \n\t \treturn _list_to_arrays(data, columns, coerce_float=coerce_float, dtype=dtype)\n", 
" \twith io.StringIO() as ostream: \n\t \tyemitter = yaml.emitter.Emitter(ostream, width=six.MAXSIZE) \n\t \tyemitter.write_double_quoted(six.text_type(text)) \n\t \treturn ostream.getvalue()\n", 
" \ttry: \n\t \ttext = re.sub('[^\\\\w \t]', '', text) \n\t \treturn [x.strip('.').lower() for x in text.split()] \n\texcept TypeError: \n\t \treturn None\n", 
" \tresult = pd.merge(lhs, rhs, left_on=t.on_left, right_on=t.on_right, how=t.how, suffixes=t.suffixes) \n\treturn result.reset_index()[t.fields]\n", 
" \tc = _FFI.from_handle(context_handle) \n\treturn c.key_to_value(key)\n", 
" \tp = ('%s(.*?)%s' % (a, b)) \n\tp = re.compile(p, (re.DOTALL | re.I)) \n\treturn [m for m in p.findall(string)]\n", 
" \ttry: \n\t \t(obj + '') \n\texcept (TypeError, ValueError): \n\t \treturn False \n\treturn True\n", 
" \treturn isinstance(value, basestring)\n", 
" \treturn isinstance(value, basestring)\n", 
" \treturn isinstance(value, basestring)\n", 
" \treturn isinstance(value, basestring)\n", 
" \topt = a.not_options.union(b.not_options) \n\tv = NotVariable('?', opt) \n\treturn U.merge(v, a, b)\n", 
" \tdiff = [] \n\tfor x in list2: \n\t \tif (x not in list1): \n\t \t \tdiff.append(x) \n\treturn diff\n", 
" \tfunc = ((use_sudo and sudo) or run) \n\tif isinstance(text, basestring): \n\t \ttext = [text] \n\tfor line in text: \n\t \tregex = (('^' + _escape_for_regex(line)) + ('' if partial else '$')) \n\t \tif (exists(filename, use_sudo=use_sudo) and line and contains(filename, regex, use_sudo=use_sudo, escape=False, shell=shell)): \n\t \t \tcontinue \n\t \tline = (line.replace(\"'\", \"'\\\\\\\\''\") if escape else line) \n\t \tfunc((\"echo \t'%s' \t>> \t%s\" % (line, _expand_path(filename))))\n", 
" \tdiff = [] \n\tfor x in list2: \n\t \tif (x not in list1): \n\t \t \tdiff.append(x) \n\treturn diff\n", 
" \tfor t in tuple_of_tuples: \n\t \tif (t[0] == key): \n\t \t \treturn t[1] \n\telse: \n\t \treturn key\n", 
" \treturn environ.get('HTTP_SECRETKEY', '')\n", 
" \tif (not items): \n\t \treturn '\\n' \n\t(matrix, info) = compute_item_matrix(items, row_first=row_first, separator_size=len(separator), displaywidth=displaywidth) \n\tif spread: \n\t \tseparator = separator.ljust(int(info['optimal_separator_width'])) \n\tfmatrix = [filter(None, x) for x in matrix] \n\tsjoin = (lambda x: separator.join([y.ljust(w, ' \t') for (y, w) in zip(x, info['column_widths'])])) \n\treturn ('\\n'.join(map(sjoin, fmatrix)) + '\\n')\n", 
" \tfor _ele in lst: \n\t \tfor match_key in _MATCH_KEYS: \n\t \t \tif (_ele.get(match_key) == ele.get(match_key)): \n\t \t \t \treturn _ele\n", 
" \trpc = rotate_async(image_data, degrees, output_encoding=output_encoding, quality=quality, correct_orientation=correct_orientation, rpc=rpc, transparent_substitution_rgb=transparent_substitution_rgb) \n\treturn rpc.get_result()\n", 
" \tif (u',' in symbols): \n\t \tsymbol_list = symbols.split(u',') \n\telse: \n\t \tsymbol_list = symbols.split() \n\treturn [Nonterminal(s.strip()) for s in symbol_list]\n", 
" \tpy_file = __file__.replace('.pyc', '.py') \n\tdir_paths = [os.path.abspath(os.path.dirname(os.path.realpath(py_file))), os.path.abspath(os.path.dirname(py_file))] \n\tfor dir_path in dir_paths: \n\t \tsibling_path = os.path.join(dir_path, sibling) \n\t \tif os.path.exists(sibling_path): \n\t \t \treturn dir_path \n\traise ValueError(('Could \tnot \tdetermine \tdirectory \tthat \tcontains \tboth, \tthis \tfile \tand \t%s.' % sibling))\n", 
" \tas_string = ''.join(mask) \n\tr = (('[^' + re.escape(as_string)) + ']') \n\treturn re.compile(r)\n", 
" \treturn _compile(pattern, flags, kwargs).split(string, maxsplit, concurrent)\n", 
" \treturn re.sub('(?<=\\\\w)([A-Z])', ' \t\\\\1', s)\n", 
" \tdef _encode_multipart_formdata(fields, files, encoding='utf-8'): \n\t \t'fields \tis \ta \tsequence \tof \t(name, \tvalue) \telements \tfor \tregular\\n \t \t \t \t \t \t \t \tform \tfields. \tfiles \tis \ta \tsequence \tof \t(name, \tfilename, \tvalue)\\n \t \t \t \t \t \t \t \telements \tfor \tdata \tto \tbe \tuploaded \tas \tfiles\\n\\n \t \t \t \t \t \t \t \tReturn \t(content_type, \tbody) \tready \tfor \thttplib.HTTP \tinstance\\n \t \t \t \t \t \t \t \t' \n\t \tBOUNDARY = u'----------ThIs_Is_tHe_bouNdaRY_$' \n\t \tCRLF = u'\\r\\n' \n\t \tL = [] \n\t \tfor (key, value) in fields: \n\t \t \tL.append((u'--' + BOUNDARY)) \n\t \t \tL.append((u'Content-Disposition: \tform-data; \tname=\"%s\"' % key)) \n\t \t \tL.append((u'Content-Type: \ttext/plain;charset=%s' % encoding)) \n\t \t \tL.append(u'Content-Transfer-Encoding: \t8bit') \n\t \t \tL.append(u'') \n\t \t \tL.append(value) \n\t \tfor (key, filename, value) in files: \n\t \t \tL.append((u'--' + BOUNDARY)) \n\t \t \tmsg = u'Content-Disposition: \tform-data; \tname=\"%s\"; \tfilename=\"%s\"' \n\t \t \tL.append((msg % (key, filename))) \n\t \t \tmsg = u'Content-Type: \t%s;charset=%s' \n\t \t \tL.append((msg % (_get_content_type(filename), encoding))) \n\t \t \tL.append(u'Content-Transfer-Encoding: \tbase64') \n\t \t \tL.append(u'') \n\t \t \tL.append(base64.b64encode(value).decode()) \n\t \tL.append(((u'--' + BOUNDARY) + u'--')) \n\t \tL.append(u'') \n\t \tbody = CRLF.join(L) \n\t \tcontent_type = (u'multipart/form-data; \tboundary=%s' % BOUNDARY) \n\t \treturn (content_type, body) \n\tdef _get_content_type(filename): \n\t \treturn (mimetypes.guess_type(filename)[0] or 'application/octet-stream') \n\t(content_type, body) = _encode_multipart_formdata(fields, files) \n\tif (https is True): \n\t \tconn = httplib.HTTPSConnection(host, timeout=timeout) \n\telse: \n\t \tconn = httplib.HTTPConnection(host, timeout=timeout) \n\theaders = {u'User-Agent': userAgent, u'Charset': encoding, u'Content-Type': content_type} \n\tif (basicAuth and (type(basicAuth) == str)): \n\t \tuser_cred = base64.encodestring(basicAuth).replace('\\n', '') \n\t \theaders.update({u'Authorization': (u'Basic \t%s' % user_cred)}) \n\ttry: \n\t \tconn.request(u'POST', selector, body, headers) \n\texcept Exception: \n\t \tmsg = 'connection \terror \t(possible \ttimeout \tafter \t%ss)' \n\t \treturn ((-1), (msg % str(timeout)), 'timeout \tor \terror') \n\ttry: \n\t \tresult = conn.getresponse() \n\texcept Exception: \n\t \treturn ((-1), 'connection \terror \t(can \tbe \t\"socket.error: \t[Errno \t54] \tConnection \treset \tby \tpeer\")') \n\treturn (result.status, result.reason, result.read())\n", 
" \tf_bytes = np.zeros(3, dtype=np.uint8) \n\tf_bytes[0] = ((val >> 16) & 255) \n\tf_bytes[1] = ((val >> 8) & 255) \n\tf_bytes[2] = (val & 255) \n\tfid.write(f_bytes.tostring())\n", 
" \treturn valmap((lambda v: tuple((OwnershipPeriod(a.start, b.start, a.sid, a.value) for (a, b) in sliding_window(2, concatv(sorted(v), [OwnershipPeriod(pd.Timestamp.max.tz_localize('utc'), None, None, None)]))))), mappings)\n", 
" \tprotected_name = metadata.protect_name(column_name) \n\tpossible_index_values = [protected_name, ('values(%s)' % protected_name)] \n\tfor index_metadata in table.indexes.values(): \n\t \toptions = dict(index_metadata.index_options) \n\t \tif (options.get('target') in possible_index_values): \n\t \t \treturn index_metadata.name\n", 
" \tif isinstance(x, Decimal): \n\t \treturn float(x) \n\treturn x\n", 
" \tomitNone = False \n\tif (NoneIsLast is None): \n\t \tNoneIsLast = True \n\t \tomitNone = True \n\tn = len(x) \n\tix = range(n) \n\tif (None not in x): \n\t \tix.sort(reverse=decreasing, key=(lambda j: x[j])) \n\telse: \n\t \tdef key(i, x=x): \n\t \t \telem = x[i] \n\t \t \tif (decreasing == NoneIsLast): \n\t \t \t \treturn ((not (elem is None)), elem) \n\t \t \telse: \n\t \t \t \treturn ((elem is None), elem) \n\t \tix = range(n) \n\t \tix.sort(key=key, reverse=decreasing) \n\tif omitNone: \n\t \tn = len(x) \n\t \tfor i in range((n - 1), (-1), (-1)): \n\t \t \tif (x[ix[i]] is None): \n\t \t \t \tn -= 1 \n\t \treturn ix[:n] \n\treturn ix\n", 
" \tax = gca() \n\tif (not isinstance(ax, PolarAxes)): \n\t \traise RuntimeError(u'rgrids \tonly \tdefined \tfor \tpolar \taxes') \n\tif (len(args) == 0): \n\t \tlines = ax.yaxis.get_gridlines() \n\t \tlabels = ax.yaxis.get_ticklabels() \n\telse: \n\t \t(lines, labels) = ax.set_rgrids(*args, **kwargs) \n\treturn (silent_list(u'Line2D \trgridline', lines), silent_list(u'Text \trgridlabel', labels))\n", 
" \tif (file.startswith('\\\\') or file.startswith('/')): \n\t \treturn True \n\ttry: \n\t \tif (file[1:3] == ':\\\\'): \n\t \t \treturn True \n\texcept: \n\t \tpass \n\treturn False\n", 
" \t_list.sort((lambda a, b: (a[indexkey] < b[indexkey]))) \n\treturn _list\n", 
" \treturn IMPL.key_pair_count_by_user(context, user_id)\n", 
" \tif (not isinstance(index, _Frame)): \n\t \tindex = df[index] \n\tpartitions = index.map_partitions(partitioning_index, npartitions=(npartitions or df.npartitions), meta=pd.Series([0])) \n\tdf2 = df.assign(_partitions=partitions) \n\tdf3 = rearrange_by_column(df2, '_partitions', npartitions=npartitions, max_branch=max_branch, shuffle=shuffle, compute=compute) \n\tdf4 = df3.map_partitions(drop_columns, '_partitions', df.columns.dtype) \n\treturn df4\n", 
" \tarr = arr.fillna('') \n\ttry: \n\t \tarr = ((sep + arr) + sep) \n\texcept TypeError: \n\t \tarr = ((sep + arr.astype(str)) + sep) \n\ttags = set() \n\tfor ts in arr.str.split(sep): \n\t \ttags.update(ts) \n\ttags = sorted((tags - set(['']))) \n\tdummies = np.empty((len(arr), len(tags)), dtype=np.int64) \n\tfor (i, t) in enumerate(tags): \n\t \tpat = ((sep + t) + sep) \n\t \tdummies[:, i] = lib.map_infer(arr.values, (lambda x: (pat in x))) \n\treturn (dummies, tags)\n", 
" \tmessages = [] \n\tfor app in ([app_name] if app_name else frappe.get_installed_apps()): \n\t \tif os.path.exists(frappe.get_app_path(app, u'public')): \n\t \t \tfor (basepath, folders, files) in os.walk(frappe.get_app_path(app, u'public')): \n\t \t \t \tif (u'frappe/public/js/lib' in basepath): \n\t \t \t \t \tcontinue \n\t \t \t \tfor fname in files: \n\t \t \t \t \tif (fname.endswith(u'.js') or fname.endswith(u'.html')): \n\t \t \t \t \t \tmessages.extend(get_messages_from_file(os.path.join(basepath, fname))) \n\treturn messages\n", 
" \treturn ('#%02x%02x%02x' % tuple([round((val * 255)) for val in rgb]))\n", 
" \tp = subprocess.Popen(cmdline, shell=True, stdout=stdout, stderr=stderr) \n\t(stdout, stderr) = p.communicate() \n\tif (p.returncode != 0): \n\t \traise RuntimeError(stderr) \n\tif stderr: \n\t \tif PY3: \n\t \t \tstderr = str(stderr, (sys.stderr.encoding or sys.getfilesystemencoding())) \n\t \twarn(stderr) \n\tif PY3: \n\t \tstdout = str(stdout, (sys.stdout.encoding or sys.getfilesystemencoding())) \n\treturn stdout.strip()\n", 
" \ttry: \n\t \titem = Q.get(True, timeout) \n\texcept Queue.Empty: \n\t \treturn None \n\treturn item\n", 
" \tif (palette is not None): \n\t \tkwargs['palette'] = palette \n\tkwargs['columns'] = columns \n\tkwargs['bin'] = bin \n\treturn ColorAttr(**kwargs)\n", 
" \t(patt, regex, replacement) = _parse_hook(s) \n\ttry: \n\t \tre.compile(regex) \n\texcept re.error as e: \n\t \traise ParseException(('Malformed \treplacement \tregex: \t%s' % str(e.message))) \n\treturn (patt, regex, replacement)\n", 
" \treturn 'The \tlength \tof \tthat \tstring \tis \t{} \tcharacters.'.format(len(text))\n", 
" \ttmp = map((lambda x: struct.pack('B', random.randrange(0, 256, 1))), ([''] * l)) \n\treturn ''.join(tmp)\n", 
" \treturn 'The \tlength \tof \tthat \tstring \tis \t{} \tcharacters.'.format(len(text))\n", 
" \treturn 'The \tlength \tof \tthat \tstring \tis \t{} \tcharacters.'.format(len(text))\n", 
" \treturn 'The \tlength \tof \tthat \tstring \tis \t{} \tcharacters.'.format(len(text))\n", 
" \tif salt.utils.is_windows(): \n\t \treturn url \n\tscheme = urlparse(url).scheme \n\tif (not scheme): \n\t \treturn url.lstrip('|') \n\telif (scheme == 'salt'): \n\t \t(path, saltenv) = parse(url) \n\t \treturn create(path.lstrip('|'), saltenv) \n\telse: \n\t \treturn url\n", 
" \treturn url_quote(string, charset, errors, (safe + ' \t'), '+').replace(' \t', '+')\n", 
" \tfrom os.path import join, isdir, islink \n\ttry: \n\t \tnames = listdir(top) \n\texcept error as err: \n\t \tif (onerror is not None): \n\t \t \tonerror(err) \n\t \treturn \n\t(dirs, nondirs) = ([], []) \n\tfor name in names: \n\t \tif isdir(join(top, name)): \n\t \t \tdirs.append(name) \n\t \telse: \n\t \t \tnondirs.append(name) \n\tif topdown: \n\t \t(yield (top, dirs, nondirs)) \n\tfor name in dirs: \n\t \tpath = join(top, name) \n\t \tif (not islink(path)): \n\t \t \tfor x in walk(path, topdown, onerror): \n\t \t \t \t(yield x) \n\tif (not topdown): \n\t \t(yield (top, dirs, nondirs))\n", 
" \tresult = pd.merge(lhs, rhs, left_on=t.on_left, right_on=t.on_right, how=t.how, suffixes=t.suffixes) \n\treturn result.reset_index()[t.fields]\n", 
" \treturn sorted((pn for pn in p if p[pn]), key=(lambda pn: p[pn]))\n", 
" \treturn sorted((pn for pn in p if p[pn]), key=(lambda pn: p[pn]))\n", 
" \tif (out is None): \n\t \tout = numpy.zeros(A.shape) \n\tindicator1 = (A >= B) \n\tindicator2 = numpy.logical_not(indicator1) \n\tout[indicator1] = (A[indicator1] + numpy.log1p(numpy.exp((B[indicator1] - A[indicator1])))) \n\tout[indicator2] = (B[indicator2] + numpy.log1p(numpy.exp((A[indicator2] - B[indicator2])))) \n\treturn out\n", 
" \tassert (len(arrtys) == len(arrs) == len(arr_shapes) == len(arr_strides)) \n\tzero = cgutils.intp_t(0) \n\tret = _empty_nd_impl(context, builder, retty, ret_shapes) \n\tret_strides = cgutils.unpack_tuple(builder, ret.strides) \n\tcopy_offsets = [] \n\tfor arr_sh in arr_shapes: \n\t \toffset = zero \n\t \tfor (dim, (size, stride)) in enumerate(zip(arr_sh, ret_strides)): \n\t \t \tis_axis = builder.icmp_signed('==', axis.type(dim), axis) \n\t \t \taddend = builder.mul(size, stride) \n\t \t \toffset = builder.select(is_axis, builder.add(offset, addend), offset) \n\t \tcopy_offsets.append(offset) \n\tret_data = ret.data \n\tfor (arrty, arr, arr_sh, arr_st, offset) in zip(arrtys, arrs, arr_shapes, arr_strides, copy_offsets): \n\t \tarr_data = arr.data \n\t \tloop_nest = cgutils.loop_nest(builder, arr_sh, cgutils.intp_t, order=retty.layout) \n\t \twith loop_nest as indices: \n\t \t \tsrc_ptr = cgutils.get_item_pointer2(builder, arr_data, arr_sh, arr_st, arrty.layout, indices) \n\t \t \tval = load_item(context, builder, arrty, src_ptr) \n\t \t \tval = context.cast(builder, val, arrty.dtype, retty.dtype) \n\t \t \tdest_ptr = cgutils.get_item_pointer2(builder, ret_data, ret_shapes, ret_strides, retty.layout, indices) \n\t \t \tstore_item(context, builder, retty, val, dest_ptr) \n\t \tret_data = cgutils.pointer_add(builder, ret_data, offset) \n\treturn ret\n", 
" \tassert (len(arrtys) == len(arrs) == len(arr_shapes) == len(arr_strides)) \n\tzero = cgutils.intp_t(0) \n\tret = _empty_nd_impl(context, builder, retty, ret_shapes) \n\tret_strides = cgutils.unpack_tuple(builder, ret.strides) \n\tcopy_offsets = [] \n\tfor arr_sh in arr_shapes: \n\t \toffset = zero \n\t \tfor (dim, (size, stride)) in enumerate(zip(arr_sh, ret_strides)): \n\t \t \tis_axis = builder.icmp_signed('==', axis.type(dim), axis) \n\t \t \taddend = builder.mul(size, stride) \n\t \t \toffset = builder.select(is_axis, builder.add(offset, addend), offset) \n\t \tcopy_offsets.append(offset) \n\tret_data = ret.data \n\tfor (arrty, arr, arr_sh, arr_st, offset) in zip(arrtys, arrs, arr_shapes, arr_strides, copy_offsets): \n\t \tarr_data = arr.data \n\t \tloop_nest = cgutils.loop_nest(builder, arr_sh, cgutils.intp_t, order=retty.layout) \n\t \twith loop_nest as indices: \n\t \t \tsrc_ptr = cgutils.get_item_pointer2(builder, arr_data, arr_sh, arr_st, arrty.layout, indices) \n\t \t \tval = load_item(context, builder, arrty, src_ptr) \n\t \t \tval = context.cast(builder, val, arrty.dtype, retty.dtype) \n\t \t \tdest_ptr = cgutils.get_item_pointer2(builder, ret_data, ret_shapes, ret_strides, retty.layout, indices) \n\t \t \tstore_item(context, builder, retty, val, dest_ptr) \n\t \tret_data = cgutils.pointer_add(builder, ret_data, offset) \n\treturn ret\n", 
" \tassert (len(arrtys) == len(arrs) == len(arr_shapes) == len(arr_strides)) \n\tzero = cgutils.intp_t(0) \n\tret = _empty_nd_impl(context, builder, retty, ret_shapes) \n\tret_strides = cgutils.unpack_tuple(builder, ret.strides) \n\tcopy_offsets = [] \n\tfor arr_sh in arr_shapes: \n\t \toffset = zero \n\t \tfor (dim, (size, stride)) in enumerate(zip(arr_sh, ret_strides)): \n\t \t \tis_axis = builder.icmp_signed('==', axis.type(dim), axis) \n\t \t \taddend = builder.mul(size, stride) \n\t \t \toffset = builder.select(is_axis, builder.add(offset, addend), offset) \n\t \tcopy_offsets.append(offset) \n\tret_data = ret.data \n\tfor (arrty, arr, arr_sh, arr_st, offset) in zip(arrtys, arrs, arr_shapes, arr_strides, copy_offsets): \n\t \tarr_data = arr.data \n\t \tloop_nest = cgutils.loop_nest(builder, arr_sh, cgutils.intp_t, order=retty.layout) \n\t \twith loop_nest as indices: \n\t \t \tsrc_ptr = cgutils.get_item_pointer2(builder, arr_data, arr_sh, arr_st, arrty.layout, indices) \n\t \t \tval = load_item(context, builder, arrty, src_ptr) \n\t \t \tval = context.cast(builder, val, arrty.dtype, retty.dtype) \n\t \t \tdest_ptr = cgutils.get_item_pointer2(builder, ret_data, ret_shapes, ret_strides, retty.layout, indices) \n\t \t \tstore_item(context, builder, retty, val, dest_ptr) \n\t \tret_data = cgutils.pointer_add(builder, ret_data, offset) \n\treturn ret\n", 
" \tassert (len(arrtys) == len(arrs) == len(arr_shapes) == len(arr_strides)) \n\tzero = cgutils.intp_t(0) \n\tret = _empty_nd_impl(context, builder, retty, ret_shapes) \n\tret_strides = cgutils.unpack_tuple(builder, ret.strides) \n\tcopy_offsets = [] \n\tfor arr_sh in arr_shapes: \n\t \toffset = zero \n\t \tfor (dim, (size, stride)) in enumerate(zip(arr_sh, ret_strides)): \n\t \t \tis_axis = builder.icmp_signed('==', axis.type(dim), axis) \n\t \t \taddend = builder.mul(size, stride) \n\t \t \toffset = builder.select(is_axis, builder.add(offset, addend), offset) \n\t \tcopy_offsets.append(offset) \n\tret_data = ret.data \n\tfor (arrty, arr, arr_sh, arr_st, offset) in zip(arrtys, arrs, arr_shapes, arr_strides, copy_offsets): \n\t \tarr_data = arr.data \n\t \tloop_nest = cgutils.loop_nest(builder, arr_sh, cgutils.intp_t, order=retty.layout) \n\t \twith loop_nest as indices: \n\t \t \tsrc_ptr = cgutils.get_item_pointer2(builder, arr_data, arr_sh, arr_st, arrty.layout, indices) \n\t \t \tval = load_item(context, builder, arrty, src_ptr) \n\t \t \tval = context.cast(builder, val, arrty.dtype, retty.dtype) \n\t \t \tdest_ptr = cgutils.get_item_pointer2(builder, ret_data, ret_shapes, ret_strides, retty.layout, indices) \n\t \t \tstore_item(context, builder, retty, val, dest_ptr) \n\t \tret_data = cgutils.pointer_add(builder, ret_data, offset) \n\treturn ret\n", 
" \tif ('@' in proxy_settings): \n\t \tprotocol = proxy_settings.split(':')[0] \n\t \tnetloc = proxy_settings.split('@')[1] \n\t \treturn ('%s://%s' % (protocol, netloc)) \n\telse: \n\t \treturn proxy_settings\n", 
" \t(head_res, baseline_res) = head_res.align(baseline_res) \n\tratio = (head_res['timing'] / baseline_res['timing']) \n\ttotals = DataFrame({HEAD_COL: head_res['timing'], BASE_COL: baseline_res['timing'], 'ratio': ratio, 'name': baseline_res.name}, columns=[HEAD_COL, BASE_COL, 'ratio', 'name']) \n\ttotals = totals.ix[(totals[HEAD_COL] > args.min_duration)] \n\ttotals = totals.dropna().sort('ratio').set_index('name') \n\treturn totals\n", 
" \tif (Authenticated not in request.effective_principals): \n\t \terror_msg = 'Please \tauthenticate \tyourself \tto \tuse \tthis \tendpoint.' \n\t \tresponse = http_error(httpexceptions.HTTPUnauthorized(), errno=ERRORS.MISSING_AUTH_TOKEN, message=error_msg) \n\t \tresponse.headers.extend(forget(request)) \n\t \treturn response \n\tif (response.content_type != 'application/json'): \n\t \terror_msg = 'This \tuser \tcannot \taccess \tthis \tresource.' \n\t \tresponse = http_error(httpexceptions.HTTPForbidden(), errno=ERRORS.FORBIDDEN, message=error_msg) \n\treturn reapply_cors(request, response)\n", 
" \tT = current.T \n\tsettings.base.prepopulate.append('default') \n\tsettings.base.guided_tour = True \n\tsettings.L10n.languages = OrderedDict([('ar', '\\xd8\\xa7\\xd9\\x84\\xd8\\xb9\\xd8\\xb1\\xd8\\xa8\\xd9\\x8a\\xd8\\xa9'), ('bs', 'Bosanski'), ('en', 'English'), ('fr', 'Fran\\xc3\\xa7ais'), ('de', 'Deutsch'), ('el', '\\xce\\xb5\\xce\\xbb\\xce\\xbb\\xce\\xb7\\xce\\xbd\\xce\\xb9\\xce\\xba\\xce\\xac'), ('es', 'Espa\\xc3\\xb1ol'), ('it', 'Italiano'), ('ja', '\\xe6\\x97\\xa5\\xe6\\x9c\\xac\\xe8\\xaa\\x9e'), ('km', '\\xe1\\x9e\\x97\\xe1\\x9e\\xb6\\xe1\\x9e\\x9f\\xe1\\x9e\\xb6\\xe1\\x9e\\x81\\xe1\\x9f\\x92\\xe1\\x9e\\x98\\xe1\\x9f\\x82\\xe1\\x9e\\x9a'), ('ko', '\\xed\\x95\\x9c\\xea\\xb5\\xad\\xec\\x96\\xb4'), ('mn', '\\xd0\\x9c\\xd0\\xbe\\xd0\\xbd\\xd0\\xb3\\xd0\\xbe\\xd0\\xbb \t\\xd1\\x85\\xd1\\x8d\\xd0\\xbb'), ('my', '\\xe1\\x80\\x99\\xe1\\x80\\xbc\\xe1\\x80\\x94\\xe1\\x80\\xba\\xe1\\x80\\x99\\xe1\\x80\\xac\\xe1\\x80\\x85\\xe1\\x80\\xac'), ('ne', '\\xe0\\xa4\\xa8\\xe0\\xa5\\x87\\xe0\\xa4\\xaa\\xe0\\xa4\\xbe\\xe0\\xa4\\xb2\\xe0\\xa5\\x80'), ('prs', '\\xd8\\xaf\\xd8\\xb1\\xdb\\x8c'), ('ps', '\\xd9\\xbe\\xda\\x9a\\xd8\\xaa\\xd9\\x88'), ('pt', 'Portugu\\xc3\\xaas'), ('pt-br', 'Portugu\\xc3\\xaas \t(Brasil)'), ('ru', '\\xd1\\x80\\xd1\\x83\\xd1\\x81\\xd1\\x81\\xd0\\xba\\xd0\\xb8\\xd0\\xb9'), ('tet', 'Tetum'), ('tl', 'Tagalog'), ('tr', 'T\\xc3\\xbcrk\\xc3\\xa7e'), ('ur', '\\xd8\\xa7\\xd8\\xb1\\xd8\\xaf\\xd9\\x88'), ('vi', 'Ti\\xe1\\xba\\xbfng \tVi\\xe1\\xbb\\x87t'), ('zh-cn', '\\xe4\\xb8\\xad\\xe6\\x96\\x87 \t(\\xe7\\xae\\x80\\xe4\\xbd\\x93)'), ('zh-tw', '\\xe4\\xb8\\xad\\xe6\\x96\\x87 \t(\\xe7\\xb9\\x81\\xe9\\xab\\x94)')]) \n\tsettings.L10n.decimal_separator = '.' \n\tsettings.gis.geonames_username = 'eden_test' \n\tsettings.modules = OrderedDict([('default', Storage(name_nice=T('Home'), restricted=False, access=None, module_type=None)), ('admin', Storage(name_nice=T('Administration'), restricted=True, access='|1|', module_type=None)), ('appadmin', Storage(name_nice=T('Administration'), restricted=True, module_type=None)), ('errors', Storage(name_nice=T('Ticket \tViewer'), restricted=False, module_type=None)), ('sync', Storage(name_nice=T('Synchronization'), restricted=True, access='|1|', module_type=None)), ('tour', Storage(name_nice=T('Guided \tTour \tFunctionality'), module_type=None)), ('translate', Storage(name_nice=T('Translation \tFunctionality'), module_type=None)), ('gis', Storage(name_nice=T('Map'), restricted=True, module_type=6)), ('pr', Storage(name_nice=T('Person \tRegistry'), restricted=True, access='|1|', module_type=10)), ('org', Storage(name_nice=T('Organizations'), restricted=True, module_type=1)), ('hrm', Storage(name_nice=T('Staff'), restricted=True, module_type=2)), ('vol', Storage(name_nice=T('Volunteers'), restricted=True, module_type=2)), ('cms', Storage(name_nice=T('Content \tManagement'), restricted=True, module_type=10)), ('doc', Storage(name_nice=T('Documents'), restricted=True, module_type=10)), ('msg', Storage(name_nice=T('Messaging'), restricted=True, module_type=None)), ('supply', Storage(name_nice=T('Supply \tChain \tManagement'), restricted=True, module_type=None)), ('inv', Storage(name_nice=T('Warehouses'), restricted=True, module_type=4)), ('asset', Storage(name_nice=T('Assets'), restricted=True, module_type=5)), ('vehicle', Storage(name_nice=T('Vehicles'), restricted=True, module_type=10)), ('req', Storage(name_nice=T('Requests'), restricted=True, module_type=10)), ('project', Storage(name_nice=T('Projects'), restricted=True, module_type=2)), ('survey', Storage(name_nice=T('Surveys'), restricted=True, module_type=5)), ('cr', Storage(name_nice=T('Shelters'), restricted=True, module_type=10)), ('hms', Storage(name_nice=T('Hospitals'), restricted=True, module_type=10)), ('dvr', Storage(name_nice=T('Disaster \tVictim \tRegistry'), restricted=True, module_type=10)), ('event', Storage(name_nice=T('Events'), restricted=True, module_type=10)), ('transport', Storage(name_nice=T('Transport'), restricted=True, module_type=10)), ('stats', Storage(name_nice=T('Statistics'), restricted=True, module_type=None)), ('member', Storage(name_nice=T('Members'), restricted=True, module_type=10)), ('budget', Storage(name_nice=T('Budgeting \tModule'), restricted=True, module_type=10))])\n", 
" \ti = symbols('i', below_fermi=True, cls=Dummy) \n\ta = symbols('a', above_fermi=True, cls=Dummy) \n\tt_ai = AntiSymmetricTensor('t', (a,), (i,)) \n\tai = NO((Fd(a) * F(i))) \n\t(i, j) = symbols('i,j', below_fermi=True, cls=Dummy) \n\t(a, b) = symbols('a,b', above_fermi=True, cls=Dummy) \n\tt_abij = AntiSymmetricTensor('t', (a, b), (i, j)) \n\tabji = NO((((Fd(a) * Fd(b)) * F(j)) * F(i))) \n\tT1 = (t_ai * ai) \n\tT2 = ((Rational(1, 4) * t_abij) * abji) \n\treturn (T1, T2)\n", 
" \tif s2: \n\t \tresult = CodeRange(ord(s1), (ord(s2) + 1)) \n\t \tresult.str = ('Range(%s,%s)' % (s1, s2)) \n\telse: \n\t \tranges = [] \n\t \tfor i in range(0, len(s1), 2): \n\t \t \tranges.append(CodeRange(ord(s1[i]), (ord(s1[(i + 1)]) + 1))) \n\t \tresult = Alt(*ranges) \n\t \tresult.str = ('Range(%s)' % repr(s1)) \n\treturn result\n", 
" \trepl_fields_re = re.compile('\\\\{[^\\\\}]*\\\\}') \n\treturn sorted(repl_fields_re.findall(s))\n", 
" \trepl_fields_re = re.compile('\\\\{[^\\\\}]*\\\\}') \n\treturn sorted(repl_fields_re.findall(s))\n", 
" \t_slugify_strip_re = re.compile('[^\\\\w\\\\s-]') \n\t_slugify_hyphenate_re = re.compile('[-\\\\s]+') \n\tif (not isinstance(value, unicode)): \n\t \tvalue = unicode(value) \n\tvalue = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore') \n\tvalue = unicode(_slugify_strip_re.sub('', value).strip().lower()) \n\treturn _slugify_hyphenate_re.sub('-', value)\n", 
" \tws = '' \n\tconversion_data = ((np.bool, np.int8, np.int8), (np.uint8, np.int8, np.int16), (np.uint16, np.int16, np.int32), (np.uint32, np.int32, np.int64)) \n\tfloat32_max = struct.unpack('<f', '\\xff\\xff\\xff~')[0] \n\tfloat64_max = struct.unpack('<d', '\\xff\\xff\\xff\\xff\\xff\\xff\\xdf\\x7f')[0] \n\tfor col in data: \n\t \tdtype = data[col].dtype \n\t \tfor c_data in conversion_data: \n\t \t \tif (dtype == c_data[0]): \n\t \t \t \tif (data[col].max() <= np.iinfo(c_data[1]).max): \n\t \t \t \t \tdtype = c_data[1] \n\t \t \t \telse: \n\t \t \t \t \tdtype = c_data[2] \n\t \t \t \tif (c_data[2] == np.float64): \n\t \t \t \t \tif (data[col].max() >= (2 ** 53)): \n\t \t \t \t \t \tws = (precision_loss_doc % ('uint64', 'float64')) \n\t \t \t \tdata[col] = data[col].astype(dtype) \n\t \tif (dtype == np.int8): \n\t \t \tif ((data[col].max() > 100) or (data[col].min() < (-127))): \n\t \t \t \tdata[col] = data[col].astype(np.int16) \n\t \telif (dtype == np.int16): \n\t \t \tif ((data[col].max() > 32740) or (data[col].min() < (-32767))): \n\t \t \t \tdata[col] = data[col].astype(np.int32) \n\t \telif (dtype == np.int64): \n\t \t \tif ((data[col].max() <= 2147483620) and (data[col].min() >= (-2147483647))): \n\t \t \t \tdata[col] = data[col].astype(np.int32) \n\t \t \telse: \n\t \t \t \tdata[col] = data[col].astype(np.float64) \n\t \t \t \tif ((data[col].max() >= (2 ** 53)) or (data[col].min() <= (- (2 ** 53)))): \n\t \t \t \t \tws = (precision_loss_doc % ('int64', 'float64')) \n\t \telif (dtype in (np.float32, np.float64)): \n\t \t \tvalue = data[col].max() \n\t \t \tif np.isinf(value): \n\t \t \t \tmsg = 'Column \t{0} \thas \ta \tmaximum \tvalue \tof \tinfinity \twhich \tis \toutside \tthe \trange \tsupported \tby \tStata.' \n\t \t \t \traise ValueError(msg.format(col)) \n\t \t \tif ((dtype == np.float32) and (value > float32_max)): \n\t \t \t \tdata[col] = data[col].astype(np.float64) \n\t \t \telif (dtype == np.float64): \n\t \t \t \tif (value > float64_max): \n\t \t \t \t \tmsg = 'Column \t{0} \thas \ta \tmaximum \tvalue \t({1}) \toutside \tthe \trange \tsupported \tby \tStata \t({1})' \n\t \t \t \t \traise ValueError(msg.format(col, value, float64_max)) \n\tif ws: \n\t \timport warnings \n\t \twarnings.warn(ws, PossiblePrecisionLoss) \n\treturn data\n", 
" \tfrom markdown2 import markdown \n\tjenv = frappe.get_jenv() \n\tsource = jenv.loader.get_source(jenv, page_info.template)[0] \n\thtml = u'' \n\tif page_info.template.endswith(u'.md'): \n\t \tsource = markdown(source) \n\tif (page_info.template.endswith(u'.html') or page_info.template.endswith(u'.md')): \n\t \tif ((u'</body>' not in source) and (u'{% \tblock' not in source)): \n\t \t \tpage_info.only_content = True \n\t \t \t(js, css) = (u'', u'') \n\t \t \tjs_path = os.path.join(page_info.basepath, (page_info.basename + u'.js')) \n\t \t \tif os.path.exists(js_path): \n\t \t \t \tjs = unicode(open(js_path, u'r').read(), u'utf-8') \n\t \t \tcss_path = os.path.join(page_info.basepath, (page_info.basename + u'.css')) \n\t \t \tif os.path.exists(css_path): \n\t \t \t \tcss = unicode(open(css_path, u'r').read(), u'utf-8') \n\t \t \thtml = u'{% \textends \t\"templates/web.html\" \t%}' \n\t \t \tif css: \n\t \t \t \thtml += ((u'\\n{% \tblock \tstyle \t%}\\n<style>\\n' + css) + u'\\n</style>\\n{% \tendblock \t%}') \n\t \t \thtml += ((u'\\n{% \tblock \tpage_content \t%}\\n' + source) + u'\\n{% \tendblock \t%}') \n\t \t \tif js: \n\t \t \t \thtml += ((u'\\n{% \tblock \tscript \t%}<script>' + js) + u'\\n</script>\\n{% \tendblock \t%}') \n\t \telse: \n\t \t \thtml = source \n\tpage_info.source = html \n\tsetup_index(page_info)\n", 
" \tpath = '' \n\ttry: \n\t \tmembership = user.membership \n\texcept Membership.DoesNotExist: \n\t \treturn '' \n\tif membership.city: \n\t \tpath += ('%s' % membership.city) \n\tif membership.region: \n\t \tif membership.city: \n\t \t \tpath += ', \t' \n\t \tpath += ('%s' % membership.region) \n\tif membership.country: \n\t \tif membership.region: \n\t \t \tpath += ' \t' \n\t \telif membership.city: \n\t \t \tpath += ', \t' \n\t \tpath += ('%s' % membership.country) \n\treturn path\n", 
" \treturn Effect(Sudo(command=command, log_command_filter=log_command_filter))\n", 
" \tif (key in my_dict): \n\t \tmy_dict[key].append(element) \n\telse: \n\t \tmy_dict[key] = [element]\n", 
" \tdirectlyProvides(object, directlyProvidedBy(object), *interfaces)\n", 
" \tfor nvitem in thelist: \n\t \tif isinstance(nvitem, dict): \n\t \t \t(name, value) = next(six.iteritems(nvitem)) \n\t \t \tif ((names is None) or (name in names)): \n\t \t \t \t(yield (nvitem, name, value))\n", 
" \tvalues = getattr(settings, setting_name) \n\tvalue = type(values)((value,)) \n\tsetattr(settings, setting_name, (value + values))\n", 
" \tclean = b(rws(t)) \n\tif ((len(clean) % 2) == 1): \n\t \tprint clean \n\t \traise ValueError('Even \tnumber \tof \tcharacters \texpected') \n\treturn a2b_hex(clean)\n", 
" \tif (columns is None): \n\t \tcolumns = list(df.select_dtypes(['object', 'category']).columns) \n\telif is_scalar(columns): \n\t \tcolumns = [columns] \n\tcategories = [get_categories(df[col]) for col in columns] \n\tif (index is False): \n\t \tindex = None \n\telse: \n\t \tindex = get_categories(df.index, (index is None)) \n\tvalues = compute(index, *categories, **kwargs) \n\tcategories = {c: v for (c, v) in zip(columns, values[1:]) if (v is not None)} \n\tif ((not len(categories)) and (index is None)): \n\t \treturn df \n\treturn df.map_partitions(_categorize_block, categories, values[0])\n", 
" \tif (not (1 <= col_idx <= 18278)): \n\t \tmsg = ('Column \tindex \tout \tof \tbounds: \t%s' % col_idx) \n\t \traise ColumnStringIndexException(msg) \n\tordinals = [] \n\ttemp = col_idx \n\twhile temp: \n\t \t(quotient, remainder) = divmod(temp, 26) \n\t \tif (remainder == 0): \n\t \t \tquotient -= 1 \n\t \t \tremainder = 26 \n\t \tordinals.append((remainder + 64)) \n\t \ttemp = quotient \n\tordinals.reverse() \n\treturn ''.join([chr(ordinal) for ordinal in ordinals])\n", 
" \tretval = {} \n\tcounter = 0 \n\twhile (counter < len(list)): \n\t \tretval[list[counter]] = list[(counter + 1)] \n\t \tcounter += 2 \n\t__debug('__options2hash \treturning:', retval) \n\treturn retval\n", 
" \tif atype.startswith('java.lang'): \n\t \tatype = atype.replace('java.lang.', '') \n\tres = TYPE_DESCRIPTOR.get(atype.lstrip('java.lang')) \n\tif (res is None): \n\t \tif (atype[0] == 'L'): \n\t \t \tres = atype[1:(-1)].replace('/', '.') \n\t \telif (atype[0] == '['): \n\t \t \tif (size is None): \n\t \t \t \tres = ('%s[]' % get_type(atype[1:])) \n\t \t \telse: \n\t \t \t \tres = ('%s[%s]' % (get_type(atype[1:]), size)) \n\t \telse: \n\t \t \tres = atype \n\treturn res\n", 
" \tif (v.owner is not None): \n\t \tn = v.owner \n\t \tif (isinstance(n.op, (GpuDimShuffle, DimShuffle)) and (n.op.new_order == (('x',) * nd))): \n\t \t \treturn grab_cpu_scalar(n.inputs[0], n.inputs[0].ndim) \n\t \telif isinstance(n.op, (GpuFromHost, HostFromGpu)): \n\t \t \treturn grab_cpu_scalar(n.inputs[0], nd) \n\t \telse: \n\t \t \treturn None \n\telif (isinstance(v, Constant) and (v.broadcastable == ((True,) * nd))): \n\t \treturn v.dimshuffle(())\n", 
" \tif (v.owner is not None): \n\t \tn = v.owner \n\t \tif (isinstance(n.op, (GpuDimShuffle, DimShuffle)) and (n.op.new_order == (('x',) * nd))): \n\t \t \treturn grab_cpu_scalar(n.inputs[0], n.inputs[0].ndim) \n\t \telif isinstance(n.op, (GpuFromHost, HostFromGpu)): \n\t \t \treturn grab_cpu_scalar(n.inputs[0], nd) \n\t \telse: \n\t \t \treturn None \n\telif (isinstance(v, Constant) and (v.broadcastable == ((True,) * nd))): \n\t \treturn v.dimshuffle(())\n", 
" \tif (v.owner is not None): \n\t \tn = v.owner \n\t \tif (isinstance(n.op, (GpuDimShuffle, DimShuffle)) and (n.op.new_order == (('x',) * nd))): \n\t \t \treturn grab_cpu_scalar(n.inputs[0], n.inputs[0].ndim) \n\t \telif isinstance(n.op, (GpuFromHost, HostFromGpu)): \n\t \t \treturn grab_cpu_scalar(n.inputs[0], nd) \n\t \telse: \n\t \t \treturn None \n\telif (isinstance(v, Constant) and (v.broadcastable == ((True,) * nd))): \n\t \treturn v.dimshuffle(())\n", 
" \tif (v.owner is not None): \n\t \tn = v.owner \n\t \tif (isinstance(n.op, (GpuDimShuffle, DimShuffle)) and (n.op.new_order == (('x',) * nd))): \n\t \t \treturn grab_cpu_scalar(n.inputs[0], n.inputs[0].ndim) \n\t \telif isinstance(n.op, (GpuFromHost, HostFromGpu)): \n\t \t \treturn grab_cpu_scalar(n.inputs[0], nd) \n\t \telse: \n\t \t \treturn None \n\telif (isinstance(v, Constant) and (v.broadcastable == ((True,) * nd))): \n\t \treturn v.dimshuffle(())\n", 
" \tmatch_rate = len(variable_name) \n\tif variable_name.startswith(variable_name_db): \n\t \tmatch_rate += (len(variable_name) / 2) \n\treturn match_rate\n", 
" \tservice_obj = bus_get_object(bus, SS_PATH) \n\tservice_iface = dbus.Interface(service_obj, SERVICE_IFACE) \n\t(locked, unlocked) = service_iface.SearchItems(attributes, signature='a{ss}') \n\tfor item_path in (locked + unlocked): \n\t \t(yield Item(bus, item_path))\n", 
" \tif (len(args) == 0): \n\t \treturn '' \n\tparts = [printf_format_for_type(x['type'], types) for x in args] \n\treturn ', \t'.join(parts)\n", 
" \treturn ' \t'.join((str(i) for i in listObj))\n", 
" \tfor i in xrange(len(seq)): \n\t \tseq.pop()\n", 
" \tdata = [_app_path(u'share/git-cola/bin', u'*'), _app_path(u'share/git-cola/icons', u'*.png'), _app_path(u'share/git-cola/icons', u'*.svg'), _app_path(u'share/git-cola/icons/dark', u'*.png'), _app_path(u'share/git-cola/icons/dark', u'*.svg'), _app_path(u'share/appdata', u'*.xml'), _app_path(u'share/applications', u'*.desktop'), _app_path(u'share/doc/git-cola', u'*.rst'), _app_path(u'share/doc/git-cola', u'*.html'), _package(u'cola'), _package(u'cola.models'), _package(u'cola.widgets')] \n\tif vendor_libs: \n\t \tdata.extend([_package(u'qtpy'), _package(u'qtpy._patch')]) \n\tdata.extend([_app_path(localedir, u'git-cola.mo') for localedir in glob(u'share/locale/*/LC_MESSAGES')]) \n\treturn data\n", 
" \tif (count is None): \n\t \tcount = (-1) \n\tif (not eval_ctx.autoescape): \n\t \treturn unicode(s).replace(unicode(old), unicode(new), count) \n\tif (hasattr(old, '__html__') or (hasattr(new, '__html__') and (not hasattr(s, '__html__')))): \n\t \ts = escape(s) \n\telse: \n\t \ts = soft_unicode(s) \n\treturn s.replace(soft_unicode(old), soft_unicode(new), count)\n", 
" \tif isinstance(value, string_type): \n\t \traise VdtTypeError(value) \n\treturn [is_string(mem) for mem in is_list(value, min, max)]\n", 
" \tresult = CodeRanges(chars_to_ranges(s)) \n\tresult.str = ('Any(%s)' % repr(s)) \n\treturn result\n", 
" \ttry: \n\t \t(1 // 0) \n\texcept: \n\t \treturn traceback.extract_stack()[(-2)][2]\n", 
" \tws = '' \n\tconversion_data = ((np.bool, np.int8, np.int8), (np.uint8, np.int8, np.int16), (np.uint16, np.int16, np.int32), (np.uint32, np.int32, np.int64)) \n\tfloat32_max = struct.unpack('<f', '\\xff\\xff\\xff~')[0] \n\tfloat64_max = struct.unpack('<d', '\\xff\\xff\\xff\\xff\\xff\\xff\\xdf\\x7f')[0] \n\tfor col in data: \n\t \tdtype = data[col].dtype \n\t \tfor c_data in conversion_data: \n\t \t \tif (dtype == c_data[0]): \n\t \t \t \tif (data[col].max() <= np.iinfo(c_data[1]).max): \n\t \t \t \t \tdtype = c_data[1] \n\t \t \t \telse: \n\t \t \t \t \tdtype = c_data[2] \n\t \t \t \tif (c_data[2] == np.float64): \n\t \t \t \t \tif (data[col].max() >= (2 ** 53)): \n\t \t \t \t \t \tws = (precision_loss_doc % ('uint64', 'float64')) \n\t \t \t \tdata[col] = data[col].astype(dtype) \n\t \tif (dtype == np.int8): \n\t \t \tif ((data[col].max() > 100) or (data[col].min() < (-127))): \n\t \t \t \tdata[col] = data[col].astype(np.int16) \n\t \telif (dtype == np.int16): \n\t \t \tif ((data[col].max() > 32740) or (data[col].min() < (-32767))): \n\t \t \t \tdata[col] = data[col].astype(np.int32) \n\t \telif (dtype == np.int64): \n\t \t \tif ((data[col].max() <= 2147483620) and (data[col].min() >= (-2147483647))): \n\t \t \t \tdata[col] = data[col].astype(np.int32) \n\t \t \telse: \n\t \t \t \tdata[col] = data[col].astype(np.float64) \n\t \t \t \tif ((data[col].max() >= (2 ** 53)) or (data[col].min() <= (- (2 ** 53)))): \n\t \t \t \t \tws = (precision_loss_doc % ('int64', 'float64')) \n\t \telif (dtype in (np.float32, np.float64)): \n\t \t \tvalue = data[col].max() \n\t \t \tif np.isinf(value): \n\t \t \t \tmsg = 'Column \t{0} \thas \ta \tmaximum \tvalue \tof \tinfinity \twhich \tis \toutside \tthe \trange \tsupported \tby \tStata.' \n\t \t \t \traise ValueError(msg.format(col)) \n\t \t \tif ((dtype == np.float32) and (value > float32_max)): \n\t \t \t \tdata[col] = data[col].astype(np.float64) \n\t \t \telif (dtype == np.float64): \n\t \t \t \tif (value > float64_max): \n\t \t \t \t \tmsg = 'Column \t{0} \thas \ta \tmaximum \tvalue \t({1}) \toutside \tthe \trange \tsupported \tby \tStata \t({1})' \n\t \t \t \t \traise ValueError(msg.format(col, value, float64_max)) \n\tif ws: \n\t \timport warnings \n\t \twarnings.warn(ws, PossiblePrecisionLoss) \n\treturn data\n", 
" \tb = as_int(b) \n\tn = as_int(n) \n\tif (b <= 1): \n\t \traise ValueError('b \tmust \tbe \t>= \t2') \n\telse: \n\t \t(x, y) = (abs(n), []) \n\t \twhile (x >= b): \n\t \t \t(x, r) = divmod(x, b) \n\t \t \ty.append(r) \n\t \ty.append(x) \n\t \ty.append(((- b) if (n < 0) else b)) \n\t \ty.reverse() \n\t \treturn y\n", 
" \tif (not ((type(bytes) is types.ListType) or (type(bytes) is types.StringType))): \n\t \traise TypeError('You \tmust \tpass \ta \tstring \tor \ta \tlist') \n\tinteger = 0 \n\tfor byte in bytes: \n\t \tinteger *= 256 \n\t \tif (type(byte) is types.StringType): \n\t \t \tbyte = ord(byte) \n\t \tinteger += byte \n\treturn integer\n", 
" \tif (not num): \n\t \treturn None \n\tif num.isdigit(): \n\t \treturn int(num) \n\tif (not re.match('\\\\w\\\\w:\\\\w\\\\w:\\\\w\\\\w', num)): \n\t \treturn None \n\ttry: \n\t \treturn int(num.replace(':', ''), 16) \n\texcept ValueError: \n\t \treturn None\n", 
" \twhile 1: \n\t \tline = f.readline() \n\t \tif (not line): \n\t \t \tbreak \n\t \t(yield line[:(-1)])\n", 
" \ttry: \n\t \tvalue = int(value) \n\texcept ValueError: \n\t \treturn value \n\tt = (_('th'), _('st'), _('nd'), _('rd'), _('th'), _('th'), _('th'), _('th'), _('th'), _('th')) \n\tif ((value % 100) in (11, 12, 13)): \n\t \treturn ('%d%s' % (value, t[0])) \n\treturn ('%d%s' % (value, t[(value % 10)]))\n", 
" \ttemplates = set() \n\tfor root in dirs: \n\t \tfor (dirpath, dirnames, filenames) in os.walk(root): \n\t \t \tfor f in filenames: \n\t \t \t \tif (len([True for e in TEMPLATE_EXTENSIONS if f.endswith(e)]) > 0): \n\t \t \t \t \tt = make_template_info(os.path.join(dirpath, f), dirs) \n\t \t \t \t \tt.all_templates = templates \n\t \t \t \t \ttemplates.add(t) \n\treturn templates\n", 
" \ttemplates = set() \n\tfor root in dirs: \n\t \tfor (dirpath, dirnames, filenames) in os.walk(root): \n\t \t \tfor f in filenames: \n\t \t \t \tif (len([True for e in TEMPLATE_EXTENSIONS if f.endswith(e)]) > 0): \n\t \t \t \t \tt = make_template_info(os.path.join(dirpath, f), dirs) \n\t \t \t \t \tt.all_templates = templates \n\t \t \t \t \ttemplates.add(t) \n\treturn templates\n", 
" \ttemplates = set() \n\tfor root in dirs: \n\t \tfor (dirpath, dirnames, filenames) in os.walk(root): \n\t \t \tfor f in filenames: \n\t \t \t \tif (len([True for e in TEMPLATE_EXTENSIONS if f.endswith(e)]) > 0): \n\t \t \t \t \tt = make_template_info(os.path.join(dirpath, f), dirs) \n\t \t \t \t \tt.all_templates = templates \n\t \t \t \t \ttemplates.add(t) \n\treturn templates\n", 
" \tfrom mpl_toolkits.axes_grid.inset_locator import inset_axes \n\tbbox = axis.get_window_extent() \n\tratio = (bbox.width / bbox.height) \n\tax = inset_axes(axis, width=(str((30 / ratio)) + '%'), height='30%', loc=loc) \n\t(pos_x, pos_y) = _prepare_topomap(pos, ax) \n\tax.scatter(pos_x, pos_y, color=colors, s=25, marker='.', zorder=1) \n\tfor idx in bads: \n\t \tax.scatter(pos_x[idx], pos_y[idx], s=5, marker='.', color='w', zorder=1) \n\tif isinstance(outlines, dict): \n\t \t_draw_outlines(ax, outlines)\n", 
" \tipaddr_filter_out = _filter_ipaddr(value, options=options, version=version) \n\tif (not ipaddr_filter_out): \n\t \treturn \n\treturn [ipaddress.ip_interface(ip_a) for ip_a in ipaddr_filter_out]\n", 
" \tipaddr_filter_out = _filter_ipaddr(value, options=options, version=version) \n\tif (not ipaddr_filter_out): \n\t \treturn \n\treturn [ipaddress.ip_interface(ip_a) for ip_a in ipaddr_filter_out]\n", 
" \tipaddr_filter_out = _filter_ipaddr(value, options=options, version=version) \n\tif (not ipaddr_filter_out): \n\t \treturn \n\treturn [ipaddress.ip_interface(ip_a) for ip_a in ipaddr_filter_out]\n", 
" \tif (not str): \n\t \ttry: \n\t \t \tbits = as_int(bits) \n\t \t \treturn [(1 if (i == '1') else 0) for i in bin(n)[2:].rjust(bits, '0')] \n\t \texcept ValueError: \n\t \t \treturn variations(list(range(2)), n, repetition=True) \n\telse: \n\t \ttry: \n\t \t \tbits = as_int(bits) \n\t \t \treturn bin(n)[2:].rjust(bits, '0') \n\t \texcept ValueError: \n\t \t \treturn (bin(i)[2:].rjust(n, '0') for i in range((2 ** n)))\n", 
" \tglobal _open_log_files, _log_file_dir \n\tpath = get_path(_log_file_dir, filename) \n\tif (path not in _open_log_files): \n\t \tclose_log_file(filename) \n\t \ttry: \n\t \t \tos.makedirs(os.path.dirname(path)) \n\t \texcept OSError: \n\t \t \tpass \n\t \t_open_log_files[path] = open(path, 'w') \n\ttimestr = time.strftime('%Y-%m-%d \t%H:%M:%S') \n\t_open_log_files[path].write(('%s: \t%s\\n' % (timestr, line))) \n\t_open_log_files[path].flush()\n", 
" \tif atomp(items): \n\t \treturn items \n\tif (len(s) == 0): \n\t \treturn items[0] \n\tlst = ([0] * s[0]) \n\tstride = s[0] \n\tfor i in range(s[0]): \n\t \tlst[i] = _fa(items[i::stride], s[1:]) \n\treturn lst\n", 
" \tsaved_pos = source.pos \n\tif source.match('{'): \n\t \tname = source.get_while((ALPHA | set(' \t'))) \n\t \tif source.match('}'): \n\t \t \ttry: \n\t \t \t \tvalue = unicodedata.lookup(name) \n\t \t \t \treturn ord(value) \n\t \t \texcept KeyError: \n\t \t \t \traise error('undefined \tcharacter \tname', source.string, source.pos) \n\tsource.pos = saved_pos \n\treturn None\n", 
" \tif (t.strip() in ['void \t*', 'char \t*']): \n\t \treturn t.strip() \n\ttry: \n\t \treturn t[:t.rindex('*')].strip() \n\texcept: \n\t \treturn t.strip()\n", 
" \t(count, sum) = _stats(input, labels, index) \n\treturn (sum / numpy.asanyarray(count).astype(numpy.float))\n", 
" \tlslices = ([0] * ndim) \n\trslices = ([0] * ndim) \n\tfor n in range(ndim): \n\t \tl = shape[n] \n\t \tslicelen = (randrange(1, (l + 1)) if (l > 0) else 0) \n\t \tlslices[n] = randslice_from_slicelen(slicelen, l) \n\t \trslices[n] = randslice_from_slicelen(slicelen, l) \n\treturn (tuple(lslices), tuple(rslices))\n", 
" \tif ((str is unicode) and isinstance(s, bytes)): \n\t \ts = s.decode('utf8') \n\treturn jsonmod.loads(s, **kwargs)\n", 
" \tvar_resolve = Variable(arg).resolve \n\tdecorated = [(var_resolve(item), item) for item in value] \n\tdecorated.sort() \n\tdecorated.reverse() \n\treturn [item[1] for item in decorated]\n", 
" \t_list.sort((lambda a, b: (a[indexkey] < b[indexkey]))) \n\treturn _list\n", 
" \t@none_if_empty \n\tdef regexp_to_list_lambda(value): \n\t \tresult = re.findall(pattern, value) \n\t \tif (result == []): \n\t \t \treturn None \n\t \treturn result \n\treturn regexp_to_list_lambda\n", 
" \tfor char in SPECIAL_CHARS: \n\t \t(yield (get_char_description(char), char)) \n\tcode = language.code.replace(u'_', u'-').split(u'-')[0] \n\tif (code in EXTRA_CHARS): \n\t \tfor char in EXTRA_CHARS[code]: \n\t \t \t(yield (get_char_description(char), char)) \n\t(yield get_quote(code, DOUBLE_OPEN, _(u'Opening \tdouble \tquote'))) \n\t(yield get_quote(code, DOUBLE_CLOSE, _(u'Closing \tdouble \tquote'))) \n\t(yield get_quote(code, SINGLE_OPEN, _(u'Opening \tsingle \tquote'))) \n\t(yield get_quote(code, SINGLE_CLOSE, _(u'Closing \tsingle \tquote'))) \n\tif (code in HYPHEN_LANGS): \n\t \t(yield (_(u'Hyphen'), u'-')) \n\tif (code in EN_DASH_LANGS): \n\t \t(yield (_(u'En \tdash'), u'\\u2013')) \n\tif (code in EM_DASH_LANGS): \n\t \t(yield (_(u'Em \tdash'), u'\\u2014'))\n", 
" \tif (date_str is None): \n\t \treturn None \n\tupload_date = None \n\tdate_str = date_str.replace(u',', u' \t') \n\tdate_str = re.sub(u'(?i)\\\\s*(?:AM|PM)(?:\\\\s+[A-Z]+)?', u'', date_str) \n\t(_, date_str) = extract_timezone(date_str) \n\tfor expression in date_formats(day_first): \n\t \ttry: \n\t \t \tupload_date = datetime.datetime.strptime(date_str, expression).strftime(u'%Y%m%d') \n\t \texcept ValueError: \n\t \t \tpass \n\tif (upload_date is None): \n\t \ttimetuple = email.utils.parsedate_tz(date_str) \n\t \tif timetuple: \n\t \t \ttry: \n\t \t \t \tupload_date = datetime.datetime(*timetuple[:6]).strftime(u'%Y%m%d') \n\t \t \texcept ValueError: \n\t \t \t \tpass \n\tif (upload_date is not None): \n\t \treturn compat_str(upload_date)\n", 
" \tcount = 0 \n\tfor i in a: \n\t \tif (i == b): \n\t \t \tcount += 1 \n\treturn count\n", 
" \tcontent_type = environ.get('CONTENT_TYPE', '') \n\tif (content_type != 'application/json'): \n\t \traise HTTPError(406, 'JSON \trequired') \n\ttry: \n\t \treturn json.loads(read_body(environ)) \n\texcept ValueError as exc: \n\t \traise HTTPError(400, exc)\n", 
" \t(temp_filename, headers) = urllib.urlretrieve(url, filename, progress_hook) \n\treturn temp_filename\n", 
" \tif str_in.startswith(('\"', \"'\")): \n\t \treturn shlex.split(str_in) \n\telse: \n\t \tcomponents = str_in.split(' \t', 1) \n\t \tif (len(components) > 1): \n\t \t \treturn ([components[0]] + SplitIntoComponents(components[1])) \n\t \telse: \n\t \t \treturn components\n", 
" \tif (len(txt) > size): \n\t \ttxt = ''.join([re.sub(u'\\\\.(?P<ends>[^.]*)$', '.\\n\\n\\\\g<ends>', txt[i:(i + size)], 1) for i in xrange(0, len(txt), size)]) \n\treturn txt\n", 
" \treturn re.split('[\\\\s,]+', cstr)\n", 
" \tsaved_pos = source.pos \n\tif source.match('{'): \n\t \tname = source.get_while((ALPHA | set(' \t'))) \n\t \tif source.match('}'): \n\t \t \ttry: \n\t \t \t \tvalue = unicodedata.lookup(name) \n\t \t \t \treturn ord(value) \n\t \t \texcept KeyError: \n\t \t \t \traise error('undefined \tcharacter \tname', source.string, source.pos) \n\tsource.pos = saved_pos \n\treturn None\n", 
" \tmapping_keys_re = re.compile('\\\\%\\\\([^\\\\)]*\\\\)\\\\w') \n\treturn sorted(mapping_keys_re.findall(s))\n", 
" \treturn subprocess.call(cmd, shell=True)\n", 
" \tdd = defaultdict(int) \n\tunique_list = [] \n\tfor val in lst: \n\t \tdd[val] += 1 \n\tfor val in lst: \n\t \tif (dd[val] == 1): \n\t \t \tunique_list.append(val) \n\treturn unique_list\n", 
" \treturn [elem for elem in lst if (not predicate(elem))]\n", 
" \t_list.sort((lambda a, b: (a[indexkey] < b[indexkey]))) \n\treturn _list\n", 
" \t_list.sort((lambda a, b: (a[indexkey] < b[indexkey]))) \n\treturn _list\n", 
" \t_list.sort((lambda a, b: (a[indexkey] < b[indexkey]))) \n\treturn _list\n", 
" \t_list.sort((lambda a, b: (a[indexkey] < b[indexkey]))) \n\treturn _list\n", 
" \tws = '' \n\tconversion_data = ((np.bool, np.int8, np.int8), (np.uint8, np.int8, np.int16), (np.uint16, np.int16, np.int32), (np.uint32, np.int32, np.int64)) \n\tfloat32_max = struct.unpack('<f', '\\xff\\xff\\xff~')[0] \n\tfloat64_max = struct.unpack('<d', '\\xff\\xff\\xff\\xff\\xff\\xff\\xdf\\x7f')[0] \n\tfor col in data: \n\t \tdtype = data[col].dtype \n\t \tfor c_data in conversion_data: \n\t \t \tif (dtype == c_data[0]): \n\t \t \t \tif (data[col].max() <= np.iinfo(c_data[1]).max): \n\t \t \t \t \tdtype = c_data[1] \n\t \t \t \telse: \n\t \t \t \t \tdtype = c_data[2] \n\t \t \t \tif (c_data[2] == np.float64): \n\t \t \t \t \tif (data[col].max() >= (2 ** 53)): \n\t \t \t \t \t \tws = (precision_loss_doc % ('uint64', 'float64')) \n\t \t \t \tdata[col] = data[col].astype(dtype) \n\t \tif (dtype == np.int8): \n\t \t \tif ((data[col].max() > 100) or (data[col].min() < (-127))): \n\t \t \t \tdata[col] = data[col].astype(np.int16) \n\t \telif (dtype == np.int16): \n\t \t \tif ((data[col].max() > 32740) or (data[col].min() < (-32767))): \n\t \t \t \tdata[col] = data[col].astype(np.int32) \n\t \telif (dtype == np.int64): \n\t \t \tif ((data[col].max() <= 2147483620) and (data[col].min() >= (-2147483647))): \n\t \t \t \tdata[col] = data[col].astype(np.int32) \n\t \t \telse: \n\t \t \t \tdata[col] = data[col].astype(np.float64) \n\t \t \t \tif ((data[col].max() >= (2 ** 53)) or (data[col].min() <= (- (2 ** 53)))): \n\t \t \t \t \tws = (precision_loss_doc % ('int64', 'float64')) \n\t \telif (dtype in (np.float32, np.float64)): \n\t \t \tvalue = data[col].max() \n\t \t \tif np.isinf(value): \n\t \t \t \tmsg = 'Column \t{0} \thas \ta \tmaximum \tvalue \tof \tinfinity \twhich \tis \toutside \tthe \trange \tsupported \tby \tStata.' \n\t \t \t \traise ValueError(msg.format(col)) \n\t \t \tif ((dtype == np.float32) and (value > float32_max)): \n\t \t \t \tdata[col] = data[col].astype(np.float64) \n\t \t \telif (dtype == np.float64): \n\t \t \t \tif (value > float64_max): \n\t \t \t \t \tmsg = 'Column \t{0} \thas \ta \tmaximum \tvalue \t({1}) \toutside \tthe \trange \tsupported \tby \tStata \t({1})' \n\t \t \t \t \traise ValueError(msg.format(col, value, float64_max)) \n\tif ws: \n\t \timport warnings \n\t \twarnings.warn(ws, PossiblePrecisionLoss) \n\treturn data\n", 
" \treturn _na_map(sep.join, arr)\n", 
" \tif (type(name) is TupleType): \n\t \treturn ('(%s)' % ', \t'.join([normalize_parameter_name(n) for n in name])) \n\telse: \n\t \treturn name\n", 
" \treturn response.data\n", 
" \tfor key in keys: \n\t \tremoveTrueFromDictionary(dictionary, key)\n", 
" \treturn ashour(truncate_second(dt, (measure * 3600)))\n", 
" \t_list.sort((lambda a, b: (a[indexkey] < b[indexkey]))) \n\treturn _list\n", 
" \ttry: \n\t \treturn datetime.utcfromtimestamp(int(timestamp)).replace(tzinfo=utc) \n\texcept (ValueError, TypeError): \n\t \treturn None\n", 
" \text_dot = ('.' + ext) \n\treturn (os.path.splitext(path)[0] + ext_dot)\n", 
" \tfor child in GlobalObject().root.childsmanager._childs.values(): \n\t \tchild.callbackChildNotForResult('sreload') \n\treturn 'reload'\n", 
" \tif (not num): \n\t \treturn None \n\tif num.isdigit(): \n\t \treturn int(num) \n\tif (not re.match('\\\\w\\\\w:\\\\w\\\\w:\\\\w\\\\w', num)): \n\t \treturn None \n\ttry: \n\t \treturn int(num.replace(':', ''), 16) \n\texcept ValueError: \n\t \treturn None\n", 
" \tm = FLOAT_REGEX.search(value) \n\tif (m is not None): \n\t \treturn float(value) \n\treturn long(value)\n", 
" \tvalues = [_to_csv_chunk(d, **kwargs) for d in df.to_delayed()] \n\tvalues = write_bytes(values, filename, name_function, compression, encoding=None) \n\tif compute: \n\t \tdelayed(values).compute(get=get) \n\telse: \n\t \treturn values\n", 
" \treturn decode(urllib.unquote(value), *args, **kwargs)\n", 
" \tif six.PY2: \n\t \treturn ord(s[index]) \n\treturn s[index]\n", 
" \tnewhash = None \n\torighash = None \n\tfmlogger.debug(newfile) \n\tif create_new: \n\t \twhile os.path.exists(newfile): \n\t \t \t(base, fname, ext) = split_filename(newfile) \n\t \t \ts = re.search(u'_c[0-9]{4,4}$', fname) \n\t \t \ti = 0 \n\t \t \tif s: \n\t \t \t \ti = (int(s.group()[2:]) + 1) \n\t \t \t \tfname = (fname[:(-6)] + (u'_c%04d' % i)) \n\t \t \telse: \n\t \t \t \tfname += (u'_c%04d' % i) \n\t \t \tnewfile = (((base + os.sep) + fname) + ext) \n\tif (hashmethod is None): \n\t \thashmethod = config.get(u'execution', u'hash_method').lower() \n\tkeep = False \n\tif os.path.lexists(newfile): \n\t \tif os.path.islink(newfile): \n\t \t \tif all(((os.readlink(newfile) == os.path.realpath(originalfile)), (not use_hardlink), (not copy))): \n\t \t \t \tkeep = True \n\t \telif posixpath.samefile(newfile, originalfile): \n\t \t \tkeep = True \n\t \telse: \n\t \t \tif (hashmethod == u'timestamp'): \n\t \t \t \thashfn = hash_timestamp \n\t \t \telif (hashmethod == u'content'): \n\t \t \t \thashfn = hash_infile \n\t \t \tnewhash = hashfn(newfile) \n\t \t \tfmlogger.debug((u'File: \t%s \talready \texists,%s, \tcopy:%d' % (newfile, newhash, copy))) \n\t \t \torighash = hashfn(originalfile) \n\t \t \tkeep = (newhash == orighash) \n\t \tif keep: \n\t \t \tfmlogger.debug((u'File: \t%s \talready \texists, \tnot \toverwriting, \tcopy:%d' % (newfile, copy))) \n\t \telse: \n\t \t \tos.unlink(newfile) \n\tif ((not keep) and use_hardlink): \n\t \ttry: \n\t \t \tfmlogger.debug((u'Linking \tFile: \t%s->%s' % (newfile, originalfile))) \n\t \t \tos.link(os.path.realpath(originalfile), newfile) \n\t \texcept OSError: \n\t \t \tuse_hardlink = False \n\t \telse: \n\t \t \tkeep = True \n\tif ((not keep) and (not copy) and (os.name == u'posix')): \n\t \ttry: \n\t \t \tfmlogger.debug((u'Symlinking \tFile: \t%s->%s' % (newfile, originalfile))) \n\t \t \tos.symlink(originalfile, newfile) \n\t \texcept OSError: \n\t \t \tcopy = True \n\t \telse: \n\t \t \tkeep = True \n\tif (not keep): \n\t \ttry: \n\t \t \tfmlogger.debug((u'Copying \tFile: \t%s->%s' % (newfile, originalfile))) \n\t \t \tshutil.copyfile(originalfile, newfile) \n\t \texcept shutil.Error as e: \n\t \t \tfmlogger.warn(e.message) \n\tif copy_related_files: \n\t \trelated_file_pairs = (get_related_files(f, include_this_file=False) for f in (originalfile, newfile)) \n\t \tfor (alt_ofile, alt_nfile) in zip(*related_file_pairs): \n\t \t \tif os.path.exists(alt_ofile): \n\t \t \t \tcopyfile(alt_ofile, alt_nfile, copy, hashmethod=hashmethod, use_hardlink=use_hardlink, copy_related_files=False) \n\treturn newfile\n", 
" \tl = line.lower() \n\ts = '' \n\ti = 0 \n\tnextCap = 1 \n\twhile (i < len(l)): \n\t \tc = l[i] \n\t \tif ((c >= 'a') and (c <= 'z') and nextCap): \n\t \t \tc = c.upper() \n\t \t \tnextCap = 0 \n\t \telif ((c == ' \t') or (c == '.') or (c == ',') or (c == ';') or (c == ':') or (c == ' DCTB ') or (c == '-') or (c == '_')): \n\t \t \tnextCap = 1 \n\t \ts += c \n\t \ti += 1 \n\treturn s\n", 
" \tskey = get_key(__opts__) \n\treturn skey.delete_key(match_dict=match)\n", 
" \tglobal logfp, log \n\tif (logfile and (not logfp)): \n\t \ttry: \n\t \t \tlogfp = open(logfile, 'a') \n\t \texcept IOError: \n\t \t \tpass \n\tif (not logfp): \n\t \tlog = nolog \n\telse: \n\t \tlog = dolog \n\tlog(*allargs)\n", 
" \tglobal logfp, log \n\tif (logfile and (not logfp)): \n\t \ttry: \n\t \t \tlogfp = open(logfile, 'a') \n\t \texcept IOError: \n\t \t \tpass \n\tif (not logfp): \n\t \tlog = nolog \n\telse: \n\t \tlog = dolog \n\tlog(*allargs)\n", 
" \tglobal _open_log_files, _log_file_dir \n\tpath = get_path(_log_file_dir, filename) \n\tif (path not in _open_log_files): \n\t \tclose_log_file(filename) \n\t \ttry: \n\t \t \tos.makedirs(os.path.dirname(path)) \n\t \texcept OSError: \n\t \t \tpass \n\t \t_open_log_files[path] = open(path, 'w') \n\ttimestr = time.strftime('%Y-%m-%d \t%H:%M:%S') \n\t_open_log_files[path].write(('%s: \t%s\\n' % (timestr, line))) \n\t_open_log_files[path].flush()\n", 
" \tif (not is_unicode(string)): \n\t \tfor codec in CODECS: \n\t \t \ttry: \n\t \t \t \tunic = to_text_string(string, codec) \n\t \t \texcept UnicodeError: \n\t \t \t \tpass \n\t \t \texcept TypeError: \n\t \t \t \tbreak \n\t \t \telse: \n\t \t \t \treturn unic \n\treturn string\n", 
" \tconverter = (converter or ModelConverter()) \n\tfield_args = (field_args or {}) \n\tmodel_fields = ((f.attname, f) for f in model._meta.fields) \n\tif only: \n\t \tmodel_fields = (x for x in model_fields if (x[0] in only)) \n\telif exclude: \n\t \tmodel_fields = (x for x in model_fields if (x[0] not in exclude)) \n\tfield_dict = {} \n\tfor (name, model_field) in model_fields: \n\t \tfield = converter.convert(model, model_field, field_args.get(name)) \n\t \tif (field is not None): \n\t \t \tfield_dict[name] = field \n\treturn field_dict\n", 
" \t(n, m) = (len(s), len(sub)) \n\thsub_digest = md5(sub.encode('utf-8')).digest() \n\toffsets = [] \n\tif (m > n): \n\t \treturn offsets \n\tfor i in range(((n - m) + 1)): \n\t \tif (md5(s[i:(i + m)].encode('utf-8')).digest() == hsub_digest): \n\t \t \tif (s[i:(i + m)] == sub): \n\t \t \t \toffsets.append(i) \n\treturn offsets\n", 
" \tfiles = glob.glob(os.path.join(directory, '*')) \n\tresults = [] \n\tfor f in files: \n\t \tif regex.match(os.path.basename(f)): \n\t \t \tresults.append(f) \n\treturn results\n", 
" \tif source: \n\t \tlog.info('Caching \t{0}'.format(source)) \n\t \tcached_files = __salt__['cp.get_file'](path=source, dest=path, saltenv=salt_env, makedirs=True) \n\t \tif (not cached_files): \n\t \t \terror = 'Failed \tto \tcache \t{0}'.format(source) \n\t \t \tlog.error(error) \n\t \t \traise CommandExecutionError(error) \n\tif config_data_source: \n\t \tlog.info('Caching \t{0}'.format(config_data_source)) \n\t \tcached_files = __salt__['cp.get_file'](path=config_data_source, dest=config_data, saltenv=salt_env, makedirs=True) \n\t \tif (not cached_files): \n\t \t \terror = 'Failed \tto \tcache \t{0}'.format(config_data_source) \n\t \t \tlog.error(error) \n\t \t \traise CommandExecutionError(error) \n\tif (not os.path.exists(path)): \n\t \terror = '\"{0} \tnot \tfound.'.format(path) \n\t \tlog.error(error) \n\t \traise CommandExecutionError(error) \n\tif (config_name is None): \n\t \tconfig_name = os.path.splitext(os.path.basename(path))[0] \n\tcwd = os.path.dirname(path) \n\tcmd = [path] \n\tif script_parameters: \n\t \tcmd.append(script_parameters) \n\tcmd.append('| \tSelect-Object \t-Property \tFullName, \tExtension, \tExists, \t@{Name=\"LastWriteTime\";Expression={Get-Date \t($_.LastWriteTime) \t-Format \tg}}') \n\tcmd = ' \t'.join(cmd) \n\tret = _pshell(cmd, cwd) \n\tif ret: \n\t \tif ret.get('Exists'): \n\t \t \tlog.info('DSC \tCompile \tConfig: \t{0}'.format(ret)) \n\t \t \treturn ret \n\tcmd = ['.', path] \n\tif script_parameters: \n\t \tcmd.append(script_parameters) \n\tcmd.extend([';', config_name]) \n\tif config_data: \n\t \tcmd.append(config_data) \n\tcmd.append('| \tSelect-Object \t-Property \tFullName, \tExtension, \tExists, \t@{Name=\"LastWriteTime\";Expression={Get-Date \t($_.LastWriteTime) \t-Format \tg}}') \n\tcmd = ' \t'.join(cmd) \n\tret = _pshell(cmd, cwd) \n\tif ret: \n\t \tif ret.get('Exists'): \n\t \t \tlog.info('DSC \tCompile \tConfig: \t{0}'.format(ret)) \n\t \t \treturn ret \n\terror = 'Failed \tto \tcompile \tconfig: \t{0}'.format(path) \n\terror += '\\nReturned: \t{0}'.format(ret) \n\tlog.error('DSC \tCompile \tConfig: \t{0}'.format(error)) \n\traise CommandExecutionError(error)\n", 
" \treturn sorted((pn for pn in p if p[pn]), key=(lambda pn: p[pn]))\n", 
" \tservice_instance = salt.utils.vmware.get_service_instance(host=host, username=username, password=password, protocol=protocol, port=port) \n\treturn salt.utils.vmware.list_datacenters(service_instance)\n", 
" \tif dt: \n\t \tif (dt.tzinfo is None): \n\t \t \treturn dt.replace(tzinfo=dateutil.tz.tzutc()) \n\t \treturn dt.astimezone(dateutil.tz.tzutc()) \n\telse: \n\t \treturn None\n", 
" \tif (not isinstance(ary, types.Buffer)): \n\t \treturn \n\tndim = ary.ndim \n\tleft_indices = [] \n\tright_indices = [] \n\tellipsis_met = False \n\tadvanced = False \n\thas_integer = False \n\tif (not isinstance(idx, types.BaseTuple)): \n\t \tidx = [idx] \n\tfor ty in idx: \n\t \tif (ty is types.ellipsis): \n\t \t \tif ellipsis_met: \n\t \t \t \traise TypeError(('only \tone \tellipsis \tallowed \tin \tarray \tindex \t(got \t%s)' % (idx,))) \n\t \t \tellipsis_met = True \n\t \telif isinstance(ty, types.SliceType): \n\t \t \tpass \n\t \telif isinstance(ty, types.Integer): \n\t \t \tty = (types.intp if ty.signed else types.uintp) \n\t \t \tndim -= 1 \n\t \t \thas_integer = True \n\t \telif (isinstance(ty, types.Array) and (ty.ndim == 0) and isinstance(ty.dtype, types.Integer)): \n\t \t \tndim -= 1 \n\t \t \thas_integer = True \n\t \telif (isinstance(ty, types.Array) and (ty.ndim == 1) and isinstance(ty.dtype, (types.Integer, types.Boolean))): \n\t \t \tif (advanced or has_integer): \n\t \t \t \traise NotImplementedError('only \tone \tadvanced \tindex \tsupported') \n\t \t \tadvanced = True \n\t \telse: \n\t \t \traise TypeError(('unsupported \tarray \tindex \ttype \t%s \tin \t%s' % (ty, idx))) \n\t \t(right_indices if ellipsis_met else left_indices).append(ty) \n\tif (advanced and (not isinstance(ary, types.Array))): \n\t \treturn \n\tall_indices = (left_indices + right_indices) \n\tif ellipsis_met: \n\t \tassert (right_indices[0] is types.ellipsis) \n\t \tdel right_indices[0] \n\tn_indices = (len(all_indices) - ellipsis_met) \n\tif (n_indices > ary.ndim): \n\t \traise TypeError(('cannot \tindex \t%s \twith \t%d \tindices: \t%s' % (ary, n_indices, idx))) \n\tif ((n_indices == ary.ndim) and (ndim == 0) and (not ellipsis_met)): \n\t \tres = ary.dtype \n\telif advanced: \n\t \tres = ary.copy(ndim=ndim, layout='C', readonly=False) \n\telse: \n\t \tif ary.slice_is_copy: \n\t \t \treturn \n\t \tlayout = ary.layout \n\t \tdef keeps_contiguity(ty, is_innermost): \n\t \t \treturn ((ty is types.ellipsis) or isinstance(ty, types.Integer) or (is_innermost and isinstance(ty, types.SliceType) and (not ty.has_step))) \n\t \tdef check_contiguity(outer_indices): \n\t \t \t'\\n \t \t \t \t \t \t \t \t \t \t \t \tWhether \tindexing \twith \tthe \tgiven \tindices \t(from \touter \tto \tinner \tin\\n \t \t \t \t \t \t \t \t \t \t \t \tphysical \tlayout \torder) \tcan \tkeep \tan \tarray \tcontiguous.\\n \t \t \t \t \t \t \t \t \t \t \t \t' \n\t \t \tfor ty in outer_indices[:(-1)]: \n\t \t \t \tif (not keeps_contiguity(ty, False)): \n\t \t \t \t \treturn False \n\t \t \tif (outer_indices and (not keeps_contiguity(outer_indices[(-1)], True))): \n\t \t \t \treturn False \n\t \t \treturn True \n\t \tif (layout == 'C'): \n\t \t \tif (n_indices == ary.ndim): \n\t \t \t \tleft_indices = (left_indices + right_indices) \n\t \t \t \tright_indices = [] \n\t \t \tif right_indices: \n\t \t \t \tlayout = 'A' \n\t \t \telif (not check_contiguity(left_indices)): \n\t \t \t \tlayout = 'A' \n\t \telif (layout == 'F'): \n\t \t \tif (n_indices == ary.ndim): \n\t \t \t \tright_indices = (left_indices + right_indices) \n\t \t \t \tleft_indices = [] \n\t \t \tif left_indices: \n\t \t \t \tlayout = 'A' \n\t \t \telif (not check_contiguity(right_indices[::(-1)])): \n\t \t \t \tlayout = 'A' \n\t \tres = ary.copy(ndim=ndim, layout=layout) \n\tif isinstance(idx, types.BaseTuple): \n\t \tidx = types.BaseTuple.from_types(all_indices) \n\telse: \n\t \t(idx,) = all_indices \n\treturn Indexing(idx, res, advanced)\n", 
" \t(nr, nc) = shape(A) \n\t(column_means, _) = scale(A) \n\treturn make_matrix(nr, nc, (lambda i, j: (A[i][j] - column_means[j])))\n", 
" \topcode = find_module('opcode') \n\ttargets = (['_unknown_opcode'] * 256) \n\tfor (opname, op) in opcode.opmap.items(): \n\t \ttargets[op] = ('TARGET_%s' % opname) \n\tf.write('static \tvoid \t*opcode_targets[256] \t= \t{\\n') \n\tf.write(',\\n'.join([(' \t \t \t \t&&%s' % s) for s in targets])) \n\tf.write('\\n};\\n')\n", 
" \treturn extract_diag(X).sum()\n", 
" \tresult = [] \n\tif ((not _addtail) and (self.text is not None)): \n\t \tresult.append(self.text) \n\tfor elem in self: \n\t \tresult.extend(elem.textlist(True)) \n\tif (_addtail and (self.tail is not None)): \n\t \tresult.append(self.tail) \n\treturn result\n", 
" \tregex = re.compile(regex) \n\tif str[0].isdigit(): \n\t \tstr = (replace + str) \n\treturn regex.sub(replace, str)\n", 
" \tif str_in.startswith(('\"', \"'\")): \n\t \treturn shlex.split(str_in) \n\telse: \n\t \tcomponents = str_in.split(' \t', 1) \n\t \tif (len(components) > 1): \n\t \t \treturn ([components[0]] + SplitIntoComponents(components[1])) \n\t \telse: \n\t \t \treturn components\n", 
" \tif (name == 'stdout'): \n\t \treturn sys.stdout \n\telif (name == 'stderr'): \n\t \treturn sys.stderr \n\telif (name == 'stdin'): \n\t \treturn sys.stdin \n\telse: \n\t \tif (encoding is not None): \n\t \t \timport codecs \n\t \t \tf = codecs.open(name, mode, encoding) \n\t \telse: \n\t \t \tf = open(name, mode) \n\t \tif ('w' in mode): \n\t \t \tos.chmod(name, 384) \n\t \treturn f\n", 
" \tcumsum = 0 \n\tfor x in xs: \n\t \tcumsum += x \n\t \t(yield cumsum)\n", 
" \treturn concatenate([cupy.expand_dims(x, axis) for x in tup], axis)\n"

]